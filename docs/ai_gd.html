<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Thomas E. Gorman">
<meta name="dcterms.date" content="2024-11-05">

<title>Group Decision Lit – CCL Projects</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-29f71986bfdb98847d3932762ebabacd.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="Assets/Style/calloutTG.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ai_gd.html">LLM Literature</a></li><li class="breadcrumb-item"><a href="./ai_gd.html">Group Decision Lit</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">CCL Projects</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/tegorman13/ccl" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">LLM Literature</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_gd.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Group Decision Lit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Individual decision lit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./llm_energy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLM Energy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_interaction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Interactive AI Lit</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Misc</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transactive Memory Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Samuel_Project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Samuel’s Project</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Driving.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Driving Lit</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#relevant-papers" id="toc-relevant-papers" class="nav-link active" data-scroll-target="#relevant-papers">Relevant Papers</a>
  <ul class="collapse">
  <li><a href="#ai-can-help-humans-find-common-ground-in-democratic-deliberation." id="toc-ai-can-help-humans-find-common-ground-in-democratic-deliberation." class="nav-link" data-scroll-target="#ai-can-help-humans-find-common-ground-in-democratic-deliberation.">AI can help humans find common ground in democratic deliberation.</a></li>
  <li><a href="#task-allocation-in-teams-as-a-multi-armed-bandit." id="toc-task-allocation-in-teams-as-a-multi-armed-bandit." class="nav-link" data-scroll-target="#task-allocation-in-teams-as-a-multi-armed-bandit.">Task Allocation in Teams as a Multi-Armed Bandit.</a></li>
  <li><a href="#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness." id="toc-human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness." class="nav-link" data-scroll-target="#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.">Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness.</a></li>
  <li><a href="#large-language-models-empowered-agent-based-modeling-and-simulation-a-survey-and-perspectives." id="toc-large-language-models-empowered-agent-based-modeling-and-simulation-a-survey-and-perspectives." class="nav-link" data-scroll-target="#large-language-models-empowered-agent-based-modeling-and-simulation-a-survey-and-perspectives.">Large language models empowered agent-based modeling and simulation: A survey and perspectives.</a></li>
  <li><a href="#building-machines-that-learn-and-think-with-people" id="toc-building-machines-that-learn-and-think-with-people" class="nav-link" data-scroll-target="#building-machines-that-learn-and-think-with-people">Building Machines that Learn and Think with People</a></li>
  <li><a href="#large-language-models-for-collective-problem-solving-insights-into-group-consensus-decision-making" id="toc-large-language-models-for-collective-problem-solving-insights-into-group-consensus-decision-making" class="nav-link" data-scroll-target="#large-language-models-for-collective-problem-solving-insights-into-group-consensus-decision-making">Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making</a></li>
  <li><a href="#exploring-collaborative-decision-making-a-quasi-experimental-study-of-human-and-generative-ai-interaction." id="toc-exploring-collaborative-decision-making-a-quasi-experimental-study-of-human-and-generative-ai-interaction." class="nav-link" data-scroll-target="#exploring-collaborative-decision-making-a-quasi-experimental-study-of-human-and-generative-ai-interaction.">Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction.</a></li>
  <li><a href="#how-large-language-models-can-reshape-collective-intelligence" id="toc-how-large-language-models-can-reshape-collective-intelligence" class="nav-link" data-scroll-target="#how-large-language-models-can-reshape-collective-intelligence">How large language models can reshape collective intelligence</a></li>
  <li><a href="#towards-human-ai-deliberation-design-and-evaluation-of-llm-empowered-deliberative-ai-for-ai-assisted-decision-making" id="toc-towards-human-ai-deliberation-design-and-evaluation-of-llm-empowered-deliberative-ai-for-ai-assisted-decision-making" class="nav-link" data-scroll-target="#towards-human-ai-deliberation-design-and-evaluation-of-llm-empowered-deliberative-ai-for-ai-assisted-decision-making">Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making</a></li>
  <li><a href="#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate." id="toc-enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate." class="nav-link" data-scroll-target="#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.">Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate.</a></li>
  <li><a href="#the-wisdom-of-partisan-crowds-comparing-collective-intelligence-in-humans-and-llm-based-agents" id="toc-the-wisdom-of-partisan-crowds-comparing-collective-intelligence-in-humans-and-llm-based-agents" class="nav-link" data-scroll-target="#the-wisdom-of-partisan-crowds-comparing-collective-intelligence-in-humans-and-llm-based-agents">The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents</a></li>
  <li><a href="#collective-innovation-in-groups-of-large-language-models." id="toc-collective-innovation-in-groups-of-large-language-models." class="nav-link" data-scroll-target="#collective-innovation-in-groups-of-large-language-models.">Collective Innovation in Groups of Large Language Models.</a></li>
  <li><a href="#evaluating-llm-agent-group-dynamics-against-human-group-dynamics-a-case-study-on-wisdom-of-partisan-crowds" id="toc-evaluating-llm-agent-group-dynamics-against-human-group-dynamics-a-case-study-on-wisdom-of-partisan-crowds" class="nav-link" data-scroll-target="#evaluating-llm-agent-group-dynamics-against-human-group-dynamics-a-case-study-on-wisdom-of-partisan-crowds">Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds</a></li>
  <li><a href="#exploring-collaboration-mechanisms-for-llm-agents-a-social-psychology-view" id="toc-exploring-collaboration-mechanisms-for-llm-agents-a-social-psychology-view" class="nav-link" data-scroll-target="#exploring-collaboration-mechanisms-for-llm-agents-a-social-psychology-view">Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View</a></li>
  <li><a href="#llm-deliberation-evaluating-llms-with-interactive-multi-agent-negotiation-games." id="toc-llm-deliberation-evaluating-llms-with-interactive-multi-agent-negotiation-games." class="nav-link" data-scroll-target="#llm-deliberation-evaluating-llms-with-interactive-multi-agent-negotiation-games.">LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games.</a></li>
  <li><a href="#llm-voting-human-choices-and-ai-collective-decision-making" id="toc-llm-voting-human-choices-and-ai-collective-decision-making" class="nav-link" data-scroll-target="#llm-voting-human-choices-and-ai-collective-decision-making">LLM Voting: Human Choices and AI Collective Decision Making</a></li>
  <li><a href="#embodied-llm-agents-learn-to-cooperate-in-organized-teams" id="toc-embodied-llm-agents-learn-to-cooperate-in-organized-teams" class="nav-link" data-scroll-target="#embodied-llm-agents-learn-to-cooperate-in-organized-teams">Embodied LLM Agents Learn to Cooperate in Organized Teams</a></li>
  <li><a href="#measuring-latent-trust-patterns-in-large-language-models-in-the-context-of-human-ai-teaming" id="toc-measuring-latent-trust-patterns-in-large-language-models-in-the-context-of-human-ai-teaming" class="nav-link" data-scroll-target="#measuring-latent-trust-patterns-in-large-language-models-in-the-context-of-human-ai-teaming">Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming</a></li>
  <li><a href="#a-survey-on-human-ai-teaming-with-large-pre-trained-models" id="toc-a-survey-on-human-ai-teaming-with-large-pre-trained-models" class="nav-link" data-scroll-target="#a-survey-on-human-ai-teaming-with-large-pre-trained-models">A Survey on Human-AI Teaming with Large Pre-Trained Models</a></li>
  <li><a href="#talk2care-an-llm-based-voice-assistant-for-communication-between-healthcare-providers-and-older-adults." id="toc-talk2care-an-llm-based-voice-assistant-for-communication-between-healthcare-providers-and-older-adults." class="nav-link" data-scroll-target="#talk2care-an-llm-based-voice-assistant-for-communication-between-healthcare-providers-and-older-adults.">Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults.</a></li>
  <li><a href="#conversational-agent-dynamics-with-minority-opinion-and-cognitive-conflict-in-small-group-decision-making." id="toc-conversational-agent-dynamics-with-minority-opinion-and-cognitive-conflict-in-small-group-decision-making." class="nav-link" data-scroll-target="#conversational-agent-dynamics-with-minority-opinion-and-cognitive-conflict-in-small-group-decision-making.">Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making.</a></li>
  <li><a href="#a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration" id="toc-a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration" class="nav-link" data-scroll-target="#a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration">A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="ai_gd.md"><i class="bi bi-file-code"></i>Github (GFM)</a></li><li><a href="ai_gd.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li><li><a href="gd_ai_lit_notes.docx"><i class="bi bi-file-word"></i>MS Word</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ai_gd.html">LLM Literature</a></li><li class="breadcrumb-item"><a href="./ai_gd.html">Group Decision Lit</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Group Decision Lit</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://tegorman13.github.io/">Thomas E. Gorman</a> </p>
  </div>
  <div class="quarto-title-meta-contents">
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 5, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="relevant-papers" class="level1">
<h1>Relevant Papers</h1>
<section id="ai-can-help-humans-find-common-ground-in-democratic-deliberation." class="level2">
<h2 class="anchored" data-anchor-id="ai-can-help-humans-find-common-ground-in-democratic-deliberation.">AI can help humans find common ground in democratic deliberation.</h2>
<p>Tessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., &amp; Summerfield, C. (2024). <strong>AI can help humans find common ground in democratic deliberation.</strong> Science, 386(6719), eadq2852. https://doi.org/10.1126/science.adq2852</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Finding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.</p>
</div>
</details>
<div id="fig-tessler" class="quarto-float quarto-figure quarto-figure-center anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-tessler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<p><a href="images/tessler1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure&nbsp;1: Figures from @tesslerAICanHelp2024"><img src="images/tessler1.png" class="img-fluid figure-img"></a></p>
<p><a href="images/tessler2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;1: Figures from @tesslerAICanHelp2024"><img src="images/tessler2.png" class="img-fluid figure-img"></a></p>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-tessler-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Figures from <span class="citation" data-cites="tesslerAICanHelp2024">Tessler et al. (<a href="#ref-tesslerAICanHelp2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="task-allocation-in-teams-as-a-multi-armed-bandit." class="level2">
<h2 class="anchored" data-anchor-id="task-allocation-in-teams-as-a-multi-armed-bandit.">Task Allocation in Teams as a Multi-Armed Bandit.</h2>
<p>Marjieh, R., Gokhale, A., Bullo, F., &amp; Griffiths, T. L. (2024). <strong>Task Allocation in Teams as a Multi-Armed Bandit.</strong> https://cocosci.princeton.edu/papers/marjieh2024task.pdf</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Humans rely on efficient distribution of resources to transcend the abilities of individuals. Successful task allocation, whether in small teams or across large institutions, depends on individuals’ ability to discern their own and others’ strengths and weaknesses, and to optimally act on them. This dependence creates a tension between exploring the capabilities of others and exploiting the knowledge acquired so far, which can be challenging. How do people navigate this tension? To address this question, we propose a novel task allocation paradigm in which a human agent is asked to repeatedly allocate tasks in three distinct classes (categorizing a blurry image, detecting a noisy voice command, and solving an anagram) between themselves and two other (bot) team members to maximize team performance. We show that this problem can be recast as a combinatorial multi-armed bandit which allows us to compare people’s performance against two well-known strategies, Thompson Sampling and Upper Confidence Bound (UCB). We find that humans are able to successfully integrate information about the capabilities of different team members to infer optimal allocations, and in some cases perform on par with these optimal strategies. Our approach opens up new avenues for studying the mechanisms underlying collective cooperation in teams.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/marjieh_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure from @marjiehTaskAllocationTeams2024"><img src="images/marjieh_24_img.png" class="img-fluid figure-img" alt="Figure from Marjieh et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="marjiehTaskAllocationTeams2024">Marjieh et al. (<a href="#ref-marjiehTaskAllocationTeams2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness." class="level2">
<h2 class="anchored" data-anchor-id="human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.">Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness.</h2>
<p>Bienefeld, N., Kolbe, M., Camen, G., Huser, D., &amp; Buehler, P. K. (2023). <strong>Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness.</strong> Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>In this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team’s ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team’s transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Bienefeld_23_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure from @bienefeldHumanAITeamingLeveraging2023"><img src="images/Bienefeld_23_img.png" class="img-fluid figure-img" alt="Figure from Bienefeld et al. (2023)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="bienefeldHumanAITeamingLeveraging2023">Bienefeld et al. (<a href="#ref-bienefeldHumanAITeamingLeveraging2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
<p><br>
<br>
</p>
</section>
<section id="large-language-models-empowered-agent-based-modeling-and-simulation-a-survey-and-perspectives." class="level2">
<h2 class="anchored" data-anchor-id="large-language-models-empowered-agent-based-modeling-and-simulation-a-survey-and-perspectives.">Large language models empowered agent-based modeling and simulation: A survey and perspectives.</h2>
<p>Gao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., &amp; Li, Y. (2024). <strong>Large language models empowered agent-based modeling and simulation: A survey and perspectives.</strong> Humanities and Social Sciences Communications, 11(1), 1–24. https://doi.org/10.1057/s41599-024-03611-3</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Agent-based modeling and simulation have evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Recently, integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, discussing their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments, and how these works address the above challenges. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/LLM-Agent-Based-Modeling-and-Simulation.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Gao_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure from @gaoLargeLanguageModels2024"><img src="images/Gao_24_img.png" class="img-fluid figure-img" alt="Figure from C. Gao et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="gaoLargeLanguageModels2024">C. Gao et al. (<a href="#ref-gaoLargeLanguageModels2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="building-machines-that-learn-and-think-with-people" class="level2">
<h2 class="anchored" data-anchor-id="building-machines-that-learn-and-think-with-people">Building Machines that Learn and Think with People</h2>
<p>Collins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., &amp; Griffiths, T. L. (2024). <strong>Building machines that learn and think with people.</strong> Nature Human Behaviour, 8(10), 1851–1863. https://doi.org/10.1038/s41562-024-01991-9</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
What do we want from machine intelligence? We envision machines that are not just tools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable, and trustworthy systems that think with us. Current artificial intelligence (AI) systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called “thought partners,” systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and AI thought partners can engage and propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.
<p></p>
</div>
</details>
<div id="fig-collins" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-collins-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Collins_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;2: Figures from @collinsBuildingMachinesThat2024a"><img src="images/Collins_24_img.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Collins2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure&nbsp;2: Figures from @collinsBuildingMachinesThat2024a"><img src="images/Collins2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Collins3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure&nbsp;2: Figures from @collinsBuildingMachinesThat2024a"><img src="images/Collins3.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-collins-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Figures from <span class="citation" data-cites="collinsBuildingMachinesThat2024a">Collins et al. (<a href="#ref-collinsBuildingMachinesThat2024a" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
<p><br>
</p>
</section>
<section id="large-language-models-for-collective-problem-solving-insights-into-group-consensus-decision-making" class="level2">
<h2 class="anchored" data-anchor-id="large-language-models-for-collective-problem-solving-insights-into-group-consensus-decision-making">Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making</h2>
<p>Du, Y., Rajivan, P., &amp; Gonzalez, C. C. (2024). <strong>Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making.</strong> https://escholarship.org/uc/item/6s060914</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Large Language models (LLM) exhibit human-like proficiency in various tasks such as translation, question answering, essay writing, and programming. Emerging research explores the use of LLMs in collective problem-solving endeavors, such as tasks where groups try to uncover clues through discussions. Although prior work has investigated individual problem-solving tasks, leveraging LLM-powered agents for group consensus and decision-making remains largely unexplored. This research addresses this gap by (1) proposing an algorithm to enable free-form conversation in groups of LLM agents, (2) creating metrics to evaluate the human-likeness of the generated dialogue and problem-solving performance, and (3) evaluating LLM agent groups against human groups using an open source dataset. Our results reveal that LLM groups outperform human groups in problem-solving tasks. LLM groups also show a greater improvement in scores after participating in free discussions. In particular, analyses indicate that LLM agent groups exhibit more disagreements, complex statements, and a propensity for positive statements compared to human groups. The results shed light on the potential of LLMs to facilitate collective reasoning and provide insight into the dynamics of group interactions involving synthetic LLM agents.</p>
</div>
</details>
<div id="fig-du" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-du-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/du_24_img1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;3: Figure from @duLargeLanguageModels2024"><img src="images/du_24_img1.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/du_24_img2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;3: Figure from @duLargeLanguageModels2024"><img src="images/du_24_img2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-du-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Figure from <span class="citation" data-cites="duLargeLanguageModels2024">Du et al. (<a href="#ref-duLargeLanguageModels2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="exploring-collaborative-decision-making-a-quasi-experimental-study-of-human-and-generative-ai-interaction." class="level2">
<h2 class="anchored" data-anchor-id="exploring-collaborative-decision-making-a-quasi-experimental-study-of-human-and-generative-ai-interaction.">Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction.</h2>
<p>Hao, X., Demir, E., &amp; Eyers, D. (2024). <strong>Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction.</strong> Technology in Society, 78, 102662. https://doi.org/10.1016/j.techsoc.2024.102662</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>This paper explores the effects of integrating Generative Artificial Intelligence (GAI) into decision-making processes within organizations, employing a quasi-experimental pretest-posttest design. The study examines the synergistic interaction between Human Intelligence (HI) and GAI across four group decision-making scenarios within three global organizations renowned for their cutting-edge operational techniques. The research progresses through several phases: identifying research problems, collecting baseline data on decision-making, implementing AI interventions, and evaluating the outcomes post-intervention to identify shifts in performance. The results demonstrate that GAI effectively reduces human cognitive burdens and mitigates heuristic biases by offering data-driven support and predictive analytics, grounded in System 2 reasoning. This is particularly valuable in complex situations characterized by unfamiliarity and information overload, where intuitive, System 1 thinking is less effective. However, the study also uncovers challenges related to GAI integration, such as potential over-reliance on technology, intrinsic biases particularly ‘out-of-the-box’ thinking without contextual creativity. To address these issues, this paper proposes an innovative strategic framework for HI-GAI collaboration that emphasizes transparency, accountability, and inclusiveness.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Hao_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure from @haoExploringCollaborativeDecisionmaking2024"><img src="images/Hao_24_img.png" class="img-fluid figure-img" alt="Figure from Hao et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="haoExploringCollaborativeDecisionmaking2024">Hao et al. (<a href="#ref-haoExploringCollaborativeDecisionmaking2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
<p><br>
<br>
</p>
</section>
<section id="how-large-language-models-can-reshape-collective-intelligence" class="level2">
<h2 class="anchored" data-anchor-id="how-large-language-models-can-reshape-collective-intelligence">How large language models can reshape collective intelligence</h2>
<p>Burton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). <strong>How large language models can reshape collective intelligence.</strong> Nature Human Behaviour, 1–13. https://doi.org/10.1038/s41562-024-01959-9</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Collective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals—even experts—resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the ‘wisdom of crowds’, online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans’ ability to collectively tackle complex problems.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Burton_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="@burtonHowLargeLanguage2024"><img src="images/Burton_24_img.png" class="img-fluid figure-img" alt="Burton et al. (2024)"></a></p>
<figcaption><span class="citation" data-cites="burtonHowLargeLanguage2024">Burton et al. (<a href="#ref-burtonHowLargeLanguage2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
<p><br>
</p>
</section>
<section id="towards-human-ai-deliberation-design-and-evaluation-of-llm-empowered-deliberative-ai-for-ai-assisted-decision-making" class="level2">
<h2 class="anchored" data-anchor-id="towards-human-ai-deliberation-design-and-evaluation-of-llm-empowered-deliberative-ai-for-ai-assisted-decision-making">Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making</h2>
<p>Ma, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., &amp; Ma, X. (2024). <strong>Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making</strong> (arXiv:2403.16812). arXiv. http://arxiv.org/abs/2403.16812</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>In AI-assisted decision-making, humans often passively review AI’s suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional explainable AI (XAI) assistants in improving humans’ appropriate reliance and task performance. Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design.</p>
</div>
</details>
<div id="fig-ma" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ma-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Ma_Ming1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;4: Figure from @maHumanAIDeliberationDesign2024"><img src="images/Ma_Ming1.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Ma_Ming2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;4: Figure from @maHumanAIDeliberationDesign2024"><img src="images/Ma_Ming2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Ma_Ming3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;4: Figure from @maHumanAIDeliberationDesign2024"><img src="images/Ma_Ming3.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Ma_Ming4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;4: Figure from @maHumanAIDeliberationDesign2024"><img src="images/Ma_Ming4.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Ma_Ming5a.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;4: Figure from @maHumanAIDeliberationDesign2024"><img src="images/Ma_Ming5a.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Ma_Ming5b.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="Figure&nbsp;4: Figure from @maHumanAIDeliberationDesign2024"><img src="images/Ma_Ming5b.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ma-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Figure from <span class="citation" data-cites="maHumanAIDeliberationDesign2024">Ma et al. (<a href="#ref-maHumanAIDeliberationDesign2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate." class="level2">
<h2 class="anchored" data-anchor-id="enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.">Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate.</h2>
<p>Chiang, C.-W., Lu, Z., Li, Z., &amp; Yin, M. (2024). <strong>Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate.</strong> Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Group decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil’s advocate in the AI-assisted group deci- sion making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities ex- hibited by modern large language models (LLMs), we design four different styles of devil’s advocate powered by LLMs, varying their interactivity (i.e., interactive vs.&nbsp;non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil’s advocates that argue against the AI model’s decision recommenda- tion have the potential to promote groups’ appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil’s advocate usually does not lead to substantial increases in people’s perceived workload for completing the group decision making tasks, while interactive LLM-powered devil’s advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/chiang_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="Figure from @chiangEnhancingAIAssistedGroup2024"><img src="images/chiang_24_img.png" class="img-fluid figure-img" alt="Figure from Chiang et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="chiangEnhancingAIAssistedGroup2024">Chiang et al. (<a href="#ref-chiangEnhancingAIAssistedGroup2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="the-wisdom-of-partisan-crowds-comparing-collective-intelligence-in-humans-and-llm-based-agents" class="level2">
<h2 class="anchored" data-anchor-id="the-wisdom-of-partisan-crowds-comparing-collective-intelligence-in-humans-and-llm-based-agents">The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents</h2>
<p>Chuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., &amp; Rogers, T. T. (2024). <strong>The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents.</strong> https://escholarship.org/uc/item/3k67x8s5</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Human groups are able to converge to more accurate beliefs through deliberation, even in the presence of polarization and partisan bias — a phenomenon known as the “wisdom of partisan crowds.” Large Language Models (LLMs) are increasingly being used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation, as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompting and lack of details in personas. Conversely, fine-tuning on human data appears to enhance convergence. These findings show the potential and limitations of LLM-based agents as a model of human collective intelligence.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Chuang_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20" title="@chuangWisdomPartisanCrowds2024"><img src="images/Chuang_24_img.png" class="img-fluid figure-img" alt="Chuang et al. (2024)"></a></p>
<figcaption><span class="citation" data-cites="chuangWisdomPartisanCrowds2024">Chuang et al. (<a href="#ref-chuangWisdomPartisanCrowds2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="collective-innovation-in-groups-of-large-language-models." class="level2">
<h2 class="anchored" data-anchor-id="collective-innovation-in-groups-of-large-language-models.">Collective Innovation in Groups of Large Language Models.</h2>
<p>Nisioti, E., Risi, S., Momennejad, I., Oudeyer, P.-Y., &amp; Moulin-Frier, C. (2024, July 7). <strong>Collective Innovation in Groups of Large Language Models.</strong> ALIFE 2024: Proceedings of the 2024 Artificial Life Conference. https://doi.org/10.1162/isal_a_00730</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Human culture relies on collective innovation: our ability to continuously explore how existing elements in our environment can be combined to create new ones. Language is hypothesized to play a key role in human culture, driving individual cognitive capacities and shaping communication. Yet the majority of models of collective innovation assign no cognitive capacities or language abilities to agents. Here, we contribute a computational study of collective innovation where agents are Large Language Models (LLMs) that play Little Alchemy 2, a creative video game originally developed for humans that, as we argue, captures useful aspects of innovation landscapes not present in previous test-beds. We, first, study an LLM in isolation and discover that it exhibits both useful skills and crucial limitations. We, then, study groups of LLMs that share information related to their behaviour and focus on the effect of social connectivity on collective performance. In agreement with previous human and computational studies, we observe that groups with dynamic connectivity out-compete fully-connected groups. Our work reveals opportunities and challenges for future studies of collective innovation that are becoming increasingly relevant as Generative Artificial Intelligence algorithms and humans innovate alongside each other.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Nisioti_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21" title="@nisiotiCollectiveInnovationGroups2024"><img src="images/Nisioti_24_img.png" class="img-fluid figure-img" alt="Nisioti et al. (2024)"></a></p>
<figcaption><span class="citation" data-cites="nisiotiCollectiveInnovationGroups2024">Nisioti et al. (<a href="#ref-nisiotiCollectiveInnovationGroups2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="evaluating-llm-agent-group-dynamics-against-human-group-dynamics-a-case-study-on-wisdom-of-partisan-crowds" class="level2">
<h2 class="anchored" data-anchor-id="evaluating-llm-agent-group-dynamics-against-human-group-dynamics-a-case-study-on-wisdom-of-partisan-crowds">Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds</h2>
<p>Chuang, Y.-S., Suresh, S., Harlalka, N., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., &amp; Rogers, T. T. (2023). <strong>Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds</strong> http://arxiv.org/abs/2311.09665</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>This study investigates the potential of Large Language Models (LLMs) to simulate human group dynamics, particularly within politically charged contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to role-play as Democrat and Republican personas, engaging in a structured interaction akin to human group study. Our approach evaluates how agents’ responses evolve through social influence. Our key findings indicate that LLM agents role-playing detailed personas and without Chain-of-Thought (CoT) reasoning closely align with human behaviors, while having CoT reasoning hurts the alignment. However, incorporating explicit biases into agent prompts does not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning LLMs with human data shows promise in achieving human-like behavior but poses a risk of overfitting certain behaviors. These findings show the potential and limitations of using LLM agents in modeling human group phenomena.</p>
</div>
</details>
<p><span class="citation" data-cites="chuangEvaluatingLLMAgent2023">Chuang et al. (<a href="#ref-chuangEvaluatingLLMAgent2023" role="doc-biblioref">2023</a>)</span></p>
</section>
<section id="exploring-collaboration-mechanisms-for-llm-agents-a-social-psychology-view" class="level2">
<h2 class="anchored" data-anchor-id="exploring-collaboration-mechanisms-for-llm-agents-a-social-psychology-view">Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View</h2>
<p>Zhang, J., Xu, X., Zhang, N., Liu, R., Hooi, B., &amp; Deng, S. (2024). <strong>Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View</strong> (arXiv:2310.02124). arXiv. http://arxiv.org/abs/2310.02124</p>
<p><a href="https://www.zjukg.org/project/MachineSoM/">https://www.zjukg.org/project/MachineSoM/</a></p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique ‘societies’ comprised of LLM agents, where each agent is characterized by a specific ‘trait’ (easy-going or overconfident) and engages in collaboration with a distinct ‘thinking pattern’ (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest humanlike social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We have shared our code and datasets1, hoping to catalyze further research in this promising avenue.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Zhang_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-22" title="@zhangExploringCollaborationMechanisms2024"><img src="images/Zhang_24_img.png" class="img-fluid figure-img" alt="Zhang et al. (2024)"></a></p>
<figcaption><span class="citation" data-cites="zhangExploringCollaborationMechanisms2024">Zhang et al. (<a href="#ref-zhangExploringCollaborationMechanisms2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="llm-deliberation-evaluating-llms-with-interactive-multi-agent-negotiation-games." class="level2">
<h2 class="anchored" data-anchor-id="llm-deliberation-evaluating-llms-with-interactive-multi-agent-negotiation-games.">LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games.</h2>
<p>Abdelnabi, S., Gomaa, A., Sivaprasad, S., Schönherr, L., &amp; Fritz, M. (2023). <strong>LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games.</strong> https://doi.org/10.60882/cispa.25233028.v1</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs’ reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and earlier models. Importantly, we test the generalization to new games and setups. Finally, we show that these games can help evaluate other critical aspects, such as the interaction dynamics between agents in the presence of greedy and adversarial players.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Abdelnabi_23_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-23" title="@abdelnabiLLMDeliberationEvaluatingLLMs2023"><img src="images/Abdelnabi_23_img.png" class="img-fluid figure-img" alt="Abdelnabi et al. (2023)"></a></p>
<figcaption><span class="citation" data-cites="abdelnabiLLMDeliberationEvaluatingLLMs2023">Abdelnabi et al. (<a href="#ref-abdelnabiLLMDeliberationEvaluatingLLMs2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="llm-voting-human-choices-and-ai-collective-decision-making" class="level2">
<h2 class="anchored" data-anchor-id="llm-voting-human-choices-and-ai-collective-decision-making">LLM Voting: Human Choices and AI Collective Decision Making</h2>
<p>Yang, J. C., Dailisan, D., Korecki, M., Hausladen, C. I., &amp; Helbing, D. (2024). <strong>LLM Voting: Human Choices and AI Collective Decision Making</strong> (arXiv:2402.01766). arXiv. http://arxiv.org/abs/2402.01766</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Yang_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-24" title="@yangLLMVotingHuman2024"><img src="images/Yang_24_img.png" class="img-fluid figure-img" alt="J. C. Yang et al. (2024)"></a></p>
<figcaption><span class="citation" data-cites="yangLLMVotingHuman2024">J. C. Yang et al. (<a href="#ref-yangLLMVotingHuman2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="embodied-llm-agents-learn-to-cooperate-in-organized-teams" class="level2">
<h2 class="anchored" data-anchor-id="embodied-llm-agents-learn-to-cooperate-in-organized-teams">Embodied LLM Agents Learn to Cooperate in Organized Teams</h2>
<p>Guo, X., Huang, K., Liu, J., Fan, W., Vélez, N., Wu, Q., Wang, H., Griffiths, T. L., &amp; Wang, M. (2024). <strong>Embodied LLM Agents Learn to Cooperate in Organized Teams</strong> (arXiv:2403.12482). arXiv. http://arxiv.org/abs/2403.12482</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Guo_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-25" title="@guoEmbodiedLLMAgents2024"><img src="images/Guo_24_img.png" class="img-fluid figure-img" alt="Guo et al. (2024)"></a></p>
<figcaption><span class="citation" data-cites="guoEmbodiedLLMAgents2024">Guo et al. (<a href="#ref-guoEmbodiedLLMAgents2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="measuring-latent-trust-patterns-in-large-language-models-in-the-context-of-human-ai-teaming" class="level2">
<h2 class="anchored" data-anchor-id="measuring-latent-trust-patterns-in-large-language-models-in-the-context-of-human-ai-teaming">Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming</h2>
<p>Koehl, D., &amp; Vangsness, L. (2023). <strong>Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming.</strong> Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 67. https://doi.org/10.1177/21695067231192869</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Qualitative self-report methods such as think-aloud procedures and open-ended response questions can provide valuable data to human factors research. These measures come with analytic weaknesses, such as researcher bias, intra- and inter-rater reliability concerns, and time-consuming coding protocols. A possible solution exists in the latent semantic patterns that exist in machine learning large language models. These semantic patterns could be used to analyze qualitative responses. This exploratory research compared the statistical quality of automated sentence coding using large language models to the benchmarks of self-report and behavioral measures within the context of trust in automation research. The results indicated that three large language models show promise as tools for analyzing qualitative responses. The study also provides insight on minimum sample sizes for model creation and offers recommendations for further validating the robustness of large language models as research tools.</p>
</div>
</details>
<div id="fig-koehl" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-koehl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Koehl_23_img1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-26" title="Figure&nbsp;5: @koehlMeasuringLatentTrust2023a"><img src="images/Koehl_23_img1.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Koehl_23_img2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-27" title="Figure&nbsp;5: @koehlMeasuringLatentTrust2023a"><img src="images/Koehl_23_img2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-koehl-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: <span class="citation" data-cites="koehlMeasuringLatentTrust2023a">Koehl &amp; Vangsness (<a href="#ref-koehlMeasuringLatentTrust2023a" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="a-survey-on-human-ai-teaming-with-large-pre-trained-models" class="level2">
<h2 class="anchored" data-anchor-id="a-survey-on-human-ai-teaming-with-large-pre-trained-models">A Survey on Human-AI Teaming with Large Pre-Trained Models</h2>
<p>Vats, V., Nizam, M. B., Liu, M., Wang, Z., Ho, R., Prasad, M. S., Titterton, V., Malreddy, S. V., Aggarwal, R., Xu, Y., Ding, L., Mehta, J., Grinnell, N., Liu, L., Zhong, S., Gandamani, D. N., Tang, X., Ghosalkar, R., Shen, C., … Davis, J. (2024). <strong>A Survey on Human-AI Teaming with Large Pre-Trained Models</strong> (arXiv:2403.04931). arXiv. http://arxiv.org/abs/2403.04931</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI Teaming, providing insights for future research, policy development, and strategic implementations aimed at harnessing the full potential of this collaboration for research and societal benefit.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Vats_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-28" title="Table from @vatsSurveyHumanAITeaming2024"><img src="images/Vats_24_img.png" class="img-fluid figure-img" alt="Table from Vats et al. (2024)"></a></p>
<figcaption>Table from <span class="citation" data-cites="vatsSurveyHumanAITeaming2024">Vats et al. (<a href="#ref-vatsSurveyHumanAITeaming2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="talk2care-an-llm-based-voice-assistant-for-communication-between-healthcare-providers-and-older-adults." class="level2">
<h2 class="anchored" data-anchor-id="talk2care-an-llm-based-voice-assistant-for-communication-between-healthcare-providers-and-older-adults.">Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults.</h2>
<p>Yang, Z., Xu, X., Yao, B., Rogers, E., Zhang, S., Intille, S., Shara, N., Gao, G. G., &amp; Wang, D. (2024). <strong>Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults.</strong> Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(2), 1–35. https://doi.org/10.1145/3659625</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Despite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs’ role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered conversational interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults’ conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers’ efforts and time. We envision our work as an initial exploration of LLMs’ capability in the intersection of healthcare and interpersonal communication.</p>
</div>
</details>
<p><a href="images/yang1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-29"><img src="images/yang1.png" class="img-fluid"></a></p>
<p><a href="images/yang2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-30"><img src="images/yang2.png" class="img-fluid"></a></p>
<div id="fig-yang" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-yang-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<a href="images/yang3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-31" title="Figure&nbsp;6: Figures from @yangTalk2CareLLMbasedVoice2024"><img src="images/yang3.png" class="img-fluid figure-img"></a>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-yang-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;6: Figures from <span class="citation" data-cites="yangTalk2CareLLMbasedVoice2024">Z. Yang et al. (<a href="#ref-yangTalk2CareLLMbasedVoice2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
<hr>
</section>
<section id="conversational-agent-dynamics-with-minority-opinion-and-cognitive-conflict-in-small-group-decision-making." class="level2">
<h2 class="anchored" data-anchor-id="conversational-agent-dynamics-with-minority-opinion-and-cognitive-conflict-in-small-group-decision-making.">Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making.</h2>
<p>Nishida, Y., Shimojo, S., &amp; Hayashi, Y. (2024). <strong>Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making.</strong> Japanese Psychological Research. https://doi.org/10.1111/jpr.12552</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>This study investigated the impact of group discussions with text-based conversational agents on risk-taking decision-making, which has been under-researched. We also focused on the influence of opinion patterns presented by the agents during discussions and attitudes toward these agents. Through an online experiment, 430 participants read a decision-seeking scenario and expressed the degree of risk they were willing to take. After viewing the text-based opinions of six agents and having a discussion with the agents, participants expressed the degree of risk they were willing to take for the same scenario. The result showed that participants’ risk-taking decisions shifted toward the agents’ group opinions, regardless of whether the agents’ opinions tended to be risky or cautious. Additionally, when the agents’ group opinions were more risk-biased and included a minority opinion, a significant association existed between the degree of the participants’ shift to a riskier decision and their positive attitudes toward the agents. The agents’ group opinions guided participants toward both risky and cautious decisions, and participants’ attitudes toward the agents were associated with their decision-making, albeit to a limited extent.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Nishida.png" class="lightbox" data-gallery="quarto-lightbox-gallery-32" title="Figure from @nishidaConversationalAgentDynamics2024"><img src="images/Nishida.png" class="img-fluid figure-img" alt="Figure from Nishida et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="nishidaConversationalAgentDynamics2024">Nishida et al. (<a href="#ref-nishidaConversationalAgentDynamics2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration" class="level2">
<h2 class="anchored" data-anchor-id="a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration">A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration</h2>
<p>Gao, J., Gebreegziabher, S. A., Choo, K. T. W., Li, T. J.-J., Perrault, S. T., &amp; Malone, T. W. (2024). <strong>A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration.</strong> Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–11. https://doi.org/10.1145/3613905.3650786</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>With ChatGPT’s release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow - planning, facilitating, iterating, and testing - to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the “5W1H” guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.</p>
</div>
</details>
<div id="fig-gao" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-gao-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Gao1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-33" title="Figure&nbsp;7: Figures from @gaoTaxonomyHumanLLMInteraction2024"><img src="images/Gao1.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Gao2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-34" title="Figure&nbsp;7: Figures from @gaoTaxonomyHumanLLMInteraction2024"><img src="images/Gao2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Gao3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-35" title="Figure&nbsp;7: Figures from @gaoTaxonomyHumanLLMInteraction2024"><img src="images/Gao3.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Gao4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-36" title="Figure&nbsp;7: Figures from @gaoTaxonomyHumanLLMInteraction2024"><img src="images/Gao4.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-gao-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;7: Figures from <span class="citation" data-cites="gaoTaxonomyHumanLLMInteraction2024">J. Gao et al. (<a href="#ref-gaoTaxonomyHumanLLMInteraction2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
<div style="page-break-after: always;"></div>
</section>
<section id="references" class="level2">
<h2 class="anchored" data-anchor-id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-abdelnabiLLMDeliberationEvaluatingLLMs2023" class="csl-entry" role="listitem">
Abdelnabi, S., Gomaa, A., Sivaprasad, S., Schönherr, L., &amp; Fritz, M. (2023). <em><span>LLM-Deliberation</span>: <span>Evaluating LLMs</span> with <span>Interactive Multi-Agent Negotiation Games</span>.</em> <a href="https://doi.org/10.60882/cispa.25233028.v1">https://doi.org/10.60882/cispa.25233028.v1</a>
</div>
<div id="ref-bienefeldHumanAITeamingLeveraging2023" class="csl-entry" role="listitem">
Bienefeld, N., Kolbe, M., Camen, G., Huser, D., &amp; Buehler, P. K. (2023). Human-<span>AI</span> teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. <em>Frontiers in Psychology</em>, <em>14</em>. <a href="https://doi.org/10.3389/fpsyg.2023.1208019">https://doi.org/10.3389/fpsyg.2023.1208019</a>
</div>
<div id="ref-burtonHowLargeLanguage2024" class="csl-entry" role="listitem">
Burton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). How large language models can reshape collective intelligence. <em>Nature Human Behaviour</em>, 1–13. <a href="https://doi.org/10.1038/s41562-024-01959-9">https://doi.org/10.1038/s41562-024-01959-9</a>
</div>
<div id="ref-chiangEnhancingAIAssistedGroup2024" class="csl-entry" role="listitem">
Chiang, C.-W., Lu, Z., Li, Z., &amp; Yin, M. (2024). Enhancing <span>AI-Assisted Group Decision Making</span> through <span>LLM-Powered Devil</span>’s <span>Advocate</span>. <em>Proceedings of the 29th <span>International Conference</span> on <span>Intelligent User Interfaces</span></em>, 103–119. <a href="https://doi.org/10.1145/3640543.3645199">https://doi.org/10.1145/3640543.3645199</a>
</div>
<div id="ref-chuangWisdomPartisanCrowds2024" class="csl-entry" role="listitem">
Chuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., &amp; Rogers, T. T. (2024). <em>The <span>Wisdom</span> of <span>Partisan Crowds</span>: <span>Comparing Collective Intelligence</span> in <span>Humans</span> and <span class="nocase">LLM-based Agents</span></em>.
</div>
<div id="ref-chuangEvaluatingLLMAgent2023" class="csl-entry" role="listitem">
Chuang, Y.-S., Suresh, S., Harlalka, N., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., &amp; Rogers, T. T. (2023). <em>Evaluating <span>LLM Agent Group Dynamics</span> against <span>Human Group Dynamics</span>: <span>A Case Study</span> on <span>Wisdom</span> of <span>Partisan Crowds</span></em> (arXiv:2311.09665). arXiv. <a href="https://arxiv.org/abs/2311.09665">https://arxiv.org/abs/2311.09665</a>
</div>
<div id="ref-collinsBuildingMachinesThat2024a" class="csl-entry" role="listitem">
Collins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., &amp; Griffiths, T. L. (2024). Building machines that learn and think with people. <em>Nature Human Behaviour</em>, <em>8</em>(10), 1851–1863. <a href="https://doi.org/10.1038/s41562-024-01991-9">https://doi.org/10.1038/s41562-024-01991-9</a>
</div>
<div id="ref-duLargeLanguageModels2024" class="csl-entry" role="listitem">
Du, Y., Rajivan, P., &amp; Gonzalez, C. C. (2024). <em>Large <span>Language Models</span> for <span>Collective Problem-Solving</span>: <span>Insights</span> into <span>Group Consensus Decision-Making</span></em>.
</div>
<div id="ref-gaoLargeLanguageModels2024" class="csl-entry" role="listitem">
Gao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., &amp; Li, Y. (2024). Large language models empowered agent-based modeling and simulation: A survey and perspectives. <em>Humanities and Social Sciences Communications</em>, <em>11</em>(1), 1–24. <a href="https://doi.org/10.1057/s41599-024-03611-3">https://doi.org/10.1057/s41599-024-03611-3</a>
</div>
<div id="ref-gaoTaxonomyHumanLLMInteraction2024" class="csl-entry" role="listitem">
Gao, J., Gebreegziabher, S. A., Choo, K. T. W., Li, T. J.-J., Perrault, S. T., &amp; Malone, T. W. (2024). A <span>Taxonomy</span> for <span>Human-LLM Interaction Modes</span>: <span>An Initial Exploration</span>. <em>Extended <span>Abstracts</span> of the <span>CHI Conference</span> on <span>Human Factors</span> in <span>Computing Systems</span></em>, 1–11. <a href="https://doi.org/10.1145/3613905.3650786">https://doi.org/10.1145/3613905.3650786</a>
</div>
<div id="ref-guoEmbodiedLLMAgents2024" class="csl-entry" role="listitem">
Guo, X., Huang, K., Liu, J., Fan, W., Vélez, N., Wu, Q., Wang, H., Griffiths, T. L., &amp; Wang, M. (2024). <em>Embodied <span>LLM Agents Learn</span> to <span>Cooperate</span> in <span>Organized Teams</span></em> (arXiv:2403.12482). arXiv. <a href="https://arxiv.org/abs/2403.12482">https://arxiv.org/abs/2403.12482</a>
</div>
<div id="ref-haoExploringCollaborativeDecisionmaking2024" class="csl-entry" role="listitem">
Hao, X., Demir, E., &amp; Eyers, D. (2024). Exploring collaborative decision-making: <span>A</span> quasi-experimental study of human and <span>Generative AI</span> interaction. <em>Technology in Society</em>, <em>78</em>, 102662. <a href="https://doi.org/10.1016/j.techsoc.2024.102662">https://doi.org/10.1016/j.techsoc.2024.102662</a>
</div>
<div id="ref-koehlMeasuringLatentTrust2023a" class="csl-entry" role="listitem">
Koehl, D., &amp; Vangsness, L. (2023). Measuring <span>Latent Trust Patterns</span> in <span>Large Language Models</span> in the <span>Context</span> of <span>Human-AI Teaming</span>. <em>Proceedings of the Human Factors and Ergonomics Society Annual Meeting</em>, <em>67</em>. <a href="https://doi.org/10.1177/21695067231192869">https://doi.org/10.1177/21695067231192869</a>
</div>
<div id="ref-maHumanAIDeliberationDesign2024" class="csl-entry" role="listitem">
Ma, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., &amp; Ma, X. (2024). <em>Towards <span>Human-AI Deliberation</span>: <span>Design</span> and <span>Evaluation</span> of <span>LLM-Empowered Deliberative AI</span> for <span>AI-Assisted Decision-Making</span></em> (arXiv:2403.16812). arXiv. <a href="https://arxiv.org/abs/2403.16812">https://arxiv.org/abs/2403.16812</a>
</div>
<div id="ref-marjiehTaskAllocationTeams2024" class="csl-entry" role="listitem">
Marjieh, R., Gokhale, A., Bullo, F., &amp; Griffiths, T. L. (2024). <em>Task <span>Allocation</span> in <span>Teams</span> as a <span>Multi-Armed Bandit</span></em>.
</div>
<div id="ref-nishidaConversationalAgentDynamics2024" class="csl-entry" role="listitem">
Nishida, Y., Shimojo, S., &amp; Hayashi, Y. (2024). Conversational <span>Agent Dynamics</span> with <span>Minority Opinion</span> and <span>Cognitive Conflict</span> in <span>Small-Group Decision-Making</span>. <em>Japanese Psychological Research</em>. <a href="https://doi.org/10.1111/jpr.12552">https://doi.org/10.1111/jpr.12552</a>
</div>
<div id="ref-nisiotiCollectiveInnovationGroups2024" class="csl-entry" role="listitem">
Nisioti, E., Risi, S., Momennejad, I., Oudeyer, P.-Y., &amp; Moulin-Frier, C. (2024, July). Collective <span>Innovation</span> in <span>Groups</span> of <span>Large Language Models</span>. <em><span>ALIFE</span> 2024: <span>Proceedings</span> of the 2024 <span>Artificial Life Conference</span></em>. <a href="https://doi.org/10.1162/isal_a_00730">https://doi.org/10.1162/isal_a_00730</a>
</div>
<div id="ref-tesslerAICanHelp2024" class="csl-entry" role="listitem">
Tessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., &amp; Summerfield, C. (2024). <span>AI</span> can help humans find common ground in democratic deliberation. <em>Science</em>, <em>386</em>(6719), eadq2852. <a href="https://doi.org/10.1126/science.adq2852">https://doi.org/10.1126/science.adq2852</a>
</div>
<div id="ref-vatsSurveyHumanAITeaming2024" class="csl-entry" role="listitem">
Vats, V., Nizam, M. B., Liu, M., Wang, Z., Ho, R., Prasad, M. S., Titterton, V., Malreddy, S. V., Aggarwal, R., Xu, Y., Ding, L., Mehta, J., Grinnell, N., Liu, L., Zhong, S., Gandamani, D. N., Tang, X., Ghosalkar, R., Shen, C., … Davis, J. (2024). <em>A <span>Survey</span> on <span>Human-AI Teaming</span> with <span>Large Pre-Trained Models</span></em> (arXiv:2403.04931). arXiv. <a href="https://arxiv.org/abs/2403.04931">https://arxiv.org/abs/2403.04931</a>
</div>
<div id="ref-yangLLMVotingHuman2024" class="csl-entry" role="listitem">
Yang, J. C., Dailisan, D., Korecki, M., Hausladen, C. I., &amp; Helbing, D. (2024). <em><span>LLM Voting</span>: <span>Human Choices</span> and <span>AI Collective Decision Making</span></em> (arXiv:2402.01766). arXiv. <a href="https://arxiv.org/abs/2402.01766">https://arxiv.org/abs/2402.01766</a>
</div>
<div id="ref-yangTalk2CareLLMbasedVoice2024" class="csl-entry" role="listitem">
Yang, Z., Xu, X., Yao, B., Rogers, E., Zhang, S., Intille, S., Shara, N., Gao, G. G., &amp; Wang, D. (2024). <span>Talk2Care</span>: <span class="nocase">An LLM-based Voice Assistant</span> for <span>Communication</span> between <span>Healthcare Providers</span> and <span>Older Adults</span>. <em>Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies</em>, <em>8</em>(2), 1–35. <a href="https://doi.org/10.1145/3659625">https://doi.org/10.1145/3659625</a>
</div>
<div id="ref-zhangExploringCollaborationMechanisms2024" class="csl-entry" role="listitem">
Zhang, J., Xu, X., Zhang, N., Liu, R., Hooi, B., &amp; Deng, S. (2024). <em>Exploring <span>Collaboration Mechanisms</span> for <span>LLM Agents</span>: <span>A Social Psychology View</span></em> (arXiv:2310.02124). arXiv. <a href="https://arxiv.org/abs/2310.02124">https://arxiv.org/abs/2310.02124</a>
</div>
</div>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/tegorman13\.github\.io\/ccl");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>