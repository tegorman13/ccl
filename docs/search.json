[
  {
    "objectID": "tms.html",
    "href": "tms.html",
    "title": "Transactive Memory Systems",
    "section": "",
    "text": "Bienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nAbstract\n\n\nIn this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team’s ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team’s transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond.\n\n\n\n\n\n\nBienefeld et al. (2023)",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "tms.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.",
    "href": "tms.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.",
    "title": "Transactive Memory Systems",
    "section": "",
    "text": "Bienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nAbstract\n\n\nIn this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team’s ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team’s transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond.\n\n\n\n\n\n\nBienefeld et al. (2023)",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "tms.html#communication-in-transactive-memory-systems-a-review-and-multidimensional-network-perspective",
    "href": "tms.html#communication-in-transactive-memory-systems-a-review-and-multidimensional-network-perspective",
    "title": "Transactive Memory Systems",
    "section": "Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective",
    "text": "Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective\nYan, B., Hollingshead, A. B., Alexander, K. S., Cruz, I., & Shaikh, S. J. (2021). Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective. Small Group Research, 52(1), 3–32. https://doi.org/10.1177/1046496420967764\n\n\nAbstract\n\n\nThe comprehensive review synthesizes 64 empirical studies on communication and transactive memory systems (TMS). The results reveal that (a) a TMS forms through communication about expertise; (b) as a TMS develops, communication to allocate information and coordinate retrieval increases, promoting information exchange; and (c) groups update their TMS through communicative learning. However, direct interpersonal communication is not necessary for TMS development or utilization. Nor do high-quality information-sharing processes always occur within developed TMS structures. For future research, we propose a multidimensional network approach to TMS that incorporates technologies, addresses member characteristics, considers multiple communication types, and situates groups in context.",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "tms.html#alignment-transactive-memory-and-collective-cognitive-systems",
    "href": "tms.html#alignment-transactive-memory-and-collective-cognitive-systems",
    "title": "Transactive Memory Systems",
    "section": "Alignment, Transactive Memory, and Collective Cognitive Systems",
    "text": "Alignment, Transactive Memory, and Collective Cognitive Systems\nTollefsen, D. P., Dale, R., & Paxton, A. (2013). Alignment, Transactive Memory, and Collective Cognitive Systems. Review of Philosophy and Psychology, 4(1), 49–64. https://doi.org/10.1007/s13164-012-0126-z\n\n\nAbstract\n\n\nResearch on linguistic interaction suggests that two or more individuals can sometimes form adaptive and cohesive systems. We describe an “alignment system” as a loosely interconnected set of cognitive processes that facilitate social interactions. As a dynamic, multi-component system, it is responsive to higher-level cognitive states such as shared beliefs and intentions (those involving collective intentionality) but can also give rise to such shared cognitive states via bottom-up processes. As an example of putative group cognition we turn to transactive memory and suggest how further research on alignment in these cases might reveal how such systems can be genuinely described as cognitive. Finally, we address a prominent critique of collective cognitive systems, arguing that there is much empirical and explanatory benefit to be gained from considering the possibility of group cognitive systems, especially in the context of small-group human interaction.",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "tms.html#building-machines-that-learn-and-think-with-people",
    "href": "tms.html#building-machines-that-learn-and-think-with-people",
    "title": "Transactive Memory Systems",
    "section": "Building Machines that Learn and Think with People",
    "text": "Building Machines that Learn and Think with People\nCollins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building Machines that Learn and Think with People (arXiv:2408.03943). arXiv. http://arxiv.org/abs/2408.03943\n\n\nAbstract\n\n\nWhat do we want from machine intelligence? We envision machines that are not just tools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable, and trustworthy systems that think with us. Current artificial intelligence (AI) systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called “thought partners,” systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and AI thought partners can engage and propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.\n\n\n\n\n\n\nFigure from Collins et al. (2024)\n\n\nArgote, L., & Ren, Y. (2012). Transactive Memory Systems: A Microfoundation of Dynamic Capabilities. Journal of Management Studies, 49(8), 1375–1382. https://doi.org/10.1111/j.1467-6486.2012.01077.x\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\nBrandon, D. P., & Hollingshead, A. B. (2004). Transactive Memory Systems in Organizations: Matching Tasks, Expertise, and People. Organization Science, 15(6), 633–644. https://doi.org/10.1287/orsc.1040.0069\nHollingshead, A. B. (1998). Communication, Learning, and Retrieval in Transactive Memory Systems. Journal of Experimental Social Psychology, 34(5), 423–442. https://doi.org/10.1006/jesp.1998.1358\nKimura, T. (2024). Virtual Teams: A Smart Literature Review of Four Decades of Research. Human Behavior and Emerging Technologies, 2024(1), 8373370. https://doi.org/10.1155/2024/8373370\nMarjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit. https://cocosci.princeton.edu/papers/marjieh2024task.pdf\nMcWilliams, D. J., & Randolph, A. B. (2024). Transactive memory systems in superteams: The effect of an intelligent assistant in virtual teams. Information Technology & People, ahead-of-print(ahead-of-print). https://doi.org/10.1108/ITP-12-2022-0918\nSamipour-Biel, S. P. (2022). A Process Model of Transactive Memory System Shared Knowledge Structure Emergence: A Computational Model in R [Ph.D., The University of Akron]. https://www.proquest.com/docview/2711844070/abstract/DBDAB24DBBB34601PQ/1\nTollefsen, D. P., Dale, R., & Paxton, A. (2013). Alignment, Transactive Memory, and Collective Cognitive Systems. Review of Philosophy and Psychology, 4(1), 49–64. https://doi.org/10.1007/s13164-012-0126-z\nUden, L., & Ting, I.-H. (Eds.). (2024). The Design of AI-Enabled Experience-Based Knowledge Management System to Facilitate Knowing and Doing in Communities of Practice (Vol. 2152). Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-63269-3\nWegner, D. M. (1995). A Computer Network Model of Human Transactive Memory. Social Cognition, 13(3), 319–339. https://doi.org/10.1521/soco.1995.13.3.319\nYan, B., Hollingshead, A. B., Alexander, K. S., Cruz, I., & Shaikh, S. J. (2021). Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective. Small Group Research, 52(1), 3–32. https://doi.org/10.1177/1046496420967764",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "chapter_structure.html",
    "href": "chapter_structure.html",
    "title": "Possible Chapter Structures",
    "section": "",
    "text": "Brainstorming chapter structures with the help of various AI assistants:",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "chapter_structure.html#alternative-0",
    "href": "chapter_structure.html#alternative-0",
    "title": "Possible Chapter Structures",
    "section": "Alternative 0",
    "text": "Alternative 0\nPotential Section Structure:\n\nIntroduction\n\nOverview of AI in group decision-making contexts\nRelevance to group dynamics research\n\nLarge Language Models (LLMs) as Decision-Making Agents\n\nCapabilities and limitations of LLMs in decision-making tasks\nComparison with human decision-making processes\n\nAI-Human Collaboration in Group Decision Making\n\nModels of human-AI teaming\nEnhancing collective intelligence through AI integration\n\nSimulating Group Dynamics with AI Agents\n\nLLM-based multi-agent simulations\nEmergent behaviors and social phenomena in AI agent groups\n\nEthical Considerations and Biases\n\nRepresentation and identity issues in AI-simulated groups\nAddressing biases in AI-assisted decision making\n\nMethodological Approaches and Challenges\n\nExperimental designs for studying AI in group contexts\nMeasuring and evaluating AI-human group performance\n\nApplications and Future Directions\n\nPotential uses in various domains (e.g., education, healthcare, policy-making)\nResearch gaps and emerging questions\n\nConclusion\n\nImplications for group dynamics research\nRecommendations for future studies\n\n\nKey Issues to Discuss:\n\nThe potential and limitations of LLMs in simulating human-like decision-making processes\nThe impact of AI agents on group dynamics, including cooperation, conflict, and consensus-building\nEthical considerations in using AI to replace or augment human participants in group studies\nThe emergence of social behaviors and phenomena in multi-agent AI systems\nMethodological challenges in designing experiments and measuring outcomes in AI-human group interactions\nThe role of AI in enhancing collective intelligence and group performance\nAddressing biases and ensuring diverse representation in AI-simulated group dynamics\nThe potential of AI to model complex social systems and inform social science research\nChallenges in aligning AI agent behavior with human social norms and expectations\nThe impact of AI on traditional theories and models of group dynamics",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "chapter_structure.html#alternative-1",
    "href": "chapter_structure.html#alternative-1",
    "title": "Possible Chapter Structures",
    "section": "Alternative 1",
    "text": "Alternative 1\n\nPotential Issues to Discuss:\n\nThe Role of AI in Collective Intelligence:\n\nHow large language models (LLMs) enhance or inhibit group decision-making (Burton et al., 2024; Cui & Yasseri, 2024).\nThe concept of hybrid human-AI collective intelligence and the synergies between human reasoning and AI-driven insights.\nOpportunities and challenges AI introduces for improving group problem-solving in both human and AI-augmented groups.\n\nHuman-AI Teaming and Decision Making:\n\nThe dynamics of human-AI collaboration and transactive memory systems in group decision-making (Bienefeld et al., 2023).\nHow introducing AI in decision-making teams affects group behavior, particularly in terms of knowledge-sharing and hypothesis generation.\n\nSimulating Human Decision-Making Using AI:\n\nUsing LLMs to simulate group dynamics and decision-making processes, such as in multi-agent negotiation games (Abdelnabi et al., 2023) or partisan group behavior (Chuang et al., 2024).\nThe limitations and opportunities of using LLMs as proxies for human participants in decision-making simulations (Aher et al., 2022).\n\nChallenges in AI-Assisted Group Decision-Making:\n\nIssues with the overreliance on AI recommendations and the role of LLM-powered devil’s advocates in correcting this overreliance (Chiang et al., 2024).\nHow AI may introduce biases or alter the dynamics of decision-making in group settings.\n\nAI’s Limitations in Group Decision Making:\n\nCurrent limitations in how AI models deal with complex social dynamics and group decision-making (Fan et al., 2024).\nSituations where LLMs fall short in reproducing nuanced human behaviors and decision-making patterns.\n\nEthical and Strategic Considerations in AI-Assisted Group Decision Making:\n\nEthical considerations around using AI for decision-making in groups, especially concerning equity and fairness.\nThe need for strategies to mitigate biases in LLMs when used for group decision-making.\n\n\n\n\n\nPossible Section Structure:\n\nIntroduction:\n\nOverview of AI in group decision-making.\nDefinition and context of group decision-making in psychology and communication studies.\nIntroduction of large language models (LLMs) and AI agents as tools in decision-making.\n\nAI and Collective Intelligence:\n\nExploration of how AI enhances collective intelligence.\nInsights from research on LLMs’ role in collective problem-solving (Burton et al., 2024).\n\nHuman-AI Teaming in Decision Making:\n\nExamination of human-AI collaboration and the dynamics of transactive memory systems (Bienefeld et al., 2023).\nCase studies on the application of AI in human decision-making teams.\n\nAI Simulations of Human Decision-Making:\n\nDiscussion of LLMs as simulators for human group dynamics (Aher et al., 2022).\nComparative analysis of AI-driven versus human-driven decision-making processes.\n\nChallenges and Limitations of AI in Group Dynamics:\n\nChallenges in AI adoption in group settings, including bias and overreliance on AI recommendations (Chiang et al., 2024).\nLimitations in AI’s ability to fully replicate human decision-making behaviors (Fan et al., 2024).\n\nEthical and Practical Considerations:\n\nEthical implications of using AI in group decision-making.\nStrategic approaches to mitigating biases and ensuring ethical outcomes.\n\nFuture Directions and Open Questions:\n\nProspective developments in AI-enhanced group decision-making.\nOpen research questions and directions for future studies in this area.",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "chapter_structure.html#alternative-2",
    "href": "chapter_structure.html#alternative-2",
    "title": "Possible Chapter Structures",
    "section": "Alternative 2",
    "text": "Alternative 2\n\nHuman-AI Teaming and Collaboration:\n\nPapers that explore how LLMs collaborate with humans in decision-making, problem-solving, and teamwork contexts.\nExample: “Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness” .\n\nLLMs as Models for Social and Group Dynamics:\n\nPapers that investigate how LLMs simulate or replicate human group behavior, including group decision-making, social interactions, and collective intelligence.\nExample: “The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents” .\n\nLLMs in Decision-Making and Game Theory:\n\nPapers that focus on the decision-making abilities of LLMs, including their performance in structured games and strategic contexts.\nExample: “Can Large Language Models Serve as Rational Players in Game Theory?” .\n\nEvaluation of LLM Capabilities and Limitations:\n\nPapers that assess the reasoning, problem-solving, and decision-making capabilities of LLMs, often by comparing them to human performance.\nExample: “LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games” .\n\nEthical and Societal Implications of LLM Use:\n\nPapers that address the ethical, societal, and policy-related considerations associated with the use of LLMs in human-centered contexts.\nExample: “Large language models cannot replace human participants because they cannot portray identity groups”.",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "chapter_structure.html#alternative-3",
    "href": "chapter_structure.html#alternative-3",
    "title": "Possible Chapter Structures",
    "section": "Alternative 3",
    "text": "Alternative 3\n\nCollective Intelligence and Decision Making This category focuses on how LLMs can be used to simulate or enhance collective intelligence, group decision-making processes, and wisdom of crowds phenomena.\nMulti-Agent Systems and Collaboration This category includes papers that explore interactions between multiple LLM agents, their collaborative behaviors, and emergent properties in multi-agent systems.\nHuman-AI Teaming and Interaction This category covers research on the integration of LLMs with human teams, human-AI collaboration, and the use of LLMs to augment human capabilities in various domains.\nSocial Behavior and Network Dynamics This category encompasses studies that investigate whether LLMs exhibit human-like social behaviors, network formation tendencies, and how they might influence or simulate social dynamics.",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "chapter_structure.html#alternative-4",
    "href": "chapter_structure.html#alternative-4",
    "title": "Possible Chapter Structures",
    "section": "Alternative 4",
    "text": "Alternative 4\n1. LLM Agents as Simulators of Human Behavior:\n\nDescription: This category encompasses papers that explore the use of LLMs to simulate individual and group human behavior in various contexts, including social dilemmas, games, and collective decision-making.\nPapers:\n\nAher et al. (2022) - Using Large Language Models to Simulate Multiple Humans\nChuang et al. (2023) - Evaluating LLM Agent Group Dynamics against Human Group Dynamics\nChuang et al. (2024) - The Wisdom of Partisan Crowds\nFan et al. (2024) - Can Large Language Models Serve as Rational Players in Game Theory?\nJin et al. (2024) - What if LLMs Have Different World Views\nLeng & Yuan (2024) - Do LLM Agents Exhibit Social Behavior?\nSun et al. - Random Silicon Sampling\nWang et al. (2024) - Large language models cannot replace human participants because they cannot portray identity groups\n\n\n2. LLM-Enhanced Collective Intelligence:\n\nDescription: This category includes papers that investigate how LLMs can be integrated into human groups or systems to augment collective intelligence, improve decision-making, and facilitate problem-solving.\nPapers:\n\nBurton et al. (2024) - How large language models can reshape collective intelligence\nChiang et al. (2024) - Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate\nCui & Yasseri (2024) - AI-enhanced Collective Intelligence\nDu et al. (2024) - Large Language Models for Collective Problem-Solving\nGruen et al. (2023) - Machine learning augmentation reduces prediction error in collective forecasting\nNisioti et al. (2024) - Collective Innovation in Groups of Large Language Models\n\n\n3. LLM Agent Interaction and Network Dynamics:\n\nDescription: This category focuses on papers that examine the interactions and emergent behaviors of multiple LLM agents, including network formation, communication patterns, and the dynamics of cooperation and competition.\nPapers:\n\nCisneros-Velarde (2024) - On the Principles behind Opinion Dynamics in Multi-Agent Systems of Large Language Models\nHuang et al. (2024) - How Far Are We on the Decision-Making of LLMs?\nKim et al. (2024) - Adaptive Collaboration Strategy for LLMs in Medical Decision Making\nMarjieh et al. (2024) - Task Allocation in Teams as a Multi-Armed Bandit\nPapachristou & Yuan (2024) - Network Formation and Dynamics Among Multi-LLMs\nPiatti et al. (2024) - Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\nZhang et al. (2024) - Simulating Classroom Education with LLM-Empowered Agents\n\n\n4. LLMs in Collaborative Tasks:\n\nDescription: If you want to further emphasize the practical applications of LLMs in collaborative settings, you could create a separate category for papers specifically addressing human-LLM collaboration in tasks like annotation or knowledge creation.\nPapers:\n\nWang et al. (2024) - Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "ai_decision.html",
    "href": "ai_decision.html",
    "title": "Individual decision lit",
    "section": "",
    "text": "Macmillan-Scott, O., & Musolesi, M. (2024). (Ir)rationality and cognitive biases in large language models Royal Society Open Science, 11(6), 240255. https://doi.org/10.1098/rsos.240255\n\n\nAbstract\n\n\nDo large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Figures from Macmillan-Scott & Musolesi (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#irrationality-and-cognitive-biases-in-large-language-models.",
    "href": "ai_decision.html#irrationality-and-cognitive-biases-in-large-language-models.",
    "title": "Individual decision lit",
    "section": "",
    "text": "Macmillan-Scott, O., & Musolesi, M. (2024). (Ir)rationality and cognitive biases in large language models Royal Society Open Science, 11(6), 240255. https://doi.org/10.1098/rsos.240255\n\n\nAbstract\n\n\nDo large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Figures from Macmillan-Scott & Musolesi (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#human-like-intuitive-behavior-and-reasoning-biases-emerged-in-large-language-models-but-disappeared-in-chatgpt",
    "href": "ai_decision.html#human-like-intuitive-behavior-and-reasoning-biases-emerged-in-large-language-models-but-disappeared-in-chatgpt",
    "title": "Individual decision lit",
    "section": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
    "text": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT\nHagendorff, T., Fabi, S., & Kosinski, M. (2023). Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. Nature Computational Science, 3(10), 833–838. https://doi.org/10.1038/s43588-023-00527-x\n\n\nAbstract\n\n\nWe design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Figures from Hagendorff et al. (2023)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#using-cognitive-psychology-to-understand-gpt-3.",
    "href": "ai_decision.html#using-cognitive-psychology-to-understand-gpt-3.",
    "title": "Individual decision lit",
    "section": "Using cognitive psychology to understand GPT-3.",
    "text": "Using cognitive psychology to understand GPT-3.\nBinz, M., & Schulz, E. (2023). Using cognitive psychology to understand GPT-3. Proceedings of the National Academy of Sciences, 120(6), e2218523120. https://doi.org/10.1073/pnas.2218523120\n\n\nAbstract\n\n\nWe study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3’s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3’s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.\n\n\n\n\n\nFigure from Binz & Schulz (2023)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#studying-and-improving-reasoning-in-humans-and-machines.",
    "href": "ai_decision.html#studying-and-improving-reasoning-in-humans-and-machines.",
    "title": "Individual decision lit",
    "section": "Studying and improving reasoning in humans and machines.",
    "text": "Studying and improving reasoning in humans and machines.\nYax, N., Anlló, H., & Palminteri, S. (2024). Studying and improving reasoning in humans and machines. Communications Psychology, 2(1), 1–16. https://doi.org/10.1038/s44271-024-00091-8\n\n\nAbstract\n\n\nIn the present study, we investigate and compare reasoning in large language models (LLMs) and humans, using a selection of cognitive psychology tools traditionally dedicated to the study of (bounded) rationality. We presented to human participants and an array of pretrained LLMs new variants of classical cognitive experiments, and cross-compared their performances. Our results showed that most of the included models presented reasoning errors akin to those frequently ascribed to error-prone, heuristic-based human reasoning. Notwithstanding this superficial similarity, an in-depth comparison between humans and LLMs indicated important differences with human-like reasoning, with models’ limitations disappearing almost entirely in more recent LLMs’ releases. Moreover, we show that while it is possible to devise strategies to induce better performance, humans and machines are not equally responsive to the same prompting schemes. We conclude by discussing the epistemological implications and challenges of comparing human and machine behavior for both artificial intelligence and cognitive psychology.\n\n\n\n\n\nFigure from Yax et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#exploring-variability-in-risk-taking-with-large-language-models.",
    "href": "ai_decision.html#exploring-variability-in-risk-taking-with-large-language-models.",
    "title": "Individual decision lit",
    "section": "Exploring variability in risk taking with large language models.",
    "text": "Exploring variability in risk taking with large language models.\nBhatia, S. (2024). Exploring variability in risk taking with large language models. Journal of Experimental Psychology: General, 153(7), 1838–1860. https://doi.org/10.1037/xge0001607\n\n\nAbstract\n\n\nWhat are the sources of individual-level differences in risk taking, and how do they depend on the domain or situation in which the decision is being made? Psychologists currently answer such questions with psychometric methods, which analyze correlations across participant responses in survey data sets. In this article, we analyze the preferences that give rise to these correlations. Our approach uses (a) large language models (LLMs) to quantify everyday risky behaviors in terms of the attributes or reasons that may describe those behaviors, and (b) decision models to map these attributes and reasons onto participant responses. We show that LLM-based decision models can explain observed correlations between behaviors in terms of the reasons different behaviors elicit and explain observed correlations between individuals in terms of the weights different individuals place on reasons, thereby providing a decision theoretic foundation for psychometric findings. Since LLMs can generate quantitative representations for nearly any naturalistic decision, they can be used to make accurate out-of-sample predictions for hundreds of everyday behaviors, predict the reasons why people may or may not want to engage in these behaviors, and interpret these reasons in terms of core psychological constructs. Our approach has important theoretical and practical implications for the study of heterogeneity in everyday behavior.\n\n\nBhatia (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#human-bias-in-ai-models-anchoring-effects-and-mitigation-strategies-in-large-language-models",
    "href": "ai_decision.html#human-bias-in-ai-models-anchoring-effects-and-mitigation-strategies-in-large-language-models",
    "title": "Individual decision lit",
    "section": "Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models",
    "text": "Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models\nNguyen, J. (2024). Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models. Journal of Behavioral and Experimental Finance, 100971. https://doi.org/10.1016/j.jbef.2024.100971\n\n\nAbstract\n\n\nThis study builds on the seminal work of Tversky and Kahneman (1974), exploring the presence and extent of anchoring bias in forecasts generated by four Large Language Models (LLMs): GPT-4, Claude 2, Gemini Pro and GPT-3.5. In contrast to recent findings of advanced reasoning capabilities in LLMs, our randomised controlled trials reveal the presence of anchoring bias across all models: forecasts are significantly influenced by prior mention of high or low values. We examine two mitigation prompting strategies, ‘Chain of Thought’ and ‘ignore previous’, finding limited and varying degrees of effectiveness. Our results extend the anchoring bias research in finance beyond human decision-making to encompass LLMs, highlighting the importance of deliberate and informed prompting in AI forecasting in both ad hoc LLM use and in crafting few-shot examples.\n\n\n\n\n\nFigure from Nguyen (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#a-turing-test-of-whether-ai-chatbots-are-behaviorally-similar-to-humans",
    "href": "ai_decision.html#a-turing-test-of-whether-ai-chatbots-are-behaviorally-similar-to-humans",
    "title": "Individual decision lit",
    "section": "A Turing test of whether AI chatbots are behaviorally similar to humans",
    "text": "A Turing test of whether AI chatbots are behaviorally similar to humans\nMei, Q., Xie, Y., Yuan, W., & Jackson, M. O. (2024). A Turing test of whether AI chatbots are behaviorally similar to humans. Proceedings of the National Academy of Sciences, 121(9), e2313925121. https://doi.org/10.1073/pnas.2313925121\n\n\nAbstract\n\n\nWe administer a Turing test to AI chatbots. We examine how chatbots behave in a suite of classic behavioral games that are designed to elicit characteristics such as trust, fairness, risk-aversion, cooperation, etc., as well as how they respond to a traditional Big-5 psychological survey that measures personality traits. ChatGPT-4 exhibits behavioral and personality traits that are statistically indistinguishable from a random human from tens of thousands of human subjects from more than 50 countries. Chatbots also modify their behavior based on previous experience and contexts “as if” they were learning from the interactions and change their behavior in response to different framings of the same strategic situation. Their behaviors are often distinct from average and modal human behaviors, in which case they tend to behave on the more altruistic and cooperative end of the distribution. We estimate that they act as if they are maximizing an average of their own and partner’s payoffs.\n\n\n\n\n\nFigure from Mei et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#deciding-fast-and-slow-the-role-of-cognitive-biases-in-ai-assisted-decision-making",
    "href": "ai_decision.html#deciding-fast-and-slow-the-role-of-cognitive-biases-in-ai-assisted-decision-making",
    "title": "Individual decision lit",
    "section": "Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making",
    "text": "Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making\nRastogi, C., Zhang, Y., Wei, D., Varshney, K. R., Dhurandhar, A., & Tomsett, R. (2022). Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 6(CSCW1), 1–22. https://doi.org/10.1145/3512930\n\n\nAbstract\n\n\nSeveral strands of research have aimed to bridge the gap between artificial intelligence (AI) and human decision-makers in AI-assisted decision-making, where humans are the consumers of AI model predictions and the ultimate decision-makers in high-stakes applications. However, people’s perception and understanding are often distorted by their cognitive biases, such as confirmation bias, anchoring bias, availability bias, to name a few. In this work, we use knowledge from the field of cognitive science to account for cognitive biases in the human-AI collaborative decision-making setting, and mitigate their negative effects on collaborative performance. To this end, we mathematically model cognitive biases and provide a general framework through which researchers and practitioners can understand the interplay between cognitive biases and human-AI accuracy. We then focus specifically on anchoring bias, a bias commonly encountered in human-AI collaboration. We implement a time-based de-anchoring strategy and conduct our first user experiment that validates its effectiveness in human-AI collaborative decision-making. With this result, we design a time allocation strategy for a resource-constrained setting that achieves optimal human-AI collaboration under some assumptions. We, then, conduct a second user experiment which shows that our time allocation strategy with explanation can effectively de-anchor the human and improve collaborative performance when the AI model has low confidence and is incorrect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Figures from Rastogi et al. (2022)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#decision-control-and-explanations-in-human-ai-collaboration-improving-user-perceptions-and-compliance",
    "href": "ai_decision.html#decision-control-and-explanations-in-human-ai-collaboration-improving-user-perceptions-and-compliance",
    "title": "Individual decision lit",
    "section": "Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance",
    "text": "Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance\nWestphal, M., Vössing, M., Satzger, G., Yom-Tov, G. B., & Rafaeli, A. (2023). Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance. Computers in Human Behavior, 144, 107714. https://doi.org/10.1016/j.chb.2023.107714\n\n\nAbstract\n\n\nHuman-AI collaboration has become common, integrating highly complex AI systems into the workplace. Still, it is often ineffective; impaired perceptions – such as low trust or limited understanding – reduce compliance with recommendations provided by the AI system. Drawing from cognitive load theory, we examine two techniques of human-AI collaboration as potential remedies. In three experimental studies, we grant users decision control by empowering them to adjust the system’s recommendations, and we offer explanations for the system’s reasoning. We find decision control positively affects user perceptions of trust and understanding, and improves user compliance with system recommendations. Next, we isolate different effects of providing explanations that may help explain inconsistent findings in recent literature: while explanations help reenact the system’s reasoning, they also increase task complexity. Further, the effectiveness of providing an explanation depends on the specific user’s cognitive ability to handle complex tasks. In summary, our study shows that users benefit from enhanced decision control, while explanations – unless appropriately designed for the specific user – may even harm user perceptions and compliance. This work bears both theoretical and practical implications for the management of human-AI collaboration.\n\n\n\n\n\nFigure from Westphal et al. (2023)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#risk-and-prosocial-behavioural-cues-elicit-human-like-response-patterns-from-ai-chatbots.",
    "href": "ai_decision.html#risk-and-prosocial-behavioural-cues-elicit-human-like-response-patterns-from-ai-chatbots.",
    "title": "Individual decision lit",
    "section": "Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots.",
    "text": "Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots.\nZhao, Y., Huang, Z., Seligman, M., & Peng, K. (2024). Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots. Scientific Reports, 14(1), 7095. https://doi.org/10.1038/s41598-024-55949-y\n\n\nAbstract\n\n\nEmotions, long deemed a distinctly human characteristic, guide a repertoire of behaviors, e.g., promoting risk-aversion under negative emotional states or generosity under positive ones. The question of whether Artificial Intelligence (AI) can possess emotions remains elusive, chiefly due to the absence of an operationalized consensus on what constitutes ‘emotion’ within AI. Adopting a pragmatic approach, this study investigated the response patterns of AI chatbots—specifically, large language models (LLMs)—to various emotional primes. We engaged AI chatbots as one would human participants, presenting scenarios designed to elicit positive, negative, or neutral emotional states. Multiple accounts of OpenAI’s ChatGPT Plus were then tasked with responding to inquiries concerning investment decisions and prosocial behaviors. Our analysis revealed that ChatGPT-4 bots, when primed with positive, negative, or neutral emotions, exhibited distinct response patterns in both risk-taking and prosocial decisions, a phenomenon less evident in the ChatGPT-3.5 iterations. This observation suggests an enhanced capacity for modulating responses based on emotional cues in more advanced LLMs. While these findings do not suggest the presence of emotions in AI, they underline the feasibility of swaying AI responses by leveraging emotional indicators.\n\n\n\n\n\nZhao et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#do-large-language-models-show-decision-heuristics-similar-to-humans-a-case-study-using-gpt-3.5",
    "href": "ai_decision.html#do-large-language-models-show-decision-heuristics-similar-to-humans-a-case-study-using-gpt-3.5",
    "title": "Individual decision lit",
    "section": "Do large language models show decision heuristics similar to humans? A case study using GPT-3.5",
    "text": "Do large language models show decision heuristics similar to humans? A case study using GPT-3.5\nSuri, G., Slater, L. R., Ziaee, A., & Nguyen, M. (2024). Do large language models show decision heuristics similar to humans? A case study using GPT-3.5. Journal of Experimental Psychology: General, 153(4), 1066–1075. https://doi.org/10.1037/xge0001547\n\n\nAbstract\n\n\nA Large Language Model (LLM) is an artificial intelligence system trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. Generative Pre-Trained Transformer (GPT)-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics and other context-sensitive responses. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (anchoring, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was influenced by anecdotal information (representativeness and availability heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively—even though both presentations contained statistically equivalent information (framing effect, Study 3); and it valued an owned item more than a newly found item even though the two items were objectively identical (endowment effect, Study 4). In each study, human participants showed similar effects. Heuristics and context-sensitive responses in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM—which lacks these processes—also shows such responses invites consideration of the possibility that language is sufficiently rich to carry these effects and may play a role in generating these effects in humans.\n\n\n\n\n\nFigure from Suri et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#can-large-language-models-capture-human-preferences",
    "href": "ai_decision.html#can-large-language-models-capture-human-preferences",
    "title": "Individual decision lit",
    "section": "Can Large Language Models Capture Human Preferences?",
    "text": "Can Large Language Models Capture Human Preferences?\nGoli, A., & Singh, A. (2024). Can Large Language Models Capture Human Preferences? Marketing Science. https://doi.org/10.1287/mksc.2023.0306\n\n\nAbstract\n\n\nWe explore the viability of large language models (LLMs), specifically OpenAI’s GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting preferences, with a focus on intertemporal choices. Leveraging the extensive literature on intertemporal discounting for benchmarking, we examine responses from LLMs across various languages and compare them with human responses, exploring preferences between smaller, sooner and larger, later rewards. Our findings reveal that both generative pretrained transformer (GPT) models demonstrate less patience than humans, with GPT-3.5 exhibiting a lexicographic preference for earlier rewards unlike human decision makers. Although GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans. Interestingly, GPT models show greater patience in languages with weak future tense references, such as German and Mandarin, aligning with the existing literature that suggests a correlation between language structure and intertemporal preferences. We demonstrate how prompting GPT to explain its decisions, a procedure we term “chain-of-thought conjoint,” can mitigate, but does not eliminate, discrepancies between LLM and human responses. Although directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings of preferences. Chain-of-thought conjoint provides a structured framework for marketers to use LLMs to identify potential attributes or factors that can explain preference heterogeneity across different customers and contexts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Figures from Goli & Singh (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#language-models-like-humans-show-content-effects-on-reasoning-tasks",
    "href": "ai_decision.html#language-models-like-humans-show-content-effects-on-reasoning-tasks",
    "title": "Individual decision lit",
    "section": "Language models, like humans, show content effects on reasoning tasks",
    "text": "Language models, like humans, show content effects on reasoning tasks\nLampinen, A. K., Dasgupta, I., Chan, S. C. Y., Sheahan, H. R., Creswell, A., Kumaran, D., McClelland, J. L., & Hill, F. (2024). Language models, like humans, show content effects on reasoning tasks. PNAS Nexus, 3(7), pgae233. https://doi.org/10.1093/pnasnexus/pgae233\n\n\nAbstract\n\n\nAbstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks but exhibit many imperfections. However, human abstract reasoning is also imperfect. Human reasoning is affected by our real-world knowledge and beliefs, and shows notable “content effects”; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns are central to debates about the fundamental nature of human intelligence. Here, we investigate whether language models—whose prior expectations capture some aspects of human knowledge—similarly mix content into their answers to logic problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art LMs, as well as humans, and find that the LMs reflect many of the same qualitative human patterns on these tasks—like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected in accuracy patterns, and in some lower-level features like the relationship between LM confidence over possible answers and human response times. However, in some cases the humans and models behave differently—particularly on the Wason task, where humans perform much worse than large models, and exhibit a distinct error pattern. Our findings have implications for understanding possible contributors to these human cognitive effects, as well as the factors that influence language model performance.\n\n\nLampinen et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#the-emergence-of-economic-rationality-of-gpt",
    "href": "ai_decision.html#the-emergence-of-economic-rationality-of-gpt",
    "title": "Individual decision lit",
    "section": "The emergence of economic rationality of GPT",
    "text": "The emergence of economic rationality of GPT\nChen, Y., Liu, T. X., Shan, Y., & Zhong, S. (2023). The emergence of economic rationality of GPT. Proceedings of the National Academy of Sciences, 120(51), e2316205120. https://doi.org/10.1073/pnas.2316205120\n\n\nAbstract\n\n\nAs large language models (LLMs) like GPT become increasingly prevalent, it is essential that we assess their capabilities beyond language processing. This paper examines the economic rationality of GPT by instructing it to make budgetary decisions in four domains: risk, time, social, and food preferences. We measure economic rationality by assessing the consistency of GPT’s decisions with utility maximization in classic revealed preference theory. We find that GPT’s decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature. Moreover, the estimated preference parameters of GPT are slightly different from human subjects and exhibit a lower degree of heterogeneity. We also find that the rationality scores are robust to the degree of randomness and demographic settings such as age and gender but are sensitive to contexts based on the language frames of the choice situations. These results suggest the potential of LLMs to make good decisions and the need to further understand their capabilities, limitations, and underlying mechanisms.\n\n\n\n\n\nFigure from Chen et al. (2023)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#the-potential-of-generative-ai-for-personalized-persuasion-at-scale.",
    "href": "ai_decision.html#the-potential-of-generative-ai-for-personalized-persuasion-at-scale.",
    "title": "Individual decision lit",
    "section": "The potential of generative AI for personalized persuasion at scale.",
    "text": "The potential of generative AI for personalized persuasion at scale.\nMatz, S. C., Teeny, J. D., Vaid, S. S., Peters, H., Harari, G. M., & Cerf, M. (2024). The potential of generative AI for personalized persuasion at scale. Scientific Reports, 14(1), 4692. https://doi.org/10.1038/s41598-024-53755-0\n\n\nAbstract\n\n\nMatching the language or content of a message to the psychological profile of its recipient (known as “personalized persuasion”) is widely considered to be one of the most effective messaging strategies. We demonstrate that the rapid advances in large language models (LLMs), like ChatGPT, could accelerate this influence by making personalized persuasion scalable. Across four studies (consisting of seven sub-studies; total N = 1788), we show that personalized messages crafted by ChatGPT exhibit significantly more influence than non-personalized messages. This was true across different domains of persuasion (e.g., marketing of consumer products, political appeals for climate action), psychological profiles (e.g., personality traits, political ideology, moral foundations), and when only providing the LLM with a single, short prompt naming or describing the targeted psychological dimension. Thus, our findings are among the first to demonstrate the potential for LLMs to automate, and thereby scale, the use of personalized persuasion in ways that enhance its effectiveness and efficiency. We discuss the implications for researchers, practitioners, and the general public.\n\n\n\n\n\nFigure from Matz et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#decision-making-paradoxes-in-humans-vs-machines-the-case-of-the-allais-and-ellsberg-paradoxes.",
    "href": "ai_decision.html#decision-making-paradoxes-in-humans-vs-machines-the-case-of-the-allais-and-ellsberg-paradoxes.",
    "title": "Individual decision lit",
    "section": "Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes.",
    "text": "Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes.\nNobandegani, A. S., Rish, I., & Shultz, T. R. (2023). Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes. Proceedings of the Annual Meeting of the Cognitive Science Society, 46. https://arxiv.org/abs/2406.11426\n\n\nAbstract\n\n\nHuman decision-making is filled with a variety of paradoxes demonstrating deviations from rationality principles. Do state-of-the-art artificial intelligence (AI) models also manifest these paradoxes when making decisions? As a case study, in this work we investigate whether GPT-4, a recently released state-of-the-art language model, would show two well-known paradoxes in human decision-making: the Allais paradox and the Ellsberg paradox. We demonstrate that GPT-4 succeeds in the two variants of the Allais paradox (the common-consequence effect and the common-ratio effect) but fails in the case of the Ellsberg paradox. We also show that providing GPT-4 with high-level normative principles allows it to succeed in the Ellsberg paradox, thus elevating GPT-4’s decision-making rationality. We discuss the implications of our work for AI rationality enhancement and AI-assisted decision-making.\n\n\nNobandegani et al. (2023)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#do-llms-exhibit-human-like-response-biases-a-case-study-in-survey-design.",
    "href": "ai_decision.html#do-llms-exhibit-human-like-response-biases-a-case-study-in-survey-design.",
    "title": "Individual decision lit",
    "section": "Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design.",
    "text": "Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design.\nTjuatja, L., Chen, V., Wu, T., Talwalkwar, A., & Neubig, G. (2024). Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design. Transactions of the Association for Computational Linguistics, 12, 1011–1026. https://doi.org/10.1162/tacl_a_00685\n\n\nAbstract\n\n\nOne widely cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording—but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of “prompts” have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior.\n\n\n\n\n\nFigure from Tjuatja et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#cognitive-ease-at-a-cost-llms-reduce-mental-effort-but-compromise-depth-in-student-scientific-inquiry",
    "href": "ai_decision.html#cognitive-ease-at-a-cost-llms-reduce-mental-effort-but-compromise-depth-in-student-scientific-inquiry",
    "title": "Individual decision lit",
    "section": "Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry",
    "text": "Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry\nStadler, M., Bannert, M., & Sailer, M. (2024). Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry. Computers in Human Behavior, 160, 108386. https://doi.org/10.1016/j.chb.2024.108386\n\n\nAbstract\n\n\nThis study explores the cognitive load and learning outcomes associated with using large language models (LLMs) versus traditional search engines for information gathering during learning. A total of 91 university students were randomly assigned to either use ChatGPT3.5 or Google to research the socio-scientific issue of nanoparticles in sunscreen to derive valid recommendations and justifications. The study aimed to investigate potential differences in cognitive load, as well as the quality and homogeneity of the students’ recommendations and justifications. Results indicated that students using LLMs experienced significantly lower cognitive load. However, despite this reduction, these students demonstrated lower-quality reasoning and argumentation in their final recommendations compared to those who used traditional search engines. Further, the homogeneity of the recommendations and justifications did not differ significantly between the two groups, suggesting that LLMs did not restrict the diversity of students’ perspectives. These findings highlight the nuanced implications of digital tools on learning, suggesting that while LLMs can decrease the cognitive burden associated with information gathering during a learning task, they may not promote deeper engagement with content necessary for high-quality learning per se.\n\n\nStadler et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "Driving.html",
    "href": "Driving.html",
    "title": "Driving Lit",
    "section": "",
    "text": "Basu, C., & Singhal, M. (n.d.). Trust Dynamics in Human Autonomous Vehicle Interaction: A Review of Trust Models.\nBrunyé, T. T., Gardony, A., Mahoney, C. R., & Taylor, H. A. (2012). Going to town: Visualized perspectives and navigation through virtual environments. Computers in Human Behavior, 28(1), 257–266. https://doi.org/10.1016/j.chb.2011.09.008\nBrunyé, T. T., Rapp, D. N., & Taylor, H. A. (2008). Representational flexibility and specificity following spatial descriptions of real-world environments. Cognition, 108(2), 418–443. https://doi.org/10.1016/j.cognition.2008.03.005\nCui, C., Ma, Y., Cao, X., Ye, W., Zhou, Y., Liang, K., Chen, J., Lu, J., Yang, Z., Liao, K.-D., Gao, T., Li, E., Tang, K., Cao, Z., Zhou, T., Liu, A., Yan, X., Mei, S., Cao, J., … Zheng, C. (2024). A Survey on Multimodal Large Language Models for Autonomous Driving. 2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), 958–979. https://doi.org/10.1109/WACVW60836.2024.00106\nDíaz-Álvarez, A., Clavijo, M., Jiménez, F., Talavera, E., & Serradilla, F. (2018). Modelling the human lane-change execution behaviour through Multilayer Perceptrons and Convolutional Neural Networks. Transportation Research Part F: Traffic Psychology and Behaviour, 56, 134–148. https://doi.org/10.1016/j.trf.2018.04.004\nEisma, Y. B., Eijssen, D. J., & de Winter, J. C. F. (2022). What Attracts the Driver’s Eye? Attention as a Function of Task and Events. Information, 13(7), Article 7. https://doi.org/10.3390/info13070333\nFajen, B. R. (2005). The Scaling of Information to Action in Visually Guided Braking. Journal of Experimental Psychology: Human Perception and Performance, 31(5), 1107–1123. https://doi.org/10.1037/0096-1523.31.5.1107\nHe, J., McCarley, J. S., & Kramer, A. F. (2014). Lane Keeping Under Cognitive Load: Performance Changes and Mechanisms. Human Factors, 56(2), 414–426. https://doi.org/10.1177/0018720813485978\nHills, T. T., & Kenett, Y. N. (n.d.). Is the Mind a Network? Maps, Vehicles, and Skyhooks in Cognitive Network Science. Topics in Cognitive Science, n/a(n/a). https://doi.org/10.1111/tops.12570\nHowell, W. C., & Kerkar, S. P. (1982). A test of task influences in uncertainty measurement. Organizational Behavior and Human Performance, 30(3), 365–390. https://doi.org/10.1016/0030-5073(82)90226-4\nHuang, S. H., Held, D., Abbeel, P., & Dragan, A. D. (2019). Enabling robots to communicate their objectives. Autonomous Robots, 43(2), 309–326. https://doi.org/10.1007/s10514-018-9771-0\nHuo, D., Ma, J., & Chang, R. (2020). Lane-changing-decision characteristics and the allocation of visual attention of drivers with an angry driving style. Transportation Research Part F: Traffic Psychology and Behaviour, 71, 62–75. https://doi.org/10.1016/j.trf.2020.03.008\nJiang, L., Chen, D., Li, Z., & Wang, Y. (2022). Risk Representation, Perception, and Propensity in an Integrated Human Lane-Change Decision Model. IEEE Transactions on Intelligent Transportation Systems, 23(12), 23474–23487. IEEE Transactions on Intelligent Transportation Systems. https://doi.org/10.1109/TITS.2022.3207182\nKamaruddin, N., Abdul Rahman, A. W., Mohamad Halim, K. I., & Mohd Noh, M. H. I. (2018). Driver Behaviour State Recognition based on Speech. TELKOMNIKA (Telecommunication Computing Electronics and Control), 16(2), 852. https://doi.org/10.12928/telkomnika.v16i2.8416\nKolekar, S., de Winter, J., & Abbink, D. (2017). A human-like steering model: Sensitive to uncertainty in the environment. 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1487–1492. https://doi.org/10.1109/SMC.2017.8122824\nKolekar, S., de Winter, J., & Abbink, D. (2020). Human-like driving behaviour emerges from a risk-based driver model. Nature Communications, 11(1), Article 1. https://data.4tu.nl/articles/dataset/Driver_s_Risk_Fields_DRF_-_Model_data/12705950/1. https://doi.org/10.1038/s41467-020-18353-4\nKujanpää, K., Baimukashev, D., Zhu, S., Azam, S., Munir, F., Alcan, G., & Kyrki, V. (2024). Challenges of Data-Driven Simulation of Diverse and Consistent Human Driving Behaviors (arXiv:2401.03236). arXiv. http://arxiv.org/abs/2401.03236\nLappi, O. (2022). Gaze Strategies in Driving–An Ecological Approach. Frontiers in Psychology, 13. https://doi.org/10.3389/fpsyg.2022.821440\nLiang, Y., Reyes, M. L., & Lee, J. D. (2007). Real-Time Detection of Driver Cognitive Distraction Using Support Vector Machines. IEEE Transactions on Intelligent Transportation Systems, 8(2), 340–350. IEEE Transactions on Intelligent Transportation Systems. https://doi.org/10.1109/TITS.2007.895298\nLiu, R., Zhao, X., Yuan, T., Li, H., Bu, T., Zhu, X., & Ma, J. (2024). A human-like response model for following vehicles in lane-changing scenario. Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering, 238(4), 760–773. https://doi.org/10.1177/09544070221135384\nLu, C., Lu, H., Chen, D., Wang, H., Li, P., & Gong, J. (2023). Human-like decision making for lane change based on the cognitive map and hierarchical reinforcement learning. Transportation Research Part C: Emerging Technologies, 156, 104328. https://doi.org/10.1016/j.trc.2023.104328\nMamo, W. G., Alhajyaseen, W. K. M., Brijs, K., Dirix, H., Vanroelen, G., Hussain, Q., Brijs, T., & Ross, V. (2024). The impact of cognitive load on a lane change task (LCT) among male autistic individuals: A driving simulator study. Transportation Research Part F: Traffic Psychology and Behaviour, 106, 27–43. https://doi.org/10.1016/j.trf.2024.07.030\nMarkkula, G., Boer, E., Romano, R., & Merat, N. (2018a). Sustained sensorimotor control as intermittent decisions about prediction errors: Computational framework and application to ground vehicle steering. Biological Cybernetics, 112(3), 181–207. https://doi.org/10.1007/s00422-017-0743-9\nMarkkula, G., Boer, E., Romano, R., & Merat, N. (2018b). Sustained sensorimotor control as intermittent decisions about prediction errors: Computational framework and application to ground vehicle steering. Biological Cybernetics, 112(3), 181–207. https://doi.org/10.1007/s00422-017-0743-9\nMeir, A., & Oron-Gilad, T. (2020). Understanding complex traffic road scenes: The case of child-pedestrians’ hazard perception. Journal of Safety Research, 72, 111–126. https://doi.org/10.1016/j.jsr.2019.12.014\nMerat, N., Jamson, A. H., Lai, F. C. H., & Carsten, O. (2012). Highly Automated Driving, Secondary Task Performance, and Driver State. Human Factors, 54(5), 762–771. https://doi.org/10.1177/0018720812442087\nMuslim, H., Itoh, M., Liang, C. K., Antona-Makoshi, J., & Uchida, N. (2021). Effects of gender, age, experience, and practice on driver reaction and acceptance of traffic jam chauffeur systems. Scientific Reports, 11(1), 17874. https://doi.org/10.1038/s41598-021-97374-5\nPekkanen, J., Lappi, O., Rinkkala, P., Tuhkanen, S., Frantsi, R., & Summala, H. (2018). A computational model for driver’s cognitive state, visual perception and intermittent attention in a distracted car following task. Royal Society Open Science, 5(9), 180194. https://doi.org/10.1098/rsos.180194\nPing, P., Sheng, Y., Qin, W., Miyajima, C., & Takeda, K. (2018). Modeling Driver Risk Perception on City Roads Using Deep Learning. IEEE Access, 6, 68850–68866. IEEE Access. https://doi.org/10.1109/ACCESS.2018.2879887\nRecarte, M. A., & Nunes, L. M. (2003). Mental workload while driving: Effects on visual search, discrimination, and decision making. Journal of Experimental Psychology: Applied, 9(2), 119–137. https://doi.org/10.1037/1076-898X.9.2.119\nReimer, B., Donmez, B., Lavallière, M., Mehler, B., Coughlin, J. F., & Teasdale, N. (2013). Impact of age and cognitive demand on lane choice and changing under actual highway conditions. Accident Analysis & Prevention, 52, 125–132. https://doi.org/10.1016/j.aap.2012.12.008\nRondora, M. E. S., Pirdavani, A., & Larocca, A. P. C. (2022). Driver Behavioral Classification on Curves Based on the Relationship between Speed, Trajectories, and Eye Movements: A Driving Simulator Study. Sustainability, 14(10), Article 10. https://doi.org/10.3390/su14106241\nSalvucci, D. D. (2006). Modeling Driver Behavior in a Cognitive Architecture. Human Factors, 48(2), 362–380. https://doi.org/10.1518/001872006777724417\nSalvucci, D. D., Mandalia, H. M., Kuge, N., & Yamamura, T. (2007). Lane-Change Detection Using a Computational Driver Model. Human Factors, 49(3), 532–542. https://doi.org/10.1518/001872007X200157\nSantoso, G. P., & Maulina, D. (2019). Human errors in traffic accidents: Differences between car drivers and motorcyclists’ experience. Psychological Research on Urban Society, 2(2), 118. https://doi.org/10.7454/proust.v2i2.69\nShimada, H., Uemura, K., Makizako, H., Doi, T., Lee, S., & Suzuki, T. (2016). Performance on the flanker task predicts driving cessation in older adults. International Journal of Geriatric Psychiatry, 31(2), 169–175. https://doi.org/10.1002/gps.4308\nSrinivasan, A. R., Hasan, M., Lin, Y.-S., Leonetti, M., Billington, J., Romano, R., & Markkula, G. (2021). Comparing merging behaviors observed in naturalistic data with behaviors generated by a machine learned model. 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), 3787–3792. https://doi.org/10.1109/ITSC48978.2021.9564791\nTan, X., & Zhang, Y. (2024). A Computational Cognitive Model of Driver Response Time for Scheduled Freeway Exiting Takeovers in Conditionally Automated Vehicles. Human Factors, 66(5), 1583–1599. https://doi.org/10.1177/00187208221143028\nTango, F., & Botta, M. (2013). Real-Time Detection System of Driver Distraction Using Machine Learning. IEEE Transactions on Intelligent Transportation Systems, 14(2), 894–905. IEEE Transactions on Intelligent Transportation Systems. https://doi.org/10.1109/TITS.2013.2247760\nTawari, A., & Kang, B. (2017). A computational framework for driver’s visual attention using a fully convolutional architecture. 2017 IEEE Intelligent Vehicles Symposium (IV), 887–894. https://doi.org/10.1109/IVS.2017.7995828\nTsirtsis, S., Rodriguez, M. G., & Gerstenberg, T. (2024). Towards a computational model of responsibility judgments in sequential human-AI collaboration. https://doi.org/10.31234/osf.io/m4yad\nWang, B., Duan, H., Feng, Y., Chen, X., Fu, Y., Mo, Z., & Di, X. (2024). Can LLMs Understand Social Norms in Autonomous Driving Games? (arXiv:2408.12680). arXiv. http://arxiv.org/abs/2408.12680\nWang, W., Zhao, D., Han, W., & Xi, J. (2018). A Learning-Based Approach for Lane Departure Warning Systems With a Personalized Driver Model. IEEE Transactions on Vehicular Technology, 67(10), 9145–9157. IEEE Transactions on Vehicular Technology. https://doi.org/10.1109/TVT.2018.2854406\nWood, K., & Simons, D. J. (2019). The spatial allocation of attention in an interactive environment. Cognitive Research: Principles and Implications, 4(1), 13. https://doi.org/10.1186/s41235-019-0164-5\nXing, Y., Lv, C., Cao, D., & Hang, P. (2021). Toward human-vehicle collaboration: Review and perspectives on human-centered collaborative automated driving. Transportation Research Part C: Emerging Technologies, 128, 103199. https://doi.org/10.1016/j.trc.2021.103199\nXing, Y., Lv, C., Wang, H., Wang, H., Ai, Y., Cao, D., Velenis, E., & Wang, F.-Y. (2019). Driver Lane Change Intention Inference for Intelligent Vehicles: Framework, Survey, and Challenges. IEEE Transactions on Vehicular Technology, 68(5), 4377–4390. IEEE Transactions on Vehicular Technology. https://doi.org/10.1109/TVT.2019.2903299\nYue, J., Manocha, D., & Wang, H. (2022). Human Trajectory Prediction via Neural Social Physics. In S. Avidan, G. Brostow, M. Cissé, G. M. Farinella, & T. Hassner (Eds.), Computer Vision – ECCV 2022 (Vol. 13694, pp. 376–394). Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-19830-4_22\nZgonnikov, A., Abbink, D., & Markkula, G. (2024). Should I Stay or Should I Go? Cognitive Modeling of Left-Turn Gap Acceptance Decisions in Human Drivers. Human Factors, 66(5), 1399–1413. https://doi.org/10.1177/00187208221144561\nZhao, D., Lam, H., Peng, H., Bao, S., LeBlanc, D. J., Nobukawa, K., & Pan, C. S. (2017). Accelerated Evaluation of Automated Vehicles Safety in Lane-Change Scenarios Based on Importance Sampling Techniques. IEEE Transactions on Intelligent Transportation Systems, 18(3), 595–607. IEEE Transactions on Intelligent Transportation Systems. https://doi.org/10.1109/TITS.2016.2582208",
    "crumbs": [
      "Misc",
      "Driving Lit"
    ]
  },
  {
    "objectID": "ai_gd.html",
    "href": "ai_gd.html",
    "title": "Literature Notes",
    "section": "",
    "text": "Marjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit. https://cocosci.princeton.edu/papers/marjieh2024task.pdf\n\n\nAbstract\n\n\nHumans rely on efficient distribution of resources to transcend the abilities of individuals. Successful task allocation, whether in small teams or across large institutions, depends on individuals’ ability to discern their own and others’ strengths and weaknesses, and to optimally act on them. This dependence creates a tension between exploring the capabilities of others and exploiting the knowledge acquired so far, which can be challenging. How do people navigate this tension? To address this question, we propose a novel task allocation paradigm in which a human agent is asked to repeatedly allocate tasks in three distinct classes (categorizing a blurry image, detecting a noisy voice command, and solving an anagram) between themselves and two other (bot) team members to maximize team performance. We show that this problem can be recast as a combinatorial multi-armed bandit which allows us to compare people’s performance against two well-known strategies, Thompson Sampling and Upper Confidence Bound (UCB). We find that humans are able to successfully integrate information about the capabilities of different team members to infer optimal allocations, and in some cases perform on par with these optimal strategies. Our approach opens up new avenues for studying the mechanisms underlying collective cooperation in teams.\n\n\n\n\n\nFigure from Marjieh et al. (2024)\n\n\n\n\n\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nAbstract\n\n\nIn this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team’s ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team’s transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond.\n\n\n\n\n\nFigure from Bienefeld et al. (2023)\n\n\n\n\n\n\n\n\nGao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., & Li, Y. (2024). Large language models empowered agent-based modeling and simulation: A survey and perspectives. Humanities and Social Sciences Communications, 11(1), 1–24. https://doi.org/10.1057/s41599-024-03611-3\n\n\nAbstract\n\n\nAgent-based modeling and simulation have evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Recently, integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, discussing their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments, and how these works address the above challenges. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/LLM-Agent-Based-Modeling-and-Simulation.\n\n\n\n\n\nFigure from Gao et al. (2024)\n\n\n\n\n\nCollins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building Machines that Learn and Think with People (arXiv:2408.03943). arXiv. http://arxiv.org/abs/2408.03943\n\n\nAbstract\n\n\nWhat do we want from machine intelligence? We envision machines that are not just tools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable, and trustworthy systems that think with us. Current artificial intelligence (AI) systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called “thought partners,” systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and AI thought partners can engage and propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.\n\n\n\n\n\n\nFigure from Collins et al. (2024)\n\n\n\n\n\n\n\nDu, Y., Rajivan, P., & Gonzalez, C. C. (2024). Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making. https://escholarship.org/uc/item/6s060914\n\n\nAbstract\n\n\nLarge Language models (LLM) exhibit human-like proficiency in various tasks such as translation, question answering, essay writing, and programming. Emerging research explores the use of LLMs in collective problem-solving endeavors, such as tasks where groups try to uncover clues through discussions. Although prior work has investigated individual problem-solving tasks, leveraging LLM-powered agents for group consensus and decision-making remains largely unexplored. This research addresses this gap by (1) proposing an algorithm to enable free-form conversation in groups of LLM agents, (2) creating metrics to evaluate the human-likeness of the generated dialogue and problem-solving performance, and (3) evaluating LLM agent groups against human groups using an open source dataset. Our results reveal that LLM groups outperform human groups in problem-solving tasks. LLM groups also show a greater improvement in scores after participating in free discussions. In particular, analyses indicate that LLM agent groups exhibit more disagreements, complex statements, and a propensity for positive statements compared to human groups. The results shed light on the potential of LLMs to facilitate collective reasoning and provide insight into the dynamics of group interactions involving synthetic LLM agents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Figure from Du et al. (2024)\n\n\n\n\n\n\nHao, X., Demir, E., & Eyers, D. (2024). Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction. Technology in Society, 78, 102662. https://doi.org/10.1016/j.techsoc.2024.102662\n\n\nAbstract\n\n\nThis paper explores the effects of integrating Generative Artificial Intelligence (GAI) into decision-making processes within organizations, employing a quasi-experimental pretest-posttest design. The study examines the synergistic interaction between Human Intelligence (HI) and GAI across four group decision-making scenarios within three global organizations renowned for their cutting-edge operational techniques. The research progresses through several phases: identifying research problems, collecting baseline data on decision-making, implementing AI interventions, and evaluating the outcomes post-intervention to identify shifts in performance. The results demonstrate that GAI effectively reduces human cognitive burdens and mitigates heuristic biases by offering data-driven support and predictive analytics, grounded in System 2 reasoning. This is particularly valuable in complex situations characterized by unfamiliarity and information overload, where intuitive, System 1 thinking is less effective. However, the study also uncovers challenges related to GAI integration, such as potential over-reliance on technology, intrinsic biases particularly ‘out-of-the-box’ thinking without contextual creativity. To address these issues, this paper proposes an innovative strategic framework for HI-GAI collaboration that emphasizes transparency, accountability, and inclusiveness.\n\n\n\n\n\nFigure from Hao et al. (2024)\n\n\n\n\n\n\n\n\nBurton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). How large language models can reshape collective intelligence. Nature Human Behaviour, 1–13. https://doi.org/10.1038/s41562-024-01959-9\n\n\nAbstract\n\n\nCollective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals—even experts—resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the ‘wisdom of crowds’, online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans’ ability to collectively tackle complex problems.\n\n\n\n\n\nBurton et al. (2024)\n\n\n\n\n\n\n\nChiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nAbstract\n\n\nGroup decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil’s advocate in the AI-assisted group deci- sion making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities ex- hibited by modern large language models (LLMs), we design four different styles of devil’s advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil’s advocates that argue against the AI model’s decision recommenda- tion have the potential to promote groups’ appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil’s advocate usually does not lead to substantial increases in people’s perceived workload for completing the group decision making tasks, while interactive LLM-powered devil’s advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.\n\n\n\n\n\nFigure from Chiang et al. (2024)\n\n\n\n\n\nChuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2024). The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents. https://escholarship.org/uc/item/3k67x8s5\n\n\nAbstract\n\n\nHuman groups are able to converge to more accurate beliefs through deliberation, even in the presence of polarization and partisan bias — a phenomenon known as the “wisdom of partisan crowds.” Large Language Models (LLMs) are increasingly being used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation, as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompting and lack of details in personas. Conversely, fine-tuning on human data appears to enhance convergence. These findings show the potential and limitations of LLM-based agents as a model of human collective intelligence.\n\n\n\n\n\nChuang et al. (2024)\n\n\n\n\n\nNisioti, E., Risi, S., Momennejad, I., Oudeyer, P.-Y., & Moulin-Frier, C. (2024, July 7). Collective Innovation in Groups of Large Language Models. ALIFE 2024: Proceedings of the 2024 Artificial Life Conference. https://doi.org/10.1162/isal_a_00730\n\n\nAbstract\n\n\nHuman culture relies on collective innovation: our ability to continuously explore how existing elements in our environment can be combined to create new ones. Language is hypothesized to play a key role in human culture, driving individual cognitive capacities and shaping communication. Yet the majority of models of collective innovation assign no cognitive capacities or language abilities to agents. Here, we contribute a computational study of collective innovation where agents are Large Language Models (LLMs) that play Little Alchemy 2, a creative video game originally developed for humans that, as we argue, captures useful aspects of innovation landscapes not present in previous test-beds. We, first, study an LLM in isolation and discover that it exhibits both useful skills and crucial limitations. We, then, study groups of LLMs that share information related to their behaviour and focus on the effect of social connectivity on collective performance. In agreement with previous human and computational studies, we observe that groups with dynamic connectivity out-compete fully-connected groups. Our work reveals opportunities and challenges for future studies of collective innovation that are becoming increasingly relevant as Generative Artificial Intelligence algorithms and humans innovate alongside each other.\n\n\n\n\n\nNisioti et al. (2024)\n\n\n\n\n\nChuang, Y.-S., Suresh, S., Harlalka, N., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2023). Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds http://arxiv.org/abs/2311.09665\n\n\nAbstract\n\n\nThis study investigates the potential of Large Language Models (LLMs) to simulate human group dynamics, particularly within politically charged contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to role-play as Democrat and Republican personas, engaging in a structured interaction akin to human group study. Our approach evaluates how agents’ responses evolve through social influence. Our key findings indicate that LLM agents role-playing detailed personas and without Chain-of-Thought (CoT) reasoning closely align with human behaviors, while having CoT reasoning hurts the alignment. However, incorporating explicit biases into agent prompts does not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning LLMs with human data shows promise in achieving human-like behavior but poses a risk of overfitting certain behaviors. These findings show the potential and limitations of using LLM agents in modeling human group phenomena.\n\n\nChuang et al. (2023)\n\n\n\nZhang, J., Xu, X., Zhang, N., Liu, R., Hooi, B., & Deng, S. (2024). Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View (arXiv:2310.02124). arXiv. http://arxiv.org/abs/2310.02124\nhttps://www.zjukg.org/project/MachineSoM/\n\n\nAbstract\n\n\nAs Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique ‘societies’ comprised of LLM agents, where each agent is characterized by a specific ‘trait’ (easy-going or overconfident) and engages in collaboration with a distinct ‘thinking pattern’ (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest humanlike social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We have shared our code and datasets1, hoping to catalyze further research in this promising avenue.\n\n\n\n\n\nZhang et al. (2024)\n\n\n\n\n\nAbdelnabi, S., Gomaa, A., Sivaprasad, S., Schönherr, L., & Fritz, M. (2023). LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. https://doi.org/10.60882/cispa.25233028.v1\n\n\nAbstract\n\n\nThere is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs’ reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and earlier models. Importantly, we test the generalization to new games and setups. Finally, we show that these games can help evaluate other critical aspects, such as the interaction dynamics between agents in the presence of greedy and adversarial players.\n\n\n\n\n\nAbdelnabi et al. (2023)\n\n\n\n\n\nYang, J. C., Dailisan, D., Korecki, M., Hausladen, C. I., & Helbing, D. (2024). LLM Voting: Human Choices and AI Collective Decision Making (arXiv:2402.01766). arXiv. http://arxiv.org/abs/2402.01766\n\n\nAbstract\n\n\nThis paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.\n\n\n\n\n\nYang et al. (2024)\n\n\n\n\n\nGuo, X., Huang, K., Liu, J., Fan, W., Vélez, N., Wu, Q., Wang, H., Griffiths, T. L., & Wang, M. (2024). Embodied LLM Agents Learn to Cooperate in Organized Teams (arXiv:2403.12482). arXiv. http://arxiv.org/abs/2403.12482\n\n\nAbstract\n\n\nLarge Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.\n\n\n\n\n\nGuo et al. (2024)\n\n\n\n\n\nKoehl, D., & Vangsness, L. (2023). Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 67. https://doi.org/10.1177/21695067231192869\n\n\nAbstract\n\n\nQualitative self-report methods such as think-aloud procedures and open-ended response questions can provide valuable data to human factors research. These measures come with analytic weaknesses, such as researcher bias, intra- and inter-rater reliability concerns, and time-consuming coding protocols. A possible solution exists in the latent semantic patterns that exist in machine learning large language models. These semantic patterns could be used to analyze qualitative responses. This exploratory research compared the statistical quality of automated sentence coding using large language models to the benchmarks of self-report and behavioral measures within the context of trust in automation research. The results indicated that three large language models show promise as tools for analyzing qualitative responses. The study also provides insight on minimum sample sizes for model creation and offers recommendations for further validating the robustness of large language models as research tools.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Koehl & Vangsness (2023)\n\n\n\n\n\n\nVats, V., Nizam, M. B., Liu, M., Wang, Z., Ho, R., Prasad, M. S., Titterton, V., Malreddy, S. V., Aggarwal, R., Xu, Y., Ding, L., Mehta, J., Grinnell, N., Liu, L., Zhong, S., Gandamani, D. N., Tang, X., Ghosalkar, R., Shen, C., … Davis, J. (2024). A Survey on Human-AI Teaming with Large Pre-Trained Models (arXiv:2403.04931). arXiv. http://arxiv.org/abs/2403.04931\n\n\nAbstract\n\n\nIn the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI Teaming, providing insights for future research, policy development, and strategic implementations aimed at harnessing the full potential of this collaboration for research and societal benefit.\n\n\n\n\n\nTable from Vats et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#task-allocation-in-teams-as-a-multi-armed-bandit.",
    "href": "ai_gd.html#task-allocation-in-teams-as-a-multi-armed-bandit.",
    "title": "Literature Notes",
    "section": "",
    "text": "Marjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit. https://cocosci.princeton.edu/papers/marjieh2024task.pdf\n\n\nAbstract\n\n\nHumans rely on efficient distribution of resources to transcend the abilities of individuals. Successful task allocation, whether in small teams or across large institutions, depends on individuals’ ability to discern their own and others’ strengths and weaknesses, and to optimally act on them. This dependence creates a tension between exploring the capabilities of others and exploiting the knowledge acquired so far, which can be challenging. How do people navigate this tension? To address this question, we propose a novel task allocation paradigm in which a human agent is asked to repeatedly allocate tasks in three distinct classes (categorizing a blurry image, detecting a noisy voice command, and solving an anagram) between themselves and two other (bot) team members to maximize team performance. We show that this problem can be recast as a combinatorial multi-armed bandit which allows us to compare people’s performance against two well-known strategies, Thompson Sampling and Upper Confidence Bound (UCB). We find that humans are able to successfully integrate information about the capabilities of different team members to infer optimal allocations, and in some cases perform on par with these optimal strategies. Our approach opens up new avenues for studying the mechanisms underlying collective cooperation in teams.\n\n\n\n\n\nFigure from Marjieh et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.",
    "href": "ai_gd.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.",
    "title": "Literature Notes",
    "section": "",
    "text": "Bienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nAbstract\n\n\nIn this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team’s ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team’s transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond.\n\n\n\n\n\nFigure from Bienefeld et al. (2023)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#large-language-models-empowered-agent-based-modeling-and-simulation-a-survey-and-perspectives.",
    "href": "ai_gd.html#large-language-models-empowered-agent-based-modeling-and-simulation-a-survey-and-perspectives.",
    "title": "Literature Notes",
    "section": "",
    "text": "Gao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., & Li, Y. (2024). Large language models empowered agent-based modeling and simulation: A survey and perspectives. Humanities and Social Sciences Communications, 11(1), 1–24. https://doi.org/10.1057/s41599-024-03611-3\n\n\nAbstract\n\n\nAgent-based modeling and simulation have evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Recently, integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, discussing their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments, and how these works address the above challenges. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/LLM-Agent-Based-Modeling-and-Simulation.\n\n\n\n\n\nFigure from Gao et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#building-machines-that-learn-and-think-with-people",
    "href": "ai_gd.html#building-machines-that-learn-and-think-with-people",
    "title": "Literature Notes",
    "section": "",
    "text": "Collins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building Machines that Learn and Think with People (arXiv:2408.03943). arXiv. http://arxiv.org/abs/2408.03943\n\n\nAbstract\n\n\nWhat do we want from machine intelligence? We envision machines that are not just tools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable, and trustworthy systems that think with us. Current artificial intelligence (AI) systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called “thought partners,” systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and AI thought partners can engage and propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.\n\n\n\n\n\n\nFigure from Collins et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#large-language-models-for-collective-problem-solving-insights-into-group-consensus-decision-making",
    "href": "ai_gd.html#large-language-models-for-collective-problem-solving-insights-into-group-consensus-decision-making",
    "title": "Literature Notes",
    "section": "",
    "text": "Du, Y., Rajivan, P., & Gonzalez, C. C. (2024). Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making. https://escholarship.org/uc/item/6s060914\n\n\nAbstract\n\n\nLarge Language models (LLM) exhibit human-like proficiency in various tasks such as translation, question answering, essay writing, and programming. Emerging research explores the use of LLMs in collective problem-solving endeavors, such as tasks where groups try to uncover clues through discussions. Although prior work has investigated individual problem-solving tasks, leveraging LLM-powered agents for group consensus and decision-making remains largely unexplored. This research addresses this gap by (1) proposing an algorithm to enable free-form conversation in groups of LLM agents, (2) creating metrics to evaluate the human-likeness of the generated dialogue and problem-solving performance, and (3) evaluating LLM agent groups against human groups using an open source dataset. Our results reveal that LLM groups outperform human groups in problem-solving tasks. LLM groups also show a greater improvement in scores after participating in free discussions. In particular, analyses indicate that LLM agent groups exhibit more disagreements, complex statements, and a propensity for positive statements compared to human groups. The results shed light on the potential of LLMs to facilitate collective reasoning and provide insight into the dynamics of group interactions involving synthetic LLM agents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Figure from Du et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#exploring-collaborative-decision-making-a-quasi-experimental-study-of-human-and-generative-ai-interaction.",
    "href": "ai_gd.html#exploring-collaborative-decision-making-a-quasi-experimental-study-of-human-and-generative-ai-interaction.",
    "title": "Literature Notes",
    "section": "",
    "text": "Hao, X., Demir, E., & Eyers, D. (2024). Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction. Technology in Society, 78, 102662. https://doi.org/10.1016/j.techsoc.2024.102662\n\n\nAbstract\n\n\nThis paper explores the effects of integrating Generative Artificial Intelligence (GAI) into decision-making processes within organizations, employing a quasi-experimental pretest-posttest design. The study examines the synergistic interaction between Human Intelligence (HI) and GAI across four group decision-making scenarios within three global organizations renowned for their cutting-edge operational techniques. The research progresses through several phases: identifying research problems, collecting baseline data on decision-making, implementing AI interventions, and evaluating the outcomes post-intervention to identify shifts in performance. The results demonstrate that GAI effectively reduces human cognitive burdens and mitigates heuristic biases by offering data-driven support and predictive analytics, grounded in System 2 reasoning. This is particularly valuable in complex situations characterized by unfamiliarity and information overload, where intuitive, System 1 thinking is less effective. However, the study also uncovers challenges related to GAI integration, such as potential over-reliance on technology, intrinsic biases particularly ‘out-of-the-box’ thinking without contextual creativity. To address these issues, this paper proposes an innovative strategic framework for HI-GAI collaboration that emphasizes transparency, accountability, and inclusiveness.\n\n\n\n\n\nFigure from Hao et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#how-large-language-models-can-reshape-collective-intelligence",
    "href": "ai_gd.html#how-large-language-models-can-reshape-collective-intelligence",
    "title": "Literature Notes",
    "section": "",
    "text": "Burton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). How large language models can reshape collective intelligence. Nature Human Behaviour, 1–13. https://doi.org/10.1038/s41562-024-01959-9\n\n\nAbstract\n\n\nCollective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals—even experts—resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the ‘wisdom of crowds’, online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans’ ability to collectively tackle complex problems.\n\n\n\n\n\nBurton et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.",
    "href": "ai_gd.html#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.",
    "title": "Literature Notes",
    "section": "",
    "text": "Chiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nAbstract\n\n\nGroup decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil’s advocate in the AI-assisted group deci- sion making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities ex- hibited by modern large language models (LLMs), we design four different styles of devil’s advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil’s advocates that argue against the AI model’s decision recommenda- tion have the potential to promote groups’ appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil’s advocate usually does not lead to substantial increases in people’s perceived workload for completing the group decision making tasks, while interactive LLM-powered devil’s advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.\n\n\n\n\n\nFigure from Chiang et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#the-wisdom-of-partisan-crowds-comparing-collective-intelligence-in-humans-and-llm-based-agents",
    "href": "ai_gd.html#the-wisdom-of-partisan-crowds-comparing-collective-intelligence-in-humans-and-llm-based-agents",
    "title": "Literature Notes",
    "section": "",
    "text": "Chuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2024). The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents. https://escholarship.org/uc/item/3k67x8s5\n\n\nAbstract\n\n\nHuman groups are able to converge to more accurate beliefs through deliberation, even in the presence of polarization and partisan bias — a phenomenon known as the “wisdom of partisan crowds.” Large Language Models (LLMs) are increasingly being used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation, as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompting and lack of details in personas. Conversely, fine-tuning on human data appears to enhance convergence. These findings show the potential and limitations of LLM-based agents as a model of human collective intelligence.\n\n\n\n\n\nChuang et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#collective-innovation-in-groups-of-large-language-models.",
    "href": "ai_gd.html#collective-innovation-in-groups-of-large-language-models.",
    "title": "Literature Notes",
    "section": "",
    "text": "Nisioti, E., Risi, S., Momennejad, I., Oudeyer, P.-Y., & Moulin-Frier, C. (2024, July 7). Collective Innovation in Groups of Large Language Models. ALIFE 2024: Proceedings of the 2024 Artificial Life Conference. https://doi.org/10.1162/isal_a_00730\n\n\nAbstract\n\n\nHuman culture relies on collective innovation: our ability to continuously explore how existing elements in our environment can be combined to create new ones. Language is hypothesized to play a key role in human culture, driving individual cognitive capacities and shaping communication. Yet the majority of models of collective innovation assign no cognitive capacities or language abilities to agents. Here, we contribute a computational study of collective innovation where agents are Large Language Models (LLMs) that play Little Alchemy 2, a creative video game originally developed for humans that, as we argue, captures useful aspects of innovation landscapes not present in previous test-beds. We, first, study an LLM in isolation and discover that it exhibits both useful skills and crucial limitations. We, then, study groups of LLMs that share information related to their behaviour and focus on the effect of social connectivity on collective performance. In agreement with previous human and computational studies, we observe that groups with dynamic connectivity out-compete fully-connected groups. Our work reveals opportunities and challenges for future studies of collective innovation that are becoming increasingly relevant as Generative Artificial Intelligence algorithms and humans innovate alongside each other.\n\n\n\n\n\nNisioti et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#evaluating-llm-agent-group-dynamics-against-human-group-dynamics-a-case-study-on-wisdom-of-partisan-crowds",
    "href": "ai_gd.html#evaluating-llm-agent-group-dynamics-against-human-group-dynamics-a-case-study-on-wisdom-of-partisan-crowds",
    "title": "Literature Notes",
    "section": "",
    "text": "Chuang, Y.-S., Suresh, S., Harlalka, N., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2023). Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds http://arxiv.org/abs/2311.09665\n\n\nAbstract\n\n\nThis study investigates the potential of Large Language Models (LLMs) to simulate human group dynamics, particularly within politically charged contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to role-play as Democrat and Republican personas, engaging in a structured interaction akin to human group study. Our approach evaluates how agents’ responses evolve through social influence. Our key findings indicate that LLM agents role-playing detailed personas and without Chain-of-Thought (CoT) reasoning closely align with human behaviors, while having CoT reasoning hurts the alignment. However, incorporating explicit biases into agent prompts does not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning LLMs with human data shows promise in achieving human-like behavior but poses a risk of overfitting certain behaviors. These findings show the potential and limitations of using LLM agents in modeling human group phenomena.\n\n\nChuang et al. (2023)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#exploring-collaboration-mechanisms-for-llm-agents-a-social-psychology-view",
    "href": "ai_gd.html#exploring-collaboration-mechanisms-for-llm-agents-a-social-psychology-view",
    "title": "Literature Notes",
    "section": "",
    "text": "Zhang, J., Xu, X., Zhang, N., Liu, R., Hooi, B., & Deng, S. (2024). Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View (arXiv:2310.02124). arXiv. http://arxiv.org/abs/2310.02124\nhttps://www.zjukg.org/project/MachineSoM/\n\n\nAbstract\n\n\nAs Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique ‘societies’ comprised of LLM agents, where each agent is characterized by a specific ‘trait’ (easy-going or overconfident) and engages in collaboration with a distinct ‘thinking pattern’ (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest humanlike social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We have shared our code and datasets1, hoping to catalyze further research in this promising avenue.\n\n\n\n\n\nZhang et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#llm-deliberation-evaluating-llms-with-interactive-multi-agent-negotiation-games.",
    "href": "ai_gd.html#llm-deliberation-evaluating-llms-with-interactive-multi-agent-negotiation-games.",
    "title": "Literature Notes",
    "section": "",
    "text": "Abdelnabi, S., Gomaa, A., Sivaprasad, S., Schönherr, L., & Fritz, M. (2023). LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. https://doi.org/10.60882/cispa.25233028.v1\n\n\nAbstract\n\n\nThere is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs’ reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and earlier models. Importantly, we test the generalization to new games and setups. Finally, we show that these games can help evaluate other critical aspects, such as the interaction dynamics between agents in the presence of greedy and adversarial players.\n\n\n\n\n\nAbdelnabi et al. (2023)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#llm-voting-human-choices-and-ai-collective-decision-making",
    "href": "ai_gd.html#llm-voting-human-choices-and-ai-collective-decision-making",
    "title": "Literature Notes",
    "section": "",
    "text": "Yang, J. C., Dailisan, D., Korecki, M., Hausladen, C. I., & Helbing, D. (2024). LLM Voting: Human Choices and AI Collective Decision Making (arXiv:2402.01766). arXiv. http://arxiv.org/abs/2402.01766\n\n\nAbstract\n\n\nThis paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.\n\n\n\n\n\nYang et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#embodied-llm-agents-learn-to-cooperate-in-organized-teams",
    "href": "ai_gd.html#embodied-llm-agents-learn-to-cooperate-in-organized-teams",
    "title": "Literature Notes",
    "section": "",
    "text": "Guo, X., Huang, K., Liu, J., Fan, W., Vélez, N., Wu, Q., Wang, H., Griffiths, T. L., & Wang, M. (2024). Embodied LLM Agents Learn to Cooperate in Organized Teams (arXiv:2403.12482). arXiv. http://arxiv.org/abs/2403.12482\n\n\nAbstract\n\n\nLarge Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.\n\n\n\n\n\nGuo et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#measuring-latent-trust-patterns-in-large-language-models-in-the-context-of-human-ai-teaming",
    "href": "ai_gd.html#measuring-latent-trust-patterns-in-large-language-models-in-the-context-of-human-ai-teaming",
    "title": "Literature Notes",
    "section": "",
    "text": "Koehl, D., & Vangsness, L. (2023). Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 67. https://doi.org/10.1177/21695067231192869\n\n\nAbstract\n\n\nQualitative self-report methods such as think-aloud procedures and open-ended response questions can provide valuable data to human factors research. These measures come with analytic weaknesses, such as researcher bias, intra- and inter-rater reliability concerns, and time-consuming coding protocols. A possible solution exists in the latent semantic patterns that exist in machine learning large language models. These semantic patterns could be used to analyze qualitative responses. This exploratory research compared the statistical quality of automated sentence coding using large language models to the benchmarks of self-report and behavioral measures within the context of trust in automation research. The results indicated that three large language models show promise as tools for analyzing qualitative responses. The study also provides insight on minimum sample sizes for model creation and offers recommendations for further validating the robustness of large language models as research tools.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Koehl & Vangsness (2023)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  },
  {
    "objectID": "ai_gd.html#a-survey-on-human-ai-teaming-with-large-pre-trained-models",
    "href": "ai_gd.html#a-survey-on-human-ai-teaming-with-large-pre-trained-models",
    "title": "Literature Notes",
    "section": "",
    "text": "Vats, V., Nizam, M. B., Liu, M., Wang, Z., Ho, R., Prasad, M. S., Titterton, V., Malreddy, S. V., Aggarwal, R., Xu, Y., Ding, L., Mehta, J., Grinnell, N., Liu, L., Zhong, S., Gandamani, D. N., Tang, X., Ghosalkar, R., Shen, C., … Davis, J. (2024). A Survey on Human-AI Teaming with Large Pre-Trained Models (arXiv:2403.04931). arXiv. http://arxiv.org/abs/2403.04931\n\n\nAbstract\n\n\nIn the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI Teaming, providing insights for future research, policy development, and strategic implementations aimed at harnessing the full potential of this collaboration for research and societal benefit.\n\n\n\n\n\nTable from Vats et al. (2024)",
    "crumbs": [
      "AI Group Decision Making",
      "Literature Notes"
    ]
  }
]