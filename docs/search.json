[
  {
    "objectID": "tms.html",
    "href": "tms.html",
    "title": "Transactive Memory Systems",
    "section": "",
    "text": "https://tegorman13.github.io/ccl/tms.html",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "tms.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.",
    "href": "tms.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.",
    "title": "Transactive Memory Systems",
    "section": "Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness.",
    "text": "Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness.\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nAbstract\n\n\nIn this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team’s ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team’s transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond.\n\n\n\n\n\n\nBienefeld et al. (2023)",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "tms.html#communication-in-transactive-memory-systems-a-review-and-multidimensional-network-perspective",
    "href": "tms.html#communication-in-transactive-memory-systems-a-review-and-multidimensional-network-perspective",
    "title": "Transactive Memory Systems",
    "section": "Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective",
    "text": "Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective\nYan, B., Hollingshead, A. B., Alexander, K. S., Cruz, I., & Shaikh, S. J. (2021). Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective. Small Group Research, 52(1), 3–32. https://doi.org/10.1177/1046496420967764\n\n\nAbstract\n\n\nThe comprehensive review synthesizes 64 empirical studies on communication and transactive memory systems (TMS). The results reveal that (a) a TMS forms through communication about expertise; (b) as a TMS develops, communication to allocate information and coordinate retrieval increases, promoting information exchange; and (c) groups update their TMS through communicative learning. However, direct interpersonal communication is not necessary for TMS development or utilization. Nor do high-quality information-sharing processes always occur within developed TMS structures. For future research, we propose a multidimensional network approach to TMS that incorporates technologies, addresses member characteristics, considers multiple communication types, and situates groups in context.",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "tms.html#alignment-transactive-memory-and-collective-cognitive-systems",
    "href": "tms.html#alignment-transactive-memory-and-collective-cognitive-systems",
    "title": "Transactive Memory Systems",
    "section": "Alignment, Transactive Memory, and Collective Cognitive Systems",
    "text": "Alignment, Transactive Memory, and Collective Cognitive Systems\nTollefsen, D. P., Dale, R., & Paxton, A. (2013). Alignment, Transactive Memory, and Collective Cognitive Systems. Review of Philosophy and Psychology, 4(1), 49–64. https://doi.org/10.1007/s13164-012-0126-z\n\n\nAbstract\n\n\nResearch on linguistic interaction suggests that two or more individuals can sometimes form adaptive and cohesive systems. We describe an “alignment system” as a loosely interconnected set of cognitive processes that facilitate social interactions. As a dynamic, multi-component system, it is responsive to higher-level cognitive states such as shared beliefs and intentions (those involving collective intentionality) but can also give rise to such shared cognitive states via bottom-up processes. As an example of putative group cognition we turn to transactive memory and suggest how further research on alignment in these cases might reveal how such systems can be genuinely described as cognitive. Finally, we address a prominent critique of collective cognitive systems, arguing that there is much empirical and explanatory benefit to be gained from considering the possibility of group cognitive systems, especially in the context of small-group human interaction.",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "tms.html#building-machines-that-learn-and-think-with-people",
    "href": "tms.html#building-machines-that-learn-and-think-with-people",
    "title": "Transactive Memory Systems",
    "section": "Building Machines that Learn and Think with People",
    "text": "Building Machines that Learn and Think with People\nCollins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building machines that learn and think with people. Nature Human Behaviour, 8(10), 1851–1863. https://doi.org/10.1038/s41562-024-01991-9\n\n\nAbstract\n\n\nWhat do we want from machine intelligence? We envision machines that are not just tools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable, and trustworthy systems that think with us. Current artificial intelligence (AI) systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called “thought partners,” systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and AI thought partners can engage and propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Figures from Collins et al. (2024)",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "chapter_structure.html",
    "href": "chapter_structure.html",
    "title": "Possible Chapter Structures",
    "section": "",
    "text": "Brainstorming chapter structures with the help of various AI assistants:",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "chapter_structure.html#alternative-0",
    "href": "chapter_structure.html#alternative-0",
    "title": "Possible Chapter Structures",
    "section": "Alternative 0",
    "text": "Alternative 0\nPotential Section Structure:\n\nIntroduction\n\nOverview of AI in group decision-making contexts\nRelevance to group dynamics research\n\nLarge Language Models (LLMs) as Decision-Making Agents\n\nCapabilities and limitations of LLMs in decision-making tasks\nComparison with human decision-making processes\n\nAI-Human Collaboration in Group Decision Making\n\nModels of human-AI teaming\nEnhancing collective intelligence through AI integration\n\nSimulating Group Dynamics with AI Agents\n\nLLM-based multi-agent simulations\nEmergent behaviors and social phenomena in AI agent groups\n\nEthical Considerations and Biases\n\nRepresentation and identity issues in AI-simulated groups\nAddressing biases in AI-assisted decision making\n\nMethodological Approaches and Challenges\n\nExperimental designs for studying AI in group contexts\nMeasuring and evaluating AI-human group performance\n\nApplications and Future Directions\n\nPotential uses in various domains (e.g., education, healthcare, policy-making)\nResearch gaps and emerging questions\n\nConclusion\n\nImplications for group dynamics research\nRecommendations for future studies\n\n\nKey Issues to Discuss:\n\nThe potential and limitations of LLMs in simulating human-like decision-making processes\nThe impact of AI agents on group dynamics, including cooperation, conflict, and consensus-building\nEthical considerations in using AI to replace or augment human participants in group studies\nThe emergence of social behaviors and phenomena in multi-agent AI systems\nMethodological challenges in designing experiments and measuring outcomes in AI-human group interactions\nThe role of AI in enhancing collective intelligence and group performance\nAddressing biases and ensuring diverse representation in AI-simulated group dynamics\nThe potential of AI to model complex social systems and inform social science research\nChallenges in aligning AI agent behavior with human social norms and expectations\nThe impact of AI on traditional theories and models of group dynamics",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "chapter_structure.html#alternative-1",
    "href": "chapter_structure.html#alternative-1",
    "title": "Possible Chapter Structures",
    "section": "Alternative 1",
    "text": "Alternative 1\n\nPotential Issues to Discuss:\n\nThe Role of AI in Collective Intelligence:\n\nHow large language models (LLMs) enhance or inhibit group decision-making (Burton et al., 2024; Cui & Yasseri, 2024).\nThe concept of hybrid human-AI collective intelligence and the synergies between human reasoning and AI-driven insights.\nOpportunities and challenges AI introduces for improving group problem-solving in both human and AI-augmented groups.\n\nHuman-AI Teaming and Decision Making:\n\nThe dynamics of human-AI collaboration and transactive memory systems in group decision-making (Bienefeld et al., 2023).\nHow introducing AI in decision-making teams affects group behavior, particularly in terms of knowledge-sharing and hypothesis generation.\n\nSimulating Human Decision-Making Using AI:\n\nUsing LLMs to simulate group dynamics and decision-making processes, such as in multi-agent negotiation games (Abdelnabi et al., 2023) or partisan group behavior (Chuang et al., 2024).\nThe limitations and opportunities of using LLMs as proxies for human participants in decision-making simulations (Aher et al., 2022).\n\nChallenges in AI-Assisted Group Decision-Making:\n\nIssues with the overreliance on AI recommendations and the role of LLM-powered devil’s advocates in correcting this overreliance (Chiang et al., 2024).\nHow AI may introduce biases or alter the dynamics of decision-making in group settings.\n\nAI’s Limitations in Group Decision Making:\n\nCurrent limitations in how AI models deal with complex social dynamics and group decision-making (Fan et al., 2024).\nSituations where LLMs fall short in reproducing nuanced human behaviors and decision-making patterns.\n\nEthical and Strategic Considerations in AI-Assisted Group Decision Making:\n\nEthical considerations around using AI for decision-making in groups, especially concerning equity and fairness.\nThe need for strategies to mitigate biases in LLMs when used for group decision-making.\n\n\n\n\n\nPossible Section Structure:\n\nIntroduction:\n\nOverview of AI in group decision-making.\nDefinition and context of group decision-making in psychology and communication studies.\nIntroduction of large language models (LLMs) and AI agents as tools in decision-making.\n\nAI and Collective Intelligence:\n\nExploration of how AI enhances collective intelligence.\nInsights from research on LLMs’ role in collective problem-solving (Burton et al., 2024).\n\nHuman-AI Teaming in Decision Making:\n\nExamination of human-AI collaboration and the dynamics of transactive memory systems (Bienefeld et al., 2023).\nCase studies on the application of AI in human decision-making teams.\n\nAI Simulations of Human Decision-Making:\n\nDiscussion of LLMs as simulators for human group dynamics (Aher et al., 2022).\nComparative analysis of AI-driven versus human-driven decision-making processes.\n\nChallenges and Limitations of AI in Group Dynamics:\n\nChallenges in AI adoption in group settings, including bias and overreliance on AI recommendations (Chiang et al., 2024).\nLimitations in AI’s ability to fully replicate human decision-making behaviors (Fan et al., 2024).\n\nEthical and Practical Considerations:\n\nEthical implications of using AI in group decision-making.\nStrategic approaches to mitigating biases and ensuring ethical outcomes.\n\nFuture Directions and Open Questions:\n\nProspective developments in AI-enhanced group decision-making.\nOpen research questions and directions for future studies in this area.",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "chapter_structure.html#alternative-2",
    "href": "chapter_structure.html#alternative-2",
    "title": "Possible Chapter Structures",
    "section": "Alternative 2",
    "text": "Alternative 2\n\nHuman-AI Teaming and Collaboration:\n\nPapers that explore how LLMs collaborate with humans in decision-making, problem-solving, and teamwork contexts.\nExample: “Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness” .\n\nLLMs as Models for Social and Group Dynamics:\n\nPapers that investigate how LLMs simulate or replicate human group behavior, including group decision-making, social interactions, and collective intelligence.\nExample: “The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents” .\n\nLLMs in Decision-Making and Game Theory:\n\nPapers that focus on the decision-making abilities of LLMs, including their performance in structured games and strategic contexts.\nExample: “Can Large Language Models Serve as Rational Players in Game Theory?” .\n\nEvaluation of LLM Capabilities and Limitations:\n\nPapers that assess the reasoning, problem-solving, and decision-making capabilities of LLMs, often by comparing them to human performance.\nExample: “LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games” .\n\nEthical and Societal Implications of LLM Use:\n\nPapers that address the ethical, societal, and policy-related considerations associated with the use of LLMs in human-centered contexts.\nExample: “Large language models cannot replace human participants because they cannot portray identity groups”.",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "chapter_structure.html#alternative-3",
    "href": "chapter_structure.html#alternative-3",
    "title": "Possible Chapter Structures",
    "section": "Alternative 3",
    "text": "Alternative 3\n\nCollective Intelligence and Decision Making This category focuses on how LLMs can be used to simulate or enhance collective intelligence, group decision-making processes, and wisdom of crowds phenomena.\nMulti-Agent Systems and Collaboration This category includes papers that explore interactions between multiple LLM agents, their collaborative behaviors, and emergent properties in multi-agent systems.\nHuman-AI Teaming and Interaction This category covers research on the integration of LLMs with human teams, human-AI collaboration, and the use of LLMs to augment human capabilities in various domains.\nSocial Behavior and Network Dynamics This category encompasses studies that investigate whether LLMs exhibit human-like social behaviors, network formation tendencies, and how they might influence or simulate social dynamics.",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "chapter_structure.html#alternative-4",
    "href": "chapter_structure.html#alternative-4",
    "title": "Possible Chapter Structures",
    "section": "Alternative 4",
    "text": "Alternative 4\n1. LLM Agents as Simulators of Human Behavior:\n\nDescription: This category encompasses papers that explore the use of LLMs to simulate individual and group human behavior in various contexts, including social dilemmas, games, and collective decision-making.\nPapers:\n\nAher et al. (2022) - Using Large Language Models to Simulate Multiple Humans\nChuang et al. (2023) - Evaluating LLM Agent Group Dynamics against Human Group Dynamics\nChuang et al. (2024) - The Wisdom of Partisan Crowds\nFan et al. (2024) - Can Large Language Models Serve as Rational Players in Game Theory?\nJin et al. (2024) - What if LLMs Have Different World Views\nLeng & Yuan (2024) - Do LLM Agents Exhibit Social Behavior?\nSun et al. - Random Silicon Sampling\nWang et al. (2024) - Large language models cannot replace human participants because they cannot portray identity groups\n\n\n2. LLM-Enhanced Collective Intelligence:\n\nDescription: This category includes papers that investigate how LLMs can be integrated into human groups or systems to augment collective intelligence, improve decision-making, and facilitate problem-solving.\nPapers:\n\nBurton et al. (2024) - How large language models can reshape collective intelligence\nChiang et al. (2024) - Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate\nCui & Yasseri (2024) - AI-enhanced Collective Intelligence\nDu et al. (2024) - Large Language Models for Collective Problem-Solving\nGruen et al. (2023) - Machine learning augmentation reduces prediction error in collective forecasting\nNisioti et al. (2024) - Collective Innovation in Groups of Large Language Models\n\n\n3. LLM Agent Interaction and Network Dynamics:\n\nDescription: This category focuses on papers that examine the interactions and emergent behaviors of multiple LLM agents, including network formation, communication patterns, and the dynamics of cooperation and competition.\nPapers:\n\nCisneros-Velarde (2024) - On the Principles behind Opinion Dynamics in Multi-Agent Systems of Large Language Models\nHuang et al. (2024) - How Far Are We on the Decision-Making of LLMs?\nKim et al. (2024) - Adaptive Collaboration Strategy for LLMs in Medical Decision Making\nMarjieh et al. (2024) - Task Allocation in Teams as a Multi-Armed Bandit\nPapachristou & Yuan (2024) - Network Formation and Dynamics Among Multi-LLMs\nPiatti et al. (2024) - Cooperate or Collapse: Emergence of Sustainable Cooperation in a Society of LLM Agents\nZhang et al. (2024) - Simulating Classroom Education with LLM-Empowered Agents\n\n\n4. LLMs in Collaborative Tasks:\n\nDescription: If you want to further emphasize the practical applications of LLMs in collaborative settings, you could create a separate category for papers specifically addressing human-LLM collaboration in tasks like annotation or knowledge creation.\nPapers:\n\nWang et al. (2024) - Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels",
    "crumbs": [
      "AI Group Decision Making",
      "Possible Chapter Structures"
    ]
  },
  {
    "objectID": "ai_decision.html",
    "href": "ai_decision.html",
    "title": "Individual decision lit",
    "section": "",
    "text": "Buçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21. https://doi.org/10.1145/3449287\n\n\nAbstract\n\n\nPeople supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI’s suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.\n\n\n\n\n\nFigure from Buçinca et al. (2021)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#irrationality-and-cognitive-biases-in-large-language-models.",
    "href": "ai_decision.html#irrationality-and-cognitive-biases-in-large-language-models.",
    "title": "Individual decision lit",
    "section": "(Ir)rationality and cognitive biases in large language models.",
    "text": "(Ir)rationality and cognitive biases in large language models.\nMacmillan-Scott, O., & Musolesi, M. (2024). (Ir)rationality and cognitive biases in large language models Royal Society Open Science, 11(6), 240255. https://doi.org/10.1098/rsos.240255\n\n\nAbstract\n\n\nDo large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Figures from Macmillan-Scott & Musolesi (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#human-like-intuitive-behavior-and-reasoning-biases-emerged-in-large-language-models-but-disappeared-in-chatgpt",
    "href": "ai_decision.html#human-like-intuitive-behavior-and-reasoning-biases-emerged-in-large-language-models-but-disappeared-in-chatgpt",
    "title": "Individual decision lit",
    "section": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
    "text": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT\nHagendorff, T., Fabi, S., & Kosinski, M. (2023). Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. Nature Computational Science, 3(10), 833–838. https://doi.org/10.1038/s43588-023-00527-x\n\n\nAbstract\n\n\nWe design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Figures from Hagendorff et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#using-cognitive-psychology-to-understand-gpt-3.",
    "href": "ai_decision.html#using-cognitive-psychology-to-understand-gpt-3.",
    "title": "Individual decision lit",
    "section": "Using cognitive psychology to understand GPT-3.",
    "text": "Using cognitive psychology to understand GPT-3.\nBinz, M., & Schulz, E. (2023). Using cognitive psychology to understand GPT-3. Proceedings of the National Academy of Sciences, 120(6), e2218523120. https://doi.org/10.1073/pnas.2218523120\n\n\nAbstract\n\n\nWe study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3’s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3’s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.\n\n\n\n\n\nFigure from Binz & Schulz (2023)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#studying-and-improving-reasoning-in-humans-and-machines.",
    "href": "ai_decision.html#studying-and-improving-reasoning-in-humans-and-machines.",
    "title": "Individual decision lit",
    "section": "Studying and improving reasoning in humans and machines.",
    "text": "Studying and improving reasoning in humans and machines.\nYax, N., Anlló, H., & Palminteri, S. (2024). Studying and improving reasoning in humans and machines. Communications Psychology, 2(1), 1–16. https://doi.org/10.1038/s44271-024-00091-8\n\n\nAbstract\n\n\nIn the present study, we investigate and compare reasoning in large language models (LLMs) and humans, using a selection of cognitive psychology tools traditionally dedicated to the study of (bounded) rationality. We presented to human participants and an array of pretrained LLMs new variants of classical cognitive experiments, and cross-compared their performances. Our results showed that most of the included models presented reasoning errors akin to those frequently ascribed to error-prone, heuristic-based human reasoning. Notwithstanding this superficial similarity, an in-depth comparison between humans and LLMs indicated important differences with human-like reasoning, with models’ limitations disappearing almost entirely in more recent LLMs’ releases. Moreover, we show that while it is possible to devise strategies to induce better performance, humans and machines are not equally responsive to the same prompting schemes. We conclude by discussing the epistemological implications and challenges of comparing human and machine behavior for both artificial intelligence and cognitive psychology.\n\n\n\n\n\nFigure from Yax et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#exploring-variability-in-risk-taking-with-large-language-models.",
    "href": "ai_decision.html#exploring-variability-in-risk-taking-with-large-language-models.",
    "title": "Individual decision lit",
    "section": "Exploring variability in risk taking with large language models.",
    "text": "Exploring variability in risk taking with large language models.\nBhatia, S. (2024). Exploring variability in risk taking with large language models. Journal of Experimental Psychology: General, 153(7), 1838–1860. https://doi.org/10.1037/xge0001607\n\n\nAbstract\n\n\nWhat are the sources of individual-level differences in risk taking, and how do they depend on the domain or situation in which the decision is being made? Psychologists currently answer such questions with psychometric methods, which analyze correlations across participant responses in survey data sets. In this article, we analyze the preferences that give rise to these correlations. Our approach uses (a) large language models (LLMs) to quantify everyday risky behaviors in terms of the attributes or reasons that may describe those behaviors, and (b) decision models to map these attributes and reasons onto participant responses. We show that LLM-based decision models can explain observed correlations between behaviors in terms of the reasons different behaviors elicit and explain observed correlations between individuals in terms of the weights different individuals place on reasons, thereby providing a decision theoretic foundation for psychometric findings. Since LLMs can generate quantitative representations for nearly any naturalistic decision, they can be used to make accurate out-of-sample predictions for hundreds of everyday behaviors, predict the reasons why people may or may not want to engage in these behaviors, and interpret these reasons in terms of core psychological constructs. Our approach has important theoretical and practical implications for the study of heterogeneity in everyday behavior.\n\n\nBhatia (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#human-bias-in-ai-models-anchoring-effects-and-mitigation-strategies-in-large-language-models",
    "href": "ai_decision.html#human-bias-in-ai-models-anchoring-effects-and-mitigation-strategies-in-large-language-models",
    "title": "Individual decision lit",
    "section": "Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models",
    "text": "Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models\nNguyen, J. (2024). Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models. Journal of Behavioral and Experimental Finance, 100971. https://doi.org/10.1016/j.jbef.2024.100971\n\n\nAbstract\n\n\nThis study builds on the seminal work of Tversky and Kahneman (1974), exploring the presence and extent of anchoring bias in forecasts generated by four Large Language Models (LLMs): GPT-4, Claude 2, Gemini Pro and GPT-3.5. In contrast to recent findings of advanced reasoning capabilities in LLMs, our randomised controlled trials reveal the presence of anchoring bias across all models: forecasts are significantly influenced by prior mention of high or low values. We examine two mitigation prompting strategies, ‘Chain of Thought’ and ‘ignore previous’, finding limited and varying degrees of effectiveness. Our results extend the anchoring bias research in finance beyond human decision-making to encompass LLMs, highlighting the importance of deliberate and informed prompting in AI forecasting in both ad hoc LLM use and in crafting few-shot examples.\n\n\n\n\n\nFigure from Nguyen (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#a-turing-test-of-whether-ai-chatbots-are-behaviorally-similar-to-humans",
    "href": "ai_decision.html#a-turing-test-of-whether-ai-chatbots-are-behaviorally-similar-to-humans",
    "title": "Individual decision lit",
    "section": "A Turing test of whether AI chatbots are behaviorally similar to humans",
    "text": "A Turing test of whether AI chatbots are behaviorally similar to humans\nMei, Q., Xie, Y., Yuan, W., & Jackson, M. O. (2024). A Turing test of whether AI chatbots are behaviorally similar to humans. Proceedings of the National Academy of Sciences, 121(9), e2313925121. https://doi.org/10.1073/pnas.2313925121\n\n\nAbstract\n\n\nWe administer a Turing test to AI chatbots. We examine how chatbots behave in a suite of classic behavioral games that are designed to elicit characteristics such as trust, fairness, risk-aversion, cooperation, etc., as well as how they respond to a traditional Big-5 psychological survey that measures personality traits. ChatGPT-4 exhibits behavioral and personality traits that are statistically indistinguishable from a random human from tens of thousands of human subjects from more than 50 countries. Chatbots also modify their behavior based on previous experience and contexts “as if” they were learning from the interactions and change their behavior in response to different framings of the same strategic situation. Their behaviors are often distinct from average and modal human behaviors, in which case they tend to behave on the more altruistic and cooperative end of the distribution. We estimate that they act as if they are maximizing an average of their own and partner’s payoffs.\n\n\n\n\n\nFigure from Mei et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#deciding-fast-and-slow-the-role-of-cognitive-biases-in-ai-assisted-decision-making",
    "href": "ai_decision.html#deciding-fast-and-slow-the-role-of-cognitive-biases-in-ai-assisted-decision-making",
    "title": "Individual decision lit",
    "section": "Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making",
    "text": "Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making\nRastogi, C., Zhang, Y., Wei, D., Varshney, K. R., Dhurandhar, A., & Tomsett, R. (2022). Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 6(CSCW1), 1–22. https://doi.org/10.1145/3512930\n\n\nAbstract\n\n\nSeveral strands of research have aimed to bridge the gap between artificial intelligence (AI) and human decision-makers in AI-assisted decision-making, where humans are the consumers of AI model predictions and the ultimate decision-makers in high-stakes applications. However, people’s perception and understanding are often distorted by their cognitive biases, such as confirmation bias, anchoring bias, availability bias, to name a few. In this work, we use knowledge from the field of cognitive science to account for cognitive biases in the human-AI collaborative decision-making setting, and mitigate their negative effects on collaborative performance. To this end, we mathematically model cognitive biases and provide a general framework through which researchers and practitioners can understand the interplay between cognitive biases and human-AI accuracy. We then focus specifically on anchoring bias, a bias commonly encountered in human-AI collaboration. We implement a time-based de-anchoring strategy and conduct our first user experiment that validates its effectiveness in human-AI collaborative decision-making. With this result, we design a time allocation strategy for a resource-constrained setting that achieves optimal human-AI collaboration under some assumptions. We, then, conduct a second user experiment which shows that our time allocation strategy with explanation can effectively de-anchor the human and improve collaborative performance when the AI model has low confidence and is incorrect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Figures from Rastogi et al. (2022)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#decision-control-and-explanations-in-human-ai-collaboration-improving-user-perceptions-and-compliance",
    "href": "ai_decision.html#decision-control-and-explanations-in-human-ai-collaboration-improving-user-perceptions-and-compliance",
    "title": "Individual decision lit",
    "section": "Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance",
    "text": "Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance\nWestphal, M., Vössing, M., Satzger, G., Yom-Tov, G. B., & Rafaeli, A. (2023). Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance. Computers in Human Behavior, 144, 107714. https://doi.org/10.1016/j.chb.2023.107714\n\n\nAbstract\n\n\nHuman-AI collaboration has become common, integrating highly complex AI systems into the workplace. Still, it is often ineffective; impaired perceptions – such as low trust or limited understanding – reduce compliance with recommendations provided by the AI system. Drawing from cognitive load theory, we examine two techniques of human-AI collaboration as potential remedies. In three experimental studies, we grant users decision control by empowering them to adjust the system’s recommendations, and we offer explanations for the system’s reasoning. We find decision control positively affects user perceptions of trust and understanding, and improves user compliance with system recommendations. Next, we isolate different effects of providing explanations that may help explain inconsistent findings in recent literature: while explanations help reenact the system’s reasoning, they also increase task complexity. Further, the effectiveness of providing an explanation depends on the specific user’s cognitive ability to handle complex tasks. In summary, our study shows that users benefit from enhanced decision control, while explanations – unless appropriately designed for the specific user – may even harm user perceptions and compliance. This work bears both theoretical and practical implications for the management of human-AI collaboration.\n\n\n\n\n\nFigure from Westphal et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#risk-and-prosocial-behavioural-cues-elicit-human-like-response-patterns-from-ai-chatbots.",
    "href": "ai_decision.html#risk-and-prosocial-behavioural-cues-elicit-human-like-response-patterns-from-ai-chatbots.",
    "title": "Individual decision lit",
    "section": "Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots.",
    "text": "Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots.\nZhao, Y., Huang, Z., Seligman, M., & Peng, K. (2024). Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots. Scientific Reports, 14(1), 7095. https://doi.org/10.1038/s41598-024-55949-y\n\n\nAbstract\n\n\nEmotions, long deemed a distinctly human characteristic, guide a repertoire of behaviors, e.g., promoting risk-aversion under negative emotional states or generosity under positive ones. The question of whether Artificial Intelligence (AI) can possess emotions remains elusive, chiefly due to the absence of an operationalized consensus on what constitutes ‘emotion’ within AI. Adopting a pragmatic approach, this study investigated the response patterns of AI chatbots—specifically, large language models (LLMs)—to various emotional primes. We engaged AI chatbots as one would human participants, presenting scenarios designed to elicit positive, negative, or neutral emotional states. Multiple accounts of OpenAI’s ChatGPT Plus were then tasked with responding to inquiries concerning investment decisions and prosocial behaviors. Our analysis revealed that ChatGPT-4 bots, when primed with positive, negative, or neutral emotions, exhibited distinct response patterns in both risk-taking and prosocial decisions, a phenomenon less evident in the ChatGPT-3.5 iterations. This observation suggests an enhanced capacity for modulating responses based on emotional cues in more advanced LLMs. While these findings do not suggest the presence of emotions in AI, they underline the feasibility of swaying AI responses by leveraging emotional indicators.\n\n\n\n\n\nZhao et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#do-large-language-models-show-decision-heuristics-similar-to-humans-a-case-study-using-gpt-3.5",
    "href": "ai_decision.html#do-large-language-models-show-decision-heuristics-similar-to-humans-a-case-study-using-gpt-3.5",
    "title": "Individual decision lit",
    "section": "Do large language models show decision heuristics similar to humans? A case study using GPT-3.5",
    "text": "Do large language models show decision heuristics similar to humans? A case study using GPT-3.5\nSuri, G., Slater, L. R., Ziaee, A., & Nguyen, M. (2024). Do large language models show decision heuristics similar to humans? A case study using GPT-3.5. Journal of Experimental Psychology: General, 153(4), 1066–1075. https://doi.org/10.1037/xge0001547\n\n\nAbstract\n\n\nA Large Language Model (LLM) is an artificial intelligence system trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. Generative Pre-Trained Transformer (GPT)-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics and other context-sensitive responses. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (anchoring, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was influenced by anecdotal information (representativeness and availability heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively—even though both presentations contained statistically equivalent information (framing effect, Study 3); and it valued an owned item more than a newly found item even though the two items were objectively identical (endowment effect, Study 4). In each study, human participants showed similar effects. Heuristics and context-sensitive responses in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM—which lacks these processes—also shows such responses invites consideration of the possibility that language is sufficiently rich to carry these effects and may play a role in generating these effects in humans.\n\n\n\n\n\nFigure from Suri et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#can-large-language-models-capture-human-preferences",
    "href": "ai_decision.html#can-large-language-models-capture-human-preferences",
    "title": "Individual decision lit",
    "section": "Can Large Language Models Capture Human Preferences?",
    "text": "Can Large Language Models Capture Human Preferences?\nGoli, A., & Singh, A. (2024). Can Large Language Models Capture Human Preferences? Marketing Science. https://doi.org/10.1287/mksc.2023.0306\n\n\nAbstract\n\n\nWe explore the viability of large language models (LLMs), specifically OpenAI’s GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting preferences, with a focus on intertemporal choices. Leveraging the extensive literature on intertemporal discounting for benchmarking, we examine responses from LLMs across various languages and compare them with human responses, exploring preferences between smaller, sooner and larger, later rewards. Our findings reveal that both generative pretrained transformer (GPT) models demonstrate less patience than humans, with GPT-3.5 exhibiting a lexicographic preference for earlier rewards unlike human decision makers. Although GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans. Interestingly, GPT models show greater patience in languages with weak future tense references, such as German and Mandarin, aligning with the existing literature that suggests a correlation between language structure and intertemporal preferences. We demonstrate how prompting GPT to explain its decisions, a procedure we term “chain-of-thought conjoint,” can mitigate, but does not eliminate, discrepancies between LLM and human responses. Although directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings of preferences. Chain-of-thought conjoint provides a structured framework for marketers to use LLMs to identify potential attributes or factors that can explain preference heterogeneity across different customers and contexts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Figures from Goli & Singh (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#language-models-like-humans-show-content-effects-on-reasoning-tasks",
    "href": "ai_decision.html#language-models-like-humans-show-content-effects-on-reasoning-tasks",
    "title": "Individual decision lit",
    "section": "Language models, like humans, show content effects on reasoning tasks",
    "text": "Language models, like humans, show content effects on reasoning tasks\nLampinen, A. K., Dasgupta, I., Chan, S. C. Y., Sheahan, H. R., Creswell, A., Kumaran, D., McClelland, J. L., & Hill, F. (2024). Language models, like humans, show content effects on reasoning tasks. PNAS Nexus, 3(7), pgae233. https://doi.org/10.1093/pnasnexus/pgae233\n\n\nAbstract\n\n\nAbstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks but exhibit many imperfections. However, human abstract reasoning is also imperfect. Human reasoning is affected by our real-world knowledge and beliefs, and shows notable “content effects”; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns are central to debates about the fundamental nature of human intelligence. Here, we investigate whether language models—whose prior expectations capture some aspects of human knowledge—similarly mix content into their answers to logic problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art LMs, as well as humans, and find that the LMs reflect many of the same qualitative human patterns on these tasks—like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected in accuracy patterns, and in some lower-level features like the relationship between LM confidence over possible answers and human response times. However, in some cases the humans and models behave differently—particularly on the Wason task, where humans perform much worse than large models, and exhibit a distinct error pattern. Our findings have implications for understanding possible contributors to these human cognitive effects, as well as the factors that influence language model performance.\n\n\nLampinen et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#the-emergence-of-economic-rationality-of-gpt",
    "href": "ai_decision.html#the-emergence-of-economic-rationality-of-gpt",
    "title": "Individual decision lit",
    "section": "The emergence of economic rationality of GPT",
    "text": "The emergence of economic rationality of GPT\nChen, Y., Liu, T. X., Shan, Y., & Zhong, S. (2023). The emergence of economic rationality of GPT. Proceedings of the National Academy of Sciences, 120(51), e2316205120. https://doi.org/10.1073/pnas.2316205120\n\n\nAbstract\n\n\nAs large language models (LLMs) like GPT become increasingly prevalent, it is essential that we assess their capabilities beyond language processing. This paper examines the economic rationality of GPT by instructing it to make budgetary decisions in four domains: risk, time, social, and food preferences. We measure economic rationality by assessing the consistency of GPT’s decisions with utility maximization in classic revealed preference theory. We find that GPT’s decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature. Moreover, the estimated preference parameters of GPT are slightly different from human subjects and exhibit a lower degree of heterogeneity. We also find that the rationality scores are robust to the degree of randomness and demographic settings such as age and gender but are sensitive to contexts based on the language frames of the choice situations. These results suggest the potential of LLMs to make good decisions and the need to further understand their capabilities, limitations, and underlying mechanisms.\n\n\n\n\n\nFigure from Chen et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#the-potential-of-generative-ai-for-personalized-persuasion-at-scale.",
    "href": "ai_decision.html#the-potential-of-generative-ai-for-personalized-persuasion-at-scale.",
    "title": "Individual decision lit",
    "section": "The potential of generative AI for personalized persuasion at scale.",
    "text": "The potential of generative AI for personalized persuasion at scale.\nMatz, S. C., Teeny, J. D., Vaid, S. S., Peters, H., Harari, G. M., & Cerf, M. (2024). The potential of generative AI for personalized persuasion at scale. Scientific Reports, 14(1), 4692. https://doi.org/10.1038/s41598-024-53755-0\n\n\nAbstract\n\n\nMatching the language or content of a message to the psychological profile of its recipient (known as “personalized persuasion”) is widely considered to be one of the most effective messaging strategies. We demonstrate that the rapid advances in large language models (LLMs), like ChatGPT, could accelerate this influence by making personalized persuasion scalable. Across four studies (consisting of seven sub-studies; total N = 1788), we show that personalized messages crafted by ChatGPT exhibit significantly more influence than non-personalized messages. This was true across different domains of persuasion (e.g., marketing of consumer products, political appeals for climate action), psychological profiles (e.g., personality traits, political ideology, moral foundations), and when only providing the LLM with a single, short prompt naming or describing the targeted psychological dimension. Thus, our findings are among the first to demonstrate the potential for LLMs to automate, and thereby scale, the use of personalized persuasion in ways that enhance its effectiveness and efficiency. We discuss the implications for researchers, practitioners, and the general public.\n\n\n\n\n\nFigure from Matz et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#decision-making-paradoxes-in-humans-vs-machines-the-case-of-the-allais-and-ellsberg-paradoxes.",
    "href": "ai_decision.html#decision-making-paradoxes-in-humans-vs-machines-the-case-of-the-allais-and-ellsberg-paradoxes.",
    "title": "Individual decision lit",
    "section": "Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes.",
    "text": "Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes.\nNobandegani, A. S., Rish, I., & Shultz, T. R. (2023). Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes. Proceedings of the Annual Meeting of the Cognitive Science Society, 46. https://arxiv.org/abs/2406.11426\n\n\nAbstract\n\n\nHuman decision-making is filled with a variety of paradoxes demonstrating deviations from rationality principles. Do state-of-the-art artificial intelligence (AI) models also manifest these paradoxes when making decisions? As a case study, in this work we investigate whether GPT-4, a recently released state-of-the-art language model, would show two well-known paradoxes in human decision-making: the Allais paradox and the Ellsberg paradox. We demonstrate that GPT-4 succeeds in the two variants of the Allais paradox (the common-consequence effect and the common-ratio effect) but fails in the case of the Ellsberg paradox. We also show that providing GPT-4 with high-level normative principles allows it to succeed in the Ellsberg paradox, thus elevating GPT-4’s decision-making rationality. We discuss the implications of our work for AI rationality enhancement and AI-assisted decision-making.\n\n\nNobandegani et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#do-llms-exhibit-human-like-response-biases-a-case-study-in-survey-design.",
    "href": "ai_decision.html#do-llms-exhibit-human-like-response-biases-a-case-study-in-survey-design.",
    "title": "Individual decision lit",
    "section": "Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design.",
    "text": "Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design.\nTjuatja, L., Chen, V., Wu, T., Talwalkwar, A., & Neubig, G. (2024). Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design. Transactions of the Association for Computational Linguistics, 12, 1011–1026. https://doi.org/10.1162/tacl_a_00685\n\n\nAbstract\n\n\nOne widely cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording—but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of “prompts” have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior.\n\n\n\n\n\nFigure from Tjuatja et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#cognitive-ease-at-a-cost-llms-reduce-mental-effort-but-compromise-depth-in-student-scientific-inquiry",
    "href": "ai_decision.html#cognitive-ease-at-a-cost-llms-reduce-mental-effort-but-compromise-depth-in-student-scientific-inquiry",
    "title": "Individual decision lit",
    "section": "Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry",
    "text": "Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry\nStadler, M., Bannert, M., & Sailer, M. (2024). Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry. Computers in Human Behavior, 160, 108386. https://doi.org/10.1016/j.chb.2024.108386\n\n\nAbstract\n\n\nThis study explores the cognitive load and learning outcomes associated with using large language models (LLMs) versus traditional search engines for information gathering during learning. A total of 91 university students were randomly assigned to either use ChatGPT3.5 or Google to research the socio-scientific issue of nanoparticles in sunscreen to derive valid recommendations and justifications. The study aimed to investigate potential differences in cognitive load, as well as the quality and homogeneity of the students’ recommendations and justifications. Results indicated that students using LLMs experienced significantly lower cognitive load. However, despite this reduction, these students demonstrated lower-quality reasoning and argumentation in their final recommendations compared to those who used traditional search engines. Further, the homogeneity of the recommendations and justifications did not differ significantly between the two groups, suggesting that LLMs did not restrict the diversity of students’ perspectives. These findings highlight the nuanced implications of digital tools on learning, suggesting that while LLMs can decrease the cognitive burden associated with information gathering during a learning task, they may not promote deeper engagement with content necessary for high-quality learning per se.\n\n\nStadler et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#cognitive-llms-towards-integrating-cognitive-architectures-and-large-language-models-for-manufacturing-decision-making",
    "href": "ai_decision.html#cognitive-llms-towards-integrating-cognitive-architectures-and-large-language-models-for-manufacturing-decision-making",
    "title": "Individual decision lit",
    "section": "Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making",
    "text": "Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making\nWu, S., Oltramari, A., Francis, J., Giles, C. L., & Ritter, F. E. (2024). Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making (arXiv:2408.09176). arXiv. http://arxiv.org/abs/2408.09176\n\n\nAbstract\n\n\nResolving the dichotomy between the human-like yet constrained reasoning processes of Cognitive Architectures and the broad but often noisy inference behavior of Large Language Models (LLMs) remains a challenging but exciting pursuit, for enabling reliable machine reasoning capabilities in production systems. Because Cognitive Architectures are famously developed for the purpose of modeling the internal mechanisms of human cognitive decision-making at a computational level, new investigations consider the goal of informing LLMs with the knowledge necessary for replicating such processes, e.g., guided perception, memory, goal-setting, and action. Previous approaches that use LLMs for grounded decision-making struggle with complex reasoning tasks that require slower, deliberate cognition over fast and intuitive inference—reporting issues related to the lack of sufficient grounding, as in hallucination. To resolve these challenges, we introduce LLM-ACTR, a novel neurosymbolic architecture that provides human-aligned and versatile decision-making by integrating the ACT-R Cognitive Architecture with LLMs. Our framework extracts and embeds knowledge of ACT-R’s internal decision-making process as latent neural representations, injects this information into trainable LLM adapter layers, and fine-tunes the LLMs for downstream prediction. Our experiments on novel Design for Manufacturing tasks show both improved task performance as well as improved grounded decision-making capability of our approach, compared to LLM-only baselines that leverage chain-of-thought reasoning strategies.",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#large-language-models-amplify-human-biases-in-moral-decision-making",
    "href": "ai_decision.html#large-language-models-amplify-human-biases-in-moral-decision-making",
    "title": "Individual decision lit",
    "section": "Large Language Models Amplify Human Biases in Moral Decision-Making",
    "text": "Large Language Models Amplify Human Biases in Moral Decision-Making\nCheung, V., Maier, M., & Lieder, F. (2024). Large Language Models Amplify Human Biases in Moral Decision-Making (https://osf.io/3kvjd/). https://doi.org/10.31234/osf.io/aj46b\n\n\nAbstract\n\n\nAs large language models (LLMs) become more widely used, people increasingly rely on them to make or advise on moral decisions. Some researchers even propose using LLMs as participants in psychology experiments. It is therefore important to understand how well LLMs make moral decisions and how they compare to humans. We investigated this question in realistic moral dilemmas using prompts where GPT-4, Llama 3, and Claude 3 give advice and where they emulate a research participant. In Study 1, we compared responses from LLMs to a representative US sample (N = 285) for 22 dilemmas: social dilemmas that pitted self-interest against the greater good, and moral dilemmas that pitted utilitarian cost-benefit reasoning against deontological rules. In social dilemmas, LLMs were more altruistic than participants. In moral dilemmas, LLMs exhibited stronger omission bias than participants: they usually endorsed inaction over action. In Study 2 (N = 490, preregistered), we replicated this omission bias and document an additional bias: unlike humans, LLMs (except GPT-4o) tended to answer “no” in moral dilemmas, whereby the phrasing of the question influences the decision even when physical action remains the same. Our findings show that LLM moral decision-making amplifies human biases and introduces potentially problematic biases.\n\n\n\n\n\nFigure from Cheung et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "Driving.html",
    "href": "Driving.html",
    "title": "Driving Lit",
    "section": "",
    "text": "Basu, C., & Singhal, M. (n.d.). Trust Dynamics in Human Autonomous Vehicle Interaction: A Review of Trust Models.\nBrunyé, T. T., Gardony, A., Mahoney, C. R., & Taylor, H. A. (2012). Going to town: Visualized perspectives and navigation through virtual environments. Computers in Human Behavior, 28(1), 257–266. https://doi.org/10.1016/j.chb.2011.09.008\nBrunyé, T. T., Rapp, D. N., & Taylor, H. A. (2008). Representational flexibility and specificity following spatial descriptions of real-world environments. Cognition, 108(2), 418–443. https://doi.org/10.1016/j.cognition.2008.03.005\nCui, C., Ma, Y., Cao, X., Ye, W., Zhou, Y., Liang, K., Chen, J., Lu, J., Yang, Z., Liao, K.-D., Gao, T., Li, E., Tang, K., Cao, Z., Zhou, T., Liu, A., Yan, X., Mei, S., Cao, J., … Zheng, C. (2024). A Survey on Multimodal Large Language Models for Autonomous Driving. 2024 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), 958–979. https://doi.org/10.1109/WACVW60836.2024.00106\nDíaz-Álvarez, A., Clavijo, M., Jiménez, F., Talavera, E., & Serradilla, F. (2018). Modelling the human lane-change execution behaviour through Multilayer Perceptrons and Convolutional Neural Networks. Transportation Research Part F: Traffic Psychology and Behaviour, 56, 134–148. https://doi.org/10.1016/j.trf.2018.04.004\nEisma, Y. B., Eijssen, D. J., & de Winter, J. C. F. (2022). What Attracts the Driver’s Eye? Attention as a Function of Task and Events. Information, 13(7), Article 7. https://doi.org/10.3390/info13070333\nFajen, B. R. (2005). The Scaling of Information to Action in Visually Guided Braking. Journal of Experimental Psychology: Human Perception and Performance, 31(5), 1107–1123. https://doi.org/10.1037/0096-1523.31.5.1107\nHe, J., McCarley, J. S., & Kramer, A. F. (2014). Lane Keeping Under Cognitive Load: Performance Changes and Mechanisms. Human Factors, 56(2), 414–426. https://doi.org/10.1177/0018720813485978\nHills, T. T., & Kenett, Y. N. (n.d.). Is the Mind a Network? Maps, Vehicles, and Skyhooks in Cognitive Network Science. Topics in Cognitive Science, n/a(n/a). https://doi.org/10.1111/tops.12570\nHowell, W. C., & Kerkar, S. P. (1982). A test of task influences in uncertainty measurement. Organizational Behavior and Human Performance, 30(3), 365–390. https://doi.org/10.1016/0030-5073(82)90226-4\nHuang, S. H., Held, D., Abbeel, P., & Dragan, A. D. (2019). Enabling robots to communicate their objectives. Autonomous Robots, 43(2), 309–326. https://doi.org/10.1007/s10514-018-9771-0\nHuo, D., Ma, J., & Chang, R. (2020). Lane-changing-decision characteristics and the allocation of visual attention of drivers with an angry driving style. Transportation Research Part F: Traffic Psychology and Behaviour, 71, 62–75. https://doi.org/10.1016/j.trf.2020.03.008\nJiang, L., Chen, D., Li, Z., & Wang, Y. (2022). Risk Representation, Perception, and Propensity in an Integrated Human Lane-Change Decision Model. IEEE Transactions on Intelligent Transportation Systems, 23(12), 23474–23487. IEEE Transactions on Intelligent Transportation Systems. https://doi.org/10.1109/TITS.2022.3207182\nKamaruddin, N., Abdul Rahman, A. W., Mohamad Halim, K. I., & Mohd Noh, M. H. I. (2018). Driver Behaviour State Recognition based on Speech. TELKOMNIKA (Telecommunication Computing Electronics and Control), 16(2), 852. https://doi.org/10.12928/telkomnika.v16i2.8416\nKolekar, S., de Winter, J., & Abbink, D. (2017). A human-like steering model: Sensitive to uncertainty in the environment. 2017 IEEE International Conference on Systems, Man, and Cybernetics (SMC), 1487–1492. https://doi.org/10.1109/SMC.2017.8122824\nKolekar, S., de Winter, J., & Abbink, D. (2020). Human-like driving behaviour emerges from a risk-based driver model. Nature Communications, 11(1), Article 1. https://data.4tu.nl/articles/dataset/Driver_s_Risk_Fields_DRF_-_Model_data/12705950/1. https://doi.org/10.1038/s41467-020-18353-4\nKujanpää, K., Baimukashev, D., Zhu, S., Azam, S., Munir, F., Alcan, G., & Kyrki, V. (2024). Challenges of Data-Driven Simulation of Diverse and Consistent Human Driving Behaviors (arXiv:2401.03236). arXiv. http://arxiv.org/abs/2401.03236\nLappi, O. (2022). Gaze Strategies in Driving–An Ecological Approach. Frontiers in Psychology, 13. https://doi.org/10.3389/fpsyg.2022.821440\nLiang, Y., Reyes, M. L., & Lee, J. D. (2007). Real-Time Detection of Driver Cognitive Distraction Using Support Vector Machines. IEEE Transactions on Intelligent Transportation Systems, 8(2), 340–350. IEEE Transactions on Intelligent Transportation Systems. https://doi.org/10.1109/TITS.2007.895298\nLiu, R., Zhao, X., Yuan, T., Li, H., Bu, T., Zhu, X., & Ma, J. (2024). A human-like response model for following vehicles in lane-changing scenario. Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering, 238(4), 760–773. https://doi.org/10.1177/09544070221135384\nLu, C., Lu, H., Chen, D., Wang, H., Li, P., & Gong, J. (2023). Human-like decision making for lane change based on the cognitive map and hierarchical reinforcement learning. Transportation Research Part C: Emerging Technologies, 156, 104328. https://doi.org/10.1016/j.trc.2023.104328\nMamo, W. G., Alhajyaseen, W. K. M., Brijs, K., Dirix, H., Vanroelen, G., Hussain, Q., Brijs, T., & Ross, V. (2024). The impact of cognitive load on a lane change task (LCT) among male autistic individuals: A driving simulator study. Transportation Research Part F: Traffic Psychology and Behaviour, 106, 27–43. https://doi.org/10.1016/j.trf.2024.07.030\nMarkkula, G., Boer, E., Romano, R., & Merat, N. (2018a). Sustained sensorimotor control as intermittent decisions about prediction errors: Computational framework and application to ground vehicle steering. Biological Cybernetics, 112(3), 181–207. https://doi.org/10.1007/s00422-017-0743-9\nMarkkula, G., Boer, E., Romano, R., & Merat, N. (2018b). Sustained sensorimotor control as intermittent decisions about prediction errors: Computational framework and application to ground vehicle steering. Biological Cybernetics, 112(3), 181–207. https://doi.org/10.1007/s00422-017-0743-9\nMeir, A., & Oron-Gilad, T. (2020). Understanding complex traffic road scenes: The case of child-pedestrians’ hazard perception. Journal of Safety Research, 72, 111–126. https://doi.org/10.1016/j.jsr.2019.12.014\nMerat, N., Jamson, A. H., Lai, F. C. H., & Carsten, O. (2012). Highly Automated Driving, Secondary Task Performance, and Driver State. Human Factors, 54(5), 762–771. https://doi.org/10.1177/0018720812442087\nMuslim, H., Itoh, M., Liang, C. K., Antona-Makoshi, J., & Uchida, N. (2021). Effects of gender, age, experience, and practice on driver reaction and acceptance of traffic jam chauffeur systems. Scientific Reports, 11(1), 17874. https://doi.org/10.1038/s41598-021-97374-5\nPekkanen, J., Lappi, O., Rinkkala, P., Tuhkanen, S., Frantsi, R., & Summala, H. (2018). A computational model for driver’s cognitive state, visual perception and intermittent attention in a distracted car following task. Royal Society Open Science, 5(9), 180194. https://doi.org/10.1098/rsos.180194\nPing, P., Sheng, Y., Qin, W., Miyajima, C., & Takeda, K. (2018). Modeling Driver Risk Perception on City Roads Using Deep Learning. IEEE Access, 6, 68850–68866. IEEE Access. https://doi.org/10.1109/ACCESS.2018.2879887\nRecarte, M. A., & Nunes, L. M. (2003). Mental workload while driving: Effects on visual search, discrimination, and decision making. Journal of Experimental Psychology: Applied, 9(2), 119–137. https://doi.org/10.1037/1076-898X.9.2.119\nReimer, B., Donmez, B., Lavallière, M., Mehler, B., Coughlin, J. F., & Teasdale, N. (2013). Impact of age and cognitive demand on lane choice and changing under actual highway conditions. Accident Analysis & Prevention, 52, 125–132. https://doi.org/10.1016/j.aap.2012.12.008\nRondora, M. E. S., Pirdavani, A., & Larocca, A. P. C. (2022). Driver Behavioral Classification on Curves Based on the Relationship between Speed, Trajectories, and Eye Movements: A Driving Simulator Study. Sustainability, 14(10), Article 10. https://doi.org/10.3390/su14106241\nSalvucci, D. D. (2006). Modeling Driver Behavior in a Cognitive Architecture. Human Factors, 48(2), 362–380. https://doi.org/10.1518/001872006777724417\nSalvucci, D. D., Mandalia, H. M., Kuge, N., & Yamamura, T. (2007). Lane-Change Detection Using a Computational Driver Model. Human Factors, 49(3), 532–542. https://doi.org/10.1518/001872007X200157\nSantoso, G. P., & Maulina, D. (2019). Human errors in traffic accidents: Differences between car drivers and motorcyclists’ experience. Psychological Research on Urban Society, 2(2), 118. https://doi.org/10.7454/proust.v2i2.69\nShimada, H., Uemura, K., Makizako, H., Doi, T., Lee, S., & Suzuki, T. (2016). Performance on the flanker task predicts driving cessation in older adults. International Journal of Geriatric Psychiatry, 31(2), 169–175. https://doi.org/10.1002/gps.4308\nSrinivasan, A. R., Hasan, M., Lin, Y.-S., Leonetti, M., Billington, J., Romano, R., & Markkula, G. (2021). Comparing merging behaviors observed in naturalistic data with behaviors generated by a machine learned model. 2021 IEEE International Intelligent Transportation Systems Conference (ITSC), 3787–3792. https://doi.org/10.1109/ITSC48978.2021.9564791\nTan, X., & Zhang, Y. (2024). A Computational Cognitive Model of Driver Response Time for Scheduled Freeway Exiting Takeovers in Conditionally Automated Vehicles. Human Factors, 66(5), 1583–1599. https://doi.org/10.1177/00187208221143028\nTango, F., & Botta, M. (2013). Real-Time Detection System of Driver Distraction Using Machine Learning. IEEE Transactions on Intelligent Transportation Systems, 14(2), 894–905. IEEE Transactions on Intelligent Transportation Systems. https://doi.org/10.1109/TITS.2013.2247760\nTawari, A., & Kang, B. (2017). A computational framework for driver’s visual attention using a fully convolutional architecture. 2017 IEEE Intelligent Vehicles Symposium (IV), 887–894. https://doi.org/10.1109/IVS.2017.7995828\nTsirtsis, S., Rodriguez, M. G., & Gerstenberg, T. (2024). Towards a computational model of responsibility judgments in sequential human-AI collaboration. https://doi.org/10.31234/osf.io/m4yad\nWang, B., Duan, H., Feng, Y., Chen, X., Fu, Y., Mo, Z., & Di, X. (2024). Can LLMs Understand Social Norms in Autonomous Driving Games? (arXiv:2408.12680). arXiv. http://arxiv.org/abs/2408.12680\nWang, W., Zhao, D., Han, W., & Xi, J. (2018). A Learning-Based Approach for Lane Departure Warning Systems With a Personalized Driver Model. IEEE Transactions on Vehicular Technology, 67(10), 9145–9157. IEEE Transactions on Vehicular Technology. https://doi.org/10.1109/TVT.2018.2854406\nWood, K., & Simons, D. J. (2019). The spatial allocation of attention in an interactive environment. Cognitive Research: Principles and Implications, 4(1), 13. https://doi.org/10.1186/s41235-019-0164-5\nXing, Y., Lv, C., Cao, D., & Hang, P. (2021). Toward human-vehicle collaboration: Review and perspectives on human-centered collaborative automated driving. Transportation Research Part C: Emerging Technologies, 128, 103199. https://doi.org/10.1016/j.trc.2021.103199\nXing, Y., Lv, C., Wang, H., Wang, H., Ai, Y., Cao, D., Velenis, E., & Wang, F.-Y. (2019). Driver Lane Change Intention Inference for Intelligent Vehicles: Framework, Survey, and Challenges. IEEE Transactions on Vehicular Technology, 68(5), 4377–4390. IEEE Transactions on Vehicular Technology. https://doi.org/10.1109/TVT.2019.2903299\nYue, J., Manocha, D., & Wang, H. (2022). Human Trajectory Prediction via Neural Social Physics. In S. Avidan, G. Brostow, M. Cissé, G. M. Farinella, & T. Hassner (Eds.), Computer Vision – ECCV 2022 (Vol. 13694, pp. 376–394). Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-19830-4_22\nZgonnikov, A., Abbink, D., & Markkula, G. (2024). Should I Stay or Should I Go? Cognitive Modeling of Left-Turn Gap Acceptance Decisions in Human Drivers. Human Factors, 66(5), 1399–1413. https://doi.org/10.1177/00187208221144561\nZhao, D., Lam, H., Peng, H., Bao, S., LeBlanc, D. J., Nobukawa, K., & Pan, C. S. (2017). Accelerated Evaluation of Automated Vehicles Safety in Lane-Change Scenarios Based on Importance Sampling Techniques. IEEE Transactions on Intelligent Transportation Systems, 18(3), 595–607. IEEE Transactions on Intelligent Transportation Systems. https://doi.org/10.1109/TITS.2016.2582208",
    "crumbs": [
      "Misc",
      "Driving Lit"
    ]
  },
  {
    "objectID": "Samuel_Project.html",
    "href": "Samuel_Project.html",
    "title": "Samuel’s Project",
    "section": "",
    "text": "Click on one of the tasks to activate, then use the arrow keys to control the car\n\n\n\n\n\n\nSam’s original mockup",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#original",
    "href": "Samuel_Project.html#original",
    "title": "Samuel’s Project",
    "section": "Original",
    "text": "Original\n\n\n\n\nSam’s original mockup",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#tasks-used-in-other-studes",
    "href": "Samuel_Project.html#tasks-used-in-other-studes",
    "title": "Samuel’s Project",
    "section": "Tasks used in Other Studes",
    "text": "Tasks used in Other Studes\n\nStructuring Knowledge with Cognitive Maps and Cognitive Graphs.\nPeer, M., Brunec, I. K., Newcombe, N. S., & Epstein, R. A. (2021). Structuring Knowledge with Cognitive Maps and Cognitive Graphs. Trends in Cognitive Sciences, 25(1), 37–54. https://doi.org/10.1016/j.tics.2020.10.004\n\n\nAbstract\n\n\nHumans and animals use mental representations of the spatial structure of the world to navigate. The classical view is that these representations take the form of Euclidean cognitive maps, but alternative theories suggest that they are cognitive graphs consisting of locations connected by paths. We review evidence suggesting that both map-like and graph-like representations exist in the mind/brain that rely on partially overlapping neural systems. Maps and graphs can operate simultaneously or separately, and they may be applied to both spatial and nonspatial knowledge. By providing structural frameworks for complex information, cognitive maps and cognitive graphs may provide fundamental organizing schemata that allow us to navigate in physical, social, and conceptual spaces.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Figures from Peer et al. (2021)\n\n\n\n\n\n\nWormholes in virtual space: From cognitive maps to cognitive graphs\nWarren, W. H., Rothman, D. B., Schnapp, B. H., & Ericson, J. D. (2017). Wormholes in virtual space: From cognitive maps to cognitive graphs. Cognition, 166, 152–163. https://doi.org/10.1016/j.cognition.2017.05.020\n\n\nAbstract\n\n\nHumans and other animals build up spatial knowledge of the environment on the basis of visual information and path integration. We compare three hypotheses about the geometry of this knowledge of navigation space: (a) ‘cognitive map’ with metric Euclidean structure and a consistent coordinate system, (b) ‘topological graph’ or network of paths between places, and (c) ‘labelled graph’ incorporating local metric information about path lengths and junction angles. In two experiments, participants walked in a non-Euclidean environment, a virtual hedge maze containing two ‘wormholes’ that visually rotated and teleported them between locations. During training, they learned the metric locations of eight target objects from a ‘home’ location, which were visible individually. During testing, shorter wormhole routes to a target were preferred, and novel shortcuts were directional, contrary to the topological hypothesis. Shortcuts were strongly biased by the wormholes, with mean constant errors of 37° and 41° (45° expected), revealing violations of the metric postulates in spatial knowledge. In addition, shortcuts to targets near wormholes shifted relative to flanking targets, revealing ‘rips’ (86% of cases), ‘folds’ (91%), and ordinal reversals (66%) in spatial knowledge. Moreover, participants were completely unaware of these geometric inconsistencies, reflecting a surprising insensitivity to Euclidean structure. The probability of the shortcut data under the Euclidean map model and labelled graph model indicated decisive support for the latter (BFGM&gt;100). We conclude that knowledge of navigation space is best characterized by a labelled graph, in which local metric information is approximate, geometrically inconsistent, and not embedded in a common coordinate system. This class of ‘cognitive graph’ models supports route finding, novel detours, and rough shortcuts, and has the potential to unify a range of data on spatial navigation.\n\n\n\n\n\nFigure from Warren et al. 2017\n\n\n\n\n\nRoute effects in city-based survey knowledge estimates\nKrukar, J., Navas Medrano, S., & Schwering, A. (2023). Route effects in city-based survey knowledge estimates. Cognitive Processing, 24(2), 213–231. https://doi.org/10.1007/s10339-022-01122-0\n\n\nAbstract\n\n\nWhen studying wayfinding in urban environments, researchers are often interested in obtaining measures of participants’ survey knowledge, i.e., their estimate of distant locations relative to other places. Previous work showed that distance estimations are consistently biased when no direct route is available to the queried target or when participants follow a detour. Here we investigated whether a corresponding bias is manifested in two other popular measures of survey knowledge: a pointing task and a sketchmapping task. The aim of this study was to investigate whether there is a systematic bias in pointing/sketchmapping performance associated with the preferred route choice in an applied urban setting. The results were mixed. We found moderate evidence for the presence of a systematic bias, but only for a subset of urban locations. When two plausible routes to the target were available, survey knowledge estimates were significantly biased in the direction of the route chosen by the participant. When only one plausible route was available, we did not find a statistically significant pattern. The results may have methodological implications for spatial cognition studies in applied urban settings that might be obtaining systematically biased survey knowledge estimates at some urban locations. Researchers should be aware that the choice of urban locations from which pointing and sketchmapping are performed might systematically distort the results, in particular when two plausible but diverging routes to the target are visible from the location.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Figures from Krukar et al. (2023)\n\n\n\n\n\n\nSpatial decision dynamics during wayfinding: Intersections prompt the decision-making process.\nBrunyé, T. T., Gardony, A. L., Holmes, A., & Taylor, H. A. (2018). Spatial decision dynamics during wayfinding: Intersections prompt the decision-making process. Cognitive Research: Principles and Implications, 3(1), 13. https://doi.org/10.1186/s41235-018-0098-3\n\n\nAbstract\n\n\nIntersections are critical decision points for wayfinders, but it is unknown how decision dynamics unfold during pedestrian wayfinding. Some research implies that pedestrians leverage available visual cues to actively compare options while in an intersection, whereas other research suggests that people strive to make decisions long before overt responses are required. Two experiments examined these possibilities while participants navigated virtual desktop environments, assessing information-seeking behavior (Experiment 1) and movement dynamics (Experiment 2) while approaching intersections. In Experiment 1, we found that participants requested navigation guidance while in path segments approaching an intersection and the guidance facilitated choice behavior. In Experiment 2, we found that participants tended to orient themselves toward an upcoming turn direction before entering an intersection, particularly as they became more familiar with the environment. Some of these patterns were modulated by individual differences in spatial ability, sense of direction, spatial strategies, and gender. Together, we provide novel evidence that deciding whether to continue straight or turn involves a dynamic, distributed decision-making process that is prompted by upcoming intersections and modulated by individual differences and environmental experience. We discuss implications of these results for spatial decision-making theory and the development of innovative adaptive, beacon-based navigation guidance systems.\n\n\n\n\n\nFigure from Brunyé et al. (2018)\n\n\n\nEricson, J. D., & Warren, W. H. (2020). Probing the invariant structure of spatial knowledge: Support for the cognitive graph hypothesis. Cognition, 200, 104276. https://doi.org/10.1016/j.cognition.2020.104276\n\n\nAbstract\n\n\nWe tested four hypotheses about the structure of spatial knowledge used for navigation: (1) the Euclidean hypothesis, a geometrically consistent map; (2) the Neighborhood hypothesis, adjacency relations between spatial regions, based on visible boundaries; (3) the Cognitive Graph hypothesis, a network of paths between places, labeled with approximate local distances and angles; and (4) the Constancy hypothesis, whatever geometric properties are invariant during learning. In two experiments, different groups of participants learned three virtual hedge mazes, which varied specific geometric properties (Euclidean Control Maze, Elastic Maze with stretching paths, Swap Maze with alternating paths to the same place). Spatial knowledge was then tested using three navigation tasks (metric shortcuts on empty ground plane, neighborhood shortcuts with visible boundaries, route task in corridors). They yielded the following results: (a) Metric shortcuts were insensitive to detectable shifts in target location, inconsistent with the Euclidean hypothesis. (b) Neighborhood shortcuts were constrained by visible boundaries in the Elastic Maze, but not in the Swap Maze, contrary to the Neighborhood and Constancy hypotheses. (c) The route task indicated that a graph of the maze was acquired in all environments, including knowledge of local path lengths. We conclude that primary spatial knowledge is consistent with the Cognitive Graph hypothesis. Neighborhoods are derived from the graph, and local distance and angle information is not embedded in a geometrically consistent map.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Figures from Ericson & Warren (2020)\n\n\n\n\n\n\nRational use of cognitive resources in human planning\nCallaway, F., Van Opheusden, B., Gul, S., Das, P., Krueger, P. M., Griffiths, T. L., & Lieder, F. (2022). Rational use of cognitive resources in human planning. Nature Human Behaviour, 6(8), 1112–1125. https://doi.org/10.1038/s41562-022-01332-8\nlink to code   link to paper   link to live task demo  \n\n\n\nFigure from Callaway et al. (2022)\n\n\n\n\n\nPeople construct simplified mental representations to plan.\nHo, M. K., Abel, D., Correa, C. G., Littman, M. L., Cohen, J. D., & Griffiths, T. L. (2022). People construct simplified mental representations to plan. Nature, 1–8. https://doi.org/10.1038/s41586-022-04743-9\n\n\n\nFigure from Ho et al. 2022\n\n\nHo, M. K., Abel, D., Correa, C. G., Littman, M. L., Cohen, J. D., & Griffiths, T. L. (2021). Control of mental representations in human planning. arXiv:2105.06948 [Cs]. http://arxiv.org/abs/2105.06948\n\n\n\nFigure from Ho et al. (2021)\n\n\n\n\n\nEmergent Collective Sensing in Human Groups.\nKrafft, P. M., Hawkins, R. X., Pentland, A., Goodman, N. D., & Tenenbaum, J. B. (2015). Emergent Collective Sensing in Human Groups. In CogSci. https://people.csail.mit.edu/pkrafft/papers/krafft-et-al-2015-emergent.pdf\n\n\n\nFigure from Krafft (2015)\n\n\n\n\n\nTowards a computational model of responsibility judgments in sequential human-AI collaboration\nTsirtsis, S., Gomez Rodriguez, M., & Gerstenberg, T. (2024). Towards a computational model of responsibility judgments in sequential human-AI collaboration. In Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 46). https://osf.io/preprints/psyarxiv/m4yad\n\n\n\nFigure from Tsirtsis et al. (2024)\n\n\n\n\n\nSpatial planning with long visual range benefits escape from visual predators in complex naturalistic environments.\nMugan, U., & MacIver, M. A. (2020). Spatial planning with long visual range benefits escape from visual predators in complex naturalistic environments. Nature Communications, 11(1), 3057. https://doi.org/10.1038/s41467-020-16102-1\nlive task demo\ncode repository\n\n\n\n\nFigure from Mugan & MacIver (2020)\n\n\n\n\n\nPattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets.\nLekschas, F., Behrisch, M., Bach, B., Kerpedjiev, P., Gehlenborg, N., & Pfister, H. (2020). Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets. IEEE Transactions on Visualization and Computer Graphics, 26(1), 611–621. IEEE Transactions on Visualization and Computer Graphics. https://doi.org/10.1109/TVCG.2019.2934555\nlink to code on github\nproject page\n\n\nAbstract\n\n\nWe present Scalable Insets, a technique for interactively exploring and navigating large numbers of annotated patterns in multiscale visualizations such as gigapixel images, matrices, or maps. Exploration of many but sparsely-distributed patterns in multiscale visualizations is challenging as visual representations change across zoom levels, context and navigational cues get lost upon zooming, and navigation is time consuming. Our technique visualizes annotated patterns too small to be identifiable at certain zoom levels using insets, i.e., magnified thumbnail views of the annotated patterns. Insets support users in searching, comparing, and contextualizing patterns while reducing the amount of navigation needed. They are dynamically placed either within the viewport or along the boundary of the viewport to offer a compromise between locality and context preservation. Annotated patterns are interactively clustered by location and type. They are visually represented as an aggregated inset to provide scalable exploration within a single viewport. In a controlled user study with 18 participants, we found that Scalable Insets can speed up visual search and improve the accuracy of pattern comparison at the cost of slower frequency estimation compared to a baseline technique. A second study with 6 experts in the field of genomics showed that Scalable Insets is easy to learn and provides first insights into how Scalable Insets can be applied in an open-ended data exploration scenario.\n\n\n\n\n\nFigure from Lekschas et al. (2020)\n\n\n\n\nPersonality Traits and Spatial Skills Are Related to Group Dynamics and Success During Collective Wayfinding\nBrunyé, T. T., Hendel, D., Gardony, A. L., Hussey, E. K., & Taylor, H. A. (2024). Personality traits and spatial skills are related to group dynamics and success during collective wayfinding. Collective Spatial Cognition, 60-99.\n\n\nAbstract\n\n\nThis chapter reviews and identifies gaps in research examining collective navigation and describes the results of a small study aimed at better elucidating the independent and interactive roles of personality and spatial skill in guiding group wayfinding dynamics, wayfinding performance, and spatial memory. In this study, individuals, dyads, and triads completed a series of individual differences tasks and questionnaires, and an individual or shared (dyads and triads) virtual wayfinding experience involving planning and executing routes between origin and destination pairs. Navigators were provided with a single digital map that they could share during the task; patterns of map sharing, virtual navigation, and wayfinding performance were logged. Higher spatial anxiety was associated with more map viewing among group members, higher scores on questionnaires assessing autism-type traits were associated with lower group cohesion, higher group heterogeneity was associated with lower group cohesion and lower path efficiency, and triads tended to have poorer memory for the location of goal locations relative to individuals and dyads. Results speak to the inherent complexity and dynamics of collective navigation, the need for understanding individual differences in guiding group behavior, and the value of continuing research in this domain.\n\n\nBrunyé et al. (2023)\n\n\n\nWayfinding in pairs: Comparing the planning and navigation performance of dyads and individuals in a real-world environment.\nBae, C., Montello, D., & Hegarty, M. (2024). Wayfinding in pairs: Comparing the planning and navigation performance of dyads and individuals in a real-world environment. Cognitive Research: Principles and Implications, 9(1), 40. https://doi.org/10.1186/s41235-024-00563-9\n\n\nAbstract\n\n\nNavigation is essential to life, and it is cognitively complex, drawing on abilities such as prospective and situated planning, spatial memory, location recognition, and real-time decision-making. In many cases, day-to-day navigation is embedded in a social context where cognition and behavior are shaped by others, but the great majority of existing research in spatial cognition has focused on individuals. The two studies we report here contribute to our understanding of social wayfinding, assessing the performance of paired and individual navigators on a real-world wayfinding task in which they were instructed to minimize time and distance traveled. In the first study, we recruited 30 pairs of friends (familiar dyads); in the second, we recruited 30 solo participants (individuals). We compare the two studies to the results of an earlier study of 30 pairs of strangers (unfamiliar dyads). We draw out differences in performance with respect to spatial, social, and cognitive considerations. Of the three conditions, solo participants were least successful in reaching the destination accurately on their initial attempt. Friends traveled more efficiently than either strangers or individuals. Working with a partner also appeared to lend confidence to wayfinders: dyads of either familiarity type were more persistent than individuals in the navigation task, even after encountering challenges or making incorrect attempts. Route selection was additionally impacted by route complexity and unfamiliarity with the study area. Navigators explicitly used ease of remembering as a planning criterion, and the resulting differences in route complexity likely influenced success during enacted navigation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Figures from Bae et al. (2024)\n\n\n\n\n\n\nIndividual and collective foraging in autonomous search agents with human intervention\nSchloesser, D. S., Hollenbeck, D., & Kello, C. T. (2021). Individual and collective foraging in autonomous search agents with human intervention. Scientific Reports, 11(1), Article 1. https://doi.org/10.1038/s41598-021-87717-7\n\n\nAbstract\n\n\nHumans and other complex organisms exhibit intelligent behaviors as individual agents and as groups of coordinated agents. They can switch between independent and collective modes of behavior, and flexible switching can be advantageous for adapting to ongoing changes in conditions. In the present study, we investigated the flexibility between independent and collective modes of behavior in a simulated social foraging task designed to benefit from both modes: distancing among ten foraging agents promoted faster detection of resources, whereas flocking promoted faster consumption. There was a tradeoff between faster detection versus faster consumption, but both factors contributed to foraging success. Results showed that group foraging performance among simulated agents was enhanced by loose coupling that balanced distancing and flocking among agents and enabled them to fluidly switch among a variety of groupings. We also examined the effects of more sophisticated cognitive capacities by studying how human players improve performance when they control one of the search agents. Results showed that human intervention further enhanced group performance with loosely coupled agents, and human foragers performed better when coordinating with loosely coupled agents. Humans players adapted their balance of independent versus collective search modes in response to the dynamics of simulated agents, thereby demonstrating the importance of adaptive flexibility in social foraging.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigures from Schloesser et al. (2021)\n\n\n\n\n\n\nAlharbi, A. H., Khafaga, D. S., El-kenawy, E.-S. M., Eid, M. M., Ibrahim, A., Abualigah, L., Khodadadi, N., & Abdelhamid, A. A. (2024). Optimizing electric vehicle paths to charging stations using parallel greylag goose algorithm and Restricted Boltzmann Machines. Frontiers in Energy Research, 12. https://doi.org/10.3389/fenrg.2024.1401330\nGarg, K., Kello, C. T., & Smaldino, P. E. (2022). Individual exploration and selective social learning: Balancing exploration–exploitation trade-offs in collective foraging. Journal of The Royal Society Interface, 19(189), 20210915. https://doi.org/10.1098/rsif.2021.0915\nMezey, D., Deffner, D., Kurvers, R. H. J. M., & Romanczuk, P. (2024). Visual social information use in collective foraging. PLOS Computational Biology, 20(5), e1012087. https://doi.org/10.1371/journal.pcbi.1012087",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "ai_gd.html",
    "href": "ai_gd.html",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Tessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., & Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science, 386(6719), eadq2852. https://doi.org/10.1126/science.adq2852\n\n\nAbstract\n\n\nFinding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.\n\n\n\n\n\n\n\n\n\nFigure 1: Figures from Tessler et al. (2024)\n\n\n\n\n\n\nMarjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit. https://cocosci.princeton.edu/papers/marjieh2024task.pdf\n\n\nAbstract\n\n\nHumans rely on efficient distribution of resources to transcend the abilities of individuals. Successful task allocation, whether in small teams or across large institutions, depends on individuals’ ability to discern their own and others’ strengths and weaknesses, and to optimally act on them. This dependence creates a tension between exploring the capabilities of others and exploiting the knowledge acquired so far, which can be challenging. How do people navigate this tension? To address this question, we propose a novel task allocation paradigm in which a human agent is asked to repeatedly allocate tasks in three distinct classes (categorizing a blurry image, detecting a noisy voice command, and solving an anagram) between themselves and two other (bot) team members to maximize team performance. We show that this problem can be recast as a combinatorial multi-armed bandit which allows us to compare people’s performance against two well-known strategies, Thompson Sampling and Upper Confidence Bound (UCB). We find that humans are able to successfully integrate information about the capabilities of different team members to infer optimal allocations, and in some cases perform on par with these optimal strategies. Our approach opens up new avenues for studying the mechanisms underlying collective cooperation in teams.\n\n\n\n\n\nFigure from Marjieh et al. (2024)\n\n\n\n\n\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nAbstract\n\n\nIn this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team’s ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team’s transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond.\n\n\n\n\n\nFigure from Bienefeld et al. (2023)\n\n\n\n\n\n\n\n\nGao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., & Li, Y. (2024). Large language models empowered agent-based modeling and simulation: A survey and perspectives. Humanities and Social Sciences Communications, 11(1), 1–24. https://doi.org/10.1057/s41599-024-03611-3\n\n\nAbstract\n\n\nAgent-based modeling and simulation have evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Recently, integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, discussing their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments, and how these works address the above challenges. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/LLM-Agent-Based-Modeling-and-Simulation.\n\n\n\n\n\nFigure from C. Gao et al. (2024)\n\n\n\n\n\nCollins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building machines that learn and think with people. Nature Human Behaviour, 8(10), 1851–1863. https://doi.org/10.1038/s41562-024-01991-9\n\n\nAbstract\n\n\nWhat do we want from machine intelligence? We envision machines that are not just tools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable, and trustworthy systems that think with us. Current artificial intelligence (AI) systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called “thought partners,” systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and AI thought partners can engage and propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Figures from Collins et al. (2024)\n\n\n\n\n\n\n\n\nDu, Y., Rajivan, P., & Gonzalez, C. C. (2024). Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making. https://escholarship.org/uc/item/6s060914\n\n\nAbstract\n\n\nLarge Language models (LLM) exhibit human-like proficiency in various tasks such as translation, question answering, essay writing, and programming. Emerging research explores the use of LLMs in collective problem-solving endeavors, such as tasks where groups try to uncover clues through discussions. Although prior work has investigated individual problem-solving tasks, leveraging LLM-powered agents for group consensus and decision-making remains largely unexplored. This research addresses this gap by (1) proposing an algorithm to enable free-form conversation in groups of LLM agents, (2) creating metrics to evaluate the human-likeness of the generated dialogue and problem-solving performance, and (3) evaluating LLM agent groups against human groups using an open source dataset. Our results reveal that LLM groups outperform human groups in problem-solving tasks. LLM groups also show a greater improvement in scores after participating in free discussions. In particular, analyses indicate that LLM agent groups exhibit more disagreements, complex statements, and a propensity for positive statements compared to human groups. The results shed light on the potential of LLMs to facilitate collective reasoning and provide insight into the dynamics of group interactions involving synthetic LLM agents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Figure from Du et al. (2024)\n\n\n\n\n\n\nHao, X., Demir, E., & Eyers, D. (2024). Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction. Technology in Society, 78, 102662. https://doi.org/10.1016/j.techsoc.2024.102662\n\n\nAbstract\n\n\nThis paper explores the effects of integrating Generative Artificial Intelligence (GAI) into decision-making processes within organizations, employing a quasi-experimental pretest-posttest design. The study examines the synergistic interaction between Human Intelligence (HI) and GAI across four group decision-making scenarios within three global organizations renowned for their cutting-edge operational techniques. The research progresses through several phases: identifying research problems, collecting baseline data on decision-making, implementing AI interventions, and evaluating the outcomes post-intervention to identify shifts in performance. The results demonstrate that GAI effectively reduces human cognitive burdens and mitigates heuristic biases by offering data-driven support and predictive analytics, grounded in System 2 reasoning. This is particularly valuable in complex situations characterized by unfamiliarity and information overload, where intuitive, System 1 thinking is less effective. However, the study also uncovers challenges related to GAI integration, such as potential over-reliance on technology, intrinsic biases particularly ‘out-of-the-box’ thinking without contextual creativity. To address these issues, this paper proposes an innovative strategic framework for HI-GAI collaboration that emphasizes transparency, accountability, and inclusiveness.\n\n\n\n\n\nFigure from Hao et al. (2024)\n\n\n\n\n\n\n\n\nBurton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). How large language models can reshape collective intelligence. Nature Human Behaviour, 1–13. https://doi.org/10.1038/s41562-024-01959-9\n\n\nAbstract\n\n\nCollective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals—even experts—resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the ‘wisdom of crowds’, online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans’ ability to collectively tackle complex problems.\n\n\n\n\n\nBurton et al. (2024)\n\n\n\n\n\n\n\nMa, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., & Ma, X. (2024). Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making (arXiv:2403.16812). arXiv. http://arxiv.org/abs/2403.16812\n\n\nAbstract\n\n\nIn AI-assisted decision-making, humans often passively review AI’s suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional explainable AI (XAI) assistants in improving humans’ appropriate reliance and task performance. Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Figure from Ma et al. (2024)\n\n\n\n\n\n\nChiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nAbstract\n\n\nGroup decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil’s advocate in the AI-assisted group deci- sion making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities ex- hibited by modern large language models (LLMs), we design four different styles of devil’s advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil’s advocates that argue against the AI model’s decision recommenda- tion have the potential to promote groups’ appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil’s advocate usually does not lead to substantial increases in people’s perceived workload for completing the group decision making tasks, while interactive LLM-powered devil’s advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.\n\n\n\n\n\nFigure from Chiang et al. (2024)\n\n\n\n\n\nChuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2024). The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents. https://escholarship.org/uc/item/3k67x8s5\n\n\nAbstract\n\n\nHuman groups are able to converge to more accurate beliefs through deliberation, even in the presence of polarization and partisan bias — a phenomenon known as the “wisdom of partisan crowds.” Large Language Models (LLMs) are increasingly being used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation, as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompting and lack of details in personas. Conversely, fine-tuning on human data appears to enhance convergence. These findings show the potential and limitations of LLM-based agents as a model of human collective intelligence.\n\n\n\n\n\nChuang et al. (2024)\n\n\n\n\n\nNisioti, E., Risi, S., Momennejad, I., Oudeyer, P.-Y., & Moulin-Frier, C. (2024, July 7). Collective Innovation in Groups of Large Language Models. ALIFE 2024: Proceedings of the 2024 Artificial Life Conference. https://doi.org/10.1162/isal_a_00730\n\n\nAbstract\n\n\nHuman culture relies on collective innovation: our ability to continuously explore how existing elements in our environment can be combined to create new ones. Language is hypothesized to play a key role in human culture, driving individual cognitive capacities and shaping communication. Yet the majority of models of collective innovation assign no cognitive capacities or language abilities to agents. Here, we contribute a computational study of collective innovation where agents are Large Language Models (LLMs) that play Little Alchemy 2, a creative video game originally developed for humans that, as we argue, captures useful aspects of innovation landscapes not present in previous test-beds. We, first, study an LLM in isolation and discover that it exhibits both useful skills and crucial limitations. We, then, study groups of LLMs that share information related to their behaviour and focus on the effect of social connectivity on collective performance. In agreement with previous human and computational studies, we observe that groups with dynamic connectivity out-compete fully-connected groups. Our work reveals opportunities and challenges for future studies of collective innovation that are becoming increasingly relevant as Generative Artificial Intelligence algorithms and humans innovate alongside each other.\n\n\n\n\n\nNisioti et al. (2024)\n\n\n\n\n\nChuang, Y.-S., Suresh, S., Harlalka, N., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2023). Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds http://arxiv.org/abs/2311.09665\n\n\nAbstract\n\n\nThis study investigates the potential of Large Language Models (LLMs) to simulate human group dynamics, particularly within politically charged contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to role-play as Democrat and Republican personas, engaging in a structured interaction akin to human group study. Our approach evaluates how agents’ responses evolve through social influence. Our key findings indicate that LLM agents role-playing detailed personas and without Chain-of-Thought (CoT) reasoning closely align with human behaviors, while having CoT reasoning hurts the alignment. However, incorporating explicit biases into agent prompts does not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning LLMs with human data shows promise in achieving human-like behavior but poses a risk of overfitting certain behaviors. These findings show the potential and limitations of using LLM agents in modeling human group phenomena.\n\n\nChuang et al. (2023)\n\n\n\nZhang, J., Xu, X., Zhang, N., Liu, R., Hooi, B., & Deng, S. (2024). Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View (arXiv:2310.02124). arXiv. http://arxiv.org/abs/2310.02124\nhttps://www.zjukg.org/project/MachineSoM/\n\n\nAbstract\n\n\nAs Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique ‘societies’ comprised of LLM agents, where each agent is characterized by a specific ‘trait’ (easy-going or overconfident) and engages in collaboration with a distinct ‘thinking pattern’ (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest humanlike social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We have shared our code and datasets1, hoping to catalyze further research in this promising avenue.\n\n\n\n\n\nZhang et al. (2024)\n\n\n\n\n\nAbdelnabi, S., Gomaa, A., Sivaprasad, S., Schönherr, L., & Fritz, M. (2023). LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. https://doi.org/10.60882/cispa.25233028.v1\n\n\nAbstract\n\n\nThere is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs’ reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and earlier models. Importantly, we test the generalization to new games and setups. Finally, we show that these games can help evaluate other critical aspects, such as the interaction dynamics between agents in the presence of greedy and adversarial players.\n\n\n\n\n\nAbdelnabi et al. (2023)\n\n\n\n\n\nYang, J. C., Dailisan, D., Korecki, M., Hausladen, C. I., & Helbing, D. (2024). LLM Voting: Human Choices and AI Collective Decision Making (arXiv:2402.01766). arXiv. http://arxiv.org/abs/2402.01766\n\n\nAbstract\n\n\nThis paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.\n\n\n\n\n\nJ. C. Yang et al. (2024)\n\n\n\n\n\nGuo, X., Huang, K., Liu, J., Fan, W., Vélez, N., Wu, Q., Wang, H., Griffiths, T. L., & Wang, M. (2024). Embodied LLM Agents Learn to Cooperate in Organized Teams (arXiv:2403.12482). arXiv. http://arxiv.org/abs/2403.12482\n\n\nAbstract\n\n\nLarge Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.\n\n\n\n\n\nGuo et al. (2024)\n\n\n\n\n\nKoehl, D., & Vangsness, L. (2023). Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 67. https://doi.org/10.1177/21695067231192869\n\n\nAbstract\n\n\nQualitative self-report methods such as think-aloud procedures and open-ended response questions can provide valuable data to human factors research. These measures come with analytic weaknesses, such as researcher bias, intra- and inter-rater reliability concerns, and time-consuming coding protocols. A possible solution exists in the latent semantic patterns that exist in machine learning large language models. These semantic patterns could be used to analyze qualitative responses. This exploratory research compared the statistical quality of automated sentence coding using large language models to the benchmarks of self-report and behavioral measures within the context of trust in automation research. The results indicated that three large language models show promise as tools for analyzing qualitative responses. The study also provides insight on minimum sample sizes for model creation and offers recommendations for further validating the robustness of large language models as research tools.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Koehl & Vangsness (2023)\n\n\n\n\n\n\nVats, V., Nizam, M. B., Liu, M., Wang, Z., Ho, R., Prasad, M. S., Titterton, V., Malreddy, S. V., Aggarwal, R., Xu, Y., Ding, L., Mehta, J., Grinnell, N., Liu, L., Zhong, S., Gandamani, D. N., Tang, X., Ghosalkar, R., Shen, C., … Davis, J. (2024). A Survey on Human-AI Teaming with Large Pre-Trained Models (arXiv:2403.04931). arXiv. http://arxiv.org/abs/2403.04931\n\n\nAbstract\n\n\nIn the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI Teaming, providing insights for future research, policy development, and strategic implementations aimed at harnessing the full potential of this collaboration for research and societal benefit.\n\n\n\n\n\nTable from Vats et al. (2024)\n\n\n\n\n\n\nYang, Z., Xu, X., Yao, B., Rogers, E., Zhang, S., Intille, S., Shara, N., Gao, G. G., & Wang, D. (2024). Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(2), 1–35. https://doi.org/10.1145/3659625\n\n\nAbstract\n\n\nDespite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs’ role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered conversational interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults’ conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers’ efforts and time. We envision our work as an initial exploration of LLMs’ capability in the intersection of healthcare and interpersonal communication.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Figures from Z. Yang et al. (2024)\n\n\n\n\n\n\n\nNishida, Y., Shimojo, S., & Hayashi, Y. (2024). Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making. Japanese Psychological Research. https://doi.org/10.1111/jpr.12552\n\n\nAbstract\n\n\nThis study investigated the impact of group discussions with text-based conversational agents on risk-taking decision-making, which has been under-researched. We also focused on the influence of opinion patterns presented by the agents during discussions and attitudes toward these agents. Through an online experiment, 430 participants read a decision-seeking scenario and expressed the degree of risk they were willing to take. After viewing the text-based opinions of six agents and having a discussion with the agents, participants expressed the degree of risk they were willing to take for the same scenario. The result showed that participants’ risk-taking decisions shifted toward the agents’ group opinions, regardless of whether the agents’ opinions tended to be risky or cautious. Additionally, when the agents’ group opinions were more risk-biased and included a minority opinion, a significant association existed between the degree of the participants’ shift to a riskier decision and their positive attitudes toward the agents. The agents’ group opinions guided participants toward both risky and cautious decisions, and participants’ attitudes toward the agents were associated with their decision-making, albeit to a limited extent.\n\n\n\n\n\nFigure from Nishida et al. (2024)\n\n\n\n\n\nGao, J., Gebreegziabher, S. A., Choo, K. T. W., Li, T. J.-J., Perrault, S. T., & Malone, T. W. (2024). A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration. Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–11. https://doi.org/10.1145/3613905.3650786\n\n\nAbstract\n\n\nWith ChatGPT’s release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow - planning, facilitating, iterating, and testing - to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the “5W1H” guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Figures from J. Gao et al. (2024)\n\n\n\n\n\n\n\n\n\nAbdelnabi, S., Gomaa, A., Sivaprasad, S., Schönherr, L., & Fritz, M. (2023). LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. https://doi.org/10.60882/cispa.25233028.v1\n\n\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nBurton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). How large language models can reshape collective intelligence. Nature Human Behaviour, 1–13. https://doi.org/10.1038/s41562-024-01959-9\n\n\nChiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nChuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2024). The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents.\n\n\nChuang, Y.-S., Suresh, S., Harlalka, N., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2023). Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds (arXiv:2311.09665). arXiv. https://arxiv.org/abs/2311.09665\n\n\nCollins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building machines that learn and think with people. Nature Human Behaviour, 8(10), 1851–1863. https://doi.org/10.1038/s41562-024-01991-9\n\n\nDu, Y., Rajivan, P., & Gonzalez, C. C. (2024). Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making. Proceedings of the Annual Meeting of the Cognitive Science Society, 46.\n\n\nGao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., & Li, Y. (2024). Large language models empowered agent-based modeling and simulation: A survey and perspectives. Humanities and Social Sciences Communications, 11(1), 1–24. https://doi.org/10.1057/s41599-024-03611-3\n\n\nGao, J., Gebreegziabher, S. A., Choo, K. T. W., Li, T. J.-J., Perrault, S. T., & Malone, T. W. (2024). A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration. Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–11. https://doi.org/10.1145/3613905.3650786\n\n\nGuo, X., Huang, K., Liu, J., Fan, W., Vélez, N., Wu, Q., Wang, H., Griffiths, T. L., & Wang, M. (2024). Embodied LLM Agents Learn to Cooperate in Organized Teams (arXiv:2403.12482). arXiv. https://arxiv.org/abs/2403.12482\n\n\nHao, X., Demir, E., & Eyers, D. (2024). Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction. Technology in Society, 78, 102662. https://doi.org/10.1016/j.techsoc.2024.102662\n\n\nKoehl, D., & Vangsness, L. (2023). Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 67. https://doi.org/10.1177/21695067231192869\n\n\nMa, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., & Ma, X. (2024). Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making (arXiv:2403.16812). arXiv. https://arxiv.org/abs/2403.16812\n\n\nMarjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit.\n\n\nNishida, Y., Shimojo, S., & Hayashi, Y. (2024). Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making. Japanese Psychological Research. https://doi.org/10.1111/jpr.12552\n\n\nNisioti, E., Risi, S., Momennejad, I., Oudeyer, P.-Y., & Moulin-Frier, C. (2024, July). Collective Innovation in Groups of Large Language Models. ALIFE 2024: Proceedings of the 2024 Artificial Life Conference. https://doi.org/10.1162/isal_a_00730\n\n\nTessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., & Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science, 386(6719), eadq2852. https://doi.org/10.1126/science.adq2852\n\n\nVats, V., Nizam, M. B., Liu, M., Wang, Z., Ho, R., Prasad, M. S., Titterton, V., Malreddy, S. V., Aggarwal, R., Xu, Y., Ding, L., Mehta, J., Grinnell, N., Liu, L., Zhong, S., Gandamani, D. N., Tang, X., Ghosalkar, R., Shen, C., … Davis, J. (2024). A Survey on Human-AI Teaming with Large Pre-Trained Models (arXiv:2403.04931). arXiv. https://arxiv.org/abs/2403.04931\n\n\nYang, J. C., Dailisan, D., Korecki, M., Hausladen, C. I., & Helbing, D. (2024). LLM Voting: Human Choices and AI Collective Decision Making (arXiv:2402.01766). arXiv. https://arxiv.org/abs/2402.01766\n\n\nYang, Z., Xu, X., Yao, B., Rogers, E., Zhang, S., Intille, S., Shara, N., Gao, G. G., & Wang, D. (2024). Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(2), 1–35. https://doi.org/10.1145/3659625\n\n\nZhang, J., Xu, X., Zhang, N., Liu, R., Hooi, B., & Deng, S. (2024). Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View (arXiv:2310.02124). arXiv. https://arxiv.org/abs/2310.02124",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#task-allocation-in-teams-as-a-multi-armed-bandit.",
    "href": "ai_gd.html#task-allocation-in-teams-as-a-multi-armed-bandit.",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Marjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit. https://cocosci.princeton.edu/papers/marjieh2024task.pdf\n\n\nAbstract\n\n\nHumans rely on efficient distribution of resources to transcend the abilities of individuals. Successful task allocation, whether in small teams or across large institutions, depends on individuals’ ability to discern their own and others’ strengths and weaknesses, and to optimally act on them. This dependence creates a tension between exploring the capabilities of others and exploiting the knowledge acquired so far, which can be challenging. How do people navigate this tension? To address this question, we propose a novel task allocation paradigm in which a human agent is asked to repeatedly allocate tasks in three distinct classes (categorizing a blurry image, detecting a noisy voice command, and solving an anagram) between themselves and two other (bot) team members to maximize team performance. We show that this problem can be recast as a combinatorial multi-armed bandit which allows us to compare people’s performance against two well-known strategies, Thompson Sampling and Upper Confidence Bound (UCB). We find that humans are able to successfully integrate information about the capabilities of different team members to infer optimal allocations, and in some cases perform on par with these optimal strategies. Our approach opens up new avenues for studying the mechanisms underlying collective cooperation in teams.\n\n\n\n\n\nFigure from Marjieh et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.",
    "href": "ai_gd.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Bienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nAbstract\n\n\nIn this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team’s ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team’s transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond.\n\n\n\n\n\nFigure from Bienefeld et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#large-language-models-empowered-agent-based-modeling-and-simulation-a-survey-and-perspectives.",
    "href": "ai_gd.html#large-language-models-empowered-agent-based-modeling-and-simulation-a-survey-and-perspectives.",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Gao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., & Li, Y. (2024). Large language models empowered agent-based modeling and simulation: A survey and perspectives. Humanities and Social Sciences Communications, 11(1), 1–24. https://doi.org/10.1057/s41599-024-03611-3\n\n\nAbstract\n\n\nAgent-based modeling and simulation have evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Recently, integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, discussing their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments, and how these works address the above challenges. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/LLM-Agent-Based-Modeling-and-Simulation.\n\n\n\n\n\nFigure from C. Gao et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#building-machines-that-learn-and-think-with-people",
    "href": "ai_gd.html#building-machines-that-learn-and-think-with-people",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Collins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building machines that learn and think with people. Nature Human Behaviour, 8(10), 1851–1863. https://doi.org/10.1038/s41562-024-01991-9\n\n\nAbstract\n\n\nWhat do we want from machine intelligence? We envision machines that are not just tools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable, and trustworthy systems that think with us. Current artificial intelligence (AI) systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called “thought partners,” systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and AI thought partners can engage and propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Figures from Collins et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#large-language-models-for-collective-problem-solving-insights-into-group-consensus-decision-making",
    "href": "ai_gd.html#large-language-models-for-collective-problem-solving-insights-into-group-consensus-decision-making",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Du, Y., Rajivan, P., & Gonzalez, C. C. (2024). Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making. https://escholarship.org/uc/item/6s060914\n\n\nAbstract\n\n\nLarge Language models (LLM) exhibit human-like proficiency in various tasks such as translation, question answering, essay writing, and programming. Emerging research explores the use of LLMs in collective problem-solving endeavors, such as tasks where groups try to uncover clues through discussions. Although prior work has investigated individual problem-solving tasks, leveraging LLM-powered agents for group consensus and decision-making remains largely unexplored. This research addresses this gap by (1) proposing an algorithm to enable free-form conversation in groups of LLM agents, (2) creating metrics to evaluate the human-likeness of the generated dialogue and problem-solving performance, and (3) evaluating LLM agent groups against human groups using an open source dataset. Our results reveal that LLM groups outperform human groups in problem-solving tasks. LLM groups also show a greater improvement in scores after participating in free discussions. In particular, analyses indicate that LLM agent groups exhibit more disagreements, complex statements, and a propensity for positive statements compared to human groups. The results shed light on the potential of LLMs to facilitate collective reasoning and provide insight into the dynamics of group interactions involving synthetic LLM agents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Figure from Du et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#exploring-collaborative-decision-making-a-quasi-experimental-study-of-human-and-generative-ai-interaction.",
    "href": "ai_gd.html#exploring-collaborative-decision-making-a-quasi-experimental-study-of-human-and-generative-ai-interaction.",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Hao, X., Demir, E., & Eyers, D. (2024). Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction. Technology in Society, 78, 102662. https://doi.org/10.1016/j.techsoc.2024.102662\n\n\nAbstract\n\n\nThis paper explores the effects of integrating Generative Artificial Intelligence (GAI) into decision-making processes within organizations, employing a quasi-experimental pretest-posttest design. The study examines the synergistic interaction between Human Intelligence (HI) and GAI across four group decision-making scenarios within three global organizations renowned for their cutting-edge operational techniques. The research progresses through several phases: identifying research problems, collecting baseline data on decision-making, implementing AI interventions, and evaluating the outcomes post-intervention to identify shifts in performance. The results demonstrate that GAI effectively reduces human cognitive burdens and mitigates heuristic biases by offering data-driven support and predictive analytics, grounded in System 2 reasoning. This is particularly valuable in complex situations characterized by unfamiliarity and information overload, where intuitive, System 1 thinking is less effective. However, the study also uncovers challenges related to GAI integration, such as potential over-reliance on technology, intrinsic biases particularly ‘out-of-the-box’ thinking without contextual creativity. To address these issues, this paper proposes an innovative strategic framework for HI-GAI collaboration that emphasizes transparency, accountability, and inclusiveness.\n\n\n\n\n\nFigure from Hao et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#how-large-language-models-can-reshape-collective-intelligence",
    "href": "ai_gd.html#how-large-language-models-can-reshape-collective-intelligence",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Burton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). How large language models can reshape collective intelligence. Nature Human Behaviour, 1–13. https://doi.org/10.1038/s41562-024-01959-9\n\n\nAbstract\n\n\nCollective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals—even experts—resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the ‘wisdom of crowds’, online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans’ ability to collectively tackle complex problems.\n\n\n\n\n\nBurton et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.",
    "href": "ai_gd.html#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Chiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nAbstract\n\n\nGroup decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil’s advocate in the AI-assisted group deci- sion making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities ex- hibited by modern large language models (LLMs), we design four different styles of devil’s advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil’s advocates that argue against the AI model’s decision recommenda- tion have the potential to promote groups’ appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil’s advocate usually does not lead to substantial increases in people’s perceived workload for completing the group decision making tasks, while interactive LLM-powered devil’s advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.\n\n\n\n\n\nFigure from Chiang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#the-wisdom-of-partisan-crowds-comparing-collective-intelligence-in-humans-and-llm-based-agents",
    "href": "ai_gd.html#the-wisdom-of-partisan-crowds-comparing-collective-intelligence-in-humans-and-llm-based-agents",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Chuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2024). The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents. https://escholarship.org/uc/item/3k67x8s5\n\n\nAbstract\n\n\nHuman groups are able to converge to more accurate beliefs through deliberation, even in the presence of polarization and partisan bias — a phenomenon known as the “wisdom of partisan crowds.” Large Language Models (LLMs) are increasingly being used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation, as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompting and lack of details in personas. Conversely, fine-tuning on human data appears to enhance convergence. These findings show the potential and limitations of LLM-based agents as a model of human collective intelligence.\n\n\n\n\n\nChuang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#collective-innovation-in-groups-of-large-language-models.",
    "href": "ai_gd.html#collective-innovation-in-groups-of-large-language-models.",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Nisioti, E., Risi, S., Momennejad, I., Oudeyer, P.-Y., & Moulin-Frier, C. (2024, July 7). Collective Innovation in Groups of Large Language Models. ALIFE 2024: Proceedings of the 2024 Artificial Life Conference. https://doi.org/10.1162/isal_a_00730\n\n\nAbstract\n\n\nHuman culture relies on collective innovation: our ability to continuously explore how existing elements in our environment can be combined to create new ones. Language is hypothesized to play a key role in human culture, driving individual cognitive capacities and shaping communication. Yet the majority of models of collective innovation assign no cognitive capacities or language abilities to agents. Here, we contribute a computational study of collective innovation where agents are Large Language Models (LLMs) that play Little Alchemy 2, a creative video game originally developed for humans that, as we argue, captures useful aspects of innovation landscapes not present in previous test-beds. We, first, study an LLM in isolation and discover that it exhibits both useful skills and crucial limitations. We, then, study groups of LLMs that share information related to their behaviour and focus on the effect of social connectivity on collective performance. In agreement with previous human and computational studies, we observe that groups with dynamic connectivity out-compete fully-connected groups. Our work reveals opportunities and challenges for future studies of collective innovation that are becoming increasingly relevant as Generative Artificial Intelligence algorithms and humans innovate alongside each other.\n\n\n\n\n\nNisioti et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#evaluating-llm-agent-group-dynamics-against-human-group-dynamics-a-case-study-on-wisdom-of-partisan-crowds",
    "href": "ai_gd.html#evaluating-llm-agent-group-dynamics-against-human-group-dynamics-a-case-study-on-wisdom-of-partisan-crowds",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Chuang, Y.-S., Suresh, S., Harlalka, N., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2023). Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds http://arxiv.org/abs/2311.09665\n\n\nAbstract\n\n\nThis study investigates the potential of Large Language Models (LLMs) to simulate human group dynamics, particularly within politically charged contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to role-play as Democrat and Republican personas, engaging in a structured interaction akin to human group study. Our approach evaluates how agents’ responses evolve through social influence. Our key findings indicate that LLM agents role-playing detailed personas and without Chain-of-Thought (CoT) reasoning closely align with human behaviors, while having CoT reasoning hurts the alignment. However, incorporating explicit biases into agent prompts does not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning LLMs with human data shows promise in achieving human-like behavior but poses a risk of overfitting certain behaviors. These findings show the potential and limitations of using LLM agents in modeling human group phenomena.\n\n\nChuang et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#exploring-collaboration-mechanisms-for-llm-agents-a-social-psychology-view",
    "href": "ai_gd.html#exploring-collaboration-mechanisms-for-llm-agents-a-social-psychology-view",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Zhang, J., Xu, X., Zhang, N., Liu, R., Hooi, B., & Deng, S. (2024). Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View (arXiv:2310.02124). arXiv. http://arxiv.org/abs/2310.02124\nhttps://www.zjukg.org/project/MachineSoM/\n\n\nAbstract\n\n\nAs Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique ‘societies’ comprised of LLM agents, where each agent is characterized by a specific ‘trait’ (easy-going or overconfident) and engages in collaboration with a distinct ‘thinking pattern’ (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest humanlike social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We have shared our code and datasets1, hoping to catalyze further research in this promising avenue.\n\n\n\n\n\nZhang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#llm-deliberation-evaluating-llms-with-interactive-multi-agent-negotiation-games.",
    "href": "ai_gd.html#llm-deliberation-evaluating-llms-with-interactive-multi-agent-negotiation-games.",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Abdelnabi, S., Gomaa, A., Sivaprasad, S., Schönherr, L., & Fritz, M. (2023). LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. https://doi.org/10.60882/cispa.25233028.v1\n\n\nAbstract\n\n\nThere is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs’ reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and earlier models. Importantly, we test the generalization to new games and setups. Finally, we show that these games can help evaluate other critical aspects, such as the interaction dynamics between agents in the presence of greedy and adversarial players.\n\n\n\n\n\nAbdelnabi et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#llm-voting-human-choices-and-ai-collective-decision-making",
    "href": "ai_gd.html#llm-voting-human-choices-and-ai-collective-decision-making",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Yang, J. C., Dailisan, D., Korecki, M., Hausladen, C. I., & Helbing, D. (2024). LLM Voting: Human Choices and AI Collective Decision Making (arXiv:2402.01766). arXiv. http://arxiv.org/abs/2402.01766\n\n\nAbstract\n\n\nThis paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.\n\n\n\n\n\nJ. C. Yang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#embodied-llm-agents-learn-to-cooperate-in-organized-teams",
    "href": "ai_gd.html#embodied-llm-agents-learn-to-cooperate-in-organized-teams",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Guo, X., Huang, K., Liu, J., Fan, W., Vélez, N., Wu, Q., Wang, H., Griffiths, T. L., & Wang, M. (2024). Embodied LLM Agents Learn to Cooperate in Organized Teams (arXiv:2403.12482). arXiv. http://arxiv.org/abs/2403.12482\n\n\nAbstract\n\n\nLarge Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.\n\n\n\n\n\nGuo et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#measuring-latent-trust-patterns-in-large-language-models-in-the-context-of-human-ai-teaming",
    "href": "ai_gd.html#measuring-latent-trust-patterns-in-large-language-models-in-the-context-of-human-ai-teaming",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Koehl, D., & Vangsness, L. (2023). Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 67. https://doi.org/10.1177/21695067231192869\n\n\nAbstract\n\n\nQualitative self-report methods such as think-aloud procedures and open-ended response questions can provide valuable data to human factors research. These measures come with analytic weaknesses, such as researcher bias, intra- and inter-rater reliability concerns, and time-consuming coding protocols. A possible solution exists in the latent semantic patterns that exist in machine learning large language models. These semantic patterns could be used to analyze qualitative responses. This exploratory research compared the statistical quality of automated sentence coding using large language models to the benchmarks of self-report and behavioral measures within the context of trust in automation research. The results indicated that three large language models show promise as tools for analyzing qualitative responses. The study also provides insight on minimum sample sizes for model creation and offers recommendations for further validating the robustness of large language models as research tools.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Koehl & Vangsness (2023)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#a-survey-on-human-ai-teaming-with-large-pre-trained-models",
    "href": "ai_gd.html#a-survey-on-human-ai-teaming-with-large-pre-trained-models",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Vats, V., Nizam, M. B., Liu, M., Wang, Z., Ho, R., Prasad, M. S., Titterton, V., Malreddy, S. V., Aggarwal, R., Xu, Y., Ding, L., Mehta, J., Grinnell, N., Liu, L., Zhong, S., Gandamani, D. N., Tang, X., Ghosalkar, R., Shen, C., … Davis, J. (2024). A Survey on Human-AI Teaming with Large Pre-Trained Models (arXiv:2403.04931). arXiv. http://arxiv.org/abs/2403.04931\n\n\nAbstract\n\n\nIn the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI Teaming, providing insights for future research, policy development, and strategic implementations aimed at harnessing the full potential of this collaboration for research and societal benefit.\n\n\n\n\n\nTable from Vats et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_decision.html#to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making",
    "href": "ai_decision.html#to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making",
    "title": "Individual decision lit",
    "section": "",
    "text": "Buçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21. https://doi.org/10.1145/3449287\n\n\nAbstract\n\n\nPeople supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI’s suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.\n\n\n\n\n\nFigure from Buçinca et al. (2021)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#large-language-model-recall-uncertainty-is-modulated-by-the-fan-effect.",
    "href": "ai_decision.html#large-language-model-recall-uncertainty-is-modulated-by-the-fan-effect.",
    "title": "Individual decision lit",
    "section": "Large Language Model Recall Uncertainty is Modulated by the Fan Effect.",
    "text": "Large Language Model Recall Uncertainty is Modulated by the Fan Effect.\nRoberts, J., Moore, K., Pham, T., Ewaleifoh, O., & Fisher, D. (2024). Large Language Model Recall Uncertainty is Modulated by the Fan Effect.\n\n\n\nFigure from Roberts et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_gd.html#conversational-agent-dynamics-with-minority-opinion-and-cognitive-conflict-in-small-group-decision-making.",
    "href": "ai_gd.html#conversational-agent-dynamics-with-minority-opinion-and-cognitive-conflict-in-small-group-decision-making.",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Nishida, Y., Shimojo, S., & Hayashi, Y. (2024). Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making. Japanese Psychological Research. https://doi.org/10.1111/jpr.12552\n\n\nAbstract\n\n\nThis study investigated the impact of group discussions with text-based conversational agents on risk-taking decision-making, which has been under-researched. We also focused on the influence of opinion patterns presented by the agents during discussions and attitudes toward these agents. Through an online experiment, 430 participants read a decision-seeking scenario and expressed the degree of risk they were willing to take. After viewing the text-based opinions of six agents and having a discussion with the agents, participants expressed the degree of risk they were willing to take for the same scenario. The result showed that participants’ risk-taking decisions shifted toward the agents’ group opinions, regardless of whether the agents’ opinions tended to be risky or cautious. Additionally, when the agents’ group opinions were more risk-biased and included a minority opinion, a significant association existed between the degree of the participants’ shift to a riskier decision and their positive attitudes toward the agents. The agents’ group opinions guided participants toward both risky and cautious decisions, and participants’ attitudes toward the agents were associated with their decision-making, albeit to a limited extent.\n\n\n\n\n\nFigure from Nishida et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "CCL Projects",
    "section": "",
    "text": "Transactive Memory Systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTMS LLM Task\n\n\n\n\n\n\nThomas E. Gorman\n\n\n\n\n\n\n\n\n\n\n\nSamuel’s Project\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nPossible Chapter Structures\n\n\n\n\n\n\nThomas E. Gorman\n\n\n\n\n\n\n\n\n\n\n\nLLM Energy Lit Highlights\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nInteractive AI Lit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIndividual decision lit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nGroup Decision Lit\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBehavioral Experiment Documentation Report\n\n\n\n\n\n\nSamuel Castro Martínez\n\n\n\n\n\n\n\n\n\n\n\nDriving Lit\n\n\n\n\n\n\nThomas E. Gorman\n\n\nMar 26, 2025\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "ai_decision.html#accuracy-time-tradeoffs-in-ai-assisted-decision-making-under-time-pressure.",
    "href": "ai_decision.html#accuracy-time-tradeoffs-in-ai-assisted-decision-making-under-time-pressure.",
    "title": "Individual decision lit",
    "section": "Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure.",
    "text": "Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure.\nSwaroop, S., Buçinca, Z., Gajos, K. Z., & Doshi-Velez, F. (2024). Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure. Proceedings of the 29th International Conference on Intelligent User Interfaces, 138–154. https://doi.org/10.1145/3640543.3645206\n\n\nAbstract\n\n\nIn settings where users both need high accuracy and are timepressured, such as doctors working in emergency rooms, we want to provide AI assistance that both increases decision accuracy and reduces decision-making time. Current literature focusses on how users interact with AI assistance when there is no time pressure, finding that different AI assistances have different benefits: some can reduce time taken while increasing overreliance on AI, while others do the opposite. The precise benefit can depend on both the user and task. In time-pressured scenarios, adapting when we show AI assistance is especially important: relying on the AI assistance can save time, and can therefore be beneficial when the AI is likely to be right. We would ideally adapt what AI assistance we show depending on various properties (of the task and of the user) in order to best trade off accuracy and time. We introduce a study where users have to answer a series of logic puzzles. We find that time pressure affects how users use different AI assistances, making some assistances more beneficial than others when compared to notime-pressure settings. We also find that a user’s overreliance rate is a key predictor of their behaviour: overreliers and not-overreliers use different AI assistance types differently. We find marginal correlations between a user’s overreliance rate (which is related to the user’s trust in AI recommendations) and their personality traits (Big Five Personality traits). Overall, our work suggests that AI assistances have different accuracy-time tradeoffs when people are under time pressure compared to no time pressure, and we explore how we might adapt AI assistances in this setting.\n\n\n\n\n\nFigure from Swaroop et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_gd.html#towards-human-ai-deliberation-design-and-evaluation-of-llm-empowered-deliberative-ai-for-ai-assisted-decision-making",
    "href": "ai_gd.html#towards-human-ai-deliberation-design-and-evaluation-of-llm-empowered-deliberative-ai-for-ai-assisted-decision-making",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Ma, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., & Ma, X. (2024). Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making (arXiv:2403.16812). arXiv. http://arxiv.org/abs/2403.16812\n\n\nAbstract\n\n\nIn AI-assisted decision-making, humans often passively review AI’s suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional explainable AI (XAI) assistants in improving humans’ appropriate reliance and task performance. Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Figure from Ma et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "llm_energy.html",
    "href": "llm_energy.html",
    "title": "LLM Energy Lit Highlights",
    "section": "",
    "text": "link to html version: https://tegorman13.github.io/ccl/llm_energy.html",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#sasha-creative-goal-oriented-reasoning-in-smart-homes-with-large-language-models.",
    "href": "llm_energy.html#sasha-creative-goal-oriented-reasoning-in-smart-homes-with-large-language-models.",
    "title": "LLM Energy Lit Highlights",
    "section": "Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models.",
    "text": "Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models.\nKing, E., Yu, H., Lee, S., & Julien, C. (2024). Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(1), 1–38. https://doi.org/10.1145/3643505\n\n\nAbstract\n\n\nSmart home assistants function best when user commands are direct and well-specified—e.g., “turn on the kitchen light”—or when a hard-coded routine specifies the response. In more natural communication, however, human speech is unconstrained, often describing goals (e.g., “make it cozy in here” or “help me save energy”) rather than indicating specific target devices and actions to take on those devices. Current systems fail to understand these under-specified commands since they cannot reason about devices and settings as they relate to human situations. We introduce large language models (LLMs) to this problem space, exploring their use for controlling devices and creating automation routines in response to under-specified user commands in smart homes. We empirically study the baseline quality and failure modes of LLM-created action plans with a survey of age-diverse users. We find that LLMs can reason creatively to achieve challenging goals, but they experience patterns of failure that diminish their usefulness. We address these gaps with Sasha, a smarter smart home assistant. Sasha responds to loosely-constrained commands like “make it cozy” or “help me sleep better” by executing plans to achieve user goals—e.g., setting a mood with available devices, or devising automation routines. We implement and evaluate Sasha in a hands-on user study, showing the capabilities and limitations of LLM-driven smart homes when faced with unconstrained user-generated scenarios.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Figure from King, Yu, Lee, et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#designing-home-automation-routines-using-an-llm-based-chatbot.",
    "href": "llm_energy.html#designing-home-automation-routines-using-an-llm-based-chatbot.",
    "title": "LLM Energy Lit Highlights",
    "section": "Designing Home Automation Routines Using an LLM-Based Chatbot.",
    "text": "Designing Home Automation Routines Using an LLM-Based Chatbot.\nGiudici, M., Padalino, L., Paolino, G., Paratici, I., Pascu, A. I., & Garzotto, F. (2024). Designing Home Automation Routines Using an LLM-Based Chatbot. Designs, 8(3), Article 3. https://doi.org/10.3390/designs8030043\n\n\nAbstract\n\n\nWithout any more delay, individuals are urged to adopt more sustainable behaviors to fight climate change. New digital systems mixed with engaging and gamification mechanisms could play an important role in achieving such an objective. In particular, Conversational Agents, like Smart Home Assistants, are a promising tool that encourage sustainable behaviors within household settings. In recent years, large language models (LLMs) have shown great potential in enhancing the capabilities of such assistants, making them more effective in interacting with users. We present the design and implementation of GreenIFTTT, an application empowered by GPT4 to create and control home automation routines. The agent helps users understand which energy consumption optimization routines could be created and applied to make their home appliances more environmentally sustainable. We performed an exploratory study (Italy, December 2023) with N = 13 participants to test our application’s usability and UX. The results suggest that GreenIFTTT is a usable, engaging, easy, and supportive tool, providing insight into new perspectives and usage of LLMs to create more environmentally sustainable home automation.\n\n\n\n\n\nFigure from Giudici et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#save-it-for-the-hot-day-an-llm-empowered-visual-analytics-system-for-heat-risk-management",
    "href": "llm_energy.html#save-it-for-the-hot-day-an-llm-empowered-visual-analytics-system-for-heat-risk-management",
    "title": "LLM Energy Lit Highlights",
    "section": "Save It for the “Hot” Day: An LLM-Empowered Visual Analytics System for Heat Risk Management",
    "text": "Save It for the “Hot” Day: An LLM-Empowered Visual Analytics System for Heat Risk Management\nLi, H., Kam-Kwai, W., Luo, Y., Chen, J., Liu, C., Zhang, Y., Lau, A. K. H., Qu, H., & Liu, D. (2024). Save It for the “Hot” Day: An LLM-Empowered Visual Analytics System for Heat Risk Management (arXiv:2406.03317). arXiv. http://arxiv.org/abs/2406.03317\n\n\nAbstract\n\n\nThe escalating frequency and intensity of heat-related climate events, particularly heatwaves, emphasize the pressing need for advanced heat risk management strategies. Current approaches, primarily relying on numerical models, face challenges in spatial-temporal resolution and in capturing the dynamic interplay of environmental, social, and behavioral factors affecting heat risks. This has led to difficulties in translating risk assessments into effective mitigation actions. Recognizing these problems, we introduce a novel approach leveraging the burgeoning capabilities of Large Language Models (LLMs) to extract rich and contextual insights from news reports. We hence propose an LLM-empowered visual analytics system, Havior, that integrates the precise, data-driven insights of numerical models with nuanced news report information. This hybrid approach enables a more comprehensive assessment of heat risks and better identification, assessment, and mitigation of heat-related threats. The system incorporates novel visualization designs, such as “thermoglyph” and news glyph, enhancing intuitive understanding and analysis of heat risks. The integration of LLM-based techniques also enables advanced information retrieval and semantic knowledge extraction that can be guided by experts’ analytics needs. Our case studies on two cities that faced significant heatwave events and interviews with five experts have demonstrated the usefulness of our system in providing in-depth and actionable insights for heat risk management.\n\n\n\n\n\nFigure from Li et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#follow-me-ai-energy-efficient-user-interaction-with-smart-environments",
    "href": "llm_energy.html#follow-me-ai-energy-efficient-user-interaction-with-smart-environments",
    "title": "LLM Energy Lit Highlights",
    "section": "Follow-Me AI: Energy-Efficient User Interaction with Smart Environments",
    "text": "Follow-Me AI: Energy-Efficient User Interaction with Smart Environments\nSaleh, A., Donta, P. K., Morabito, R., Motlagh, N. H., & Lovén, L. (2024). Follow-Me AI: Energy-Efficient User Interaction with Smart Environments (arXiv:2404.12486). arXiv. http://arxiv.org/abs/2404.12486\n\n\nAbstract\n\n\nThis article introduces Follow-Me AI, a concept designed to enhance user interactions with smart environments, optimize energy use, and provide better control over data captured by these environments. Through AI agents that accompany users, Follow-Me AI negotiates data management based on user consent, aligns environmental controls as well as user communication and computes resources available in the environment with user preferences, and predicts user behavior to proactively adjust the smart environment. The manuscript illustrates this concept with a detailed example of Follow-Me AI in a smart campus setting, detailing the interactions with the building’s management system for optimal comfort and efficiency. Finally, this article looks into the challenges and opportunities related to Follow-Me AI.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Figure from Saleh et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#an-llm-based-digital-twin-for-optimizing-human-in-the-loop-systems",
    "href": "llm_energy.html#an-llm-based-digital-twin-for-optimizing-human-in-the-loop-systems",
    "title": "LLM Energy Lit Highlights",
    "section": "An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems",
    "text": "An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems\nYang, H., Siew, M., & Joe-Wong, C. (2024). An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems (arXiv:2403.16809). arXiv. http://arxiv.org/abs/2403.16809\n\n\nAbstract\n\n\nThe increasing prevalence of Cyber-Physical Systems and the Internet of Things (CPS-IoT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment. For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption. Collecting realtime feedback on human preferences in such human-in-the-loop (HITL) systems, however, is difficult in practice. We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization. In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall. The aggregated thermal preferences are integrated into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which employs the LLM as a dynamic simulation of the physical environment to learn how to balance between energy savings and occupant comfort. Our results show that LLMs are capable of simulating complex population movements within large open spaces. Besides, AitL-RLdemonstrates superior performance compared to the popular existing policy of set point control, suggesting that adaptive and personalized decision-making is critical for efficient optimization in CPS-IoT applications. Through this case study, we demonstrate the potential of integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system adaptability and efficiency. The project’s code can be found on our GitHub repository.\n\n\n\n\n\nFigure from Yang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#can-private-llm-agents-synthesize-household-energy-consumption-data",
    "href": "llm_energy.html#can-private-llm-agents-synthesize-household-energy-consumption-data",
    "title": "LLM Energy Lit Highlights",
    "section": "Can Private LLM Agents Synthesize Household Energy Consumption Data?",
    "text": "Can Private LLM Agents Synthesize Household Energy Consumption Data?\nAlmashor, M., & Miyashita, Y. (2024). Can Private LLM Agents Synthesize Household Energy Consumption Data? Proceedings of the 15th ACM International Conference on Future and Sustainable Energy Systems, 664–668. https://doi.org/10.1145/3632775.3661993\n\n\nAbstract\n\n\nReproducible science requires easy access to data, especially with the rise of data-driven and increasingly complex models used within energy research. Too often however, the data to reconstruct and verify purported solutions in publications is hidden due to some combination of commercial, legal, and sensitivity issues. This early work presents our initial efforts to leverage the recent advancements in Large Language Models (LLMs) to create usable and shareable energy datasets. In particular, we’re utilising their mimicry of human behaviors, with the goal of extracting and exploring synthetic energy data through the simulation of LLM agents capable of interacting with and executing actions in controlled environments. We also analyse and visualise publicly available data in an attempt to create realistic but not quite exact copies of the originals. Our early results show some promise, with outputs that resemble the twin peak curves for household energy consumption. The hope is that our generalised approach can be used to easily replicate usable and realistic copies of otherwise secret or sensitive data.\n\n\nAlmashor & Miyashita (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#prompt-gaming-a-pilot-study-on-llm-evaluating-agent-in-a-meaningful-energy-game.",
    "href": "llm_energy.html#prompt-gaming-a-pilot-study-on-llm-evaluating-agent-in-a-meaningful-energy-game.",
    "title": "LLM Energy Lit Highlights",
    "section": "Prompt-Gaming: A Pilot Study on LLM-Evaluating Agent in a Meaningful Energy Game.",
    "text": "Prompt-Gaming: A Pilot Study on LLM-Evaluating Agent in a Meaningful Energy Game.\nIsaza-Giraldo, A., Bala, P., Campos, P. F., & Pereira, L. (2024). Prompt-Gaming: A Pilot Study on LLM-Evaluating Agent in a Meaningful Energy Game. Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems, 1–12. https://doi.org/10.1145/3613905.3650774\n\n\nAbstract\n\n\nBuilding on previous work on incorporating large language models (LLM) in gaming, we investigate the possibility of implementing LLM as evaluating agents of open-ended challenges in serious games and its potential to facilitate a meaningful experience for the player. We contribute with a sustainability game prototype in a single natural language prompt about energy communities and we tested it with 13 participants inside ChatGPT-3.5. Two participants were already aware of energy communities before the game, and eight of the remaining 11 gained valuable knowledge about the specific topic. Comparing ChatGPT-3.5 evaluations of players’ interaction with an expert’s assessment, ChatGPT-3.5 correctly evaluated 81% of player’s answers. Our results are encouraging and show the potential of using LLMs as mediating agents in educational games, while also allowing easy prototyping of games through natural language prompts.\n\n\n\n\n\nFigure from Isaza-Giraldo et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#evaluation-of-large-language-models-llms-on-the-mastery-of-knowledge-and-skills-in-the-heating-ventilation-and-air-conditioning-hvac-industry.",
    "href": "llm_energy.html#evaluation-of-large-language-models-llms-on-the-mastery-of-knowledge-and-skills-in-the-heating-ventilation-and-air-conditioning-hvac-industry.",
    "title": "LLM Energy Lit Highlights",
    "section": "Evaluation of large language models (LLMs) on the mastery of knowledge and skills in the heating, ventilation and air conditioning (HVAC) industry.",
    "text": "Evaluation of large language models (LLMs) on the mastery of knowledge and skills in the heating, ventilation and air conditioning (HVAC) industry.\nLu, J., Tian, X., Zhang, C., Zhao, Y., Zhang, J., Zhang, W., Feng, C., He, J., Wang, J., & He, F. (2024). Evaluation of large language models (LLMs) on the mastery of knowledge and skills in the heating, ventilation and air conditioning (HVAC) industry. Energy and Built Environment. https://doi.org/10.1016/j.enbenv.2024.03.010\n\n\nAbstract\n\n\nLarge language models (LLMs) have shown human-level capabilities in solving various complex tasks. However, it is still unknown whether state-of-the-art LLMs master sufficient knowledge related to heating, ventilation and air conditioning (HVAC) systems. It will be inspiring if LLMs can think and learn like professionals in the HVAC industry. Hence, this study investigates the performance of LLMs on mastering the knowledge and skills related to the HVAC industry by letting them take the ASHRAE Certified HVAC Designer examination, an authoritative examination in the HVAC industry. Three key knowledge capabilities are explored: recall, analysis and application. Twelve representative LLMs are tested such as GPT-3.5, GPT-4 and LLaMA. According to the results, GPT-4 passes the ASHRAE Certified HVAC Designer examination with scores from 74 to 78, which is higher than about half of human examinees. Besides, GPT-3.5 passes the examination twice out of five times. It demonstrates that some LLMs such as GPT-4 and GPT-3.5 have great potential to assist or replace humans in designing and operating HVAC systems. However, they still make some mistakes sometimes due to the lack of knowledge, poor reasoning capabilities and unsatisfactory equation calculation abilities. Accordingly, four future research directions are proposed to reveal how to utilize and improve LLMs in the HVAC industry: teaching LLMs to use design tools or software in the HVAC industry, enabling LLMs to read and analyze the operational data from HVAC systems, developing tailored corpuses for the HVAC industry, and assessing the performance of LLMs in real-world HVAC design and operation scenarios.\n\n\n\n\n\nFigure from Lu et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#domain-specific-large-language-models-for-fault-diagnosis-of-heating-ventilation-and-air-conditioning-systems-by-labeled-data-supervised-fine-tuning.",
    "href": "llm_energy.html#domain-specific-large-language-models-for-fault-diagnosis-of-heating-ventilation-and-air-conditioning-systems-by-labeled-data-supervised-fine-tuning.",
    "title": "LLM Energy Lit Highlights",
    "section": "Domain-specific large language models for fault diagnosis of heating, ventilation, and air conditioning systems by labeled-data-supervised fine-tuning.",
    "text": "Domain-specific large language models for fault diagnosis of heating, ventilation, and air conditioning systems by labeled-data-supervised fine-tuning.\nZhang, J., Zhang, C., Lu, J., & Zhao, Y. (2025). Domain-specific large language models for fault diagnosis of heating, ventilation, and air conditioning systems by labeled-data-supervised fine-tuning. Applied Energy, 377, 124378. https://doi.org/10.1016/j.apenergy.2024.124378\n\n\nAbstract\n\n\nLarge language models (LLMs) have exhibited great potential in fault diagnosis of heating, ventilation, and air conditioning systems. However, the fault diagnosis accuracy of LLMs is still unsatisfactory, due to the lack of effective diagnosis accuracy enhancement methods for LLMs. To fill this gap, this study proposes a LLM fine-tuning method supervised by data with fault and fault-free labels to enhance the fault diagnosis accuracy of LLMs. This method designs a LLM self-correction strategy to automatically generate a fine-tuning dataset based on the labeled data. The generated fine-tuning dataset is applied to fine-tune a LLM. Moreover, a data augmentation-based approach is put forward to adaptively update the fine-tuning dataset for iteratively developing a high-performance fine-tuned LLM. The proposed method is utilized to fine-tune the GPT-3.5 model using the air handling unit (AHU) fault dataset from the RP-1312 project. The results show that the diagnosis accuracy of the GPT-3.5 model is increased from 29.5 % to 100.0 % after model fine-tuning. Compared with the GPT-4 model, the fine-tuned GPT-3.5 model achieves a 31.1 % higher average diagnosis accuracy. The fine-tuned GPT-3.5 model is also applied to diagnose faults in two AHUs from another open-source dataset to verify the generalization ability of this model. The two AHUs have different system structures and sensor configurations compared to the AHU in the RP-1312 dataset, and this dataset is not utilized to fine-tune the GPT-3.5 model. The average diagnosis accuracy of the GPT-3.5 model is increased from 46.0 % to 99.1 % and from 38.8 % to 98.9 % for the faults in the two AHUs, respectively, after model fine-tuning. Furthermore, the proposed method is verified using two fault datasets from a variable air volume box and a chiller plant system. After fine-tuning the GPT-3.5 model using the two datasets, the average diagnosis accuracy of this model is increased from 33.0 % to 98.3 % for variable air volume box faults and from 36.0 % to 99.1 % for chiller plant system faults. This study provides an effective solution to the development of domain-specific LLMs for this domain.\n\n\nZhang et al. (2025)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "ai_decision.html#the-llm-effect-are-humans-truly-using-llms-or-are-they-being-influenced-by-them-instead",
    "href": "ai_decision.html#the-llm-effect-are-humans-truly-using-llms-or-are-they-being-influenced-by-them-instead",
    "title": "Individual decision lit",
    "section": "The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?",
    "text": "The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?\nChoi, A. S., Akter, S. S., Singh, J. P., & Anastasopoulos, A. (2024). The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead? (arXiv:2410.04699). arXiv. http://arxiv.org/abs/2410.04699\n\n\nAbstract\n\n\nLarge Language Models (LLMs) have shown capabilities close to human performance in various analytical tasks, leading researchers to use them for time and labor-intensive analyses. However, their capability to handle highly specialized and open-ended tasks in domains like policy studies remains in question. This paper investigates the efficiency and accuracy of LLMs in specialized tasks through a structured user study focusing on Human-LLM partnership. The study, conducted in two stages-Topic Discovery and Topic Assignment-integrates LLMs with expert annotators to observe the impact of LLM suggestions on what is usually human-only analysis. Results indicate that LLM-generated topic lists have significant overlap with human generated topic lists, with minor hiccups in missing document-specific topics. However, LLM suggestions may significantly improve task completion speed, but at the same time introduce anchoring bias, potentially affecting the depth and nuance of the analysis, raising a critical question about the trade-off between increased efficiency and the risk of biased analysis.\n\n\n\n\n\nFigure from Choi et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#mutual-theory-of-mind-in-human-ai-collaboration-an-empirical-study-with-llm-driven-ai-agents-in-a-real-time-shared-workspace-task",
    "href": "ai_decision.html#mutual-theory-of-mind-in-human-ai-collaboration-an-empirical-study-with-llm-driven-ai-agents-in-a-real-time-shared-workspace-task",
    "title": "Individual decision lit",
    "section": "Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task",
    "text": "Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task\nZhang, S., Wang, X., Zhang, W., Chen, Y., Gao, L., Wang, D., Zhang, W., Wang, X., & Wen, Y. (2024). Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task (arXiv:2409.08811). arXiv. http://arxiv.org/abs/2409.08811\n\n\nAbstract\n\n\nTheory of Mind (ToM) significantly impacts human collaboration and communication as a crucial capability to understand others. When AI agents with ToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises in such human-AI teams (HATs). The MToM process, which involves interactive communication and ToM-based strategy adjustment, affects the team’s performance and collaboration process. To explore the MToM process, we conducted a mixed-design experiment using a large language model-driven AI agent with ToM and communication modules in a real-time shared-workspace task. We find that the agent’s ToM capability does not significantly impact team performance but enhances human understanding of the agent and the feeling of being understood. Most participants in our study believe verbal communication increases human burden, and the results show that bidirectional communication leads to lower HAT performance. We discuss the results’ implications for designing AI agents that collaborate with humans in real-time shared workspace tasks.\n\n\n\n\n\nFigure from Zhang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_gd.html#a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration",
    "href": "ai_gd.html#a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Gao, J., Gebreegziabher, S. A., Choo, K. T. W., Li, T. J.-J., Perrault, S. T., & Malone, T. W. (2024). A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration. Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–11. https://doi.org/10.1145/3613905.3650786\n\n\nAbstract\n\n\nWith ChatGPT’s release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow - planning, facilitating, iterating, and testing - to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the “5W1H” guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Figures from J. Gao et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_gd.html#talk2care-an-llm-based-voice-assistant-for-communication-between-healthcare-providers-and-older-adults.",
    "href": "ai_gd.html#talk2care-an-llm-based-voice-assistant-for-communication-between-healthcare-providers-and-older-adults.",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Yang, Z., Xu, X., Yao, B., Rogers, E., Zhang, S., Intille, S., Shara, N., Gao, G. G., & Wang, D. (2024). Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(2), 1–35. https://doi.org/10.1145/3659625\n\n\nAbstract\n\n\nDespite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs’ role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered conversational interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults’ conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers’ efforts and time. We envision our work as an initial exploration of LLMs’ capability in the intersection of healthcare and interpersonal communication.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Figures from Z. Yang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "llm_energy.html#game-of-llms-discovering-structural-constructs-in-activities-using-large-language-models.",
    "href": "llm_energy.html#game-of-llms-discovering-structural-constructs-in-activities-using-large-language-models.",
    "title": "LLM Energy Lit Highlights",
    "section": "Game of LLMs: Discovering Structural Constructs in Activities using Large Language Models.",
    "text": "Game of LLMs: Discovering Structural Constructs in Activities using Large Language Models.\nHiremath, S. K., & Plötz, T. (2024). Game of LLMs: Discovering Structural Constructs in Activities using Large Language Models. Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing, 487–492. https://doi.org/10.1145/3675094.3678444\n\n\nAbstract\n\n\nHuman Activity Recognition is a time-series analysis problem. A popular analysis procedure used by the community assumes an optimal window length to design recognition pipelines. However, in the scenario of smart homes, where activities are of varying duration and frequency, the assumption of a constant sized window does not hold. Additionally, previous works have shown these activities to be made up of building blocks. We focus on identifying these underlying building blocks–structural constructs, with the use of large language models. Identifying these constructs can be beneficial especially in recognizing short-duration and infrequent activities, which current systems cannot recognize. We also propose the development of an activity recognition procedure that uses these building blocks to model activities, thus helping the downstream task of activity monitoring in smart homes.\n\n\n\n\n\nFigure from Hiremath & Plötz (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#thoughtful-things-building-human-centric-smart-devices-with-small-language-models",
    "href": "llm_energy.html#thoughtful-things-building-human-centric-smart-devices-with-small-language-models",
    "title": "LLM Energy Lit Highlights",
    "section": "Thoughtful Things: Building Human-Centric Smart Devices with Small Language Models",
    "text": "Thoughtful Things: Building Human-Centric Smart Devices with Small Language Models\nKing, E., Yu, H., Vartak, S., Jacob, J., Lee, S., & Julien, C. (2024). Thoughtful Things: Building Human-Centric Smart Devices with Small Language Models (arXiv:2405.03821). arXiv. http://arxiv.org/abs/2405.03821\n\n\nAbstract\n\n\nEveryday devices like light bulbs and kitchen appliances are now embedded with so many features and automated behaviors that they have become complicated to actually use. While such “smart” capabilities can better support users’ goals, the task of learning the “ins and outs” of different devices is daunting. Voice assistants aim to solve this problem by providing a natural language interface to devices, yet such assistants cannot understand loosely-constrained commands, they lack the ability to reason about and explain devices’ behaviors to users, and they rely on connectivity to intrusive cloud infrastructure. Toward addressing these issues, we propose thoughtful things: devices that leverage lightweight, on-device language models to take actions and explain their behaviors in response to unconstrained user commands. We propose an end-to-end framework that leverages formal modeling, automated training data synthesis, and generative language models to create devices that are both capable and thoughtful in the presence of unconstrained user goals and inquiries. Our framework requires no labeled data and can be deployed on-device, with no cloud dependency. We implement two thoughtful things (a lamp and a thermostat) and deploy them on real hardware, evaluating their practical performance.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Figure from King, Yu, Vartak, et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#enhancing-smart-home-interaction-through-multimodal-command-disambiguation.-personal-and-ubiquitous-computing.",
    "href": "llm_energy.html#enhancing-smart-home-interaction-through-multimodal-command-disambiguation.-personal-and-ubiquitous-computing.",
    "title": "LLM Energy",
    "section": "Enhancing smart home interaction through multimodal command disambiguation. Personal and Ubiquitous Computing.",
    "text": "Enhancing smart home interaction through multimodal command disambiguation. Personal and Ubiquitous Computing.\nCalò, T., & De Russis, L. (2024). Enhancing smart home interaction through multimodal command disambiguation. Personal and Ubiquitous Computing. https://doi.org/10.1007/s00779-024-01827-3\n\n\nAbstract\n\n\nSmart speakers are entering our homes and enriching the connected ecosystem already present in them. Home inhabitants can use those to execute relatively simple commands, e.g., turning a lamp on. Their capabilities to interpret more complex and ambiguous commands (e.g., make this room warmer) are limited, if not absent. Large language models (LLMs) can offer creative and viable solutions to enable a practical and user-acceptable interpretation of such ambiguous commands. This paper introduces an interactive disambiguation approach that integrates visual and textual cues with natural language commands. After contextualizing the approach with a use case, we test it in an experiment where users are prompted to select the appropriate cue (an image or a textual description) to clarify ambiguous commands, thereby refining the accuracy of the system’s interpretations. Outcomes from the study indicate that the disambiguation system produces responses well-aligned with user intentions, and that participants found the textual descriptions slightly more effective. Finally, interviews reveal heightened satisfaction with the smart-home system when engaging with the proposed disambiguation approach.\n\n\n\n\n\nFigure from Calò & De Russis (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#llm-based-open-domain-integrated-task-and-knowledge-assistants-with-programmable-policies",
    "href": "llm_energy.html#llm-based-open-domain-integrated-task-and-knowledge-assistants-with-programmable-policies",
    "title": "LLM Energy Lit Highlights",
    "section": "LLM-Based Open-Domain Integrated Task and Knowledge Assistants with Programmable Policies",
    "text": "LLM-Based Open-Domain Integrated Task and Knowledge Assistants with Programmable Policies\nJoshi, H., Liu, S., Chen, J., Weigle, R., & Lam, M. S. (2024). LLM-Based Open-Domain Integrated Task and Knowledge Assistants with Programmable Policies (arXiv:2407.05674). arXiv. http://arxiv.org/abs/2407.05674\n\n\nAbstract\n\n\nProgramming LLM-based knowledge and task assistants that faithfully conform to developer-provided policies is challenging. These agents must retrieve and provide consistent, accurate, and relevant information to address user’s queries and needs. Yet such agents generate unfounded responses (“hallucinate”). Traditional dialogue trees can only handle a limited number of conversation flows, making them inherently brittle. To this end, we present KITA - a programmable framework for creating task-oriented conversational agents that are designed to handle complex user interactions. Unlike LLMs, KITA provides reliable grounded responses, with controllable agent policies through its expressive specification, KITA Worksheet. In contrast to dialog trees, it is resilient to diverse user queries, helpful with knowledge sources, and offers ease of programming policies through its declarative paradigm. Through a real-user study involving 62 participants, we show that KITA beats the GPT-4 with function calling baseline by 26.1, 22.5, and 52.4 points on execution accuracy, dialogue act accuracy, and goal completion rate, respectively. We also release 22 real-user conversations with KITA manually corrected to ensure accuracy.\n\n\nJoshi et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#large-language-models-are-zero-shot-recognizers-for-activities-of-daily-living",
    "href": "llm_energy.html#large-language-models-are-zero-shot-recognizers-for-activities-of-daily-living",
    "title": "LLM Energy Lit Highlights",
    "section": "Large Language Models are Zero-Shot Recognizers for Activities of Daily Living",
    "text": "Large Language Models are Zero-Shot Recognizers for Activities of Daily Living\nCivitarese, G., Fiori, M., Choudhary, P., & Bettini, C. (2024). Large Language Models are Zero-Shot Recognizers for Activities of Daily Living (arXiv:2407.01238). arXiv. http://arxiv.org/abs/2407.01238\n\n\nAbstract\n\n\nThe sensor-based recognition of Activities of Daily Living (ADLs) in smart home environments enables several applications in the areas of energy management, safety, well-being, and healthcare. ADLs recognition is typically based on deep learning methods requiring large datasets to be trained. Recently, several studies proved that Large Language Models (LLMs) effectively capture common-sense knowledge about human activities. However, the effectiveness of LLMs for ADLs recognition in smart home environments still deserves to be investigated. In this work, we propose ADL-LLM, a novel LLM-based ADLs recognition system. ADLLLM transforms raw sensor data into textual representations, that are processed by an LLM to perform zero-shot ADLs recognition. Moreover, in the scenario where a small labeled dataset is available, ADL-LLM can also be empowered with few-shot prompting. We evaluated ADL-LLM on two public datasets, showing its effectiveness in this domain.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Figures from Civitarese et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#large-language-models-for-power-scheduling-a-user-centric-approach",
    "href": "llm_energy.html#large-language-models-for-power-scheduling-a-user-centric-approach",
    "title": "LLM Energy Lit Highlights",
    "section": "Large Language Models for Power Scheduling: A User-Centric Approach",
    "text": "Large Language Models for Power Scheduling: A User-Centric Approach\nMongaillard, T., Lasaulce, S., Hicheur, O., Zhang, C., Bariah, L., Varma, V. S., Zou, H., Zhao, Q., & Debbah, M. (2024). Large Language Models for Power Scheduling: A User-Centric Approach (arXiv:2407.00476). arXiv. http://arxiv.org/abs/2407.00476\n\n\nAbstract\n\n\nWhile traditional optimization and scheduling schemes are designed to meet fixed, predefined system requirements, future systems are moving toward user-driven approaches and personalized services, aiming to achieve high quality-of-experience (QoE) and flexibility. This challenge is particularly pronounced in wireless and digitalized energy networks, where users’ requirements have largely not been taken into consideration due to the lack of a common language between users and machines. The emergence of powerful large language models (LLMs) marks a radical departure from traditional system-centric methods into more advanced user-centric approaches by providing a natural communication interface between users and devices. In this paper, for the first time, we introduce a novel architecture for resource scheduling problems by constructing three LLM agents to convert an arbitrary user’s voice request (VRQ) into a resource allocation vector. Specifically, we design an LLM intent recognition agent to translate the request into an optimization problem (OP), an LLM OP parameter identification agent, and an LLM OP solving agent. To evaluate system performance, we construct a database of typical VRQs in the context of electric vehicle (EV) charging. As a proof of concept, we primarily use Llama 3 8B. Through testing with different prompt engineering scenarios, the obtained results demonstrate the efficiency of the proposed architecture. The conducted performance analysis allows key insights to be extracted. For instance, having a larger set of candidate OPs to model the real-world problem might degrade the final performance because of a higher recognition/OP classification noise level. All results and codes are open source.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Figures from Mongaillard et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#a-recommendation-system-for-prosumers-based-on-large-language-models.",
    "href": "llm_energy.html#a-recommendation-system-for-prosumers-based-on-large-language-models.",
    "title": "LLM Energy Lit Highlights",
    "section": "A Recommendation System for Prosumers Based on Large Language Models.",
    "text": "A Recommendation System for Prosumers Based on Large Language Models.\nOprea, S.-V., & Bâra, A. (2024). A Recommendation System for Prosumers Based on Large Language Models. Sensors, 24(11), Article 11. https://doi.org/10.3390/s24113530\n\n\nAbstract\n\n\nAs modern technologies, particularly home assistant devices and sensors, become more integrated into our daily lives, they are also making their way into the domain of energy management within our homes. Homeowners, now acting as prosumers, have access to detailed information at 15-min or even 5-min intervals, including weather forecasts, outputs from renewable energy source (RES)-based systems, appliance schedules and the current energy balance, which details any deficits or surpluses along with their quantities and the predicted prices on the local energy market (LEM). The goal for these prosumers is to reduce costs while ensuring their home’s comfort levels are maintained. However, given the complexity and the rapid decision-making required in managing this information, the need for a supportive system is evident. This is particularly true given the routine nature of these decisions, highlighting the potential for a system that provides personalized recommendations to optimize energy consumption, whether that involves adjusting the load or engaging in transactions with the LEM. In this context, we propose a recommendation system powered by large language models (LLMs), Scikit-llm and zero-shot classifiers, designed to evaluate specific scenarios and offer tailored advice for prosumers based on the available data at any given moment. Two scenarios for a prosumer of 5.9 kW are assessed using candidate labels, such as Decrease, Increase, Sell and Buy. A comparison with a content-based filtering system is provided considering the performance metrics that are relevant for prosumers.\n\n\nOprea & Bâra (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#a-conversational-agent-for-creating-automations-exploiting-large-language-models.-personal-and-ubiquitous-computing.",
    "href": "llm_energy.html#a-conversational-agent-for-creating-automations-exploiting-large-language-models.-personal-and-ubiquitous-computing.",
    "title": "LLM Energy Lit Highlights",
    "section": "A conversational agent for creating automations exploiting large language models. Personal and Ubiquitous Computing.",
    "text": "A conversational agent for creating automations exploiting large language models. Personal and Ubiquitous Computing.\nGallo, S., Paternò, F., & Malizia, A. (2024). A conversational agent for creating automations exploiting large language models. Personal and Ubiquitous Computing. https://doi.org/10.1007/s00779-024-01825-5\n\n\nAbstract\n\n\nThe proliferation of sensors and smart Internet of Things (IoT) devices in our everyday environments is reshaping our interactions with everyday objects. This change underlines the need to empower non-expert users to easily configure the behaviour of these devices to align with their preferences and habits. At the same time, recent advances in generative transformers, such as ChatGPT, have opened up new possibilities in a variety of natural language processing tasks, enhancing reasoning capabilities and conversational interactions. This paper presents RuleBot +  + , a conversational agent that exploits GPT-4 to assist the user in the creation and modification of trigger-action automations through natural language. After an introduction to motivations and related work, we present the design and implementation of RuleBot +  + and report the results of the user test in which users interacted with our solution and Home Assistant, one of the most used open-source tools for managing smart environments.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Figures from Gallo et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#a-human-on-the-loop-optimization-autoformalism-approach-for-sustainability",
    "href": "llm_energy.html#a-human-on-the-loop-optimization-autoformalism-approach-for-sustainability",
    "title": "LLM Energy Lit Highlights",
    "section": "A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability",
    "text": "A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability\nJin, M., Sel, B., Hardeep, F., & Yin, W. (2023). A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability (arXiv:2308.10380). arXiv. http://arxiv.org/abs/2308.10380\n\n\nAbstract\n\n\nThis paper outlines a natural conversational approach to solving personalized energy-related problems using large language models (LLMs). We focus on customizable optimization problems that necessitate repeated solving with slight variations in modeling and are user-specific, hence posing a challenge to devising a one-size-fits-all model. We put forward a strategy that augments an LLM with an optimization solver, enhancing its proficiency in understanding and responding to user specifications and preferences while providing nonlinear reasoning capabilities. Our approach pioneers the novel concept of human-guided optimization autoformalism, translating a natural language task specification automatically into an optimization instance. This enables LLMs to analyze, explain, and tackle a variety of instance-specific energy-related problems, pushing beyond the limits of current prompt-based techniques. Our research encompasses various commonplace tasks in the energy sector, from electric vehicle charging and Heating, Ventilation, and Air Conditioning (HVAC) control to long-term planning problems such as cost-benefit evaluations for installing rooftop solar photovoltaics (PVs) or heat pumps. This pilot study marks an essential stride towards the context-based formulation of optimization using LLMs, with the potential to democratize optimization processes. As a result, stakeholders are empowered to optimize their energy consumption, promoting sustainable energy practices customized to personal needs and preferences.\n\n\n\n\n\nFigure from Jin et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#enhancing-smart-home-interaction-through-multimodal-command-disambiguation.",
    "href": "llm_energy.html#enhancing-smart-home-interaction-through-multimodal-command-disambiguation.",
    "title": "LLM Energy Lit Highlights",
    "section": "Enhancing smart home interaction through multimodal command disambiguation.",
    "text": "Enhancing smart home interaction through multimodal command disambiguation.\nCalò, T., & De Russis, L. (2024). Enhancing smart home interaction through multimodal command disambiguation. Personal and Ubiquitous Computing. https://doi.org/10.1007/s00779-024-01827-3\n\n\nAbstract\n\n\nSmart speakers are entering our homes and enriching the connected ecosystem already present in them. Home inhabitants can use those to execute relatively simple commands, e.g., turning a lamp on. Their capabilities to interpret more complex and ambiguous commands (e.g., make this room warmer) are limited, if not absent. Large language models (LLMs) can offer creative and viable solutions to enable a practical and user-acceptable interpretation of such ambiguous commands. This paper introduces an interactive disambiguation approach that integrates visual and textual cues with natural language commands. After contextualizing the approach with a use case, we test it in an experiment where users are prompted to select the appropriate cue (an image or a textual description) to clarify ambiguous commands, thereby refining the accuracy of the system’s interpretations. Outcomes from the study indicate that the disambiguation system produces responses well-aligned with user intentions, and that participants found the textual descriptions slightly more effective. Finally, interviews reveal heightened satisfaction with the smart-home system when engaging with the proposed disambiguation approach.\n\n\n\n\n\nFigure from Calò & De Russis (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#references",
    "href": "llm_energy.html#references",
    "title": "LLM Energy Lit Highlights",
    "section": "References",
    "text": "References\n\n\nAlmashor, M., & Miyashita, Y. (2024). Can Private LLM Agents Synthesize Household Energy Consumption Data? Proceedings of the 15th ACM International Conference on Future and Sustainable Energy Systems, 664–668. https://doi.org/10.1145/3632775.3661993\n\n\nCalò, T., & De Russis, L. (2024). Enhancing smart home interaction through multimodal command disambiguation. Personal and Ubiquitous Computing. https://doi.org/10.1007/s00779-024-01827-3\n\n\nCivitarese, G., Fiori, M., Choudhary, P., & Bettini, C. (2024). Large Language Models are Zero-Shot Recognizers for Activities of Daily Living (arXiv:2407.01238). arXiv. https://arxiv.org/abs/2407.01238\n\n\nGallo, S., Paternò, F., & Malizia, A. (2024). A conversational agent for creating automations exploiting large language models. Personal and Ubiquitous Computing. https://doi.org/10.1007/s00779-024-01825-5\n\n\nGiudici, M., Padalino, L., Paolino, G., Paratici, I., Pascu, A. I., & Garzotto, F. (2024). Designing Home Automation Routines Using an LLM-Based Chatbot. Designs, 8(3), 43. https://doi.org/10.3390/designs8030043\n\n\nHiremath, S. K., & Plötz, T. (2024). Game of LLMs: Discovering Structural Constructs in Activities using Large Language Models. Companion of the 2024 on ACM International Joint Conference on Pervasive and Ubiquitous Computing, 487–492. https://doi.org/10.1145/3675094.3678444\n\n\nIsaza-Giraldo, A., Bala, P., Campos, P. F., & Pereira, L. (2024). Prompt-Gaming: A Pilot Study on LLM-Evaluating Agent in a Meaningful Energy Game. Extended Abstracts of the 2024 CHI Conference on Human Factors in Computing Systems, 1–12. https://doi.org/10.1145/3613905.3650774\n\n\nJin, M., Sel, B., Hardeep, F., & Yin, W. (2023). A Human-on-the-Loop Optimization Autoformalism Approach for Sustainability (arXiv:2308.10380). arXiv. https://arxiv.org/abs/2308.10380\n\n\nJoshi, H., Liu, S., Chen, J., Weigle, R., & Lam, M. S. (2024). LLM-Based Open-Domain Integrated Task and Knowledge Assistants with Programmable Policies (arXiv:2407.05674). arXiv. https://arxiv.org/abs/2407.05674\n\n\nKing, E., Yu, H., Lee, S., & Julien, C. (2024). Sasha: Creative Goal-Oriented Reasoning in Smart Homes with Large Language Models. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(1), 1–38. https://doi.org/10.1145/3643505\n\n\nKing, E., Yu, H., Vartak, S., Jacob, J., Lee, S., & Julien, C. (2024). Thoughtful Things: Building Human-Centric Smart Devices with Small Language Models (arXiv:2405.03821). arXiv. https://arxiv.org/abs/2405.03821\n\n\nLi, H., Kam-Kwai, W., Luo, Y., Chen, J., Liu, C., Zhang, Y., Lau, A. K. H., Qu, H., & Liu, D. (2024). Save It for the \"Hot\" Day: An LLM-Empowered Visual Analytics System for Heat Risk Management (arXiv:2406.03317). arXiv. https://arxiv.org/abs/2406.03317\n\n\nLu, J., Tian, X., Zhang, C., Zhao, Y., Zhang, J., Zhang, W., Feng, C., He, J., Wang, J., & He, F. (2024). Evaluation of large language models (LLMs) on the mastery of knowledge and skills in the heating, ventilation and air conditioning (HVAC) industry. Energy and Built Environment. https://doi.org/10.1016/j.enbenv.2024.03.010\n\n\nMongaillard, T., Lasaulce, S., Hicheur, O., Zhang, C., Bariah, L., Varma, V. S., Zou, H., Zhao, Q., & Debbah, M. (2024). Large Language Models for Power Scheduling: A User-Centric Approach (arXiv:2407.00476). arXiv. https://arxiv.org/abs/2407.00476\n\n\nOprea, S.-V., & Bâra, A. (2024). A Recommendation System for Prosumers Based on Large Language Models. Sensors, 24(11), 3530. https://doi.org/10.3390/s24113530\n\n\nRey-Jouanchicot, J., Bottaro, A., Campo, E., Bouraoui, J.-L., Vigouroux, N., & Vella, F. (2024). Leveraging Large Language Models for enhanced personalised user experience in Smart Homes (arXiv:2407.12024). arXiv. https://arxiv.org/abs/2407.12024\n\n\nRivkin, D., Hogan, F., Feriani, A., Konar, A., Sigal, A., Liu, X., & Dudek, G. (2024). AIoT Smart Home via Autonomous LLM Agents. IEEE Internet of Things Journal, 1–1. https://doi.org/10.1109/JIOT.2024.3471904\n\n\nSaleh, A., Donta, P. K., Morabito, R., Motlagh, N. H., & Lovén, L. (2024). Follow-Me AI: Energy-Efficient User Interaction with Smart Environments (arXiv:2404.12486). arXiv. https://arxiv.org/abs/2404.12486\n\n\nYang, H., Siew, M., & Joe-Wong, C. (2024). An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems (arXiv:2403.16809). arXiv. https://arxiv.org/abs/2403.16809\n\n\nYin, Z., Zhang, M., & Kawahara, D. (2024). Harmony: A Home Agent for Responsive Management and Action Optimization with a Locally Deployed Large Language Model (arXiv:2410.14252). arXiv. https://arxiv.org/abs/2410.14252\n\n\nZhang, J., Zhang, C., Lu, J., & Zhao, Y. (2025). Domain-specific large language models for fault diagnosis of heating, ventilation, and air conditioning systems by labeled-data-supervised fine-tuning. Applied Energy, 377, 124378. https://doi.org/10.1016/j.apenergy.2024.124378",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#task-allocation-in-teams-as-a-multi-armed-bandit.",
    "href": "llm_energy.html#task-allocation-in-teams-as-a-multi-armed-bandit.",
    "title": "Group Decision Lit",
    "section": "Task Allocation in Teams as a Multi-Armed Bandit.",
    "text": "Task Allocation in Teams as a Multi-Armed Bandit.\nMarjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit. https://cocosci.princeton.edu/papers/marjieh2024task.pdf\n\n\nAbstract\n\n\nHumans rely on efficient distribution of resources to transcend the abilities of individuals. Successful task allocation, whether in small teams or across large institutions, depends on individuals’ ability to discern their own and others’ strengths and weaknesses, and to optimally act on them. This dependence creates a tension between exploring the capabilities of others and exploiting the knowledge acquired so far, which can be challenging. How do people navigate this tension? To address this question, we propose a novel task allocation paradigm in which a human agent is asked to repeatedly allocate tasks in three distinct classes (categorizing a blurry image, detecting a noisy voice command, and solving an anagram) between themselves and two other (bot) team members to maximize team performance. We show that this problem can be recast as a combinatorial multi-armed bandit which allows us to compare people’s performance against two well-known strategies, Thompson Sampling and Upper Confidence Bound (UCB). We find that humans are able to successfully integrate information about the capabilities of different team members to infer optimal allocations, and in some cases perform on par with these optimal strategies. Our approach opens up new avenues for studying the mechanisms underlying collective cooperation in teams.\n\n\n\n\n\nFigure from Marjieh et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.",
    "href": "llm_energy.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.",
    "title": "Group Decision Lit",
    "section": "Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness.",
    "text": "Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness.\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nAbstract\n\n\nIn this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team’s ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team’s transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond.\n\n\n\n\n\nFigure from Bienefeld et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#large-language-models-empowered-agent-based-modeling-and-simulation-a-survey-and-perspectives.",
    "href": "llm_energy.html#large-language-models-empowered-agent-based-modeling-and-simulation-a-survey-and-perspectives.",
    "title": "Group Decision Lit",
    "section": "Large language models empowered agent-based modeling and simulation: A survey and perspectives.",
    "text": "Large language models empowered agent-based modeling and simulation: A survey and perspectives.\nGao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., & Li, Y. (2024). Large language models empowered agent-based modeling and simulation: A survey and perspectives. Humanities and Social Sciences Communications, 11(1), 1–24. https://doi.org/10.1057/s41599-024-03611-3\n\n\nAbstract\n\n\nAgent-based modeling and simulation have evolved as a powerful tool for modeling complex systems, offering insights into emergent behaviors and interactions among diverse agents. Recently, integrating large language models into agent-based modeling and simulation presents a promising avenue for enhancing simulation capabilities. This paper surveys the landscape of utilizing large language models in agent-based modeling and simulation, discussing their challenges and promising future directions. In this survey, since this is an interdisciplinary field, we first introduce the background of agent-based modeling and simulation and large language model-empowered agents. We then discuss the motivation for applying large language models to agent-based simulation and systematically analyze the challenges in environment perception, human alignment, action generation, and evaluation. Most importantly, we provide a comprehensive overview of the recent works of large language model-empowered agent-based modeling and simulation in multiple scenarios, which can be divided into four domains: cyber, physical, social, and hybrid, covering simulation of both real-world and virtual environments, and how these works address the above challenges. Finally, since this area is new and quickly evolving, we discuss the open problems and promising future directions. We summarize the representative papers along with their code repositories in https://github.com/tsinghua-fib-lab/LLM-Agent-Based-Modeling-and-Simulation.\n\n\n\n\n\nFigure from C. Gao et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#building-machines-that-learn-and-think-with-people",
    "href": "llm_energy.html#building-machines-that-learn-and-think-with-people",
    "title": "Group Decision Lit",
    "section": "Building Machines that Learn and Think with People",
    "text": "Building Machines that Learn and Think with People\nCollins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building machines that learn and think with people. Nature Human Behaviour, 8(10), 1851–1863. https://doi.org/10.1038/s41562-024-01991-9\n\n\nAbstract\n\n\nWhat do we want from machine intelligence? We envision machines that are not just tools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable, and trustworthy systems that think with us. Current artificial intelligence (AI) systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called “thought partners,” systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and AI thought partners can engage and propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 18: Figures from Collins et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#large-language-models-for-collective-problem-solving-insights-into-group-consensus-decision-making",
    "href": "llm_energy.html#large-language-models-for-collective-problem-solving-insights-into-group-consensus-decision-making",
    "title": "Group Decision Lit",
    "section": "Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making",
    "text": "Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making\nDu, Y., Rajivan, P., & Gonzalez, C. C. (2024). Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making. https://escholarship.org/uc/item/6s060914\n\n\nAbstract\n\n\nLarge Language models (LLM) exhibit human-like proficiency in various tasks such as translation, question answering, essay writing, and programming. Emerging research explores the use of LLMs in collective problem-solving endeavors, such as tasks where groups try to uncover clues through discussions. Although prior work has investigated individual problem-solving tasks, leveraging LLM-powered agents for group consensus and decision-making remains largely unexplored. This research addresses this gap by (1) proposing an algorithm to enable free-form conversation in groups of LLM agents, (2) creating metrics to evaluate the human-likeness of the generated dialogue and problem-solving performance, and (3) evaluating LLM agent groups against human groups using an open source dataset. Our results reveal that LLM groups outperform human groups in problem-solving tasks. LLM groups also show a greater improvement in scores after participating in free discussions. In particular, analyses indicate that LLM agent groups exhibit more disagreements, complex statements, and a propensity for positive statements compared to human groups. The results shed light on the potential of LLMs to facilitate collective reasoning and provide insight into the dynamics of group interactions involving synthetic LLM agents.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 19: Figure from Du et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#exploring-collaborative-decision-making-a-quasi-experimental-study-of-human-and-generative-ai-interaction.",
    "href": "llm_energy.html#exploring-collaborative-decision-making-a-quasi-experimental-study-of-human-and-generative-ai-interaction.",
    "title": "Group Decision Lit",
    "section": "Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction.",
    "text": "Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction.\nHao, X., Demir, E., & Eyers, D. (2024). Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction. Technology in Society, 78, 102662. https://doi.org/10.1016/j.techsoc.2024.102662\n\n\nAbstract\n\n\nThis paper explores the effects of integrating Generative Artificial Intelligence (GAI) into decision-making processes within organizations, employing a quasi-experimental pretest-posttest design. The study examines the synergistic interaction between Human Intelligence (HI) and GAI across four group decision-making scenarios within three global organizations renowned for their cutting-edge operational techniques. The research progresses through several phases: identifying research problems, collecting baseline data on decision-making, implementing AI interventions, and evaluating the outcomes post-intervention to identify shifts in performance. The results demonstrate that GAI effectively reduces human cognitive burdens and mitigates heuristic biases by offering data-driven support and predictive analytics, grounded in System 2 reasoning. This is particularly valuable in complex situations characterized by unfamiliarity and information overload, where intuitive, System 1 thinking is less effective. However, the study also uncovers challenges related to GAI integration, such as potential over-reliance on technology, intrinsic biases particularly ‘out-of-the-box’ thinking without contextual creativity. To address these issues, this paper proposes an innovative strategic framework for HI-GAI collaboration that emphasizes transparency, accountability, and inclusiveness.\n\n\n\n\n\nFigure from Hao et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#how-large-language-models-can-reshape-collective-intelligence",
    "href": "llm_energy.html#how-large-language-models-can-reshape-collective-intelligence",
    "title": "Group Decision Lit",
    "section": "How large language models can reshape collective intelligence",
    "text": "How large language models can reshape collective intelligence\nBurton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). How large language models can reshape collective intelligence. Nature Human Behaviour, 1–13. https://doi.org/10.1038/s41562-024-01959-9\n\n\nAbstract\n\n\nCollective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals—even experts—resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the ‘wisdom of crowds’, online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans’ ability to collectively tackle complex problems.\n\n\n\n\n\nBurton et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#towards-human-ai-deliberation-design-and-evaluation-of-llm-empowered-deliberative-ai-for-ai-assisted-decision-making",
    "href": "llm_energy.html#towards-human-ai-deliberation-design-and-evaluation-of-llm-empowered-deliberative-ai-for-ai-assisted-decision-making",
    "title": "Group Decision Lit",
    "section": "Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making",
    "text": "Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making\nMa, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., & Ma, X. (2024). Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making (arXiv:2403.16812). arXiv. http://arxiv.org/abs/2403.16812\n\n\nAbstract\n\n\nIn AI-assisted decision-making, humans often passively review AI’s suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional explainable AI (XAI) assistants in improving humans’ appropriate reliance and task performance. Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 20: Figure from Ma et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.",
    "href": "llm_energy.html#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.",
    "title": "Group Decision Lit",
    "section": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate.",
    "text": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate.\nChiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nAbstract\n\n\nGroup decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil’s advocate in the AI-assisted group deci- sion making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities ex- hibited by modern large language models (LLMs), we design four different styles of devil’s advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil’s advocates that argue against the AI model’s decision recommenda- tion have the potential to promote groups’ appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil’s advocate usually does not lead to substantial increases in people’s perceived workload for completing the group decision making tasks, while interactive LLM-powered devil’s advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.\n\n\n\n\n\nFigure from Chiang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#the-wisdom-of-partisan-crowds-comparing-collective-intelligence-in-humans-and-llm-based-agents",
    "href": "llm_energy.html#the-wisdom-of-partisan-crowds-comparing-collective-intelligence-in-humans-and-llm-based-agents",
    "title": "Group Decision Lit",
    "section": "The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents",
    "text": "The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents\nChuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2024). The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents. https://escholarship.org/uc/item/3k67x8s5\n\n\nAbstract\n\n\nHuman groups are able to converge to more accurate beliefs through deliberation, even in the presence of polarization and partisan bias — a phenomenon known as the “wisdom of partisan crowds.” Large Language Models (LLMs) are increasingly being used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation, as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompting and lack of details in personas. Conversely, fine-tuning on human data appears to enhance convergence. These findings show the potential and limitations of LLM-based agents as a model of human collective intelligence.\n\n\n\n\n\nChuang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#collective-innovation-in-groups-of-large-language-models.",
    "href": "llm_energy.html#collective-innovation-in-groups-of-large-language-models.",
    "title": "Group Decision Lit",
    "section": "Collective Innovation in Groups of Large Language Models.",
    "text": "Collective Innovation in Groups of Large Language Models.\nNisioti, E., Risi, S., Momennejad, I., Oudeyer, P.-Y., & Moulin-Frier, C. (2024, July 7). Collective Innovation in Groups of Large Language Models. ALIFE 2024: Proceedings of the 2024 Artificial Life Conference. https://doi.org/10.1162/isal_a_00730\n\n\nAbstract\n\n\nHuman culture relies on collective innovation: our ability to continuously explore how existing elements in our environment can be combined to create new ones. Language is hypothesized to play a key role in human culture, driving individual cognitive capacities and shaping communication. Yet the majority of models of collective innovation assign no cognitive capacities or language abilities to agents. Here, we contribute a computational study of collective innovation where agents are Large Language Models (LLMs) that play Little Alchemy 2, a creative video game originally developed for humans that, as we argue, captures useful aspects of innovation landscapes not present in previous test-beds. We, first, study an LLM in isolation and discover that it exhibits both useful skills and crucial limitations. We, then, study groups of LLMs that share information related to their behaviour and focus on the effect of social connectivity on collective performance. In agreement with previous human and computational studies, we observe that groups with dynamic connectivity out-compete fully-connected groups. Our work reveals opportunities and challenges for future studies of collective innovation that are becoming increasingly relevant as Generative Artificial Intelligence algorithms and humans innovate alongside each other.\n\n\n\n\n\nNisioti et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#evaluating-llm-agent-group-dynamics-against-human-group-dynamics-a-case-study-on-wisdom-of-partisan-crowds",
    "href": "llm_energy.html#evaluating-llm-agent-group-dynamics-against-human-group-dynamics-a-case-study-on-wisdom-of-partisan-crowds",
    "title": "Group Decision Lit",
    "section": "Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds",
    "text": "Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds\nChuang, Y.-S., Suresh, S., Harlalka, N., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2023). Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds http://arxiv.org/abs/2311.09665\n\n\nAbstract\n\n\nThis study investigates the potential of Large Language Models (LLMs) to simulate human group dynamics, particularly within politically charged contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to role-play as Democrat and Republican personas, engaging in a structured interaction akin to human group study. Our approach evaluates how agents’ responses evolve through social influence. Our key findings indicate that LLM agents role-playing detailed personas and without Chain-of-Thought (CoT) reasoning closely align with human behaviors, while having CoT reasoning hurts the alignment. However, incorporating explicit biases into agent prompts does not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning LLMs with human data shows promise in achieving human-like behavior but poses a risk of overfitting certain behaviors. These findings show the potential and limitations of using LLM agents in modeling human group phenomena.\n\n\nChuang et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#exploring-collaboration-mechanisms-for-llm-agents-a-social-psychology-view",
    "href": "llm_energy.html#exploring-collaboration-mechanisms-for-llm-agents-a-social-psychology-view",
    "title": "Group Decision Lit",
    "section": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View",
    "text": "Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View\nZhang, J., Xu, X., Zhang, N., Liu, R., Hooi, B., & Deng, S. (2024). Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View (arXiv:2310.02124). arXiv. http://arxiv.org/abs/2310.02124\nhttps://www.zjukg.org/project/MachineSoM/\n\n\nAbstract\n\n\nAs Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique ‘societies’ comprised of LLM agents, where each agent is characterized by a specific ‘trait’ (easy-going or overconfident) and engages in collaboration with a distinct ‘thinking pattern’ (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest humanlike social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We have shared our code and datasets1, hoping to catalyze further research in this promising avenue.\n\n\n\n\n\nZhang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#llm-deliberation-evaluating-llms-with-interactive-multi-agent-negotiation-games.",
    "href": "llm_energy.html#llm-deliberation-evaluating-llms-with-interactive-multi-agent-negotiation-games.",
    "title": "Group Decision Lit",
    "section": "LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games.",
    "text": "LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games.\nAbdelnabi, S., Gomaa, A., Sivaprasad, S., Schönherr, L., & Fritz, M. (2023). LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. https://doi.org/10.60882/cispa.25233028.v1\n\n\nAbstract\n\n\nThere is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs’ reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and earlier models. Importantly, we test the generalization to new games and setups. Finally, we show that these games can help evaluate other critical aspects, such as the interaction dynamics between agents in the presence of greedy and adversarial players.\n\n\n\n\n\nAbdelnabi et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#llm-voting-human-choices-and-ai-collective-decision-making",
    "href": "llm_energy.html#llm-voting-human-choices-and-ai-collective-decision-making",
    "title": "Group Decision Lit",
    "section": "LLM Voting: Human Choices and AI Collective Decision Making",
    "text": "LLM Voting: Human Choices and AI Collective Decision Making\nYang, J. C., Dailisan, D., Korecki, M., Hausladen, C. I., & Helbing, D. (2024). LLM Voting: Human Choices and AI Collective Decision Making (arXiv:2402.01766). arXiv. http://arxiv.org/abs/2402.01766\n\n\nAbstract\n\n\nThis paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.\n\n\n\n\n\nJ. C. Yang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#embodied-llm-agents-learn-to-cooperate-in-organized-teams",
    "href": "llm_energy.html#embodied-llm-agents-learn-to-cooperate-in-organized-teams",
    "title": "Group Decision Lit",
    "section": "Embodied LLM Agents Learn to Cooperate in Organized Teams",
    "text": "Embodied LLM Agents Learn to Cooperate in Organized Teams\nGuo, X., Huang, K., Liu, J., Fan, W., Vélez, N., Wu, Q., Wang, H., Griffiths, T. L., & Wang, M. (2024). Embodied LLM Agents Learn to Cooperate in Organized Teams (arXiv:2403.12482). arXiv. http://arxiv.org/abs/2403.12482\n\n\nAbstract\n\n\nLarge Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency.\n\n\n\n\n\nGuo et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#measuring-latent-trust-patterns-in-large-language-models-in-the-context-of-human-ai-teaming",
    "href": "llm_energy.html#measuring-latent-trust-patterns-in-large-language-models-in-the-context-of-human-ai-teaming",
    "title": "Group Decision Lit",
    "section": "Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming",
    "text": "Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming\nKoehl, D., & Vangsness, L. (2023). Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 67. https://doi.org/10.1177/21695067231192869\n\n\nAbstract\n\n\nQualitative self-report methods such as think-aloud procedures and open-ended response questions can provide valuable data to human factors research. These measures come with analytic weaknesses, such as researcher bias, intra- and inter-rater reliability concerns, and time-consuming coding protocols. A possible solution exists in the latent semantic patterns that exist in machine learning large language models. These semantic patterns could be used to analyze qualitative responses. This exploratory research compared the statistical quality of automated sentence coding using large language models to the benchmarks of self-report and behavioral measures within the context of trust in automation research. The results indicated that three large language models show promise as tools for analyzing qualitative responses. The study also provides insight on minimum sample sizes for model creation and offers recommendations for further validating the robustness of large language models as research tools.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 21: Koehl & Vangsness (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#a-survey-on-human-ai-teaming-with-large-pre-trained-models",
    "href": "llm_energy.html#a-survey-on-human-ai-teaming-with-large-pre-trained-models",
    "title": "Group Decision Lit",
    "section": "A Survey on Human-AI Teaming with Large Pre-Trained Models",
    "text": "A Survey on Human-AI Teaming with Large Pre-Trained Models\nVats, V., Nizam, M. B., Liu, M., Wang, Z., Ho, R., Prasad, M. S., Titterton, V., Malreddy, S. V., Aggarwal, R., Xu, Y., Ding, L., Mehta, J., Grinnell, N., Liu, L., Zhong, S., Gandamani, D. N., Tang, X., Ghosalkar, R., Shen, C., … Davis, J. (2024). A Survey on Human-AI Teaming with Large Pre-Trained Models (arXiv:2403.04931). arXiv. http://arxiv.org/abs/2403.04931\n\n\nAbstract\n\n\nIn the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI Teaming, providing insights for future research, policy development, and strategic implementations aimed at harnessing the full potential of this collaboration for research and societal benefit.\n\n\n\n\n\nTable from Vats et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#conversational-agent-dynamics-with-minority-opinion-and-cognitive-conflict-in-small-group-decision-making.",
    "href": "llm_energy.html#conversational-agent-dynamics-with-minority-opinion-and-cognitive-conflict-in-small-group-decision-making.",
    "title": "Group Decision Lit",
    "section": "Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making.",
    "text": "Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making.\nNishida, Y., Shimojo, S., & Hayashi, Y. (2024). Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making. Japanese Psychological Research. https://doi.org/10.1111/jpr.12552\n\n\nAbstract\n\n\nThis study investigated the impact of group discussions with text-based conversational agents on risk-taking decision-making, which has been under-researched. We also focused on the influence of opinion patterns presented by the agents during discussions and attitudes toward these agents. Through an online experiment, 430 participants read a decision-seeking scenario and expressed the degree of risk they were willing to take. After viewing the text-based opinions of six agents and having a discussion with the agents, participants expressed the degree of risk they were willing to take for the same scenario. The result showed that participants’ risk-taking decisions shifted toward the agents’ group opinions, regardless of whether the agents’ opinions tended to be risky or cautious. Additionally, when the agents’ group opinions were more risk-biased and included a minority opinion, a significant association existed between the degree of the participants’ shift to a riskier decision and their positive attitudes toward the agents. The agents’ group opinions guided participants toward both risky and cautious decisions, and participants’ attitudes toward the agents were associated with their decision-making, albeit to a limited extent.\n\n\n\n\n\nFigure from Nishida et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration",
    "href": "llm_energy.html#a-taxonomy-for-human-llm-interaction-modes-an-initial-exploration",
    "title": "Group Decision Lit",
    "section": "A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration",
    "text": "A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration\nGao, J., Gebreegziabher, S. A., Choo, K. T. W., Li, T. J.-J., Perrault, S. T., & Malone, T. W. (2024). A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration. Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–11. https://doi.org/10.1145/3613905.3650786\n\n\nAbstract\n\n\nWith ChatGPT’s release, conversational prompting has become the most popular form of human-LLM interaction. However, its effectiveness is limited for more complex tasks involving reasoning, creativity, and iteration. Through a systematic analysis of HCI papers published since 2021, we identified four key phases in the human-LLM interaction flow - planning, facilitating, iterating, and testing - to precisely understand the dynamics of this process. Additionally, we have developed a taxonomy of four primary interaction modes: Mode 1: Standard Prompting, Mode 2: User Interface, Mode 3: Context-based, and Mode 4: Agent Facilitator. This taxonomy was further enriched using the “5W1H” guideline method, which involved a detailed examination of definitions, participant roles (Who), the phases that happened (When), human objectives and LLM abilities (What), and the mechanics of each interaction mode (How). We anticipate this taxonomy will contribute to the future design and evaluation of human-LLM interaction.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 22: J. Gao et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#talk2care-an-llm-based-voice-assistant-for-communication-between-healthcare-providers-and-older-adults.",
    "href": "llm_energy.html#talk2care-an-llm-based-voice-assistant-for-communication-between-healthcare-providers-and-older-adults.",
    "title": "Group Decision Lit",
    "section": "Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults.",
    "text": "Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults.\nYang, Z., Xu, X., Yao, B., Rogers, E., Zhang, S., Intille, S., Shara, N., Gao, G. G., & Wang, D. (2024). Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(2), 1–35. https://doi.org/10.1145/3659625\n\n\nAbstract\n\n\nDespite the plethora of telehealth applications to assist home-based older adults and healthcare providers, basic messaging and phone calls are still the most common communication methods, which suffer from limited availability, information loss, and process inefficiencies. One promising solution to facilitate patient-provider communication is to leverage large language models (LLMs) with their powerful natural conversation and summarization capability. However, there is a limited understanding of LLMs’ role during the communication. We first conducted two interview studies with both older adults (N=10) and healthcare providers (N=9) to understand their needs and opportunities for LLMs in patient-provider asynchronous communication. Based on the insights, we built an LLM-powered communication system, Talk2Care, and designed interactive components for both groups: (1) For older adults, we leveraged the convenience and accessibility of voice assistants (VAs) and built an LLM-powered conversational interface for effective information collection. (2) For health providers, we built an LLM-based dashboard to summarize and present important health information based on older adults’ conversations with the VA. We further conducted two user studies with older adults and providers to evaluate the usability of the system. The results showed that Talk2Care could facilitate the communication process, enrich the health information collected from older adults, and considerably save providers’ efforts and time. We envision our work as an initial exploration of LLMs’ capability in the intersection of healthcare and interpersonal communication.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 23: Figures from Z. Yang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making",
    "href": "llm_energy.html#to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making",
    "title": "Group Decision Lit",
    "section": "To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making",
    "text": "To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making\nBuçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21. https://doi.org/10.1145/3449287\n\n\nAbstract\n\n\nPeople supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI’s suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.\n\n\n\n\n\nFigure from Buçinca et al. (2021)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#irrationality-and-cognitive-biases-in-large-language-models.",
    "href": "llm_energy.html#irrationality-and-cognitive-biases-in-large-language-models.",
    "title": "Group Decision Lit",
    "section": "(Ir)rationality and cognitive biases in large language models.",
    "text": "(Ir)rationality and cognitive biases in large language models.\nMacmillan-Scott, O., & Musolesi, M. (2024). (Ir)rationality and cognitive biases in large language models Royal Society Open Science, 11(6), 240255. https://doi.org/10.1098/rsos.240255\n\n\nAbstract\n\n\nDo large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Figures from Macmillan-Scott & Musolesi (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#human-like-intuitive-behavior-and-reasoning-biases-emerged-in-large-language-models-but-disappeared-in-chatgpt",
    "href": "llm_energy.html#human-like-intuitive-behavior-and-reasoning-biases-emerged-in-large-language-models-but-disappeared-in-chatgpt",
    "title": "Group Decision Lit",
    "section": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT",
    "text": "Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT\nHagendorff, T., Fabi, S., & Kosinski, M. (2023). Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. Nature Computational Science, 3(10), 833–838. https://doi.org/10.1038/s43588-023-00527-x\n\n\nAbstract\n\n\nWe design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Figures from Hagendorff et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#using-cognitive-psychology-to-understand-gpt-3.",
    "href": "llm_energy.html#using-cognitive-psychology-to-understand-gpt-3.",
    "title": "Group Decision Lit",
    "section": "Using cognitive psychology to understand GPT-3.",
    "text": "Using cognitive psychology to understand GPT-3.\nBinz, M., & Schulz, E. (2023). Using cognitive psychology to understand GPT-3. Proceedings of the National Academy of Sciences, 120(6), e2218523120. https://doi.org/10.1073/pnas.2218523120\n\n\nAbstract\n\n\nWe study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3’s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3’s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.\n\n\n\n\n\nFigure from Binz & Schulz (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#studying-and-improving-reasoning-in-humans-and-machines.",
    "href": "llm_energy.html#studying-and-improving-reasoning-in-humans-and-machines.",
    "title": "Group Decision Lit",
    "section": "Studying and improving reasoning in humans and machines.",
    "text": "Studying and improving reasoning in humans and machines.\nYax, N., Anlló, H., & Palminteri, S. (2024). Studying and improving reasoning in humans and machines. Communications Psychology, 2(1), 1–16. https://doi.org/10.1038/s44271-024-00091-8\n\n\nAbstract\n\n\nIn the present study, we investigate and compare reasoning in large language models (LLMs) and humans, using a selection of cognitive psychology tools traditionally dedicated to the study of (bounded) rationality. We presented to human participants and an array of pretrained LLMs new variants of classical cognitive experiments, and cross-compared their performances. Our results showed that most of the included models presented reasoning errors akin to those frequently ascribed to error-prone, heuristic-based human reasoning. Notwithstanding this superficial similarity, an in-depth comparison between humans and LLMs indicated important differences with human-like reasoning, with models’ limitations disappearing almost entirely in more recent LLMs’ releases. Moreover, we show that while it is possible to devise strategies to induce better performance, humans and machines are not equally responsive to the same prompting schemes. We conclude by discussing the epistemological implications and challenges of comparing human and machine behavior for both artificial intelligence and cognitive psychology.\n\n\n\n\n\nFigure from Yax et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#exploring-variability-in-risk-taking-with-large-language-models.",
    "href": "llm_energy.html#exploring-variability-in-risk-taking-with-large-language-models.",
    "title": "Group Decision Lit",
    "section": "Exploring variability in risk taking with large language models.",
    "text": "Exploring variability in risk taking with large language models.\nBhatia, S. (2024). Exploring variability in risk taking with large language models. Journal of Experimental Psychology: General, 153(7), 1838–1860. https://doi.org/10.1037/xge0001607\n\n\nAbstract\n\n\nWhat are the sources of individual-level differences in risk taking, and how do they depend on the domain or situation in which the decision is being made? Psychologists currently answer such questions with psychometric methods, which analyze correlations across participant responses in survey data sets. In this article, we analyze the preferences that give rise to these correlations. Our approach uses (a) large language models (LLMs) to quantify everyday risky behaviors in terms of the attributes or reasons that may describe those behaviors, and (b) decision models to map these attributes and reasons onto participant responses. We show that LLM-based decision models can explain observed correlations between behaviors in terms of the reasons different behaviors elicit and explain observed correlations between individuals in terms of the weights different individuals place on reasons, thereby providing a decision theoretic foundation for psychometric findings. Since LLMs can generate quantitative representations for nearly any naturalistic decision, they can be used to make accurate out-of-sample predictions for hundreds of everyday behaviors, predict the reasons why people may or may not want to engage in these behaviors, and interpret these reasons in terms of core psychological constructs. Our approach has important theoretical and practical implications for the study of heterogeneity in everyday behavior.\n\n\nBhatia (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#human-bias-in-ai-models-anchoring-effects-and-mitigation-strategies-in-large-language-models",
    "href": "llm_energy.html#human-bias-in-ai-models-anchoring-effects-and-mitigation-strategies-in-large-language-models",
    "title": "Group Decision Lit",
    "section": "Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models",
    "text": "Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models\nNguyen, J. (2024). Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models. Journal of Behavioral and Experimental Finance, 100971. https://doi.org/10.1016/j.jbef.2024.100971\n\n\nAbstract\n\n\nThis study builds on the seminal work of Tversky and Kahneman (1974), exploring the presence and extent of anchoring bias in forecasts generated by four Large Language Models (LLMs): GPT-4, Claude 2, Gemini Pro and GPT-3.5. In contrast to recent findings of advanced reasoning capabilities in LLMs, our randomised controlled trials reveal the presence of anchoring bias across all models: forecasts are significantly influenced by prior mention of high or low values. We examine two mitigation prompting strategies, ‘Chain of Thought’ and ‘ignore previous’, finding limited and varying degrees of effectiveness. Our results extend the anchoring bias research in finance beyond human decision-making to encompass LLMs, highlighting the importance of deliberate and informed prompting in AI forecasting in both ad hoc LLM use and in crafting few-shot examples.\n\n\n\n\n\nFigure from Nguyen (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#a-turing-test-of-whether-ai-chatbots-are-behaviorally-similar-to-humans",
    "href": "llm_energy.html#a-turing-test-of-whether-ai-chatbots-are-behaviorally-similar-to-humans",
    "title": "Group Decision Lit",
    "section": "A Turing test of whether AI chatbots are behaviorally similar to humans",
    "text": "A Turing test of whether AI chatbots are behaviorally similar to humans\nMei, Q., Xie, Y., Yuan, W., & Jackson, M. O. (2024). A Turing test of whether AI chatbots are behaviorally similar to humans. Proceedings of the National Academy of Sciences, 121(9), e2313925121. https://doi.org/10.1073/pnas.2313925121\n\n\nAbstract\n\n\nWe administer a Turing test to AI chatbots. We examine how chatbots behave in a suite of classic behavioral games that are designed to elicit characteristics such as trust, fairness, risk-aversion, cooperation, etc., as well as how they respond to a traditional Big-5 psychological survey that measures personality traits. ChatGPT-4 exhibits behavioral and personality traits that are statistically indistinguishable from a random human from tens of thousands of human subjects from more than 50 countries. Chatbots also modify their behavior based on previous experience and contexts “as if” they were learning from the interactions and change their behavior in response to different framings of the same strategic situation. Their behaviors are often distinct from average and modal human behaviors, in which case they tend to behave on the more altruistic and cooperative end of the distribution. We estimate that they act as if they are maximizing an average of their own and partner’s payoffs.\n\n\n\n\n\nFigure from Mei et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#deciding-fast-and-slow-the-role-of-cognitive-biases-in-ai-assisted-decision-making",
    "href": "llm_energy.html#deciding-fast-and-slow-the-role-of-cognitive-biases-in-ai-assisted-decision-making",
    "title": "Group Decision Lit",
    "section": "Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making",
    "text": "Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making\nRastogi, C., Zhang, Y., Wei, D., Varshney, K. R., Dhurandhar, A., & Tomsett, R. (2022). Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 6(CSCW1), 1–22. https://doi.org/10.1145/3512930\n\n\nAbstract\n\n\nSeveral strands of research have aimed to bridge the gap between artificial intelligence (AI) and human decision-makers in AI-assisted decision-making, where humans are the consumers of AI model predictions and the ultimate decision-makers in high-stakes applications. However, people’s perception and understanding are often distorted by their cognitive biases, such as confirmation bias, anchoring bias, availability bias, to name a few. In this work, we use knowledge from the field of cognitive science to account for cognitive biases in the human-AI collaborative decision-making setting, and mitigate their negative effects on collaborative performance. To this end, we mathematically model cognitive biases and provide a general framework through which researchers and practitioners can understand the interplay between cognitive biases and human-AI accuracy. We then focus specifically on anchoring bias, a bias commonly encountered in human-AI collaboration. We implement a time-based de-anchoring strategy and conduct our first user experiment that validates its effectiveness in human-AI collaborative decision-making. With this result, we design a time allocation strategy for a resource-constrained setting that achieves optimal human-AI collaboration under some assumptions. We, then, conduct a second user experiment which shows that our time allocation strategy with explanation can effectively de-anchor the human and improve collaborative performance when the AI model has low confidence and is incorrect.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Figures from Rastogi et al. (2022)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#decision-control-and-explanations-in-human-ai-collaboration-improving-user-perceptions-and-compliance",
    "href": "llm_energy.html#decision-control-and-explanations-in-human-ai-collaboration-improving-user-perceptions-and-compliance",
    "title": "Group Decision Lit",
    "section": "Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance",
    "text": "Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance\nWestphal, M., Vössing, M., Satzger, G., Yom-Tov, G. B., & Rafaeli, A. (2023). Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance. Computers in Human Behavior, 144, 107714. https://doi.org/10.1016/j.chb.2023.107714\n\n\nAbstract\n\n\nHuman-AI collaboration has become common, integrating highly complex AI systems into the workplace. Still, it is often ineffective; impaired perceptions – such as low trust or limited understanding – reduce compliance with recommendations provided by the AI system. Drawing from cognitive load theory, we examine two techniques of human-AI collaboration as potential remedies. In three experimental studies, we grant users decision control by empowering them to adjust the system’s recommendations, and we offer explanations for the system’s reasoning. We find decision control positively affects user perceptions of trust and understanding, and improves user compliance with system recommendations. Next, we isolate different effects of providing explanations that may help explain inconsistent findings in recent literature: while explanations help reenact the system’s reasoning, they also increase task complexity. Further, the effectiveness of providing an explanation depends on the specific user’s cognitive ability to handle complex tasks. In summary, our study shows that users benefit from enhanced decision control, while explanations – unless appropriately designed for the specific user – may even harm user perceptions and compliance. This work bears both theoretical and practical implications for the management of human-AI collaboration.\n\n\n\n\n\nFigure from Westphal et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#risk-and-prosocial-behavioural-cues-elicit-human-like-response-patterns-from-ai-chatbots.",
    "href": "llm_energy.html#risk-and-prosocial-behavioural-cues-elicit-human-like-response-patterns-from-ai-chatbots.",
    "title": "Group Decision Lit",
    "section": "Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots.",
    "text": "Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots.\nZhao, Y., Huang, Z., Seligman, M., & Peng, K. (2024). Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots. Scientific Reports, 14(1), 7095. https://doi.org/10.1038/s41598-024-55949-y\n\n\nAbstract\n\n\nEmotions, long deemed a distinctly human characteristic, guide a repertoire of behaviors, e.g., promoting risk-aversion under negative emotional states or generosity under positive ones. The question of whether Artificial Intelligence (AI) can possess emotions remains elusive, chiefly due to the absence of an operationalized consensus on what constitutes ‘emotion’ within AI. Adopting a pragmatic approach, this study investigated the response patterns of AI chatbots—specifically, large language models (LLMs)—to various emotional primes. We engaged AI chatbots as one would human participants, presenting scenarios designed to elicit positive, negative, or neutral emotional states. Multiple accounts of OpenAI’s ChatGPT Plus were then tasked with responding to inquiries concerning investment decisions and prosocial behaviors. Our analysis revealed that ChatGPT-4 bots, when primed with positive, negative, or neutral emotions, exhibited distinct response patterns in both risk-taking and prosocial decisions, a phenomenon less evident in the ChatGPT-3.5 iterations. This observation suggests an enhanced capacity for modulating responses based on emotional cues in more advanced LLMs. While these findings do not suggest the presence of emotions in AI, they underline the feasibility of swaying AI responses by leveraging emotional indicators.\n\n\n\n\n\nZhao et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#do-large-language-models-show-decision-heuristics-similar-to-humans-a-case-study-using-gpt-3.5",
    "href": "llm_energy.html#do-large-language-models-show-decision-heuristics-similar-to-humans-a-case-study-using-gpt-3.5",
    "title": "Group Decision Lit",
    "section": "Do large language models show decision heuristics similar to humans? A case study using GPT-3.5",
    "text": "Do large language models show decision heuristics similar to humans? A case study using GPT-3.5\nSuri, G., Slater, L. R., Ziaee, A., & Nguyen, M. (2024). Do large language models show decision heuristics similar to humans? A case study using GPT-3.5. Journal of Experimental Psychology: General, 153(4), 1066–1075. https://doi.org/10.1037/xge0001547\n\n\nAbstract\n\n\nA Large Language Model (LLM) is an artificial intelligence system trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. Generative Pre-Trained Transformer (GPT)-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics and other context-sensitive responses. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (anchoring, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was influenced by anecdotal information (representativeness and availability heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively—even though both presentations contained statistically equivalent information (framing effect, Study 3); and it valued an owned item more than a newly found item even though the two items were objectively identical (endowment effect, Study 4). In each study, human participants showed similar effects. Heuristics and context-sensitive responses in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM—which lacks these processes—also shows such responses invites consideration of the possibility that language is sufficiently rich to carry these effects and may play a role in generating these effects in humans.\n\n\n\n\n\nFigure from Suri et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#can-large-language-models-capture-human-preferences",
    "href": "llm_energy.html#can-large-language-models-capture-human-preferences",
    "title": "Group Decision Lit",
    "section": "Can Large Language Models Capture Human Preferences?",
    "text": "Can Large Language Models Capture Human Preferences?\nGoli, A., & Singh, A. (2024). Can Large Language Models Capture Human Preferences? Marketing Science. https://doi.org/10.1287/mksc.2023.0306\n\n\nAbstract\n\n\nWe explore the viability of large language models (LLMs), specifically OpenAI’s GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting preferences, with a focus on intertemporal choices. Leveraging the extensive literature on intertemporal discounting for benchmarking, we examine responses from LLMs across various languages and compare them with human responses, exploring preferences between smaller, sooner and larger, later rewards. Our findings reveal that both generative pretrained transformer (GPT) models demonstrate less patience than humans, with GPT-3.5 exhibiting a lexicographic preference for earlier rewards unlike human decision makers. Although GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans. Interestingly, GPT models show greater patience in languages with weak future tense references, such as German and Mandarin, aligning with the existing literature that suggests a correlation between language structure and intertemporal preferences. We demonstrate how prompting GPT to explain its decisions, a procedure we term “chain-of-thought conjoint,” can mitigate, but does not eliminate, discrepancies between LLM and human responses. Although directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings of preferences. Chain-of-thought conjoint provides a structured framework for marketers to use LLMs to identify potential attributes or factors that can explain preference heterogeneity across different customers and contexts.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Figures from Goli & Singh (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#language-models-like-humans-show-content-effects-on-reasoning-tasks",
    "href": "llm_energy.html#language-models-like-humans-show-content-effects-on-reasoning-tasks",
    "title": "Group Decision Lit",
    "section": "Language models, like humans, show content effects on reasoning tasks",
    "text": "Language models, like humans, show content effects on reasoning tasks\nLampinen, A. K., Dasgupta, I., Chan, S. C. Y., Sheahan, H. R., Creswell, A., Kumaran, D., McClelland, J. L., & Hill, F. (2024). Language models, like humans, show content effects on reasoning tasks. PNAS Nexus, 3(7), pgae233. https://doi.org/10.1093/pnasnexus/pgae233\n\n\nAbstract\n\n\nAbstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks but exhibit many imperfections. However, human abstract reasoning is also imperfect. Human reasoning is affected by our real-world knowledge and beliefs, and shows notable “content effects”; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns are central to debates about the fundamental nature of human intelligence. Here, we investigate whether language models—whose prior expectations capture some aspects of human knowledge—similarly mix content into their answers to logic problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art LMs, as well as humans, and find that the LMs reflect many of the same qualitative human patterns on these tasks—like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected in accuracy patterns, and in some lower-level features like the relationship between LM confidence over possible answers and human response times. However, in some cases the humans and models behave differently—particularly on the Wason task, where humans perform much worse than large models, and exhibit a distinct error pattern. Our findings have implications for understanding possible contributors to these human cognitive effects, as well as the factors that influence language model performance.\n\n\nLampinen et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#the-emergence-of-economic-rationality-of-gpt",
    "href": "llm_energy.html#the-emergence-of-economic-rationality-of-gpt",
    "title": "Group Decision Lit",
    "section": "The emergence of economic rationality of GPT",
    "text": "The emergence of economic rationality of GPT\nChen, Y., Liu, T. X., Shan, Y., & Zhong, S. (2023). The emergence of economic rationality of GPT. Proceedings of the National Academy of Sciences, 120(51), e2316205120. https://doi.org/10.1073/pnas.2316205120\n\n\nAbstract\n\n\nAs large language models (LLMs) like GPT become increasingly prevalent, it is essential that we assess their capabilities beyond language processing. This paper examines the economic rationality of GPT by instructing it to make budgetary decisions in four domains: risk, time, social, and food preferences. We measure economic rationality by assessing the consistency of GPT’s decisions with utility maximization in classic revealed preference theory. We find that GPT’s decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature. Moreover, the estimated preference parameters of GPT are slightly different from human subjects and exhibit a lower degree of heterogeneity. We also find that the rationality scores are robust to the degree of randomness and demographic settings such as age and gender but are sensitive to contexts based on the language frames of the choice situations. These results suggest the potential of LLMs to make good decisions and the need to further understand their capabilities, limitations, and underlying mechanisms.\n\n\n\n\n\nFigure from Chen et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#the-potential-of-generative-ai-for-personalized-persuasion-at-scale.",
    "href": "llm_energy.html#the-potential-of-generative-ai-for-personalized-persuasion-at-scale.",
    "title": "Group Decision Lit",
    "section": "The potential of generative AI for personalized persuasion at scale.",
    "text": "The potential of generative AI for personalized persuasion at scale.\nMatz, S. C., Teeny, J. D., Vaid, S. S., Peters, H., Harari, G. M., & Cerf, M. (2024). The potential of generative AI for personalized persuasion at scale. Scientific Reports, 14(1), 4692. https://doi.org/10.1038/s41598-024-53755-0\n\n\nAbstract\n\n\nMatching the language or content of a message to the psychological profile of its recipient (known as “personalized persuasion”) is widely considered to be one of the most effective messaging strategies. We demonstrate that the rapid advances in large language models (LLMs), like ChatGPT, could accelerate this influence by making personalized persuasion scalable. Across four studies (consisting of seven sub-studies; total N = 1788), we show that personalized messages crafted by ChatGPT exhibit significantly more influence than non-personalized messages. This was true across different domains of persuasion (e.g., marketing of consumer products, political appeals for climate action), psychological profiles (e.g., personality traits, political ideology, moral foundations), and when only providing the LLM with a single, short prompt naming or describing the targeted psychological dimension. Thus, our findings are among the first to demonstrate the potential for LLMs to automate, and thereby scale, the use of personalized persuasion in ways that enhance its effectiveness and efficiency. We discuss the implications for researchers, practitioners, and the general public.\n\n\n\n\n\nFigure from Matz et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#decision-making-paradoxes-in-humans-vs-machines-the-case-of-the-allais-and-ellsberg-paradoxes.",
    "href": "llm_energy.html#decision-making-paradoxes-in-humans-vs-machines-the-case-of-the-allais-and-ellsberg-paradoxes.",
    "title": "Group Decision Lit",
    "section": "Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes.",
    "text": "Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes.\nNobandegani, A. S., Rish, I., & Shultz, T. R. (2023). Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes. Proceedings of the Annual Meeting of the Cognitive Science Society, 46. https://arxiv.org/abs/2406.11426\n\n\nAbstract\n\n\nHuman decision-making is filled with a variety of paradoxes demonstrating deviations from rationality principles. Do state-of-the-art artificial intelligence (AI) models also manifest these paradoxes when making decisions? As a case study, in this work we investigate whether GPT-4, a recently released state-of-the-art language model, would show two well-known paradoxes in human decision-making: the Allais paradox and the Ellsberg paradox. We demonstrate that GPT-4 succeeds in the two variants of the Allais paradox (the common-consequence effect and the common-ratio effect) but fails in the case of the Ellsberg paradox. We also show that providing GPT-4 with high-level normative principles allows it to succeed in the Ellsberg paradox, thus elevating GPT-4’s decision-making rationality. We discuss the implications of our work for AI rationality enhancement and AI-assisted decision-making.\n\n\nNobandegani et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#do-llms-exhibit-human-like-response-biases-a-case-study-in-survey-design.",
    "href": "llm_energy.html#do-llms-exhibit-human-like-response-biases-a-case-study-in-survey-design.",
    "title": "Group Decision Lit",
    "section": "Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design.",
    "text": "Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design.\nTjuatja, L., Chen, V., Wu, T., Talwalkwar, A., & Neubig, G. (2024). Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design. Transactions of the Association for Computational Linguistics, 12, 1011–1026. https://doi.org/10.1162/tacl_a_00685\n\n\nAbstract\n\n\nOne widely cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording—but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of “prompts” have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior.\n\n\n\n\n\nFigure from Tjuatja et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#cognitive-ease-at-a-cost-llms-reduce-mental-effort-but-compromise-depth-in-student-scientific-inquiry",
    "href": "llm_energy.html#cognitive-ease-at-a-cost-llms-reduce-mental-effort-but-compromise-depth-in-student-scientific-inquiry",
    "title": "Group Decision Lit",
    "section": "Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry",
    "text": "Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry\nStadler, M., Bannert, M., & Sailer, M. (2024). Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry. Computers in Human Behavior, 160, 108386. https://doi.org/10.1016/j.chb.2024.108386\n\n\nAbstract\n\n\nThis study explores the cognitive load and learning outcomes associated with using large language models (LLMs) versus traditional search engines for information gathering during learning. A total of 91 university students were randomly assigned to either use ChatGPT3.5 or Google to research the socio-scientific issue of nanoparticles in sunscreen to derive valid recommendations and justifications. The study aimed to investigate potential differences in cognitive load, as well as the quality and homogeneity of the students’ recommendations and justifications. Results indicated that students using LLMs experienced significantly lower cognitive load. However, despite this reduction, these students demonstrated lower-quality reasoning and argumentation in their final recommendations compared to those who used traditional search engines. Further, the homogeneity of the recommendations and justifications did not differ significantly between the two groups, suggesting that LLMs did not restrict the diversity of students’ perspectives. These findings highlight the nuanced implications of digital tools on learning, suggesting that while LLMs can decrease the cognitive burden associated with information gathering during a learning task, they may not promote deeper engagement with content necessary for high-quality learning per se.\n\n\nStadler et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#cognitive-llms-towards-integrating-cognitive-architectures-and-large-language-models-for-manufacturing-decision-making",
    "href": "llm_energy.html#cognitive-llms-towards-integrating-cognitive-architectures-and-large-language-models-for-manufacturing-decision-making",
    "title": "Group Decision Lit",
    "section": "Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making",
    "text": "Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making\nWu, S., Oltramari, A., Francis, J., Giles, C. L., & Ritter, F. E. (2024). Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making (arXiv:2408.09176). arXiv. http://arxiv.org/abs/2408.09176\n\n\nAbstract\n\n\nResolving the dichotomy between the human-like yet constrained reasoning processes of Cognitive Architectures and the broad but often noisy inference behavior of Large Language Models (LLMs) remains a challenging but exciting pursuit, for enabling reliable machine reasoning capabilities in production systems. Because Cognitive Architectures are famously developed for the purpose of modeling the internal mechanisms of human cognitive decision-making at a computational level, new investigations consider the goal of informing LLMs with the knowledge necessary for replicating such processes, e.g., guided perception, memory, goal-setting, and action. Previous approaches that use LLMs for grounded decision-making struggle with complex reasoning tasks that require slower, deliberate cognition over fast and intuitive inference—reporting issues related to the lack of sufficient grounding, as in hallucination. To resolve these challenges, we introduce LLM-ACTR, a novel neurosymbolic architecture that provides human-aligned and versatile decision-making by integrating the ACT-R Cognitive Architecture with LLMs. Our framework extracts and embeds knowledge of ACT-R’s internal decision-making process as latent neural representations, injects this information into trainable LLM adapter layers, and fine-tunes the LLMs for downstream prediction. Our experiments on novel Design for Manufacturing tasks show both improved task performance as well as improved grounded decision-making capability of our approach, compared to LLM-only baselines that leverage chain-of-thought reasoning strategies.",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#large-language-models-amplify-human-biases-in-moral-decision-making",
    "href": "llm_energy.html#large-language-models-amplify-human-biases-in-moral-decision-making",
    "title": "Group Decision Lit",
    "section": "Large Language Models Amplify Human Biases in Moral Decision-Making",
    "text": "Large Language Models Amplify Human Biases in Moral Decision-Making\nCheung, V., Maier, M., & Lieder, F. (2024). Large Language Models Amplify Human Biases in Moral Decision-Making (https://osf.io/3kvjd/). https://doi.org/10.31234/osf.io/aj46b\n\n\nAbstract\n\n\nAs large language models (LLMs) become more widely used, people increasingly rely on them to make or advise on moral decisions. Some researchers even propose using LLMs as participants in psychology experiments. It is therefore important to understand how well LLMs make moral decisions and how they compare to humans. We investigated this question in realistic moral dilemmas using prompts where GPT-4, Llama 3, and Claude 3 give advice and where they emulate a research participant. In Study 1, we compared responses from LLMs to a representative US sample (N = 285) for 22 dilemmas: social dilemmas that pitted self-interest against the greater good, and moral dilemmas that pitted utilitarian cost-benefit reasoning against deontological rules. In social dilemmas, LLMs were more altruistic than participants. In moral dilemmas, LLMs exhibited stronger omission bias than participants: they usually endorsed inaction over action. In Study 2 (N = 490, preregistered), we replicated this omission bias and document an additional bias: unlike humans, LLMs (except GPT-4o) tended to answer “no” in moral dilemmas, whereby the phrasing of the question influences the decision even when physical action remains the same. Our findings show that LLM moral decision-making amplifies human biases and introduces potentially problematic biases.\n\n\n\n\n\nFigure from Cheung et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#large-language-model-recall-uncertainty-is-modulated-by-the-fan-effect.",
    "href": "llm_energy.html#large-language-model-recall-uncertainty-is-modulated-by-the-fan-effect.",
    "title": "Group Decision Lit",
    "section": "Large Language Model Recall Uncertainty is Modulated by the Fan Effect.",
    "text": "Large Language Model Recall Uncertainty is Modulated by the Fan Effect.\nRoberts, J., Moore, K., Pham, T., Ewaleifoh, O., & Fisher, D. (2024). Large Language Model Recall Uncertainty is Modulated by the Fan Effect.\n\n\n\nFigure from Roberts et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#accuracy-time-tradeoffs-in-ai-assisted-decision-making-under-time-pressure.",
    "href": "llm_energy.html#accuracy-time-tradeoffs-in-ai-assisted-decision-making-under-time-pressure.",
    "title": "Group Decision Lit",
    "section": "Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure.",
    "text": "Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure.\nSwaroop, S., Buçinca, Z., Gajos, K. Z., & Doshi-Velez, F. (2024). Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure. Proceedings of the 29th International Conference on Intelligent User Interfaces, 138–154. https://doi.org/10.1145/3640543.3645206\n\n\nAbstract\n\n\nIn settings where users both need high accuracy and are timepressured, such as doctors working in emergency rooms, we want to provide AI assistance that both increases decision accuracy and reduces decision-making time. Current literature focusses on how users interact with AI assistance when there is no time pressure, finding that different AI assistances have different benefits: some can reduce time taken while increasing overreliance on AI, while others do the opposite. The precise benefit can depend on both the user and task. In time-pressured scenarios, adapting when we show AI assistance is especially important: relying on the AI assistance can save time, and can therefore be beneficial when the AI is likely to be right. We would ideally adapt what AI assistance we show depending on various properties (of the task and of the user) in order to best trade off accuracy and time. We introduce a study where users have to answer a series of logic puzzles. We find that time pressure affects how users use different AI assistances, making some assistances more beneficial than others when compared to notime-pressure settings. We also find that a user’s overreliance rate is a key predictor of their behaviour: overreliers and not-overreliers use different AI assistance types differently. We find marginal correlations between a user’s overreliance rate (which is related to the user’s trust in AI recommendations) and their personality traits (Big Five Personality traits). Overall, our work suggests that AI assistances have different accuracy-time tradeoffs when people are under time pressure compared to no time pressure, and we explore how we might adapt AI assistances in this setting.\n\n\n\n\n\nFigure from Swaroop et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#the-llm-effect-are-humans-truly-using-llms-or-are-they-being-influenced-by-them-instead",
    "href": "llm_energy.html#the-llm-effect-are-humans-truly-using-llms-or-are-they-being-influenced-by-them-instead",
    "title": "Group Decision Lit",
    "section": "The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?",
    "text": "The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?\nChoi, A. S., Akter, S. S., Singh, J. P., & Anastasopoulos, A. (2024). The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead? (arXiv:2410.04699). arXiv. http://arxiv.org/abs/2410.04699\n\n\nAbstract\n\n\nLarge Language Models (LLMs) have shown capabilities close to human performance in various analytical tasks, leading researchers to use them for time and labor-intensive analyses. However, their capability to handle highly specialized and open-ended tasks in domains like policy studies remains in question. This paper investigates the efficiency and accuracy of LLMs in specialized tasks through a structured user study focusing on Human-LLM partnership. The study, conducted in two stages-Topic Discovery and Topic Assignment-integrates LLMs with expert annotators to observe the impact of LLM suggestions on what is usually human-only analysis. Results indicate that LLM-generated topic lists have significant overlap with human generated topic lists, with minor hiccups in missing document-specific topics. However, LLM suggestions may significantly improve task completion speed, but at the same time introduce anchoring bias, potentially affecting the depth and nuance of the analysis, raising a critical question about the trade-off between increased efficiency and the risk of biased analysis.\n\n\n\n\n\nFigure from Choi et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#mutual-theory-of-mind-in-human-ai-collaboration-an-empirical-study-with-llm-driven-ai-agents-in-a-real-time-shared-workspace-task",
    "href": "llm_energy.html#mutual-theory-of-mind-in-human-ai-collaboration-an-empirical-study-with-llm-driven-ai-agents-in-a-real-time-shared-workspace-task",
    "title": "Group Decision Lit",
    "section": "Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task",
    "text": "Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task\nZhang, S., Wang, X., Zhang, W., Chen, Y., Gao, L., Wang, D., Zhang, W., Wang, X., & Wen, Y. (2024). Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task (arXiv:2409.08811). arXiv. http://arxiv.org/abs/2409.08811\n\n\nAbstract\n\n\nTheory of Mind (ToM) significantly impacts human collaboration and communication as a crucial capability to understand others. When AI agents with ToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises in such human-AI teams (HATs). The MToM process, which involves interactive communication and ToM-based strategy adjustment, affects the team’s performance and collaboration process. To explore the MToM process, we conducted a mixed-design experiment using a large language model-driven AI agent with ToM and communication modules in a real-time shared-workspace task. We find that the agent’s ToM capability does not significantly impact team performance but enhances human understanding of the agent and the feeling of being understood. Most participants in our study believe verbal communication increases human burden, and the results show that bidirectional communication leads to lower HAT performance. We discuss the results’ implications for designing AI agents that collaborate with humans in real-time shared workspace tasks.\n\n\n\n\n\nFigure from S. Zhang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.-1",
    "href": "llm_energy.html#human-ai-teaming-leveraging-transactive-memory-and-speaking-up-for-enhanced-team-effectiveness.-1",
    "title": "Transactive Memory Systems",
    "section": "Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness.",
    "text": "Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness.\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nAbstract\n\n\nIn this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team’s ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team’s transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond.\n\n\n\n\n\n\nBienefeld et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#communication-in-transactive-memory-systems-a-review-and-multidimensional-network-perspective",
    "href": "llm_energy.html#communication-in-transactive-memory-systems-a-review-and-multidimensional-network-perspective",
    "title": "Transactive Memory Systems",
    "section": "Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective",
    "text": "Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective\nYan, B., Hollingshead, A. B., Alexander, K. S., Cruz, I., & Shaikh, S. J. (2021). Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective. Small Group Research, 52(1), 3–32. https://doi.org/10.1177/1046496420967764\n\n\nAbstract\n\n\nThe comprehensive review synthesizes 64 empirical studies on communication and transactive memory systems (TMS). The results reveal that (a) a TMS forms through communication about expertise; (b) as a TMS develops, communication to allocate information and coordinate retrieval increases, promoting information exchange; and (c) groups update their TMS through communicative learning. However, direct interpersonal communication is not necessary for TMS development or utilization. Nor do high-quality information-sharing processes always occur within developed TMS structures. For future research, we propose a multidimensional network approach to TMS that incorporates technologies, addresses member characteristics, considers multiple communication types, and situates groups in context.",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#alignment-transactive-memory-and-collective-cognitive-systems",
    "href": "llm_energy.html#alignment-transactive-memory-and-collective-cognitive-systems",
    "title": "Transactive Memory Systems",
    "section": "Alignment, Transactive Memory, and Collective Cognitive Systems",
    "text": "Alignment, Transactive Memory, and Collective Cognitive Systems\nTollefsen, D. P., Dale, R., & Paxton, A. (2013). Alignment, Transactive Memory, and Collective Cognitive Systems. Review of Philosophy and Psychology, 4(1), 49–64. https://doi.org/10.1007/s13164-012-0126-z\n\n\nAbstract\n\n\nResearch on linguistic interaction suggests that two or more individuals can sometimes form adaptive and cohesive systems. We describe an “alignment system” as a loosely interconnected set of cognitive processes that facilitate social interactions. As a dynamic, multi-component system, it is responsive to higher-level cognitive states such as shared beliefs and intentions (those involving collective intentionality) but can also give rise to such shared cognitive states via bottom-up processes. As an example of putative group cognition we turn to transactive memory and suggest how further research on alignment in these cases might reveal how such systems can be genuinely described as cognitive. Finally, we address a prominent critique of collective cognitive systems, arguing that there is much empirical and explanatory benefit to be gained from considering the possibility of group cognitive systems, especially in the context of small-group human interaction.",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#building-machines-that-learn-and-think-with-people-1",
    "href": "llm_energy.html#building-machines-that-learn-and-think-with-people-1",
    "title": "Transactive Memory Systems",
    "section": "Building Machines that Learn and Think with People",
    "text": "Building Machines that Learn and Think with People\nCollins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building Machines that Learn and Think with People (arXiv:2408.03943). arXiv. http://arxiv.org/abs/2408.03943\n\n\nAbstract\n\n\nWhat do we want from machine intelligence? We envision machines that are not just tools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable, and trustworthy systems that think with us. Current artificial intelligence (AI) systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called “thought partners,” systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and AI thought partners can engage and propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.\n\n\n\n\n\n\nFigure from Collins et al. (2024)\n\n\nArgote, L., & Ren, Y. (2012). Transactive Memory Systems: A Microfoundation of Dynamic Capabilities. Journal of Management Studies, 49(8), 1375–1382. https://doi.org/10.1111/j.1467-6486.2012.01077.x\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\nBrandon, D. P., & Hollingshead, A. B. (2004). Transactive Memory Systems in Organizations: Matching Tasks, Expertise, and People. Organization Science, 15(6), 633–644. https://doi.org/10.1287/orsc.1040.0069\nHollingshead, A. B. (1998). Communication, Learning, and Retrieval in Transactive Memory Systems. Journal of Experimental Social Psychology, 34(5), 423–442. https://doi.org/10.1006/jesp.1998.1358\nKimura, T. (2024). Virtual Teams: A Smart Literature Review of Four Decades of Research. Human Behavior and Emerging Technologies, 2024(1), 8373370. https://doi.org/10.1155/2024/8373370\nMarjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit. https://cocosci.princeton.edu/papers/marjieh2024task.pdf\nMcWilliams, D. J., & Randolph, A. B. (2024). Transactive memory systems in superteams: The effect of an intelligent assistant in virtual teams. Information Technology & People, ahead-of-print(ahead-of-print). https://doi.org/10.1108/ITP-12-2022-0918\nSamipour-Biel, S. P. (2022). A Process Model of Transactive Memory System Shared Knowledge Structure Emergence: A Computational Model in R [Ph.D., The University of Akron]. https://www.proquest.com/docview/2711844070/abstract/DBDAB24DBBB34601PQ/1\nTollefsen, D. P., Dale, R., & Paxton, A. (2013). Alignment, Transactive Memory, and Collective Cognitive Systems. Review of Philosophy and Psychology, 4(1), 49–64. https://doi.org/10.1007/s13164-012-0126-z\nUden, L., & Ting, I.-H. (Eds.). (2024). The Design of AI-Enabled Experience-Based Knowledge Management System to Facilitate Knowing and Doing in Communities of Practice (Vol. 2152). Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-63269-3\nWegner, D. M. (1995). A Computer Network Model of Human Transactive Memory. Social Cognition, 13(3), 319–339. https://doi.org/10.1521/soco.1995.13.3.319\nYan, B., Hollingshead, A. B., Alexander, K. S., Cruz, I., & Shaikh, S. J. (2021). Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective. Small Group Research, 52(1), 3–32. https://doi.org/10.1177/1046496420967764",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "tms.html#task-allocation-in-teams-as-a-multi-armed-bandit.",
    "href": "tms.html#task-allocation-in-teams-as-a-multi-armed-bandit.",
    "title": "Transactive Memory Systems",
    "section": "Task Allocation in Teams as a Multi-Armed Bandit.",
    "text": "Task Allocation in Teams as a Multi-Armed Bandit.\nMarjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit. https://cocosci.princeton.edu/papers/marjieh2024task.pdf\n\n\nAbstract\n\n\nHumans rely on efficient distribution of resources to transcend the abilities of individuals. Successful task allocation, whether in small teams or across large institutions, depends on individuals’ ability to discern their own and others’ strengths and weaknesses, and to optimally act on them. This dependence creates a tension between exploring the capabilities of others and exploiting the knowledge acquired so far, which can be challenging. How do people navigate this tension? To address this question, we propose a novel task allocation paradigm in which a human agent is asked to repeatedly allocate tasks in three distinct classes (categorizing a blurry image, detecting a noisy voice command, and solving an anagram) between themselves and two other (bot) team members to maximize team performance. We show that this problem can be recast as a combinatorial multi-armed bandit which allows us to compare people’s performance against two well-known strategies, Thompson Sampling and Upper Confidence Bound (UCB). We find that humans are able to successfully integrate information about the capabilities of different team members to infer optimal allocations, and in some cases perform on par with these optimal strategies. Our approach opens up new avenues for studying the mechanisms underlying collective cooperation in teams.\n\n\n\n\n\nFigure from Marjieh et al. (2024)",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "ai_gd.html#ai-can-help-humans-find-common-ground-in-democratic-deliberation.",
    "href": "ai_gd.html#ai-can-help-humans-find-common-ground-in-democratic-deliberation.",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Tessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., & Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science, 386(6719), eadq2852. https://doi.org/10.1126/science.adq2852\n\n\nAbstract\n\n\nFinding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.\n\n\n\n\n\n\n\n\n\nFigure 1: Figures from Tessler et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_decision.html#bridging-the-gulf-of-envisioning-cognitive-design-challenges-in-llm-interfaces",
    "href": "ai_decision.html#bridging-the-gulf-of-envisioning-cognitive-design-challenges-in-llm-interfaces",
    "title": "Individual decision lit",
    "section": "Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces",
    "text": "Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces\nSubramonyam, H., Pea, R., Pondoc, C. L., Agrawala, M., & Seifert, C. (2024). Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces (arXiv:2309.14459; Version 2). arXiv. http://arxiv.org/abs/2309.14459\n\n\nAbstract\n\n\nLarge language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Norman’s gulfs of execution and evaluation. To address this gap, we theorize how end-users ‘envision’ translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments: (1) knowing whether LLMs can accomplish the task, (2) how to instruct the LLM to do the task, and (3) how to evaluate the success of the LLM’s output in meeting the goal. Finally, we make recommendations to narrow the envisioning gulf in human-LLM interactions.\n\n\n\n\n\nFigure from Subramonyam et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#learning-to-guide-human-decision-makers-with-vision-language-models",
    "href": "ai_decision.html#learning-to-guide-human-decision-makers-with-vision-language-models",
    "title": "Individual decision lit",
    "section": "Learning To Guide Human Decision Makers With Vision-Language Models",
    "text": "Learning To Guide Human Decision Makers With Vision-Language Models\nBanerjee, D., Teso, S., Sayin, B., & Passerini, A. (2024). Learning To Guide Human Decision Makers With Vision-Language Models (arXiv:2403.16501). arXiv. http://arxiv.org/abs/2403.16501\n\n\nAbstract\n\n\nThere is increasing interest in developing AIs for assisting human decision-making in high-stakes tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain. Mainstream approaches team up an expert with a machine learning model to which safer decisions are offloaded, thus letting the former focus on cases that demand their attention. his separation of responsibilities setup, however, is inadequate for high-stakes scenarios. On the one hand, the expert may end up over-relying on the machine’s decisions due to anchoring bias, thus losing the human oversight that is increasingly being required by regulatory agencies to ensure trustworthy AI. On the other hand, the expert is left entirely unassisted on the (typically hardest) decisions on which the model abstained. As a remedy, we introduce learning to guide (LTG), an alternative framework in which - rather than taking control from the human expert - the machine provides guidance useful for decision making, and the human is entirely responsible for coming up with a decision. In order to ensure guidance is interpretable} and task-specific, we develop SLOG, an approach for turning any vision-language model into a capable generator of textual guidance by leveraging a modicum of human feedback. Our empirical evaluation highlights the promise of SLOG on a challenging, real-world medical diagnosis task.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Figures from Banerjee et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "tms.html#bridging-the-gulf-of-envisioning-cognitive-design-challenges-in-llm-interfaces",
    "href": "tms.html#bridging-the-gulf-of-envisioning-cognitive-design-challenges-in-llm-interfaces",
    "title": "Transactive Memory Systems",
    "section": "Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces",
    "text": "Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces\nSubramonyam, H., Pea, R., Pondoc, C. L., Agrawala, M., & Seifert, C. (2024). Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces (arXiv:2309.14459; Version 2). arXiv. http://arxiv.org/abs/2309.14459\n\n\nAbstract\n\n\nLarge language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Norman’s gulfs of execution and evaluation. To address this gap, we theorize how end-users ‘envision’ translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments: (1) knowing whether LLMs can accomplish the task, (2) how to instruct the LLM to do the task, and (3) how to evaluate the success of the LLM’s output in meeting the goal. Finally, we make recommendations to narrow the envisioning gulf in human-LLM interactions.\n\n\n\n\n\nFigure from Subramonyam et al. (2024)",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "tms.html#misc-papers",
    "href": "tms.html#misc-papers",
    "title": "Transactive Memory Systems",
    "section": "Misc Papers",
    "text": "Misc Papers\nArgote, L., & Ren, Y. (2012). Transactive Memory Systems: A Microfoundation of Dynamic Capabilities. Journal of Management Studies, 49(8), 1375–1382. https://doi.org/10.1111/j.1467-6486.2012.01077.x\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\nBrandon, D. P., & Hollingshead, A. B. (2004). Transactive Memory Systems in Organizations: Matching Tasks, Expertise, and People. Organization Science, 15(6), 633–644. https://doi.org/10.1287/orsc.1040.0069\nHollingshead, A. B. (1998). Communication, Learning, and Retrieval in Transactive Memory Systems. Journal of Experimental Social Psychology, 34(5), 423–442. https://doi.org/10.1006/jesp.1998.1358\nKimura, T. (2024). Virtual Teams: A Smart Literature Review of Four Decades of Research. Human Behavior and Emerging Technologies, 2024(1), 8373370. https://doi.org/10.1155/2024/8373370\nMarjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit. https://cocosci.princeton.edu/papers/marjieh2024task.pdf\nMcWilliams, D. J., & Randolph, A. B. (2024). Transactive memory systems in superteams: The effect of an intelligent assistant in virtual teams. Information Technology & People, ahead-of-print(ahead-of-print). https://doi.org/10.1108/ITP-12-2022-0918\nSamipour-Biel, S. P. (2022). A Process Model of Transactive Memory System Shared Knowledge Structure Emergence: A Computational Model in R [Ph.D., The University of Akron]. https://www.proquest.com/docview/2711844070/abstract/DBDAB24DBBB34601PQ/1\nTollefsen, D. P., Dale, R., & Paxton, A. (2013). Alignment, Transactive Memory, and Collective Cognitive Systems. Review of Philosophy and Psychology, 4(1), 49–64. https://doi.org/10.1007/s13164-012-0126-z\nUden, L., & Ting, I.-H. (Eds.). (2024). The Design of AI-Enabled Experience-Based Knowledge Management System to Facilitate Knowing and Doing in Communities of Practice (Vol. 2152). Springer Nature Switzerland. https://doi.org/10.1007/978-3-031-63269-3\nWegner, D. M. (1995). A Computer Network Model of Human Transactive Memory. Social Cognition, 13(3), 319–339. https://doi.org/10.1521/soco.1995.13.3.319\nYan, B., Hollingshead, A. B., Alexander, K. S., Cruz, I., & Shaikh, S. J. (2021). Communication in Transactive Memory Systems: A Review and Multidimensional Network Perspective. Small Group Research, 52(1), 3–32. https://doi.org/10.1177/1046496420967764",
    "crumbs": [
      "Misc",
      "Transactive Memory Systems"
    ]
  },
  {
    "objectID": "ai_decision.html#how-does-value-similarity-affect-human-reliance-in-ai-assisted-ethical-decision-making",
    "href": "ai_decision.html#how-does-value-similarity-affect-human-reliance-in-ai-assisted-ethical-decision-making",
    "title": "Individual decision lit",
    "section": "How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making?",
    "text": "How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making?\nNarayanan, S., Yu, G., Ho, C.-J., & Yin, M. (2023). How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making? Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, 49–57. https://doi.org/10.1145/3600211.3604709\n\n\nAbstract\n\n\nThis paper explores the impact of value similarity between humans and AI on human reliance in the context of AI-assisted ethical decision-making. Using kidney allocation as a case study, we conducted a randomized human-subject experiment where workers were presented with ethical dilemmas in various conditions, including no AI recommendations, recommendations from a similar AI, and recommendations from a dissimilar AI. We found that recommendations provided by a dissimilar AI had a higher overall effect on human decisions than recommendations from a similar AI. However, when humans and AI disagreed, participants were more likely to change their decisions when provided with recommendations from a similar AI. The effect was not due to humans’ perceptions of the AI being similar, but rather due to the AI displaying similar ethical values through its recommendations. We also conduct a preliminary analysis on the relationship between value similarity and trust, and potential shifts in ethical preferences at the population-level.\n\n\n\n\n\nFigure from Narayanan et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_interaction.html",
    "href": "ai_interaction.html",
    "title": "Interactive AI Lit",
    "section": "",
    "text": "Wang, B., Liu, J., Karimnazarov, J., & Thompson, N. (2024). Task Supportive and Personalized Human-Large Language Model Interaction: A User Study. Proceedings of the 2024 ACM SIGIR Conference on Human Information Interaction and Retrieval, 370–375. https://doi.org/10.1145/3627508.3638344\n\n\nAbstract\n\n\nLarge language model (LLM) applications, such as ChatGPT, are a powerful tool for online information-seeking (IS) and problem-solving tasks. However, users still face challenges initializing and refining prompts, and their cognitive barriers and biased perceptions further impede task completion. These issues reflect broader challenges identified within the fields of IS and interactive information retrieval (IIR). To address these, our approach integrates task context and user perceptions into human-ChatGPT interactions through prompt engineering. We developed a ChatGPT-like platform integrated with supportive functions, including perception articulation, prompt suggestion, and conversation explanation. Our findings of a user study demonstrate that the supportive functions help users manage expectations, reduce cognitive loads, better refine prompts, and increase user engagement. This research enhances our comprehension of designing proactive and user-centric systems with LLMs. It offers insights into evaluating human-LLM interactions and emphasizes potential challenges for under served users.\n\n\n\n\n\nFigure from B. Wang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "ai_interaction.html#transitioning-to-human-centered-ai-a-systematic-review-of-theories-scenarios-and-hypotheses-in-human-ai-interactions.",
    "href": "ai_interaction.html#transitioning-to-human-centered-ai-a-systematic-review-of-theories-scenarios-and-hypotheses-in-human-ai-interactions.",
    "title": "Interactive AI Lit",
    "section": "Transitioning to Human-Centered AI: A Systematic Review of Theories, Scenarios, and Hypotheses in Human-AI Interactions.",
    "text": "Transitioning to Human-Centered AI: A Systematic Review of Theories, Scenarios, and Hypotheses in Human-AI Interactions.\nWang, D., Zheng, K., Li, C., & Guo, J. (2024). Transitioning to Human-Centered AI: A Systematic Review of Theories, Scenarios, and Hypotheses in Human-AI Interactions. Proceedings of the Association for Information Science and Technology, 61(1), 673–678. https://doi.org/10.1002/pra2.1078\n\n\nAbstract\n\n\nThis study conducted a systematic review of human-AI interaction (HAI)over the past decade for the implemented theories and scenarios, and the tested hypotheses to discover the changes in the current transition to human-centered AI (HCAI). Moving from acceptance theories, Computers are social actors (CASA), anthropomorphism, and the integrative trust model are the most frequent theories. Augmentation scenarios of decision-making, teamwork, and human-AI collaborations are common in the latest HAI studies. Users’ trust, acceptance, and intention to use an AI system are the main research targets in HAI studies. These trends show a clear transition toward HCAI. This paper also discusses opportunities tied to HAI studies based on the interconnections between the various theories, scenarios, and hypotheses.\n\n\n\n\n\n\n\n\n\nFigure 1: Figures from D. Wang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "ai_interaction.html#task-supportive-and-personalized-human-large-language-model-interaction-a-user-study.",
    "href": "ai_interaction.html#task-supportive-and-personalized-human-large-language-model-interaction-a-user-study.",
    "title": "Interactive AI Lit",
    "section": "",
    "text": "Wang, B., Liu, J., Karimnazarov, J., & Thompson, N. (2024). Task Supportive and Personalized Human-Large Language Model Interaction: A User Study. Proceedings of the 2024 ACM SIGIR Conference on Human Information Interaction and Retrieval, 370–375. https://doi.org/10.1145/3627508.3638344\n\n\nAbstract\n\n\nLarge language model (LLM) applications, such as ChatGPT, are a powerful tool for online information-seeking (IS) and problem-solving tasks. However, users still face challenges initializing and refining prompts, and their cognitive barriers and biased perceptions further impede task completion. These issues reflect broader challenges identified within the fields of IS and interactive information retrieval (IIR). To address these, our approach integrates task context and user perceptions into human-ChatGPT interactions through prompt engineering. We developed a ChatGPT-like platform integrated with supportive functions, including perception articulation, prompt suggestion, and conversation explanation. Our findings of a user study demonstrate that the supportive functions help users manage expectations, reduce cognitive loads, better refine prompts, and increase user engagement. This research enhances our comprehension of designing proactive and user-centric systems with LLMs. It offers insights into evaluating human-LLM interactions and emphasizes potential challenges for under served users.\n\n\n\n\n\nFigure from B. Wang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "ai_decision.html#determinants-of-llm-assisted-decision-makin",
    "href": "ai_decision.html#determinants-of-llm-assisted-decision-makin",
    "title": "Individual decision lit",
    "section": "Determinants of LLM-assisted Decision-Makin",
    "text": "Determinants of LLM-assisted Decision-Makin\nEigner, E., & Händler, T. (2024). Determinants of LLM-assisted Decision-Making (arXiv:2402.17385). arXiv. http://arxiv.org/abs/2402.17385\n\n\nAbstract\n\n\nDecision-making is a fundamental capability in everyday life. Large Language Models (LLMs) provide multifaceted support in enhancing human decision-making processes. However, understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions. This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with LLM support. In particular, we explore the effects of technological aspects of LLMs, including transparency and prompt engineering, psychological factors such as emotions and decision-making styles, as well as decisionspecific determinants such as task difficulty and accountability. In addition, the impact of the determinants on the decision-making process is illustrated via multiple application scenarios. Drawing from our analysis, we develop a dependency framework that systematizes possible interactions in terms of reciprocal interdependencies between these determinants. Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user’s mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes. Our findings can be seen as crucial for improving decision quality in human-AI collaboration, empowering both users and organizations, and designing more effective LLM interfaces. Additionally, our work provides a foundation for future empirical investigations on the determinants of decision-making assisted by LLMs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Figures from Eigner & Händler (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#determinants-of-llm-assisted-decision-making",
    "href": "ai_decision.html#determinants-of-llm-assisted-decision-making",
    "title": "Individual decision lit",
    "section": "Determinants of LLM-assisted Decision-Making",
    "text": "Determinants of LLM-assisted Decision-Making\nEigner, E., & Händler, T. (2024). Determinants of LLM-assisted Decision-Making (arXiv:2402.17385). arXiv. http://arxiv.org/abs/2402.17385\n\n\nAbstract\n\n\nDecision-making is a fundamental capability in everyday life. Large Language Models (LLMs) provide multifaceted support in enhancing human decision-making processes. However, understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions. This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with LLM support. In particular, we explore the effects of technological aspects of LLMs, including transparency and prompt engineering, psychological factors such as emotions and decision-making styles, as well as decision specific determinants such as task difficulty and accountability. In addition, the impact of the determinants on the decision-making process is illustrated via multiple application scenarios. Drawing from our analysis, we develop a dependency framework that systematizes possible interactions in terms of reciprocal interdependencies between these determinants. Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user’s mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes. Our findings can be seen as crucial for improving decision quality in human-AI collaboration, empowering both users and organizations, and designing more effective LLM interfaces. Additionally, our work provides a foundation for future empirical investigations on the determinants of decision-making assisted by LLMs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Figures from Eigner & Händler (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#a-taxonomy-of-human-and-ml-strengths-in-decision-making-to-investigate-human-ml-complementarity.",
    "href": "ai_decision.html#a-taxonomy-of-human-and-ml-strengths-in-decision-making-to-investigate-human-ml-complementarity.",
    "title": "Individual decision lit",
    "section": "A Taxonomy of Human and ML Strengths in Decision-Making to Investigate Human-ML Complementarity.",
    "text": "A Taxonomy of Human and ML Strengths in Decision-Making to Investigate Human-ML Complementarity.\nRastogi, C., Leqi, L., Holstein, K., & Heidari, H. (2023). A Taxonomy of Human and ML Strengths in Decision-Making to Investigate Human-ML Complementarity. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 11, 127–139. https://doi.org/10.1609/hcomp.v11i1.27554\n\n\nAbstract\n\n\nHybrid human-ML systems increasingly make consequential decisions in a wide range of domains. These systems are often introduced with the expectation that the combined human-ML system will achieve complementary performance, that is, the combined decision-making system will be an improvement compared with either decision-making agent in isolation. However, empirical results have been mixed, and existing research rarely articulates the sources and mechanisms by which complementary performance is expected to arise. Our goal in this work is to provide conceptual tools to advance the way researchers reason and communicate about human-ML complementarity. Drawing upon prior literature in human psychology, machine learning, and human-computer interaction, we propose a taxonomy characterizing distinct ways in which human and ML-based decision-making can differ. In doing so, we conceptually map potential mechanisms by which combining human and ML decision-making may yield complementary performance, developing a language for the research community to reason about design of hybrid systems in any decision-making domain. To illustrate how our taxonomy can be used to investigate complementarity, we provide a mathematical aggregation framework to examine enabling conditions for complementarity. Through synthetic simulations, we demonstrate how this framework can be used to explore specific aspects of our taxonomy and shed light on the optimal mechanisms for combining human-ML judgments.\n\n\nRastogi et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#take-caution-in-using-llms-as-human-surrogates-scylla-ex-machina",
    "href": "ai_decision.html#take-caution-in-using-llms-as-human-surrogates-scylla-ex-machina",
    "title": "Individual decision lit",
    "section": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina",
    "text": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina\nGao, Y., Lee, D., Burtch, G., & Fazelpour, S. (2024). Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina (No. arXiv:2410.19599). arXiv. http://arxiv.org/abs/2410.19599\n\n\nAbstract\n\n\nHuman decision-making is filled with a variety of paradoxes demonstrating deviations from rationality principles. Do state-of-the-art artificial intelligence (AI) models also manifest these paradoxes when making decisions? As a case study, in this work we investigate whether GPT-4, a recently released state-of-the-art language model, would show two well-known paradoxes in human decision-making: the Allais paradox and the Ellsberg paradox. We demonstrate that GPT-4 succeeds in the two variants of the Allais paradox (the common-consequence effect and the common-ratio effect) but fails in the case of the Ellsberg paradox. We also show that providing GPT-4 with high-level normative principles allows it to succeed in the Ellsberg paradox, thus elevating GPT-4’s decision-making rationality. We discuss the implications of our work for AI rationality enhancement and AI-assisted decision-making.\n\n\n\n\n\nFigure from Gao et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_decision.html#towards-a-science-of-human-ai-decision-making-an-overview-of-design-space-in-empirical-human-subject-studies.",
    "href": "ai_decision.html#towards-a-science-of-human-ai-decision-making-an-overview-of-design-space-in-empirical-human-subject-studies.",
    "title": "Individual decision lit",
    "section": "Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies.",
    "text": "Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies.\nLai, V., Chen, C., Smith-Renner, A., Liao, Q. V., & Tan, C. (2023). Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies. 2023 ACM Conference on Fairness, Accountability, and Transparency, 1369–1385. https://doi.org/10.1145/3593013.3594087\n\n\nAbstract\n\n\nAI systems are adopted in numerous domains due to their increas- ingly strong predictive performance. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time-consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our work highlights the need to develop com- mon frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other’s work and produce generalizable scientific knowledge. We also hope this work will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.\n\n\n\n\n\nFigure from Lai et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_interaction.html#human-creativity-in-the-age-of-llms-randomized-experiments-on-divergent-and-convergent-thinking",
    "href": "ai_interaction.html#human-creativity-in-the-age-of-llms-randomized-experiments-on-divergent-and-convergent-thinking",
    "title": "Interactive AI Lit",
    "section": "Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking",
    "text": "Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking\nKumar, H., Vincentius, J., Jordan, E., & Anderson, A. (2024). Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking (No. arXiv:2410.03703). arXiv. http://arxiv.org/abs/2410.03703\n\n\nAbstract\n\n\nLarge language models are transforming the creative process by offering unprecedented capabilities to algorithmically generate ideas. While these tools can enhance human creativity when people co-create with them, it’s unclear how this will impact unassisted human creativity. We conducted two large pre-registered parallel experiments involving 1,100 participants attempting tasks targeting the two core components of creativity, divergent and convergent thinking. We compare the effects of two forms of large language model (LLM) assistance – a standard LLM providing direct answers and a coach-like LLM offering guidance – with a control group receiving no AI assistance, and focus particularly on how all groups perform in a final, unassisted stage. Our findings reveal that while LLM assistance can provide short-term boosts in creativity during assisted tasks, it may inadvertently hinder independent creative performance when users work without assistance, raising concerns about the long-term impact on human creativity and cognition.\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Figures from Kumar et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "ai_interaction.html#to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making",
    "href": "ai_interaction.html#to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making",
    "title": "Interactive AI Lit",
    "section": "To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making",
    "text": "To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making\nBuçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21. https://doi.org/10.1145/3449287\n\n\nAbstract\n\n\nPeople supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI’s suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.\n\n\n\n\n\nFigure from Buçinca et al. (2021)",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "ai_interaction.html#ai-can-help-humans-find-common-ground-in-democratic-deliberation.",
    "href": "ai_interaction.html#ai-can-help-humans-find-common-ground-in-democratic-deliberation.",
    "title": "Interactive AI Lit",
    "section": "AI can help humans find common ground in democratic deliberation.",
    "text": "AI can help humans find common ground in democratic deliberation.\nTessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., & Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science, 386(6719), eadq2852. https://doi.org/10.1126/science.adq2852\n\n\nAbstract\n\n\nFinding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.\n\n\n\n\n\n\n\n\n\nFigure 3: Figures from Tessler et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "ai_interaction.html#evaluating-language-models-for-mathematics-through-interactions",
    "href": "ai_interaction.html#evaluating-language-models-for-mathematics-through-interactions",
    "title": "Interactive AI Lit",
    "section": "Evaluating Language Models for Mathematics through Interactions",
    "text": "Evaluating Language Models for Mathematics through Interactions\nCollins, K. M., Jiang, A. Q., Frieder, S., Wong, L., Zilka, M., Bhatt, U., Lukasiewicz, T., Wu, Y., Tenenbaum, J. B., Hart, W., Gowers, T., Li, W., Weller, A., & Jamnik, M. (2023). Evaluating Language Models for Mathematics through Interactions (No. arXiv:2306.01694). arXiv. http://arxiv.org/abs/2306.01694\n\n\nAbstract\n\n\nThe standard methodology of evaluating large language models (LLMs) based on static pairs of inputs and outputs is insufficient for developing assistants: this kind of assessments fails to take into account the essential interactive element in their deployment, and therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analysing MathConverse, we derive a preliminary taxonomy of human behaviours and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generations, amongst other findings. Further, we identify useful scenarios and existing issues of GPT-4 in mathematical reasoning through a series of case studies contributed by expert mathematicians. We conclude with actionable takeaways for ML practitioners and mathematicians: models which communicate uncertainty, respond well to user corrections, are more interpretable and concise may constitute better assistants; interactive evaluation is a promising way to continually navigate the capability of these models; humans should be aware of language models’ algebraic fallibility, and for that reason discern where they should be used.\n\n\n\n\n\n\n\n\n\n\nFigure 4: Figures from Collins et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "ai_interaction.html#large-language-models-experimentation-interface",
    "href": "ai_interaction.html#large-language-models-experimentation-interface",
    "title": "Interactive AI Lit",
    "section": "Large Language Models Experimentation Interface",
    "text": "Large Language Models Experimentation Interface\nLaban, G., Laban, T., & Gunes, H. (2024). LEXI: Large Language Models Experimentation Interface (No. arXiv:2407.01488). arXiv. http://arxiv.org/abs/2407.01488\n\n\nAbstract\n\n\nThe recent developments in Large Language Models (LLM), mark a significant moment in the research and development of social interactions with artificial agents. These agents are widely deployed in a variety of settings, with potential impact on users. However, the study of social interactions with agents powered by LLM is still emerging, limited by access to the technology and to data, the absence of standardised interfaces, and challenges to establishing controlled experimental setups using the currently available business-oriented platforms. To answer these gaps, we developed LEXI, LLMs Experimentation Interface, an open-source tool enabling the deployment of artificial agents powered by LLM in social interaction behavioural experiments. Using a graphical interface, LEXI allows researchers to build agents, and deploy them in experimental setups along with forms and questionnaires while collecting interaction logs and self-reported data. The outcomes of usability testing indicate LEXI’s broad utility, high usability and minimum mental workload requirement, with distinctive benefits observed across disciplines. A proof-of-concept study exploring the tool’s efficacy in evaluating social HAIs was conducted, resulting in high-quality data. A comparison of empathetic versus neutral agents indicated that people perceive empathetic agents as more social, and write longer and more positive messages towards them.\n\n\nLaban et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "ai_interaction.html#human-ai-collaboration-in-cooperative-games-a-study-of-playing-codenames-with-an-llm-assistant",
    "href": "ai_interaction.html#human-ai-collaboration-in-cooperative-games-a-study-of-playing-codenames-with-an-llm-assistant",
    "title": "Interactive AI Lit",
    "section": "Human-AI Collaboration in Cooperative Games: A Study of Playing Codenames with an LLM Assistant",
    "text": "Human-AI Collaboration in Cooperative Games: A Study of Playing Codenames with an LLM Assistant\nSidji, M., Smith, W., & Rogerson, M. J. (2024). Human-AI Collaboration in Cooperative Games: A Study of Playing Codenames with an LLM Assistant. Proc. ACM Hum.-Comput. Interact., 8(CHI PLAY), 316:1-316:25. https://doi.org/10.1145/3677081\n\n\nAbstract\n\n\nPlaying partial information, restricted communication, cooperative (PIRCC) games with humans have proven challenging for AI, due to our reliance on social dynamics and sophisticated cognitive techniques. Yet, recent advances in generative AI may change this situation through new forms of human-AI collaboration. This paper investigates how teams of players interact with an AI assistant in the PIRCC game Codenames and the impact this has on cognition, social dynamics, and player experience. We observed gameplay and conducted post-game focus groups with 54 participants across ten groups. Each group played three rounds of Codenames, with an AI assistant supporting Cluegivers. We found the AI assistant enhanced players’ convergent and divergent thinking, but interfered with formation of team mental models, highlighting a tension in the use of AI in creative team scenarios. The presence of the AI challenged many players’ understanding of the ‘spirit of the game’. Furthermore, the presence of the AI assistants weakened social connections between human teammates, but strengthened connections across teams. This paper provides an empirical account of an AI assistant’s effect on cognition, social dynamics, and player experience in Codenames. We highlight the opportunities and challenges that arise when designing hybrid digital boardgames that include AI assistants.\n\n\nSidji et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "ai_interaction.html#effects-of-interacting-with-a-large-language-model-compared-with-a-human-coach-on-the-clinical-diagnostic-process-and-outcomes-among-fourth-year-medical-students-study-protocol-for-a-prospective-randomised-experiment-using-patient-vignettes",
    "href": "ai_interaction.html#effects-of-interacting-with-a-large-language-model-compared-with-a-human-coach-on-the-clinical-diagnostic-process-and-outcomes-among-fourth-year-medical-students-study-protocol-for-a-prospective-randomised-experiment-using-patient-vignettes",
    "title": "Interactive AI Lit",
    "section": "Effects of interacting with a large language model compared with a human coach on the clinical diagnostic process and outcomes among fourth-year medical students: Study protocol for a prospective, randomised experiment using patient vignettes",
    "text": "Effects of interacting with a large language model compared with a human coach on the clinical diagnostic process and outcomes among fourth-year medical students: Study protocol for a prospective, randomised experiment using patient vignettes\nKämmer, J. E., Hautz, W. E., Krummrey, G., Sauter, T. C., Penders, D., Birrenbach, T., & Bienefeld, N. (2024). Effects of interacting with a large language model compared with a human coach on the clinical diagnostic process and outcomes among fourth-year medical students: Study protocol for a prospective, randomised experiment using patient vignettes. BMJ Open, 14(7), e087469. https://doi.org/10.1136/bmjopen-2024-087469\n\n\nAbstract\n\n\nVersatile large language models (LLMs) have the potential to augment diagnostic decision-­making by assisting diagnosticians, thanks to their ability to engage in open-­ended, natural conversations and their comprehensive knowledge access. Yet the novelty of LLMs in diagnostic decision-­making introduces uncertainties regarding their impact. Clinicians unfamiliar with the use of LLMs in their professional context may rely on general attitudes towards LLMs more broadly, potentially hindering thoughtful use and critical evaluation of their input, leading to either over-­reliance and lack of critical thinking or an unwillingness to use LLMs as diagnostic aids. To address these concerns, this study examines the influence on the diagnostic process and outcomes of interacting with an LLM compared with a human coach, and of prior training vs no training for interacting with either of these ‘coaches’. Our findings aim to illuminate the potential benefits and risks of employing artificial intelligence (AI) in diagnostic decision-­making. Methods and analysis  We are conducting a prospective, randomised experiment with N=158 fourth-­year medical students from Charité Medical School, Berlin, Germany. Participants are asked to diagnose patient vignettes after being assigned to either a human coach or ChatGPT and after either training or no training (both between-­subject factors). We are specifically collecting data on the effects of using either of these ‘coaches’ and of additional training on information search, number of hypotheses entertained, diagnostic accuracy and confidence. Statistical methods will include linear mixed effects models. Exploratory analyses of the interaction patterns and attitudes towards AI will also generate more generalisable knowledge about the role of AI in medicine.\n\n\n\n\n\nFigure from Kämmer et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "ai_interaction.html#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.",
    "href": "ai_interaction.html#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.",
    "title": "Interactive AI Lit",
    "section": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate.",
    "text": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate.\nChiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nAbstract\n\n\nGroup decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil’s advocate in the AI-assisted group deci- sion making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities ex- hibited by modern large language models (LLMs), we design four different styles of devil’s advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil’s advocates that argue against the AI model’s decision recommenda- tion have the potential to promote groups’ appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil’s advocate usually does not lead to substantial increases in people’s perceived workload for completing the group decision making tasks, while interactive LLM-powered devil’s advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.\n\n\n\n\n\nFigure from Chiang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "llm_energy.html#bridging-the-gulf-of-envisioning-cognitive-design-challenges-in-llm-interfaces",
    "href": "llm_energy.html#bridging-the-gulf-of-envisioning-cognitive-design-challenges-in-llm-interfaces",
    "title": "Group Decision Lit",
    "section": "Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces",
    "text": "Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces\nSubramonyam, H., Pea, R., Pondoc, C. L., Agrawala, M., & Seifert, C. (2024). Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces (arXiv:2309.14459; Version 2). arXiv. http://arxiv.org/abs/2309.14459\n\n\nAbstract\n\n\nLarge language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Norman’s gulfs of execution and evaluation. To address this gap, we theorize how end-users ‘envision’ translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments: (1) knowing whether LLMs can accomplish the task, (2) how to instruct the LLM to do the task, and (3) how to evaluate the success of the LLM’s output in meeting the goal. Finally, we make recommendations to narrow the envisioning gulf in human-LLM interactions.\n\n\n\n\n\nFigure from Subramonyam et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#learning-to-guide-human-decision-makers-with-vision-language-models",
    "href": "llm_energy.html#learning-to-guide-human-decision-makers-with-vision-language-models",
    "title": "Group Decision Lit",
    "section": "Learning To Guide Human Decision Makers With Vision-Language Models",
    "text": "Learning To Guide Human Decision Makers With Vision-Language Models\nBanerjee, D., Teso, S., Sayin, B., & Passerini, A. (2024). Learning To Guide Human Decision Makers With Vision-Language Models (arXiv:2403.16501). arXiv. http://arxiv.org/abs/2403.16501\n\n\nAbstract\n\n\nThere is increasing interest in developing AIs for assisting human decision-making in high-stakes tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain. Mainstream approaches team up an expert with a machine learning model to which safer decisions are offloaded, thus letting the former focus on cases that demand their attention. his separation of responsibilities setup, however, is inadequate for high-stakes scenarios. On the one hand, the expert may end up over-relying on the machine’s decisions due to anchoring bias, thus losing the human oversight that is increasingly being required by regulatory agencies to ensure trustworthy AI. On the other hand, the expert is left entirely unassisted on the (typically hardest) decisions on which the model abstained. As a remedy, we introduce learning to guide (LTG), an alternative framework in which - rather than taking control from the human expert - the machine provides guidance useful for decision making, and the human is entirely responsible for coming up with a decision. In order to ensure guidance is interpretable} and task-specific, we develop SLOG, an approach for turning any vision-language model into a capable generator of textual guidance by leveraging a modicum of human feedback. Our empirical evaluation highlights the promise of SLOG on a challenging, real-world medical diagnosis task.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 11: Figures from Banerjee et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#how-does-value-similarity-affect-human-reliance-in-ai-assisted-ethical-decision-making",
    "href": "llm_energy.html#how-does-value-similarity-affect-human-reliance-in-ai-assisted-ethical-decision-making",
    "title": "Group Decision Lit",
    "section": "How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making?",
    "text": "How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making?\nNarayanan, S., Yu, G., Ho, C.-J., & Yin, M. (2023). How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making? Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, 49–57. https://doi.org/10.1145/3600211.3604709\n\n\nAbstract\n\n\nThis paper explores the impact of value similarity between humans and AI on human reliance in the context of AI-assisted ethical decision-making. Using kidney allocation as a case study, we conducted a randomized human-subject experiment where workers were presented with ethical dilemmas in various conditions, including no AI recommendations, recommendations from a similar AI, and recommendations from a dissimilar AI. We found that recommendations provided by a dissimilar AI had a higher overall effect on human decisions than recommendations from a similar AI. However, when humans and AI disagreed, participants were more likely to change their decisions when provided with recommendations from a similar AI. The effect was not due to humans’ perceptions of the AI being similar, but rather due to the AI displaying similar ethical values through its recommendations. We also conduct a preliminary analysis on the relationship between value similarity and trust, and potential shifts in ethical preferences at the population-level.\n\n\n\n\n\nFigure from Narayanan et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#determinants-of-llm-assisted-decision-making",
    "href": "llm_energy.html#determinants-of-llm-assisted-decision-making",
    "title": "Group Decision Lit",
    "section": "Determinants of LLM-assisted Decision-Making",
    "text": "Determinants of LLM-assisted Decision-Making\nEigner, E., & Händler, T. (2024). Determinants of LLM-assisted Decision-Making (arXiv:2402.17385). arXiv. http://arxiv.org/abs/2402.17385\n\n\nAbstract\n\n\nDecision-making is a fundamental capability in everyday life. Large Language Models (LLMs) provide multifaceted support in enhancing human decision-making processes. However, understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions. This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with LLM support. In particular, we explore the effects of technological aspects of LLMs, including transparency and prompt engineering, psychological factors such as emotions and decision-making styles, as well as decision specific determinants such as task difficulty and accountability. In addition, the impact of the determinants on the decision-making process is illustrated via multiple application scenarios. Drawing from our analysis, we develop a dependency framework that systematizes possible interactions in terms of reciprocal interdependencies between these determinants. Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user’s mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes. Our findings can be seen as crucial for improving decision quality in human-AI collaboration, empowering both users and organizations, and designing more effective LLM interfaces. Additionally, our work provides a foundation for future empirical investigations on the determinants of decision-making assisted by LLMs.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 12: Figures from Eigner & Händler (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#a-taxonomy-of-human-and-ml-strengths-in-decision-making-to-investigate-human-ml-complementarity.",
    "href": "llm_energy.html#a-taxonomy-of-human-and-ml-strengths-in-decision-making-to-investigate-human-ml-complementarity.",
    "title": "Group Decision Lit",
    "section": "A Taxonomy of Human and ML Strengths in Decision-Making to Investigate Human-ML Complementarity.",
    "text": "A Taxonomy of Human and ML Strengths in Decision-Making to Investigate Human-ML Complementarity.\nRastogi, C., Leqi, L., Holstein, K., & Heidari, H. (2023). A Taxonomy of Human and ML Strengths in Decision-Making to Investigate Human-ML Complementarity. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 11, 127–139. https://doi.org/10.1609/hcomp.v11i1.27554\n\n\nAbstract\n\n\nHybrid human-ML systems increasingly make consequential decisions in a wide range of domains. These systems are often introduced with the expectation that the combined human-ML system will achieve complementary performance, that is, the combined decision-making system will be an improvement compared with either decision-making agent in isolation. However, empirical results have been mixed, and existing research rarely articulates the sources and mechanisms by which complementary performance is expected to arise. Our goal in this work is to provide conceptual tools to advance the way researchers reason and communicate about human-ML complementarity. Drawing upon prior literature in human psychology, machine learning, and human-computer interaction, we propose a taxonomy characterizing distinct ways in which human and ML-based decision-making can differ. In doing so, we conceptually map potential mechanisms by which combining human and ML decision-making may yield complementary performance, developing a language for the research community to reason about design of hybrid systems in any decision-making domain. To illustrate how our taxonomy can be used to investigate complementarity, we provide a mathematical aggregation framework to examine enabling conditions for complementarity. Through synthetic simulations, we demonstrate how this framework can be used to explore specific aspects of our taxonomy and shed light on the optimal mechanisms for combining human-ML judgments.\n\n\nRastogi et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#take-caution-in-using-llms-as-human-surrogates-scylla-ex-machina",
    "href": "llm_energy.html#take-caution-in-using-llms-as-human-surrogates-scylla-ex-machina",
    "title": "Group Decision Lit",
    "section": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina",
    "text": "Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina\nGao, Y., Lee, D., Burtch, G., & Fazelpour, S. (2024). Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina (No. arXiv:2410.19599). arXiv. http://arxiv.org/abs/2410.19599\n\n\nAbstract\n\n\nHuman decision-making is filled with a variety of paradoxes demonstrating deviations from rationality principles. Do state-of-the-art artificial intelligence (AI) models also manifest these paradoxes when making decisions? As a case study, in this work we investigate whether GPT-4, a recently released state-of-the-art language model, would show two well-known paradoxes in human decision-making: the Allais paradox and the Ellsberg paradox. We demonstrate that GPT-4 succeeds in the two variants of the Allais paradox (the common-consequence effect and the common-ratio effect) but fails in the case of the Ellsberg paradox. We also show that providing GPT-4 with high-level normative principles allows it to succeed in the Ellsberg paradox, thus elevating GPT-4’s decision-making rationality. We discuss the implications of our work for AI rationality enhancement and AI-assisted decision-making.\n\n\n\n\n\nFigure from Y. Gao et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#towards-a-science-of-human-ai-decision-making-an-overview-of-design-space-in-empirical-human-subject-studies.",
    "href": "llm_energy.html#towards-a-science-of-human-ai-decision-making-an-overview-of-design-space-in-empirical-human-subject-studies.",
    "title": "Group Decision Lit",
    "section": "Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies.",
    "text": "Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies.\nLai, V., Chen, C., Smith-Renner, A., Liao, Q. V., & Tan, C. (2023). Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies. 2023 ACM Conference on Fairness, Accountability, and Transparency, 1369–1385. https://doi.org/10.1145/3593013.3594087\n\n\nAbstract\n\n\nAI systems are adopted in numerous domains due to their increas- ingly strong predictive performance. However, in high-stakes domains such as criminal justice and healthcare, full automation is often not desirable due to safety, ethical, and legal concerns, yet fully manual approaches can be inaccurate and time-consuming. As a result, there is growing interest in the research community to augment human decision making with AI assistance. Besides developing AI technologies for this purpose, the emerging field of human-AI decision making must embrace empirical approaches to form a foundational understanding of how humans interact and work with AI to make decisions. To invite and help structure research efforts towards a science of understanding and improving human-AI decision making, we survey recent literature of empirical human-subject studies on this topic. We summarize the study design choices made in over 100 papers in three important aspects: (1) decision tasks, (2) AI assistance elements, and (3) evaluation metrics. For each aspect, we summarize current trends, discuss gaps in current practices of the field, and make a list of recommendations for future research. Our work highlights the need to develop com- mon frameworks to account for the design and research spaces of human-AI decision making, so that researchers can make rigorous choices in study design, and the research community can build on each other’s work and produce generalizable scientific knowledge. We also hope this work will serve as a bridge for HCI and AI communities to work together to mutually shape the empirical science and computational technologies for human-AI decision making.\n\n\n\n\n\nFigure from Lai et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#task-supportive-and-personalized-human-large-language-model-interaction-a-user-study.",
    "href": "llm_energy.html#task-supportive-and-personalized-human-large-language-model-interaction-a-user-study.",
    "title": "Group Decision Lit",
    "section": "Task Supportive and Personalized Human-Large Language Model Interaction: A User Study.",
    "text": "Task Supportive and Personalized Human-Large Language Model Interaction: A User Study.\nWang, B., Liu, J., Karimnazarov, J., & Thompson, N. (2024). Task Supportive and Personalized Human-Large Language Model Interaction: A User Study. Proceedings of the 2024 ACM SIGIR Conference on Human Information Interaction and Retrieval, 370–375. https://doi.org/10.1145/3627508.3638344\n\n\nAbstract\n\n\nLarge language model (LLM) applications, such as ChatGPT, are a powerful tool for online information-seeking (IS) and problem-solving tasks. However, users still face challenges initializing and refining prompts, and their cognitive barriers and biased perceptions further impede task completion. These issues reflect broader challenges identified within the fields of IS and interactive information retrieval (IIR). To address these, our approach integrates task context and user perceptions into human-ChatGPT interactions through prompt engineering. We developed a ChatGPT-like platform integrated with supportive functions, including perception articulation, prompt suggestion, and conversation explanation. Our findings of a user study demonstrate that the supportive functions help users manage expectations, reduce cognitive loads, better refine prompts, and increase user engagement. This research enhances our comprehension of designing proactive and user-centric systems with LLMs. It offers insights into evaluating human-LLM interactions and emphasizes potential challenges for under served users.\n\n\n\n\n\nFigure from B. Wang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#transitioning-to-human-centered-ai-a-systematic-review-of-theories-scenarios-and-hypotheses-in-human-ai-interactions.",
    "href": "llm_energy.html#transitioning-to-human-centered-ai-a-systematic-review-of-theories-scenarios-and-hypotheses-in-human-ai-interactions.",
    "title": "Group Decision Lit",
    "section": "Transitioning to Human-Centered AI: A Systematic Review of Theories, Scenarios, and Hypotheses in Human-AI Interactions.",
    "text": "Transitioning to Human-Centered AI: A Systematic Review of Theories, Scenarios, and Hypotheses in Human-AI Interactions.\nWang, D., Zheng, K., Li, C., & Guo, J. (2024). Transitioning to Human-Centered AI: A Systematic Review of Theories, Scenarios, and Hypotheses in Human-AI Interactions. Proceedings of the Association for Information Science and Technology, 61(1), 673–678. https://doi.org/10.1002/pra2.1078\n\n\nAbstract\n\n\nThis study conducted a systematic review of human-AI interaction (HAI)over the past decade for the implemented theories and scenarios, and the tested hypotheses to discover the changes in the current transition to human-centered AI (HCAI). Moving from acceptance theories, Computers are social actors (CASA), anthropomorphism, and the integrative trust model are the most frequent theories. Augmentation scenarios of decision-making, teamwork, and human-AI collaborations are common in the latest HAI studies. Users’ trust, acceptance, and intention to use an AI system are the main research targets in HAI studies. These trends show a clear transition toward HCAI. This paper also discusses opportunities tied to HAI studies based on the interconnections between the various theories, scenarios, and hypotheses.\n\n\n\n\n\n\n\n\n\nFigure 13: Figures from D. Wang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#human-creativity-in-the-age-of-llms-randomized-experiments-on-divergent-and-convergent-thinking",
    "href": "llm_energy.html#human-creativity-in-the-age-of-llms-randomized-experiments-on-divergent-and-convergent-thinking",
    "title": "Group Decision Lit",
    "section": "Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking",
    "text": "Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking\nKumar, H., Vincentius, J., Jordan, E., & Anderson, A. (2024). Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking (No. arXiv:2410.03703). arXiv. http://arxiv.org/abs/2410.03703\n\n\nAbstract\n\n\nLarge language models are transforming the creative process by offering unprecedented capabilities to algorithmically generate ideas. While these tools can enhance human creativity when people co-create with them, it’s unclear how this will impact unassisted human creativity. We conducted two large pre-registered parallel experiments involving 1,100 participants attempting tasks targeting the two core components of creativity, divergent and convergent thinking. We compare the effects of two forms of large language model (LLM) assistance – a standard LLM providing direct answers and a coach-like LLM offering guidance – with a control group receiving no AI assistance, and focus particularly on how all groups perform in a final, unassisted stage. Our findings reveal that while LLM assistance can provide short-term boosts in creativity during assisted tasks, it may inadvertently hinder independent creative performance when users work without assistance, raising concerns about the long-term impact on human creativity and cognition.\n\n\n\n\n\n\n\n\n\n\n\nFigure 14: Figures from Kumar et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making-1",
    "href": "llm_energy.html#to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making-1",
    "title": "Group Decision Lit",
    "section": "To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making",
    "text": "To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making\nBuçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21. https://doi.org/10.1145/3449287\n\n\nAbstract\n\n\nPeople supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI’s suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.\n\n\n\n\n\nFigure from Buçinca et al. (2021)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#ai-can-help-humans-find-common-ground-in-democratic-deliberation.",
    "href": "llm_energy.html#ai-can-help-humans-find-common-ground-in-democratic-deliberation.",
    "title": "Group Decision Lit",
    "section": "AI can help humans find common ground in democratic deliberation.",
    "text": "AI can help humans find common ground in democratic deliberation.\nTessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., & Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science, 386(6719), eadq2852. https://doi.org/10.1126/science.adq2852\n\n\nAbstract\n\n\nFinding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.\n\n\n\n\n\n\n\n\n\nFigure 15: Figures from Tessler et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#evaluating-language-models-for-mathematics-through-interactions",
    "href": "llm_energy.html#evaluating-language-models-for-mathematics-through-interactions",
    "title": "Group Decision Lit",
    "section": "Evaluating Language Models for Mathematics through Interactions",
    "text": "Evaluating Language Models for Mathematics through Interactions\nCollins, K. M., Jiang, A. Q., Frieder, S., Wong, L., Zilka, M., Bhatt, U., Lukasiewicz, T., Wu, Y., Tenenbaum, J. B., Hart, W., Gowers, T., Li, W., Weller, A., & Jamnik, M. (2023). Evaluating Language Models for Mathematics through Interactions (No. arXiv:2306.01694). arXiv. http://arxiv.org/abs/2306.01694\n\n\nAbstract\n\n\nThe standard methodology of evaluating large language models (LLMs) based on static pairs of inputs and outputs is insufficient for developing assistants: this kind of assessments fails to take into account the essential interactive element in their deployment, and therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models~(InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analysing MathConverse, we derive a preliminary taxonomy of human behaviours and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generations, amongst other findings. Further, we identify useful scenarios and existing issues of GPT-4 in mathematical reasoning through a series of case studies contributed by expert mathematicians. We conclude with actionable takeaways for ML practitioners and mathematicians: models which communicate uncertainty, respond well to user corrections, are more interpretable and concise may constitute better assistants; interactive evaluation is a promising way to continually navigate the capability of these models; humans should be aware of language models’ algebraic fallibility, and for that reason discern where they should be used.\n\n\n\n\n\n\n\n\n\n\nFigure 16: Figures from Collins et al. (2023)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#large-language-models-experimentation-interface",
    "href": "llm_energy.html#large-language-models-experimentation-interface",
    "title": "Group Decision Lit",
    "section": "Large Language Models Experimentation Interface",
    "text": "Large Language Models Experimentation Interface\nLaban, G., Laban, T., & Gunes, H. (2024). LEXI: Large Language Models Experimentation Interface (No. arXiv:2407.01488). arXiv. http://arxiv.org/abs/2407.01488\n\n\nAbstract\n\n\nThe recent developments in Large Language Models (LLM), mark a significant moment in the research and development of social interactions with artificial agents. These agents are widely deployed in a variety of settings, with potential impact on users. However, the study of social interactions with agents powered by LLM is still emerging, limited by access to the technology and to data, the absence of standardised interfaces, and challenges to establishing controlled experimental setups using the currently available business-oriented platforms. To answer these gaps, we developed LEXI, LLMs Experimentation Interface, an open-source tool enabling the deployment of artificial agents powered by LLM in social interaction behavioural experiments. Using a graphical interface, LEXI allows researchers to build agents, and deploy them in experimental setups along with forms and questionnaires while collecting interaction logs and self-reported data. The outcomes of usability testing indicate LEXI’s broad utility, high usability and minimum mental workload requirement, with distinctive benefits observed across disciplines. A proof-of-concept study exploring the tool’s efficacy in evaluating social HAIs was conducted, resulting in high-quality data. A comparison of empathetic versus neutral agents indicated that people perceive empathetic agents as more social, and write longer and more positive messages towards them.\n\n\nLaban et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#human-ai-collaboration-in-cooperative-games-a-study-of-playing-codenames-with-an-llm-assistant",
    "href": "llm_energy.html#human-ai-collaboration-in-cooperative-games-a-study-of-playing-codenames-with-an-llm-assistant",
    "title": "Group Decision Lit",
    "section": "Human-AI Collaboration in Cooperative Games: A Study of Playing Codenames with an LLM Assistant",
    "text": "Human-AI Collaboration in Cooperative Games: A Study of Playing Codenames with an LLM Assistant\nSidji, M., Smith, W., & Rogerson, M. J. (2024). Human-AI Collaboration in Cooperative Games: A Study of Playing Codenames with an LLM Assistant. Proc. ACM Hum.-Comput. Interact., 8(CHI PLAY), 316:1-316:25. https://doi.org/10.1145/3677081\n\n\nAbstract\n\n\nPlaying partial information, restricted communication, cooperative (PIRCC) games with humans have proven challenging for AI, due to our reliance on social dynamics and sophisticated cognitive techniques. Yet, recent advances in generative AI may change this situation through new forms of human-AI collaboration. This paper investigates how teams of players interact with an AI assistant in the PIRCC game Codenames and the impact this has on cognition, social dynamics, and player experience. We observed gameplay and conducted post-game focus groups with 54 participants across ten groups. Each group played three rounds of Codenames, with an AI assistant supporting Cluegivers. We found the AI assistant enhanced players’ convergent and divergent thinking, but interfered with formation of team mental models, highlighting a tension in the use of AI in creative team scenarios. The presence of the AI challenged many players’ understanding of the ‘spirit of the game’. Furthermore, the presence of the AI assistants weakened social connections between human teammates, but strengthened connections across teams. This paper provides an empirical account of an AI assistant’s effect on cognition, social dynamics, and player experience in Codenames. We highlight the opportunities and challenges that arise when designing hybrid digital boardgames that include AI assistants.\n\n\nSidji et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#effects-of-interacting-with-a-large-language-model-compared-with-a-human-coach-on-the-clinical-diagnostic-process-and-outcomes-among-fourth-year-medical-students-study-protocol-for-a-prospective-randomised-experiment-using-patient-vignettes",
    "href": "llm_energy.html#effects-of-interacting-with-a-large-language-model-compared-with-a-human-coach-on-the-clinical-diagnostic-process-and-outcomes-among-fourth-year-medical-students-study-protocol-for-a-prospective-randomised-experiment-using-patient-vignettes",
    "title": "Group Decision Lit",
    "section": "Effects of interacting with a large language model compared with a human coach on the clinical diagnostic process and outcomes among fourth-year medical students: Study protocol for a prospective, randomised experiment using patient vignettes",
    "text": "Effects of interacting with a large language model compared with a human coach on the clinical diagnostic process and outcomes among fourth-year medical students: Study protocol for a prospective, randomised experiment using patient vignettes\nKämmer, J. E., Hautz, W. E., Krummrey, G., Sauter, T. C., Penders, D., Birrenbach, T., & Bienefeld, N. (2024). Effects of interacting with a large language model compared with a human coach on the clinical diagnostic process and outcomes among fourth-year medical students: Study protocol for a prospective, randomised experiment using patient vignettes. BMJ Open, 14(7), e087469. https://doi.org/10.1136/bmjopen-2024-087469\n\n\nAbstract\n\n\nVersatile large language models (LLMs) have the potential to augment diagnostic decision-­making by assisting diagnosticians, thanks to their ability to engage in open-­ended, natural conversations and their comprehensive knowledge access. Yet the novelty of LLMs in diagnostic decision-­making introduces uncertainties regarding their impact. Clinicians unfamiliar with the use of LLMs in their professional context may rely on general attitudes towards LLMs more broadly, potentially hindering thoughtful use and critical evaluation of their input, leading to either over-­reliance and lack of critical thinking or an unwillingness to use LLMs as diagnostic aids. To address these concerns, this study examines the influence on the diagnostic process and outcomes of interacting with an LLM compared with a human coach, and of prior training vs no training for interacting with either of these ‘coaches’. Our findings aim to illuminate the potential benefits and risks of employing artificial intelligence (AI) in diagnostic decision-­making. Methods and analysis  We are conducting a prospective, randomised experiment with N=158 fourth-­year medical students from Charité Medical School, Berlin, Germany. Participants are asked to diagnose patient vignettes after being assigned to either a human coach or ChatGPT and after either training or no training (both between-­subject factors). We are specifically collecting data on the effects of using either of these ‘coaches’ and of additional training on information search, number of hypotheses entertained, diagnostic accuracy and confidence. Statistical methods will include linear mixed effects models. Exploratory analyses of the interaction patterns and attitudes towards AI will also generate more generalisable knowledge about the role of AI in medicine.\n\n\n\n\n\nFigure from Kämmer et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#ai-can-help-humans-find-common-ground-in-democratic-deliberation.-1",
    "href": "llm_energy.html#ai-can-help-humans-find-common-ground-in-democratic-deliberation.-1",
    "title": "Group Decision Lit",
    "section": "AI can help humans find common ground in democratic deliberation.",
    "text": "AI can help humans find common ground in democratic deliberation.\nTessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., & Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science, 386(6719), eadq2852. https://doi.org/10.1126/science.adq2852\n\n\nAbstract\n\n\nFinding agreement through a free exchange of views is often difficult. Collective deliberation can be slow, difficult to scale, and unequally attentive to different voices. In this study, we trained an artificial intelligence (AI) to mediate human deliberation. Using participants’ personal opinions and critiques, the AI mediator iteratively generates and refines statements that express common ground among the group on social or political issues. Participants (N = 5734) preferred AI-generated statements to those written by human mediators, rating them as more informative, clear, and unbiased. Discussants often updated their views after the deliberation, converging on a shared perspective. Text embeddings revealed that successful group statements incorporated dissenting voices while respecting the majority position. These findings were replicated in a virtual citizens’ assembly involving a demographically representative sample of the UK population.\n\n\n\n\n\n\n\n\n\nFigure 17: Figures from Tessler et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "llm_energy.html#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.-1",
    "href": "llm_energy.html#enhancing-ai-assisted-group-decision-making-through-llm-powered-devils-advocate.-1",
    "title": "Group Decision Lit",
    "section": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate.",
    "text": "Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate.\nChiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nAbstract\n\n\nGroup decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil’s advocate in the AI-assisted group deci- sion making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities ex- hibited by modern large language models (LLMs), we design four different styles of devil’s advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil’s advocates that argue against the AI model’s decision recommenda- tion have the potential to promote groups’ appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil’s advocate usually does not lead to substantial increases in people’s perceived workload for completing the group decision making tasks, while interactive LLM-powered devil’s advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.\n\n\n\n\n\nFigure from Chiang et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy"
    ]
  },
  {
    "objectID": "ai_decision.html#references",
    "href": "ai_decision.html#references",
    "title": "Individual decision lit",
    "section": "References",
    "text": "References\n\n\nBanerjee, D., Teso, S., Sayin, B., & Passerini, A. (2024). Learning To Guide Human Decision Makers With Vision-Language Models (arXiv:2403.16501). arXiv. https://arxiv.org/abs/2403.16501\n\n\nBhatia, S. (2024). Exploring variability in risk taking with large language models. Journal of Experimental Psychology: General, 153(7), 1838–1860. https://doi.org/10.1037/xge0001607\n\n\nBinz, M., & Schulz, E. (2023). Using cognitive psychology to understand GPT-3. Proceedings of the National Academy of Sciences, 120(6), e2218523120. https://doi.org/10.1073/pnas.2218523120\n\n\nBuçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21. https://doi.org/10.1145/3449287\n\n\nChen, Y., Liu, T. X., Shan, Y., & Zhong, S. (2023). The emergence of economic rationality of GPT. Proceedings of the National Academy of Sciences, 120(51), e2316205120. https://doi.org/10.1073/pnas.2316205120\n\n\nCheung, V., Maier, M., & Lieder, F. (2024). Large Language Models Amplify Human Biases in Moral Decision-Making. https://doi.org/10.31234/osf.io/aj46b\n\n\nChoi, A. S., Akter, S. S., Singh, J. P., & Anastasopoulos, A. (2024). The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead? (arXiv:2410.04699). arXiv. https://arxiv.org/abs/2410.04699\n\n\nEigner, E., & Händler, T. (2024). Determinants of LLM-assisted Decision-Making (arXiv:2402.17385). arXiv. https://arxiv.org/abs/2402.17385\n\n\nGao, Y., Lee, D., Burtch, G., & Fazelpour, S. (2024). Take Caution in Using LLMs as Human Surrogates: Scylla Ex Machina (arXiv:2410.19599). arXiv. https://arxiv.org/abs/2410.19599\n\n\nGoli, A., & Singh, A. (2024). Can Large Language Models Capture Human Preferences? Marketing Science. https://doi.org/10.1287/mksc.2023.0306\n\n\nHagendorff, T., Fabi, S., & Kosinski, M. (2023). Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT. Nature Computational Science, 3(10), 833–838. https://doi.org/10.1038/s43588-023-00527-x\n\n\nLai, V., Chen, C., Smith-Renner, A., Liao, Q. V., & Tan, C. (2023). Towards a Science of Human-AI Decision Making: An Overview of Design Space in Empirical Human-Subject Studies. 2023 ACM Conference on Fairness, Accountability, and Transparency, 1369–1385. https://doi.org/10.1145/3593013.3594087\n\n\nLampinen, A. K., Dasgupta, I., Chan, S. C. Y., Sheahan, H. R., Creswell, A., Kumaran, D., McClelland, J. L., & Hill, F. (2024). Language models, like humans, show content effects on reasoning tasks. PNAS Nexus, 3(7), pgae233. https://doi.org/10.1093/pnasnexus/pgae233\n\n\nMacmillan-Scott, O., & Musolesi, M. (2024). (Ir)rationality and cognitive biases in large language models. Royal Society Open Science, 11(6), 240255. https://doi.org/10.1098/rsos.240255\n\n\nMatz, S. C., Teeny, J. D., Vaid, S. S., Peters, H., Harari, G. M., & Cerf, M. (2024). The potential of generative AI for personalized persuasion at scale. Scientific Reports, 14(1), 4692. https://doi.org/10.1038/s41598-024-53755-0\n\n\nMei, Q., Xie, Y., Yuan, W., & Jackson, M. O. (2024). A Turing test of whether AI chatbots are behaviorally similar to humans. Proceedings of the National Academy of Sciences, 121(9), e2313925121. https://doi.org/10.1073/pnas.2313925121\n\n\nNarayanan, S., Yu, G., Ho, C.-J., & Yin, M. (2023). How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making? Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, 49–57. https://doi.org/10.1145/3600211.3604709\n\n\nNguyen, J. (2024). Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models. Journal of Behavioral and Experimental Finance, 100971. https://doi.org/10.1016/j.jbef.2024.100971\n\n\nNobandegani, A. S., Rish, I., & Shultz, T. R. (2023). Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes. Proceedings of the Annual Meeting of the Cognitive Science Society, 46.\n\n\nRastogi, C., Leqi, L., Holstein, K., & Heidari, H. (2023). A Taxonomy of Human and ML Strengths in Decision-Making to Investigate Human-ML Complementarity. Proceedings of the AAAI Conference on Human Computation and Crowdsourcing, 11, 127–139. https://doi.org/10.1609/hcomp.v11i1.27554\n\n\nRastogi, C., Zhang, Y., Wei, D., Varshney, K. R., Dhurandhar, A., & Tomsett, R. (2022). Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 6(CSCW1), 1–22. https://doi.org/10.1145/3512930\n\n\nRoberts, J., Moore, K., Pham, T., Ewaleifoh, O., & Fisher, D. (2024). Large Language Model Recall Uncertainty is Modulated by the Fan Effect.\n\n\nStadler, M., Bannert, M., & Sailer, M. (2024). Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry. Computers in Human Behavior, 160, 108386. https://doi.org/10.1016/j.chb.2024.108386\n\n\nSubramonyam, H., Pea, R., Pondoc, C. L., Agrawala, M., & Seifert, C. (2024). Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces. Proceedings of the CHI Conference on Human Factors in Computing Systems, 1–19. https://arxiv.org/abs/2309.14459\n\n\nSuri, G., Slater, L. R., Ziaee, A., & Nguyen, M. (2024). Do large language models show decision heuristics similar to humans? A case study using GPT-35. Journal of Experimental Psychology: General, 153(4), 1066–1075. https://doi.org/10.1037/xge0001547\n\n\nSwaroop, S., Buçinca, Z., Gajos, K. Z., & Doshi-Velez, F. (2024). Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure. Proceedings of the 29th International Conference on Intelligent User Interfaces, 138–154. https://doi.org/10.1145/3640543.3645206\n\n\nTjuatja, L., Chen, V., Wu, T., Talwalkwar, A., & Neubig, G. (2024). Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design. Transactions of the Association for Computational Linguistics, 12, 1011–1026. https://doi.org/10.1162/tacl_a_00685\n\n\nTsirtsis, S., Rodriguez, M. G., & Gerstenberg, T. (2024). Towards a computational model of responsibility judgments in sequential human-AI collaboration. https://doi.org/10.31234/osf.io/m4yad\n\n\nWestphal, M., Vössing, M., Satzger, G., Yom-Tov, G. B., & Rafaeli, A. (2023). Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance. Computers in Human Behavior, 144, 107714. https://doi.org/10.1016/j.chb.2023.107714\n\n\nYax, N., Anlló, H., & Palminteri, S. (2024). Studying and improving reasoning in humans and machines. Communications Psychology, 2(1), 1–16. https://doi.org/10.1038/s44271-024-00091-8\n\n\nZhang, S., Wang, X., Zhang, W., Chen, Y., Gao, L., Wang, D., Zhang, W., Wang, X., & Wen, Y. (2024). Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task (arXiv:2409.08811). arXiv. https://arxiv.org/abs/2409.08811\n\n\nZhao, Y., Huang, Z., Seligman, M., & Peng, K. (2024). Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots. Scientific Reports, 14(1), 7095. https://doi.org/10.1038/s41598-024-55949-y",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "ai_gd.html#references",
    "href": "ai_gd.html#references",
    "title": "Group Decision Lit",
    "section": "",
    "text": "Abdelnabi, S., Gomaa, A., Sivaprasad, S., Schönherr, L., & Fritz, M. (2023). LLM-Deliberation: Evaluating LLMs with Interactive Multi-Agent Negotiation Games. https://doi.org/10.60882/cispa.25233028.v1\n\n\nBienefeld, N., Kolbe, M., Camen, G., Huser, D., & Buehler, P. K. (2023). Human-AI teaming: Leveraging transactive memory and speaking up for enhanced team effectiveness. Frontiers in Psychology, 14. https://doi.org/10.3389/fpsyg.2023.1208019\n\n\nBurton, J. W., Lopez-Lopez, E., Hechtlinger, S., Rahwan, Z., Aeschbach, S., Bakker, M. A., Becker, J. A., Berditchevskaia, A., Berger, J., Brinkmann, L., Flek, L., Herzog, S. M., Huang, S., Kapoor, S., Narayanan, A., Nussberger, A.-M., Yasseri, T., Nickl, P., Almaatouq, A., … Hertwig, R. (2024). How large language models can reshape collective intelligence. Nature Human Behaviour, 1–13. https://doi.org/10.1038/s41562-024-01959-9\n\n\nChiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nChuang, Y.-S., Harlalka, N., Suresh, S., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2024). The Wisdom of Partisan Crowds: Comparing Collective Intelligence in Humans and LLM-based Agents.\n\n\nChuang, Y.-S., Suresh, S., Harlalka, N., Goyal, A., Hawkins, R., Yang, S., Shah, D., Hu, J., & Rogers, T. T. (2023). Evaluating LLM Agent Group Dynamics against Human Group Dynamics: A Case Study on Wisdom of Partisan Crowds (arXiv:2311.09665). arXiv. https://arxiv.org/abs/2311.09665\n\n\nCollins, K. M., Sucholutsky, I., Bhatt, U., Chandra, K., Wong, L., Lee, M., Zhang, C. E., Zhi-Xuan, T., Ho, M., Mansinghka, V., Weller, A., Tenenbaum, J. B., & Griffiths, T. L. (2024). Building machines that learn and think with people. Nature Human Behaviour, 8(10), 1851–1863. https://doi.org/10.1038/s41562-024-01991-9\n\n\nDu, Y., Rajivan, P., & Gonzalez, C. C. (2024). Large Language Models for Collective Problem-Solving: Insights into Group Consensus Decision-Making. Proceedings of the Annual Meeting of the Cognitive Science Society, 46.\n\n\nGao, C., Lan, X., Li, N., Yuan, Y., Ding, J., Zhou, Z., Xu, F., & Li, Y. (2024). Large language models empowered agent-based modeling and simulation: A survey and perspectives. Humanities and Social Sciences Communications, 11(1), 1–24. https://doi.org/10.1057/s41599-024-03611-3\n\n\nGao, J., Gebreegziabher, S. A., Choo, K. T. W., Li, T. J.-J., Perrault, S. T., & Malone, T. W. (2024). A Taxonomy for Human-LLM Interaction Modes: An Initial Exploration. Extended Abstracts of the CHI Conference on Human Factors in Computing Systems, 1–11. https://doi.org/10.1145/3613905.3650786\n\n\nGuo, X., Huang, K., Liu, J., Fan, W., Vélez, N., Wu, Q., Wang, H., Griffiths, T. L., & Wang, M. (2024). Embodied LLM Agents Learn to Cooperate in Organized Teams (arXiv:2403.12482). arXiv. https://arxiv.org/abs/2403.12482\n\n\nHao, X., Demir, E., & Eyers, D. (2024). Exploring collaborative decision-making: A quasi-experimental study of human and Generative AI interaction. Technology in Society, 78, 102662. https://doi.org/10.1016/j.techsoc.2024.102662\n\n\nKoehl, D., & Vangsness, L. (2023). Measuring Latent Trust Patterns in Large Language Models in the Context of Human-AI Teaming. Proceedings of the Human Factors and Ergonomics Society Annual Meeting, 67. https://doi.org/10.1177/21695067231192869\n\n\nMa, S., Chen, Q., Wang, X., Zheng, C., Peng, Z., Yin, M., & Ma, X. (2024). Towards Human-AI Deliberation: Design and Evaluation of LLM-Empowered Deliberative AI for AI-Assisted Decision-Making (arXiv:2403.16812). arXiv. https://arxiv.org/abs/2403.16812\n\n\nMarjieh, R., Gokhale, A., Bullo, F., & Griffiths, T. L. (2024). Task Allocation in Teams as a Multi-Armed Bandit.\n\n\nNishida, Y., Shimojo, S., & Hayashi, Y. (2024). Conversational Agent Dynamics with Minority Opinion and Cognitive Conflict in Small-Group Decision-Making. Japanese Psychological Research. https://doi.org/10.1111/jpr.12552\n\n\nNisioti, E., Risi, S., Momennejad, I., Oudeyer, P.-Y., & Moulin-Frier, C. (2024, July). Collective Innovation in Groups of Large Language Models. ALIFE 2024: Proceedings of the 2024 Artificial Life Conference. https://doi.org/10.1162/isal_a_00730\n\n\nTessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., & Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science, 386(6719), eadq2852. https://doi.org/10.1126/science.adq2852\n\n\nVats, V., Nizam, M. B., Liu, M., Wang, Z., Ho, R., Prasad, M. S., Titterton, V., Malreddy, S. V., Aggarwal, R., Xu, Y., Ding, L., Mehta, J., Grinnell, N., Liu, L., Zhong, S., Gandamani, D. N., Tang, X., Ghosalkar, R., Shen, C., … Davis, J. (2024). A Survey on Human-AI Teaming with Large Pre-Trained Models (arXiv:2403.04931). arXiv. https://arxiv.org/abs/2403.04931\n\n\nYang, J. C., Dailisan, D., Korecki, M., Hausladen, C. I., & Helbing, D. (2024). LLM Voting: Human Choices and AI Collective Decision Making (arXiv:2402.01766). arXiv. https://arxiv.org/abs/2402.01766\n\n\nYang, Z., Xu, X., Yao, B., Rogers, E., Zhang, S., Intille, S., Shara, N., Gao, G. G., & Wang, D. (2024). Talk2Care: An LLM-based Voice Assistant for Communication between Healthcare Providers and Older Adults. Proceedings of the ACM on Interactive, Mobile, Wearable and Ubiquitous Technologies, 8(2), 1–35. https://doi.org/10.1145/3659625\n\n\nZhang, J., Xu, X., Zhang, N., Liu, R., Hooi, B., & Deng, S. (2024). Exploring Collaboration Mechanisms for LLM Agents: A Social Psychology View (arXiv:2310.02124). arXiv. https://arxiv.org/abs/2310.02124",
    "crumbs": [
      "LLM Literature",
      "Group Decision Lit"
    ]
  },
  {
    "objectID": "ai_interaction.html#references",
    "href": "ai_interaction.html#references",
    "title": "Interactive AI Lit",
    "section": "References",
    "text": "References\n\n\nBuçinca, Z., Malaya, M. B., & Gajos, K. Z. (2021). To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making. Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21. https://doi.org/10.1145/3449287\n\n\nChiang, C.-W., Lu, Z., Li, Z., & Yin, M. (2024). Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil’s Advocate. Proceedings of the 29th International Conference on Intelligent User Interfaces, 103–119. https://doi.org/10.1145/3640543.3645199\n\n\nCollins, K. M., Jiang, A. Q., Frieder, S., Wong, L., Zilka, M., Bhatt, U., Lukasiewicz, T., Wu, Y., Tenenbaum, J. B., Hart, W., Gowers, T., Li, W., Weller, A., & Jamnik, M. (2023). Evaluating Language Models for Mathematics through Interactions (arXiv:2306.01694). arXiv. https://arxiv.org/abs/2306.01694\n\n\nKämmer, J. E., Hautz, W. E., Krummrey, G., Sauter, T. C., Penders, D., Birrenbach, T., & Bienefeld, N. (2024). Effects of interacting with a large language model compared with a human coach on the clinical diagnostic process and outcomes among fourth-year medical students: Study protocol for a prospective, randomised experiment using patient vignettes. BMJ Open, 14(7), e087469. https://doi.org/10.1136/bmjopen-2024-087469\n\n\nKumar, H., Vincentius, J., Jordan, E., & Anderson, A. (2024). Human Creativity in the Age of LLMs: Randomized Experiments on Divergent and Convergent Thinking (arXiv:2410.03703). arXiv. https://arxiv.org/abs/2410.03703\n\n\nLaban, G., Laban, T., & Gunes, H. (2024). LEXI: Large Language Models Experimentation Interface (arXiv:2407.01488). arXiv. https://arxiv.org/abs/2407.01488\n\n\nSidji, M., Smith, W., & Rogerson, M. J. (2024). Human-AI Collaboration in Cooperative Games: A Study of Playing Codenames with an LLM Assistant. Proc. ACM Hum.-Comput. Interact., 8(CHI PLAY), 316:1–316:25. https://doi.org/10.1145/3677081\n\n\nTessler, M. H., Bakker, M. A., Jarrett, D., Sheahan, H., Chadwick, M. J., Koster, R., Evans, G., Campbell-Gillingham, L., Collins, T., Parkes, D. C., Botvinick, M., & Summerfield, C. (2024). AI can help humans find common ground in democratic deliberation. Science, 386(6719), eadq2852. https://doi.org/10.1126/science.adq2852\n\n\nWang, B., Liu, J., Karimnazarov, J., & Thompson, N. (2024). Task Supportive and Personalized Human-Large Language Model Interaction: A User Study. Proceedings of the 2024 ACM SIGIR Conference on Human Information Interaction and Retrieval, 370–375. https://doi.org/10.1145/3627508.3638344\n\n\nWang, D., Zheng, K., Li, C., & Guo, J. (2024). Transitioning to Human-Centered AI: A Systematic Review of Theories, Scenarios, and Hypotheses in Human-AI Interactions. Proceedings of the Association for Information Science and Technology, 61(1), 673–678. https://doi.org/10.1002/pra2.1078",
    "crumbs": [
      "LLM Literature",
      "Interactive AI Lit"
    ]
  },
  {
    "objectID": "ai_decision.html#towards-a-computational-model-of-responsibility-judgments-in-sequential-human-ai-collaboration",
    "href": "ai_decision.html#towards-a-computational-model-of-responsibility-judgments-in-sequential-human-ai-collaboration",
    "title": "Individual decision lit",
    "section": "Towards a computational model of responsibility judgments in sequential human-AI collaboration",
    "text": "Towards a computational model of responsibility judgments in sequential human-AI collaboration\nTsirtsis, S., Gomez Rodriguez, M., & Gerstenberg, T. (2024). Towards a computational model of responsibility judgments in sequential human-AI collaboration. In Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 46). https://osf.io/preprints/psyarxiv/m4yad\n\n\nAbstract\n\n\nWhen a human and an AI agent collaborate to complete a task and something goes wrong, who is responsible? Prior work has developed theories to describe how people assign responsibility to individuals in teams. However, there has been little work studying the cognitive processes that underlie responsibility judgments in human-AI collaborations, especially for tasks comprising a sequence of interdependent actions. In this work, we take a step towards filling this gap. Using semi-autonomous driving as a paradigm, we develop an environment that simulates stylized cases of human-AI collaboration using a generative model of agent behavior. We propose a model of responsibility that considers how unexpected an agent’s action was, and what would have happened had they acted differently. We test the model’s predictions empirically and find that in addition to action expectations and counterfactual considerations, participants’ responsibility judgments are also affected by how much each agent actually contributed to the outcome.\n\n\n\n\n\nFigure from Tsirtsis et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "Individual decision lit"
    ]
  },
  {
    "objectID": "llm_energy.html#aiot-smart-home-via-autonomous-llm-agents.",
    "href": "llm_energy.html#aiot-smart-home-via-autonomous-llm-agents.",
    "title": "LLM Energy Lit Highlights",
    "section": "AIoT Smart Home via Autonomous LLM Agents.",
    "text": "AIoT Smart Home via Autonomous LLM Agents.\nRivkin, D., Hogan, F., Feriani, A., Konar, A., Sigal, A., Liu, X., & Dudek, G. (2024). AIoT Smart Home via Autonomous LLM Agents. IEEE Internet of Things Journal, 1–1. IEEE Internet of Things Journal. https://doi.org/10.1109/JIOT.2024.3471904\n\n\nAbstract\n\n\nThe common-sense reasoning abilities and vast general knowledge of Large Language Models (LLMs) make them a natural fit for interpreting user requests in a smart home assistant context. LLMs, however, lack specific knowledge about the user and their home, which limits their potential impact. SAGE (Smart Home Agent with Grounded Execution), overcomes these and other limitations by using a scheme in which a user request triggers an LLM-controlled sequence of discrete actions. These actions can be used to retrieve information, interact with the user, or manipulate device states. SAGE controls this process through a dynamically constructed tree of LLM prompts, which help it decide which action to take next, whether an action was successful, and when to terminate the process. The SAGE action set augments an LLM’s capabilities to support some of the most critical requirements for a smart home assistant. These include: flexible and scalable user preference management (“Is my team playing tonight?”), access to any smart device’s full functionality without device-specific code via API reading (“Turn down the screen brightness on my dryer”), persistent device state monitoring (“Remind me to throw out the milk when I open the fridge”), natural device references using only a photo of the room (“Turn on the lamp on the dresser”), and more. We introduce a benchmark of 50 new and challenging smart home tasks where SAGE achieves a 76% success rate, significantly outperforming existing LLM-enabled baselines (30% success rate).\n\n\n\n\n\nFigure from Rivkin et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#leveraging-large-language-models-for-enhanced-personalised-user-experience-in-smart-homes",
    "href": "llm_energy.html#leveraging-large-language-models-for-enhanced-personalised-user-experience-in-smart-homes",
    "title": "LLM Energy Lit Highlights",
    "section": "Leveraging Large Language Models for enhanced personalised user experience in Smart Homes",
    "text": "Leveraging Large Language Models for enhanced personalised user experience in Smart Homes\nRey-Jouanchicot, J., Bottaro, A., Campo, E., Bouraoui, J.-L., Vigouroux, N., & Vella, F. (2024). Leveraging Large Language Models for enhanced personalised user experience in Smart Homes (No. arXiv:2407.12024). arXiv. http://arxiv.org/abs/2407.12024\n\n\nAbstract\n\n\nSmart home automation systems aim to improve the comfort and convenience of users in their living environment. However, adapting automation to user needs remains a challenge. Indeed, many systems still rely on hand-crafted routines for each smart object.This paper presents an original smart home architecture leveraging Large Language Models (LLMs) and user preferences to push the boundaries of personalisation and intuitiveness in the home environment.This article explores a human-centred approach that uses the general knowledge provided by LLMs to learn and facilitate interactions with the environment.The advantages of the proposed model are demonstrated on a set of scenarios, as well as a comparative analysis with various LLM implementations. Some metrics are assessed to determine the system’s ability to maintain comfort, safety, and user preferences. The paper details the approach to real-world implementation and evaluation.The proposed approach of using preferences shows up to 52.3% increase in average grade, and with an average processing time reduced by 35.6% on Starling 7B Alpha LLM. In addition, performance is 26.4% better than the results of the larger models without preferences, with processing time almost 20 times faster.\n\n\n\n\n\nFigure from Rey-Jouanchicot et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "llm_energy.html#harmony-a-home-agent-for-responsive-management-and-action-optimization-with-a-locally-deployed-large-language-model",
    "href": "llm_energy.html#harmony-a-home-agent-for-responsive-management-and-action-optimization-with-a-locally-deployed-large-language-model",
    "title": "LLM Energy Lit Highlights",
    "section": "Harmony: A Home Agent for Responsive Management and Action Optimization with a Locally Deployed Large Language Model",
    "text": "Harmony: A Home Agent for Responsive Management and Action Optimization with a Locally Deployed Large Language Model\nYin, Z., Zhang, M., & Kawahara, D. (2024). Harmony: A Home Agent for Responsive Management and Action Optimization with a Locally Deployed Large Language Model (No. arXiv:2410.14252). arXiv. http://arxiv.org/abs/2410.14252\n\n\nAbstract\n\n\nSince the launch of GPT-3.5, intelligent home assistant technology based on large language models (LLMs) has made significant progress. These intelligent home assistant frameworks, such as those based on high-performance LLMs like GPT-4, have greatly expanded their functional range and application scenarios by computing on the cloud, enriching user experience and diversification. In order to optimize the privacy and economy of data processing while maintaining the powerful functions of LLMs, we propose Harmony, a smart home assistant framework that uses a locally deployable small-scale LLM. Based on Llama3-8b, an open LLM that can be easily deployed on a consumer-grade PC, Harmony does not send any data to the internet during operation, ensuring local computation and privacy secured. Harmony based on Llama3-8b achieved competitive performance on our benchmark tests with the framework used in related work with GPT-4. In addition to solving the issues mentioned above, Harmony can also take actions according to the user and home status, even if the user does not issue a command. For example, when the user wants to wake up later than normal on the weekend, Harmony would open the curtains only when the user gets up or prepare the room when the user comes home without requiring user commands.\n\n\n\n\n\nFigure from Yin et al. (2024)",
    "crumbs": [
      "LLM Literature",
      "LLM Energy Lit Highlights"
    ]
  },
  {
    "objectID": "Samuel_Project.html#complexity",
    "href": "Samuel_Project.html#complexity",
    "title": "Samuel’s Project",
    "section": "Complexity",
    "text": "Complexity\n\nTask complexity moderates group synergy.\nAlmaatouq, A., Alsobay, M., Yin, M., & Watts, D. J. (2021). Task complexity moderates group synergy. Proceedings of the National Academy of Sciences, 118(36). https://doi.org/10.1073/pnas.2101062118\n\n\nAbstract\n\n\nComplexity—defined in terms of the number of components and the nature of the interdependencies between them—is clearly a relevant feature of all tasks that groups perform. Yet the role that task complexity plays in determining group performance remains poorly understood, in part because no clear language exists to express complexity in a way that allows for straightforward comparisons across tasks. Here we avoid this analytical difficulty by identifying a class of tasks for which complexity can be varied systematically while keeping all other elements of the task unchanged. We then test the effects of task complexity in a preregistered two-phase experiment in which 1,200 individuals were evaluated on a series of tasks of varying complexity (phase 1) and then randomly assigned to solve similar tasks either in interacting groups or as independent individuals (phase 2). We find that interacting groups are as fast as the fastest individual and more efficient than the most efficient individual for complex tasks but not for simpler ones. Leveraging our highly granular digital data, we define and precisely measure group process losses and synergistic gains and show that the balance between the two switches signs at intermediate values of task complexity. Finally, we find that interacting groups generate more solutions more rapidly and explore the solution space more broadly than independent problem solvers, finding higher-quality solutions than all but the highest-scoring individuals.\n\n\n\n\n\nFigure from Almaatouq et al. (2021)\n\n\n\n\n\nThe interaction between map complexity and crowd movement on navigation decisions in virtual reality.\nZhao, H., Thrash, T., Grossrieder, A., Kapadia, M., Moussaïd, M., Hölscher, C., & Schinazi, V. R. (2020). The interaction between map complexity and crowd movement on navigation decisions in virtual reality. Royal Society Open Science, 7(3), 191523. https://doi.org/10.1098/rsos.191523\n\n\nAbstract\n\n\nA carefully designed map can reduce pedestrians’ cognitive load during wayfinding and may be an especially useful navigation aid in crowded public environments. In the present paper, we report three studies that investigated the effects of map complexity and crowd movement on wayfinding time, accuracy and hesitation using both online and laboratory-based networked virtual reality (VR) platforms. In the online study, we found that simple map designs led to shorter decision times and higher accuracy compared to complex map designs. In the networked VR set-up, we found that co-present participants made very few errors. In the final VR study, we replayed the traces of participants’ avatars from the second study so that they indicated a different direction than the maps. In this scenario, we found an interaction between map design and crowd movement in terms of decision time and the distributions of locations at which participants hesitated. Together, these findings can help the designers of maps for public spaces account for the movements of real crowds.\n\n\n\n\n\nFigure from Zhao et al. (2020)\n\n\n\n\n\nTask Complexity and Performance in Individuals and Groups Without Communication.\nGulati, A., Nguyen, T. N., & Gonzalez, C. (2021). Task Complexity and Performance in Individuals and Groups Without Communication. AAAI Fall Symposium. Cham: Springer Nature Switzerland, 8.\n\n\nAbstract\n\n\nWhile groups where members communicate with each other may perform better than groups without communication, there are multiple scenarios where communication between group members is not possible. Our work analyses the impact of task complexity on individuals and groups of different sizes while solving a goal-seeking navigation task without communication. Our major goal is to determine the effect of task complexity on performance and whether agents in a group are able to coordinate to perform the task more effectively despite the lack of communication. We developed a cognitive model of each individual agent that performs the task. We compare the performance of this agent with individual human performance, who worked on the same task. We observe that the cognitive agent is able to replicate the general behavioral trends observed in humans. Using this cognitive model, we generate groups of different sizes where individual agents work in the same goal-seeking task independently and without communication. First, we observe that increasing task complexity by design does not necessarily lead to worse performance in individuals and groups. We also observe that larger groups perform better than smaller groups and individuals alone. However, individual agents within a group perform worse than an agent working on the task alone. This effect is not the result of agents within a group covering less ground in the task compared to individuals alone. Rather, it is an effect resulting from the overlap of the agents within a group. Importantly, agents learn to reduce their overlap and improve their performance without explicit communication. These results can inform the design of AI agents in humanmachine teams.\n\n\nGulati et al. (2021)\n\n\n\nEnvironmental memory boosts group formation of clueless individuals.\nDias, C. S., Trivedi, M., Volpe, G., Araújo, N. A. M., & Volpe, G. (2023). Environmental memory boosts group formation of clueless individuals. Nature Communications, 14(1), 7324. https://doi.org/10.1038/s41467-023-43099-0\n\n\nAbstract\n\n\nThe formation of groups of interacting individuals improves performance and fitness in many decentralised systems, from micro-organisms to social insects, from robotic swarms to artificial intelligence algorithms. Often, group formation and high-level coordination in these systems emerge from individuals with limited information-processing capabilities implementing low-level rules of communication to signal to each other. Here, we show that, even in a community of clueless individuals incapable of processing information and communicating, a dynamic environment can coordinate group formation by transiently storing memory of the earlier passage of individuals. Our results identify a new mechanism of indirect coordination via shared memory that is primarily promoted and reinforced by dynamic environmental factors, thus overshadowing the need for any form of explicit signalling between individuals. We expect this pathway to group formation to be relevant for understanding and controlling self-organisation and collective decision making in both living and artificial active matter in real-life environments.",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#uncertainty",
    "href": "Samuel_Project.html#uncertainty",
    "title": "Samuel’s Project",
    "section": "Uncertainty",
    "text": "Uncertainty\n\n\nFlexible social inference facilitates targeted social learning when rewards are not observable.\nHawkins, R. D., Berdahl, A. M., Pentland, A. ‘Sandy,’ Tenenbaum, J. B., Goodman, N. D., & Krafft, P. M. (2023). Flexible social inference facilitates targeted social learning when rewards are not observable. Nature Human Behaviour, 7(10), 1767–1776. https://doi.org/10.1038/s41562-023-01682-x\n\n\nAbstract\n\n\nGroups coordinate more effectively when individuals are able to learn from others’ successes. But acquiring such knowledge is not always easy, especially in real-world environments where success is hidden from public view. We suggest that social inference capacities may help bridge this gap, allowing individuals to update their beliefs about others’ underlying knowledge and success from observable trajectories of behaviour. We compared our social inference model against simpler heuristics in three studies of human behaviour in a collective-sensing task. Experiment 1 demonstrated that average performance improved as a function of group size at a rate greater than predicted by heuristic models. Experiment 2 introduced artificial agents to evaluate how individuals selectively rely on social information. Experiment 3 generalized these findings to a more complex reward landscape. Taken together, our findings provide insight into the relationship between individual social cognition and the flexibility of collective behaviour.\n\n\n\n\n\nFigure from Hawkins et al. (2023)\n\n\n\nTump, A. N., Wu, C. M., Bouhlel, I., & Goldstone, R. L. (2019). The Evolutionary Dynamics of Cooperation in Collective Search [Preprint]. http://biorxiv.org/lookup/doi/10.1101/538447\n\n\nAbstract\n\n\nHow does cooperation arise in an evolutionary context? We approach this problem using a collective search paradigm where interactions are dynamic and there is competition for rewards. Using evolutionary simulations, we ﬁnd that the unconditional sharing of information can be an evolutionary advantageous strategy without the need for conditional strategies or explicit reciprocation. Shared information acts as a recruitment signal and facilitates the formation of a self-organized group. Thus, the improved search efﬁciency of the collective bestows byproduct beneﬁts onto the original sharer. A key mechanism is a visibility radius, where individuals have unconditional access to information about neighbors within a limited distance. Our results show that for a variety of initial conditions—including populations initially devoid of prosocial individuals—and across both static and dynamic ﬁtness landscapes, we ﬁnd strong selection pressure to evolve unconditional sharing.\n\n\n\n\n\nSpatial planning with long visual range benefits escape from visual predators in complex naturalistic environments.\nMugan, U., & MacIver, M. A. (2020). Spatial planning with long visual range benefits escape from visual predators in complex naturalistic environments. Nature Communications, 11(1), 3057. https://doi.org/10.1038/s41467-020-16102-1\nlive task demo\ncode repository\n\n\n\nAbstract\n\n\nIt is uncontroversial that land animals have more elaborated cognitive abilities than their aquatic counterparts such as fish. Yet there is no apparent a-priori reason for this. A key cognitive faculty is planning. We show that in visually guided predator-prey interactions, planning provides a significant advantage, but only on land. During animal evolution, the water-to-land transition resulted in a massive increase in visual range. Simulations of behavior identify a specific type of terrestrial habitat, clustered open and closed areas (savanna-like), where the advantage of planning peaks. Our computational experiments demonstrate how this patchy terrestrial structure, in combination with enhanced visual range, can reveal and hide agents as a function of their movement and create a selective benefit for imagining, evaluating, and selecting among possible future scenarios—in short, for planning. The vertebrate invasion of land may have been an important step in their cognitive evolution.\n\n\n\n\n\nFigure from Mugan & MacIver (2020)\n\n\n\n\n\nThe form of uncertainty affects selection for social learning.\nTurner, M. A., Moya, C., Smaldino, P. E., & Jones, J. H. (2023). The form of uncertainty affects selection for social learning. Evolutionary Human Sciences, 5, e20. https://doi.org/10.1017/ehs.2023.11\n\n\nAbstract\n\n\nSocial learning is a critical adaptation for dealing with different forms of variability. Uncertainty is a severe form of variability where the space of possible decisions or probabilities of associated outcomes are unknown. We identified four theoretically important sources of uncertainty: temporal environmental variability; payoff ambiguity; selection-set size; and effective lifespan. When these combine, it is nearly impossible to fully learn about the environment. We develop an evolutionary agent-based model to test how each form of uncertainty affects the evolution of social learning. Agents perform one of several behaviours, modelled as a multi-armed bandit, to acquire payoffs. All agents learn about behavioural payoffs individually through an adaptive behaviour-choice model that uses a softmax decision rule. Use of vertical and oblique payoff-biased social learning evolved to serve as a scaffold for adaptive individual learning – they are not opposite strategies. Different types of uncertainty had varying effects. Temporal environmental variability suppressed social learning, whereas larger selection-set size promoted social learning, even when the environment changed frequently. Payoff ambiguity and lifespan interacted with other uncertainty parameters. This study begins to explain how social learning can predominate despite highly variable real-world environments when effective individual learning helps individuals recover from learning outdated social information.",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#social-influence",
    "href": "Samuel_Project.html#social-influence",
    "title": "Samuel’s Project",
    "section": "Social Influence",
    "text": "Social Influence\n\nVisual-spatial dynamics drive adaptive social learning in immersive environments\nWu, C. M., Deffner, D., Kahl, B., Meder, B., Ho, M. H., & Kurvers, R. H. J. M. (2023). Visual-spatial dynamics drive adaptive social learning in immersive environments [Preprint]. https://doi.org/10.1101/2023.06.28.546887\n\n\nAbstract\n\n\nHumans are uniquely capable social learners. Our capacity to learn from others across short and long timescales is a driving force behind the success of our species. Yet there are seemingly maladaptive patterns of human social learning, characterized by both overreliance and underreliance on social information. Recent advances in animal research have incorporated rich visual and spatial dynamics to study social learning in ecological contexts, showing how simple mechanisms can give rise to intelligent group dynamics. However, similar techniques have yet to be translated into human research, which additionally requires integrating the sophistication of human individual and social learning mechanisms. Thus, it is still largely unknown how humans dynamically adapt social learning strategies to different environments and how group dynamics emerge under realistic conditions. Here, we use a collective foraging experiment in an immersive Minecraft environment to provide unique insights into how visual-spatial interactions give rise to adaptive, specialized, and selective social learning. Our analyses show how groups adapt to the demands of the environment through specialization of learning strategies rather than homogeneity and through the adaptive deployment of selective imitation rather than indiscriminate copying. We test these mechanisms using computational modeling, providing a deeper understanding of the cognitive mechanisms that dynamically inﬂuence social decision-making in ecological contexts. All results are compared against an asocial baseline, allowing us to specify specialization and selective attention as uniquely social phenomena, which provide the adaptive foundations of human social learning.\n\n\n\n\n\nFigure from Wu et al. (2023)\n\n\n\n\n\nSpecialization and selective social attention establishes the balance between individual and social learning.\nWu, C. M., Ho, M. K., Kahl, B., Leuker, C., Meder, B., & Kurvers, R. H. J. M. (2021). Specialization and selective social attention establishes the balance between individual and social learning. Proceedings of the 43rd Annual Conference of the Cognitive Science Society, 1921–1927. https://doi.org/10.1101/2021.02.03.429553\n\n\nAbstract\n\n\nA key question individuals face in any social learning environment is when to innovate alone and when to imitate others. Previous simulation results have found that the best performing groups exhibit an intermediate balance, yet it is still largely unknown how individuals collectively negotiate this balance. We use an immersive collective foraging experiment, implemented in the Minecraft game engine, facilitating unprecedented access to spatial trajectories and visual ﬁeld data. The virtual environment imposes a limited ﬁeld of view, creating a natural trade-off between allocating visual attention towards individual innovation or to look towards peers for social imitation. By analyzing foraging patterns, social interactions (visual and spatial), and social inﬂuence, we shine new light on how groups collectively adapt to the ﬂuctuating demands of the environment through specialization and selective imitation, rather than homogeneity and indiscriminate copying of others.\n\n\n\n\n\nCollective incentives reduce over-exploitation of social information in unconstrained human groups.\nDeffner, D., Mezey, D., Kahl, B., Schakowski, A., Romanczuk, P., Wu, C. M., & Kurvers, R. H. J. M. (2024). Collective incentives reduce over-exploitation of social information in unconstrained human groups. Nature Communications, 15(1), 2683. https://doi.org/10.1038/s41467-024-47010-3\n\n\nAbstract\n\n\nCollective dynamics emerge from countless individual decisions. Yet, we poorly understand the processes governing dynamically-interacting individuals in human collectives under realistic conditions. We present a naturalistic immersive-reality experiment where groups of participants searched for rewards in different environments, studying how individuals weigh personal and social information and how this shapes individual and collective outcomes. Capturing high-resolution visual-spatial data, behavioral analyses revealed individual-level gains—but group-level losses—of high social information use and spatial proximity in environments with concentrated (vs. distributed) resources. Incentivizing participants at the group (vs. individual) level facilitated adaptation to concentrated environments, buffering apparently excessive scrounging. To infer discrete choices from unconstrained interactions and uncover the underlying decision mechanisms, we developed an unsupervised Social Hidden Markov Decision model. Computational results showed that participants were more sensitive to social information in concentrated environments frequently switching to a social relocation state where they approach successful group members. Group-level incentives reduced participants’ overall responsiveness to social information and promoted higher selectivity over time. Finally, mapping group-level spatio-temporal dynamics through time-lagged regressions revealed a collective exploration-exploitation trade-off across different timescales. Our study unravels the processes linking individual-level strategies to emerging collective dynamics, and provides tools to investigate decision-making in freely-interacting collectives.\n\n\n\n\n\nFigure from Deffner et al. (2024)\n\n\n\n\n\nInsights about the common generative rule underlying an information foraging task can be facilitated via collective search.\nNaito, A., Katahira, K., & Kameda, T. (2022). Insights about the common generative rule underlying an information foraging task can be facilitated via collective search. Scientific Reports, 12(1), 8047.\n\n\nAbstract\n\n\nSocial learning is beneficial for efficient information search in unfamiliar environments (“within-task” learning). In the real world, however, possible search spaces are often so large that decision makers are incapable of covering all options, even if they pool their information collectively. One strategy to handle such overload is developing generalizable knowledge that extends to multiple related environments (“across-task” learning). However, it is unknown whether and how social information may facilitate such across-task learning. Here, we investigated participants’ social learning processes across multiple laboratory foraging sessions in spatially correlated reward landscapes that were generated according to a common rule. The results showed that paired participants were able to improve efficiency in information search across sessions more than solo participants. Computational analysis of participants’ choice-behaviors revealed that such improvement across sessions was related to better understanding of the common generative rule. Rule understanding was correlated within a pair, suggesting that social interaction is a key to the improvement of across-task learning.\n\n\n\n\n\nFigure from Naito et al. (2022)\n\n\n\n\n\nIndividualism versus collective movement during travel.\nDoherty, C. T. M., & Laidre, M. E. (2022). Individualism versus collective movement during travel. Scientific Reports, 12(1), 7508. https://doi.org/10.1038/s41598-022-11469-1\n\n\nAbstract\n\n\nCollective movement may emerge if coordinating one’s movement with others produces a greater benefit to oneself than can be achieved alone. Experimentally, the capacity to manoeuvre simulated groups in the wild could enable powerful tests of the impact of collective movement on individual decisions. Yet such experiments are currently lacking due to the inherent difficulty of controlling whole collectives. Here we used a novel technique of experimentally simulating the movement of collectives of social hermit crabs (Coenobita compressus) in the wild. Using large architectural arrays of shells dragged across the beach, we generated synchronous collective movement and systematically varied the simulated collective’s travel direction as well as the context (i.e., danger level). With drone video from above, we then tested whether focal individuals were biased in their movement by the collective. We found that, despite considerable engagement with the collective, individuals’ direction was not significantly biased. Instead, individuals expressed substantial variability across all stimulus directions and contexts. Notably, individuals typically achieved shorter displacements in the presence of the collective versus in the presence of the control stimulus, suggesting an impact of traffic. The absence of a directional bias in individual movement due to the collective suggests that social hermit crabs are individualists, which move with a high level of opportunistic independence, likely thanks to the personal architecture and armour they carry in the form of a protective shell. Future studies can manipulate this level of armour to test its role in autonomy of movement, including the consequences of shell architecture for social decisions. Our novel experimental approach can be used to ask many further questions about how and why collective and individual movement interact.\n\n\n\n\n\nBeyond the individual: A social foraging framework to study decisions in groups.\nGarg, K., Deng, W., & Mobbs, D. (2024). Beyond the individual: A social foraging framework to study decisions in groups. OSF. https://doi.org/10.31219/osf.io/rmqyb\n\n\nAbstract\n\n\nA key goal of the behavioral sciences is to understand how agents decide between rewarding, hazardous, and conflicting options. Foraging theory, which is rooted in ecology and evolutionary theory, has helped advance this pursuit but has largely been limited to the study of the individual. In this Perspective, we extend beyond an individual. We propose social foraging as a promising avenue to study social decisions, or decisions within a social context. Recent research has already applied similar paradigms to study social behavior in naturalistic conditions. We synthesize the key socio-cognitive elements involved in social foraging that can be further studied through foraging paradigms. We then propose a social foraging framework that distinguishes between the asocial and social components involved in the decision-making process and describes their integration. Our framework bridges research across disciplines to provide a promising new avenue for the study of social behavior by linking decisions across different scales, from individuals to collectives.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Figures from Garg et al. (2024)",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#alt-2",
    "href": "Samuel_Project.html#alt-2",
    "title": "Samuel’s Project",
    "section": "Alt 2",
    "text": "Alt 2",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#alt-2---larger",
    "href": "Samuel_Project.html#alt-2---larger",
    "title": "Samuel’s Project",
    "section": "Alt 2 - larger",
    "text": "Alt 2 - larger",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#alt-2b",
    "href": "Samuel_Project.html#alt-2b",
    "title": "Samuel’s Project",
    "section": "Alt 2b",
    "text": "Alt 2b",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#alt-2c",
    "href": "Samuel_Project.html#alt-2c",
    "title": "Samuel’s Project",
    "section": "Alt 2c",
    "text": "Alt 2c",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#alt-3",
    "href": "Samuel_Project.html#alt-3",
    "title": "Samuel’s Project",
    "section": "Alt 3",
    "text": "Alt 3",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#alt-3b",
    "href": "Samuel_Project.html#alt-3b",
    "title": "Samuel’s Project",
    "section": "Alt 3b",
    "text": "Alt 3b",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#alt-3c",
    "href": "Samuel_Project.html#alt-3c",
    "title": "Samuel’s Project",
    "section": "Alt 3c",
    "text": "Alt 3c",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "Samuel_Project.html#peters-talks-on-foraging",
    "href": "Samuel_Project.html#peters-talks-on-foraging",
    "title": "Samuel’s Project",
    "section": "Peter’s Talks on Foraging",
    "text": "Peter’s Talks on Foraging\n\n\n\n\n\nhttps://www.youtube.com/watch?v=vIyFGM5p40w&ab_channel=GuillaumeInstitutdessciencescognitives\nhttps://www.youtube.com/watch?v=HiqdleY6rFg&ab_channel=MINDSummerSchool\nhttps://www.youtube.com/watch?v=t18gU5kvDFg&ab_channel=CogSci%3AInterdisciplinaryStudyoftheMind\nhttps://www.youtube.com/watch?v=Yih7YD8nikU&ab_channel=InstituteforPure%26AppliedMathematics%28IPAM%29\nhttps://www.youtube.com/watch?v=lrviTiJLoqE&ab_channel=MaxPlanckInstituteforHumanDevelopment\nhttps://www.youtube.com/watch?v=jbhWE8NxaOk&ab_channel=AugmentedIntelligenceWorkshop\nhttps://www.youtube.com/watch?v=fgGkTlF8hXc&ab_channel=sethfrey\nhttps://www.youtube.com/watch?v=lPVQe_kTm28&ab_channel=CogSci%3AInterdisciplinaryStudyoftheMind",
    "crumbs": [
      "Misc",
      "Samuel's Project"
    ]
  },
  {
    "objectID": "scm_documentation.html",
    "href": "scm_documentation.html",
    "title": "Behavioral Experiment Documentation Report",
    "section": "",
    "text": "This document outlines the workings of the JavaScript and HTML code used for a behavioral experiment on social foraging, simulating a scenario where human participants select charging stations for electric vehicles (EVs).\n\n\n\nTask apperance"
  },
  {
    "objectID": "scm_documentation.html#documenting-javascript-and-html-code-for-a-behavioral-experiment-on-social-foraging",
    "href": "scm_documentation.html#documenting-javascript-and-html-code-for-a-behavioral-experiment-on-social-foraging",
    "title": "Behavioral Experiment Documentation Report",
    "section": "",
    "text": "This document outlines the workings of the JavaScript and HTML code used for a behavioral experiment on social foraging, simulating a scenario where human participants select charging stations for electric vehicles (EVs).\n\n\n\nTask apperance"
  },
  {
    "objectID": "scm_documentation.html#conceptual-framework-and-motivation",
    "href": "scm_documentation.html#conceptual-framework-and-motivation",
    "title": "Behavioral Experiment Documentation Report",
    "section": "1. Conceptual Framework and Motivation",
    "text": "1. Conceptual Framework and Motivation\n\nProblem Space:\nEfficient allocation of limited resources (e.g., EV charging stations) presents significant challenges, especially with dynamic behavior and decision-making. This project studies human resource allocation decisions through a real-time, interactive online tool.\n\n\nKey Innovations and Advantages:\n\nReal-Time Interaction: WebSockets provide synchronized, multi-user interactions.\nCustomizability: A control panel dynamically adjusts parameters like station count, average prices, and player budgets.\nIntegration: Player behavior and station selection data are stored in a MySQL database for analysis.\n\n\n\nFuture Development Roadmap:\n\nEnhanced visualization of player actions and system states.\nIntegration of machine-learning models for behavior prediction.\nExpanded parameter customization for diverse experimental setups.\n\n\n\n\n\n2. Architecture and Implementation\n\nSystem Architecture:\nThe architecture comprises three main components:\n\nClient-Side Interface (HTML + JavaScript): Manages user interaction and displays the simulation using a dynamic game map.\nServer-Side Logic (Node.js + WebSocket): Manages connections, synchronizes game states, relays updates, and handles admin parameter changes.\nDatabase Layer (MySQL): Records player positions for post-experiment analysis.\n\n\n\n\n\n\n\nflowchart TD\n    Start([Start: Player or Admin interaction])\n    AdminParameters[\"Admin updates parameters via control panel\"]\n    SocketConnect[\"Player connects via Socket.IO\"]\n    ServerLogic[\"Server handles connection and updates game state\"]\n    DBQuery[\"Server logs positions in MySQL database\"]\n    GameStateEmit[\"Server sends updated game state to players\"]\n    UIUpdate[\"Client UI updates with new data\"]\n    ResetGame[\"Game reset event triggered by Admin or Player\"]\n    AdminServerUpdate[\"Admin updates parameters on server\"]\n    DatabaseSave[\"Admin parameters saved in JSON file\"]\n\n    Start --&gt;|Admin Input| AdminParameters\n    Start --&gt;|Player Connection| SocketConnect\n    AdminParameters --&gt;|Sends API Request| AdminServerUpdate\n    AdminServerUpdate --&gt; DatabaseSave\n    SocketConnect --&gt; ServerLogic\n    ServerLogic --&gt;|Database Logging| DBQuery\n    ServerLogic --&gt;|Real-time updates| GameStateEmit\n    DBQuery --&gt;|Feedback to server| ServerLogic\n    GameStateEmit --&gt; UIUpdate\n    ResetGame --&gt;|Emit Reset Event| ServerLogic\n    ServerLogic --&gt;|Sends Reset State| GameStateEmit\n\n\n\n\nFigure 1: example diagram 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nG\n\n\n\nUser\n\nUser\n\n\n\nClient\n\nHTML + JS\nClient\n\n\n\nUser-&gt;Client\n\n\nInput actions\n\n\n\nServer\n\nNode.js\nServer\n\n\n\nClient-&gt;Server\n\n\nSends actions\nvia WebSocket\n\n\n\nServer-&gt;Client\n\n\nBroadcasts\nupdated game state\n\n\n\nDatabase\n\n\nMySQL\nDatabase\n\n\n\nServer-&gt;Database\n\n\nLogs player\nactions\n\n\n\n\n\n\nFigure 2: example diagram 2\n\n\n\n\n\nData Flow:\n\nClient connects to server via Socket.IO.\nServer sends initial game state (players, stations).\nClient renders the game.\nPlayer interacts (moves to a station).\nClient sends player’s action to the server.\nServer updates game state and broadcasts changes to all clients.\nClients update displays.\nPlayer positions are periodically saved to MySQL.\n\nKey Data Structures:\n\ngameState (Server): Holds game state (players, stations).\n\n\n\nscript.js\n\nconst gameState = {\n    players: new Map(), // Map of players and their attributes\n    stations: [\n        { top: 8, left: 338, cost: 20 },\n        { top: 479, left: 183, cost: 30 },\n        ...\n    ],\n};\n\n\nPlayer Attributes:\n\nPosition (x, y)\nEnergy and monetary budgets\nAssigned colors for visual distinction\n\n\n\nadminParameters (Client & Server): JSON with configurable parameters (station count, mean price, budget, download speed, station size)."
  },
  {
    "objectID": "scm_documentation.html#key-functions-and-their-purpose",
    "href": "scm_documentation.html#key-functions-and-their-purpose",
    "title": "Behavioral Experiment Documentation Report",
    "section": "3. Key Functions and Their Purpose",
    "text": "3. Key Functions and Their Purpose\n\n\nscript.js\n\nconst setInitialParameters = async () =&gt; { ... }\n\nPurpose: Retrieves initial game parameters from the server or defaults. Sets initial admin panel slider values.\n\n\nscript.js\n\nupdateButton.addEventListener('click', () =&gt; { ... });\n\n\nPurpose: Sends updated parameters from the admin panel to the server.\n\n\nserver.js\n\nconst insertPlayerPosition = async (playerId, positionX, positionY) =&gt; { ... }\n\nPurpose: Records player position in the MySQL database.\nio.on('connection', (socket) =&gt; { ... });\nPurpose: Handles new player connections, initializes player data, sends initial game state, manages movement/disconnections, and the game reset.\napp.put('/admin-parameters', async (req, res) =&gt; { ... });\nPurpose: Handles and validates incoming admin parameters. Updates AdminParams, saves to admin-data.json and sends parameter information to the players."
  },
  {
    "objectID": "scm_documentation.html#key-features-and-functionality",
    "href": "scm_documentation.html#key-features-and-functionality",
    "title": "Behavioral Experiment Documentation Report",
    "section": "4. Key Features and Functionality",
    "text": "4. Key Features and Functionality\n\nFeature 1: Synchronized Game State\n\nReal-time Interaction: Socket.IO enables responsive gameplay.\nWhy: Ensures consistent user experience in multi-user settings.\nHow: Employs WebSocket to propagate real-time updates.\n\n\n\n\n 1\n\n\n\n\nserver.js\n\nio.on('connection', (socket) =&gt; {\n    const player = {\n        id: socket.id,\n        positionX: 0,\n        positionY: 235,\n        energy: 100,\n        money: 100,\n        color: getRandomColor(),\n    };\n    gameState.players.set(socket.id, player);\n    io.emit('gameState', gameState);\n});\n\n\n\n\n\n\n\nFeature 2: Dynamic Parameter Control\n\nWhy: Facilitates experiment customization without server restarts.\nHow: Parameters are adjustable via a control panel in the interface.\n\n\n\nindex.html\n\n&lt;label for=\"stationCount\"&gt;Number of Stations:&lt;/label&gt;\n&lt;input type=\"range\" id=\"stationCount\" min=\"1\" max=\"20\" value=\"5\" /&gt;\n\n\n\nFeature 3: Data Logging\n\nWhy: Enables in-depth analysis of participant behavior.\nHow: Logs player positions and actions into a MySQL database.\n\n\n\nserver.js\n\nconst insertPlayerPosition = async (playerId, positionX, positionY) =&gt; {\n    const query = `INSERT INTO player_positions (player_id, position_x, position_y) VALUES (?, ?, ?)`;\n    await mysqlConnection.execute(query, [playerId, positionX, positionY]);\n};\n\n\nAdmin Control Panel (admin.html): Dynamically adjusts parameters.\nData Persistence (MySQL): Stores player data for analysis.\nGame Reset: Allows restarting from initial conditions."
  },
  {
    "objectID": "scm_documentation.html#installation-and-getting-started",
    "href": "scm_documentation.html#installation-and-getting-started",
    "title": "Behavioral Experiment Documentation Report",
    "section": "5. Installation and Getting Started",
    "text": "5. Installation and Getting Started\n\nInstall dependencies:\n\nnpm install express socket.io mysql2\n\nStart the server:\n\nnode server.js\n\nOpen index.html: In a web browser.\nOpen the Admin Control Panel: http://localhost:3000/admin"
  },
  {
    "objectID": "scm_documentation.html#limitations",
    "href": "scm_documentation.html#limitations",
    "title": "Behavioral Experiment Documentation Report",
    "section": "6. Limitations",
    "text": "6. Limitations\n\nMatching Algorithms: Currently limited.\nScalability: May require optimizations for large numbers of players.\nGame Logic: Requires more detailed implementation of energy, charging, and monetary mechanics."
  },
  {
    "objectID": "scm_documentation.html#conclusion",
    "href": "scm_documentation.html#conclusion",
    "title": "Behavioral Experiment Documentation Report",
    "section": "7. Conclusion",
    "text": "7. Conclusion\nThis documentation outlines the structure and operation of the experiment software. Future work will focus on scalability, user experience, and integration with advanced analytics."
  },
  {
    "objectID": "scm_documentation.html#appendix",
    "href": "scm_documentation.html#appendix",
    "title": "Behavioral Experiment Documentation Report",
    "section": "Appendix",
    "text": "Appendix\n\n\n\n 2: Full server.js code.\n\n\nimport express from 'express';\nimport http from 'http';\nimport { Server } from 'socket.io';\nimport path from 'path';\nimport mysql from 'mysql2/promise';\nimport { fileURLToPath } from 'url';\nimport adminData from './admin-data.json' with { type: \"json\" };\nimport { writeFile } from 'fs/promises';\n\nconst __filename = fileURLToPath(import.meta.url);\nconst __dirname = path.dirname(__filename);\n\nconst app = express();\nconst server = http.createServer(app);\nconst io = new Server(server);\napp.use(express.json())\nlet AdminParams  = adminData;\n// Configuración del puerto\nconst PORT = process.env.PORT || 3000;\n\nconst gameState = {\n    players: new Map(),\n    stations: [\n        { top: 8, left: 338, cost: 20 },\n        { top: 479, left: 183, cost: 30 },\n        { top: 123, left: 124, cost: 15 },\n        { top: 300, left: 243, cost: 25 },\n        { top: 185, left: 479, cost: 10 },\n    ],\n};\n\nlet mysqlConnection;\nconst configureMySQL = async () =&gt; {\n    try {\n        mysqlConnection = await mysql.createConnection({\n            host: 'ec2-3-85-172-100.compute-1.amazonaws.com',\n            user: 'scastrom',\n            database: 'pathTracker',\n            password: 'samuel2024',\n        });\n        console.log('Conexión exitosa a MySQL');\n    } catch (err) {\n        console.error('Error al conectar a MySQL:', err.message);\n    }\n};"
  },
  {
    "objectID": "tms_llm.html",
    "href": "tms_llm.html",
    "title": "TMS LLM Task",
    "section": "",
    "text": "Figure 1: Simulations of Transactive Memory Systems",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#simulation-description",
    "href": "tms_llm.html#simulation-description",
    "title": "TMS LLM Task",
    "section": "Simulation Description",
    "text": "Simulation Description\nThis web application simulates a basic Transactive Memory System (TMS) within a small team of three agents. TMS refers to the shared understanding within a group about who knows what, who is good at what, and how to access that expertise. This simulation uses Large Language Models (LLMs) via an API to act as the agents, making decisions about task allocation based on their perceived knowledge of their own skills and the skills of their teammates.\nTo begin, enter the key (might be auto-entered) and click “Save” to set up the API connection. Then, configure the simulation settings (Agent Mode, Organizational Prompt, Total Rounds, Prompt History) and run the simulation to observe how different settings impact the team’s performance. You can save and compare multiple simulation runs to analyze the effects of various configurations on the team’s success in completing tasks. Press “Run next step” to proceed through the simulation one round at a time or “Run full simulation” to complete all rounds at once.\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 2: Simulations of Transactive Memory Systems",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#general-description-what-it-simulates",
    "href": "tms_llm.html#general-description-what-it-simulates",
    "title": "TMS LLM Task",
    "section": "General Description (What it Simulates)",
    "text": "General Description (What it Simulates)\nImagine a small team working together. For the team to be efficient, members shouldn’t just know things; they need to know who on the team is good at what. This knowledge of “who knows what” is called a Transactive Memory System (TMS). It allows team members to delegate tasks to the person most likely to succeed, rather than everyone trying to do everything themselves.\nThis web page provides a simplified simulation of how such a system might develop and function. It uses Artificial Intelligence (specifically, a Large Language Model or LLM like Llama 3.2 via an API) to represent the team members (“Agents”).\nHere’s the basic idea:\n\nThe Team: There are three simulated agents (Agent 1, Agent 2, Agent 3). Each agent has predefined, hidden skill levels (Good, Poor) in three areas: Math, Writing, and Logic.\nThe Tasks: In each “round” of the simulation, each agent is assigned one task type (Math, Writing, or Logic).\nThe Decision: The core of the simulation is the agent’s decision. Using its limited knowledge (based on settings), the agent (the LLM) decides whether to perform the task itself (“Self”) or delegate it to another agent it believes is better suited.\nLearning (Implicit): Agents “learn” based on the history of task outcomes they observe (either direct interactions or everything, depending on the mode). This history is fed back into the AI’s prompt in subsequent rounds, influencing future decisions.\nSuccess/Failure: Once the decision is made, the simulation checks the actual skill level of the agent who ended up with the task. Based on this skill (“Good” = 90% success, “Poor” = 10% success), the task either succeeds or fails randomly according to those probabilities.\nScoring: The team gets points only if all tasks in a round are successful.\nComparison: You can run multiple simulations with different settings (like how agents observe history, or what general instructions they get) and compare their performance using charts and data tables to see which strategies lead to better team outcomes (higher scores, better delegation).\n\nEssentially, it’s a playground to explore how different communication patterns and guidance affect a team’s ability to leverage its collective skills effectively, using AI agents as stand-ins for team members.",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#basic-simulation-procedure",
    "href": "tms_llm.html#basic-simulation-procedure",
    "title": "TMS LLM Task",
    "section": "Basic Simulation Procedure",
    "text": "Basic Simulation Procedure\n\nAPI Key: Enter your Purdue API key in the “API Key Setup” section and click “Save API Key”. This is necessary for the simulation to communicate with the AI model. The simulation area will appear once the key is saved.\nConfigure Settings:\n\nChoose the Agent Mode: How much information agents get about past rounds (“Direct Interactions Only” or “Observe Everything”).\nSelect an Organizational Prompt: General guidance given to the agents (e.g., “Focus on Best History”, “Encourage Delegation”).\nSet the Total Rounds per Sim. (e.g., 10 rounds).\nSet the Prompt History (Rounds): How many past rounds of information are included in the prompt given to the AI agent for its decision.\n(Optional) Click Customize Agent Prompts… to modify the underlying instructions given to each AI agent.\n\nRun the Simulation:\n\nClick Run next step to run one round at a time.\nClick Run full simulation to automatically run all remaining rounds. You can click Stop Auto Run (which appears) to halt this.\n\nObserve Results:\n\nThe Results Area shows the outcome of each round: who was assigned what, who they decided should do it, whether it succeeded or failed, and whether the optimal agent performed the task.\nThe Charts below (Cumulative Score, Correct Assignment Rate) update automatically, showing the performance of the current run in black alongside any saved runs you select for comparison.\n\nInspect Agents (Optional): Click the A1, A2, or A3 buttons in the “Inspect” section to open a pop-up showing what that agent has “observed” so far based on the chosen Agent Mode and the detailed history of its AI interactions (prompts and responses).\nSave Results: Once a simulation completes all its rounds, the Save Simulation Results button becomes active. Click it to store the settings, results, and logs of this run. It will appear in the “Compare Stored Runs” list.\nCompare Runs:\n\nIn the Compare Stored Runs section, check the boxes next to the saved runs you want to compare.\nThe Charts and Agent Choice Matrices will update to show data from the selected runs.\nClick View Log next to a saved run to see its detailed configuration, prompts used, and round-by-round results in a pop-up window.\nUse the Select All, Deselect All, and Delete Selected buttons to manage the comparison list.\n\nReset: Click Reset Current Run to clear the current simulation’s progress and results (without deleting saved runs) and allow you to change settings for a new run.",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#user-interface-explained",
    "href": "tms_llm.html#user-interface-explained",
    "title": "TMS LLM Task",
    "section": "User Interface Explained",
    "text": "User Interface Explained\n\nAPI Key Setup\n\nPurdue API Key Input: A password field where you enter your API key required to use the Purdue-hosted AI model.\nSave API Key Button: Saves the entered key for the current browser session and reveals the main simulation interface.\n\n\n\nSettings\n\nAgent Mode: Radio buttons controlling how much history agents observe:\n\nDirect Interactions Only: An agent only remembers the outcome of tasks it was initially assigned or tasks it ended up performing (either self-assigned or delegated to it).\nObserve Everything: Every agent sees the outcome of every task performed by any agent in the relevant history.\n\nOrganizational Prompt: A dropdown menu providing different high-level instructions added to the AI agent’s prompt:\n\nBaseline: Generic instruction to use skills and history.\nFocus on Best History: Explicitly tells the agent to prioritize delegation based on past success rates seen in the history.\nEncourage Delegation: Nudges the agent towards delegating if another agent seems significantly better based on history.\nSelf-Reliant Bias: Nudges the agent towards doing the task itself unless history strongly suggests delegation.\n\nTotal Rounds per Sim.: A number input setting the duration (number of rounds) for a single simulation run.\nPrompt History (Rounds): A number input setting how many previous rounds’ results are included in the context provided to the AI agent when it makes its decision. A value of 0 means no history is provided.\nCustomize Agent Prompts… Button: Opens a modal window allowing you to edit the base “system prompt” for each individual agent before starting a run. This allows for more fine-grained control over agent behavior beyond the standard Organizational Prompts.\n\n\n\nSimulation Control\n\nSimulation Status Label: Displays the current state of the simulation (e.g., “Ready to start”, “Running Round 5 of 10…”, “Simulation complete”).\nTotal Score Label: Shows the cumulative score for the current simulation run. Points are awarded only if all tasks succeed in a round.\nCurrent Org Instruction Display Label: Shows the currently selected Organizational Prompt setting.\nInspect Buttons (A1, A2, A3): Open the “Agent Memory” modal for the respective agent, showing their observations and interaction history for the current run.\nRun next step Button: Executes a single round of the simulation (task assignment, agent decisions via API calls, result simulation, UI update).\nRun full simulation Button: Automatically executes all remaining rounds until the simulation reaches the “Total Rounds” limit.\nStop Auto Run Button: Appears only during a full simulation run; clicking it stops the automatic execution after the current round finishes processing.\nReset Current Run Button: Stops any current execution, clears the results and state of the current run, and enables the settings so you can start a new run with potentially different parameters. Saved runs are unaffected.\nSave Simulation Results Button: Becomes active only when a simulation has completed all rounds. Clicking it stores the configuration, performance data, and logs of that run into the “Compare Stored Runs” list.\n\n\n\nResults Area\n\nThis scrollable text box displays the detailed outcomes of each round for the current simulation run as it progresses.\nRound Header (--- Round X ---): Separates results by round.\nResult Item: Shows:\n\nThe agent initially assigned the task.\nThe task type.\nThe decision made (decided: Self or decided: Agent X). It might show (original: ...) if the AI’s raw output was cleaned up or (forced from: ...) if an invalid decision was corrected.\nThe outcome (Success in green or Failure in red).\nDelegation correctness ([Optimal Solver] in green if the agent who performed the task was the best skilled, or [Suboptimal Solver (Best: Agent Y)] in red if another agent had a better chance of success).\n\n\n\n\nComparison Area\n\nCompare Stored Runs Header: Title for the section.\nAction Buttons:\n\nSelect All: Checks all checkboxes in the list below.\nDeselect All: Unchecks all checkboxes.\nDelete Selected: Permanently removes any saved runs whose checkboxes are checked. Requires confirmation.\n\nSaved Runs List: Displays previously saved simulation runs.\n\nCheckbox: Selects/deselects the run for inclusion in the comparison charts and matrices below.\nColor Box: Shows the color assigned to this run in the charts.\nLabel: A descriptive label for the run (e.g., “Run 1: direct(d3)/Base/21pts”), summarizing key settings and the final score.\nView Log Button: Opens the “Detailed Simulation Log” modal for this specific saved run.\nDelete Button: Permanently removes this single saved run. Requires confirmation.\n\n\n\n\nCharts\n\nCumulative Score Chart: A line chart showing the total score accumulated over the rounds for the currently selected saved runs and the current run (shown in black).\nCorrect Assignment Rate Chart: A line chart showing the percentage of tasks in each round that were performed by the agent with the highest skill level for that task, comparing selected saved runs and the current run (in black).\n\n\n\nAgent Choice Matrices\n\nHeader: Title for the section.\nMatrix Grid: Contains individual tables, one for each agent (Agent 1, Agent 2, Agent 3).\n\nTable Caption: Indicates which agent’s choices are being shown (e.g., “Agent 1’s Choices”).\nRows: Represent the task type the agent was assigned.\nColumns: Represent the agent that was chosen to perform the task (Agent 1, Agent 2, or Agent 3 - “Self” decisions are mapped to the deciding agent’s ID).\nCells: Show the proportion (as a percentage) of times that agent chose that specific solver when assigned that specific task, aggregated across all selected saved runs.\nHighlighted Cells (correct-choice): The cell corresponding to the theoretically optimal agent for that task row is highlighted in green. This helps visualize if agents are learning to delegate correctly based on the aggregated data.\nNo Data Placeholder: Appears if no runs are selected or if the selected runs have no data.\n\n\n\n\nModals (Pop-up Windows)\n\nInspect Agent Modal (memoryModal):\n\nOpened by the A1/A2/A3 buttons.\nShows the agent’s known skills (ground truth).\nShows the raw log of task outcomes the agent has observed based on the “Agent Mode”.\nContains a button View Detailed Interaction History which expands to show the exact System Prompt, User Prompt, and Raw LLM Response for each decision the agent made in the current run. Useful for debugging agent reasoning.\n\nCustomize Agent Prompts Modal (promptModal):\n\nOpened by the “Customize Agent Prompts…” button.\nProvides text areas to edit the base system prompt for each agent.\nReset All to Defaults: Reverts the text areas back to the standard default prompts.\nSave and Close: Saves any changes made. These changes only take effect after you Reset Current Run.\n\nDetailed Simulation Log Modal (storedLogModal):\n\nOpened by the “View Log” button next to a saved run.\nDisplays comprehensive information about a saved run.\nTabs:\n\nOverview: Shows run metadata (timestamp, ID), settings used (mode, depth, prompt, model), and summary performance (final score, avg. correct assignment rate).\nSystem Prompts: Shows the exact system prompt used for each agent during that run (highlighting if it was custom or default).\nResult Logs: Shows the detailed round-by-round results, similar to the main Results Area, but for the saved run.",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#potential-problems-and-inefficiencies",
    "href": "tms_llm.html#potential-problems-and-inefficiencies",
    "title": "TMS LLM Task",
    "section": "Potential Problems and Inefficiencies",
    "text": "Potential Problems and Inefficiencies\n\nAPI Dependency & Cost: The simulation relies heavily on external API calls to the LLM.\n\nCost: Each agent decision requires an API call, which can incur costs depending on the API provider and usage. Running many rounds or many simulations can become expensive.\nLatency: API calls take time. Running a full simulation with many rounds can be slow, especially since agent decisions within a round are currently processed sequentially.\nRate Limits: Frequent API calls might hit rate limits imposed by the provider, causing errors.\nAPI Errors: Network issues or problems on the API server side can cause failures. The current error handling is basic (logs errors, defaults to “Self”, might stop auto-run).\n\nLLM Output Parsing: The code expects the LLM to respond with exactly “Self”, “Agent 1”, “Agent 2”, or “Agent 3”. While there’s some basic cleaning (removing quotes, trailing punctuation), slightly unexpected phrasing could lead to the decision defaulting to “Self”, potentially skewing results. This is a common challenge when working with LLMs for structured output without using more robust methods like function calling.\nSequential Processing: Within runRound, the getAgentDecisionLLM function is called sequentially for each agent using await in a loop. This means Agent 2 doesn’t start thinking until Agent 1’s API call completes, etc. This increases the time taken per round.\nState Management: The simulation uses global variables (simulationState, agents, savedSimulations) to manage its state. While functional for this demo, this can become difficult to manage and debug in larger, more complex applications. Changes in one part of the code can unintentionally affect others.\nInefficient Data Storage (Deep Copy): When storing a run (storeRunBtn), it uses JSON.parse(JSON.stringify(...)) to create deep copies of the results log and decision histories. For simulations with many rounds, these logs can become large, making this copying method memory and CPU intensive.\nNo Data Persistence: The savedSimulations array exists only in memory. If you close the browser tab or refresh the page, all saved runs are lost.\nBasic Observation Model: The agent.observations log is just a simple record of outcomes. The agents don’t explicitly calculate or infer skill levels based on this data; the raw history is simply fed back to the LLM, relying on it to interpret the patterns.\nSimplified Task Model: Tasks are abstract (Math, Writing, Logic) with fixed, simple probabilistic success rates. Real-world tasks and skills are far more nuanced.\nUI Updates: While likely acceptable for this scale, updating the resultsArea and charts every round involves direct DOM manipulation, which can sometimes be less efficient than using frameworks with virtual DOMs (like React, Vue) for very complex interfaces or extremely rapid updates.\nPrompt Brittleness: The simulation’s behavior is highly sensitive to the exact wording of the system prompts, organizational instructions, and the LLM’s interpretation, which can sometimes be unpredictable or change between model versions.",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#areas-for-improvement",
    "href": "tms_llm.html#areas-for-improvement",
    "title": "TMS LLM Task",
    "section": "Areas for Improvement",
    "text": "Areas for Improvement\n\nRobust Error Handling: Implement retry logic for API calls (with exponential backoff) for transient network errors or rate limits. Provide clearer user feedback when API errors occur.\nImproved LLM Output Handling:\n\nExplore using LLM Function Calling (if the API/model supports it) to get structured JSON output instead of relying on exact string matching.\nImplement more sophisticated parsing and validation of the LLM’s text response.\n\nParallel API Calls: Modify runRound to use Promise.all to send API requests for all agent decisions within a round concurrently. This could significantly speed up simulations, provided the API rate limits allow it.\nState Management Refactoring: Encapsulate state and logic within classes or modules (e.g., a Simulation class, Agent class) to improve organization and reduce reliance on global variables.\nPersistence: Use localStorage or sessionStorage to save the savedSimulations array, allowing users to retain saved runs across browser sessions.\nData Storage Optimization: For storing large logs, consider more efficient serialization methods or potentially summarizing older data if full detail isn’t always needed for comparison.\nAdvanced Agent Memory/Inference: Implement logic where agents explicitly calculate or update estimated skill levels for themselves and others based on observed outcomes, rather than just passing raw history to the LLM.\nConfiguration Options: Allow users to configure API parameters like the specific model (config.model), temperature (config.temperature), and max tokens (config.maxTokens) via the UI.\nCode Modularity: Break the large JavaScript block into smaller, more focused functions or modules (e.g., api.js, ui.js, simulation.js, charts.js). Add more comments to explain complex sections.\nVisualization Enhancements: Add options to plot different metrics (e.g., average score per round, task success rate by type). Visualize inferred agent skill levels over time if implemented (see point 7).\nBatching UI Updates: For very long runs, consider batching DOM updates (e.g., update resultsArea only every few rounds or at the end) to potentially improve performance, though the current approach is likely fine.",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#what-this-simulation-demonstrates",
    "href": "tms_llm.html#what-this-simulation-demonstrates",
    "title": "TMS LLM Task",
    "section": "What This Simulation Demonstrates",
    "text": "What This Simulation Demonstrates\nThis web page provides a simplified, interactive simulation exploring how a TMS might emerge and function within a small team. It uses AI agents (powered by a Large Language Model like Llama 3.2 via an API) to represent team members.\nThe Core Idea:\n\nThe Team: Three simulated “Agents” (Agent 1, Agent 2, Agent 3). Each has predefined, hidden skill levels (Good or Poor) in three domains: Math, Writing, and Logic. For instance, Agent 1 might be Good at Math but Poor at Writing and Logic.\nThe Tasks: In each “round,” every agent is assigned one task type (Math, Writing, or Logic).\nThe Decision: This is the heart of the TMS simulation. Based on the information available to it (controlled by settings like “Agent Mode” and “Prompt History”), each agent (the LLM) decides whether to:\n\nPerform the task itself (“Self”).\nDelegate the task to another agent it believes is better suited (“Agent X”).\n\nLearning (Implicitly): Agents don’t have explicit memory structures that get updated like a database. Instead, they “learn” by observing the outcomes of past tasks. A summary of relevant past rounds (the “history”) is included in the prompt sent to the LLM for its next decision. This history influences its future choices. The amount and type of history observed depend on the Agent Mode setting.\nSuccess/Failure Simulation: After the decision, the simulation determines the outcome based on the actual skill level of the agent who ended up performing the task.\n\nGood Skill: 90% chance of success.\nPoor Skill: 10% chance of success.\nThe outcome (Success/Failure) is determined randomly based on these probabilities.\n\nTeam Scoring: The team earns points (+3) for a round only if all three tasks assigned in that round are completed successfully. This incentivizes effective delegation across the entire team.\nExperimentation & Comparison: You can run multiple simulations using different settings (e.g., how agents observe history, what general instructions they receive). By saving and comparing the results using charts and data tables, you can explore which conditions foster a more effective TMS, leading to better scores and more accurate task assignments.\n\nEssentially, this demo is a sandbox for observing how communication patterns (Agent Mode) and high-level guidance (Organizational Prompts) impact the ability of AI agents to develop and utilize a TMS to leverage their collective expertise.",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#running-a-simulation-basic-workflow",
    "href": "tms_llm.html#running-a-simulation-basic-workflow",
    "title": "TMS LLM Task",
    "section": "Running a Simulation: Basic Workflow",
    "text": "Running a Simulation: Basic Workflow\n\nSetup (API Key): Enter your Purdue API key in the “API Key Setup” section and click “Save”. This is essential for the simulation to communicate with the AI model. The main simulation area will appear after saving.\nConfigure Settings: Before starting, adjust the simulation parameters:\n\nAgent Mode: Controls how much task history agents observe (Direct Interactions Only vs. Observe Everything).\nOrganizational Prompt: Selects high-level guidance given to agents (e.g., Baseline, Focus on Best History, Encourage Delegation, Self-Reliant Bias).\nTotal Rounds: Sets the number of rounds for the simulation (e.g., 15).\nPrompt History: Determines how many past rounds are included in the context given to the AI for decision-making (0 means no history).\n(Optional) Customize Agent Prompts: Click the button to modify the underlying base instructions for each agent for more fine-grained control (requires Reset Run to apply).\n\nRun the Simulation:\n\nClick Run next step to execute one round at a time.\nClick Run full simulation to automatically run all remaining rounds. Use Stop Auto Run to halt mid-way.\n\nObserve Results:\n\nThe Results Area displays the detailed outcome of each round for the current run (who was assigned what, who performed it, success/failure, optimality).\nThe Charts (Cumulative Score, Correct Assignment Rate) update automatically, showing the current run’s performance (in black) alongside any selected saved runs.\n\nInspect & Debug (Optional):\n\nClick the A1, A2, A3 buttons under “Inspect” to see what an agent has observed (based on Agent Mode) and its detailed AI interaction history (prompts & responses) for the current run.\n\nSave & Compare:\n\nOnce a simulation completes, click Save Simulation Results to store its settings and outcomes.\nIn the Compare Stored Runs section, check the boxes next to saved runs to compare their performance on the charts and in the Agent Choice Matrices.\nUse View Log to examine the detailed configuration and results of a specific saved run. Manage the list using Select All, Deselect All, Delete Selected.\n\nReset: Click Reset Current Run to clear the current simulation’s progress and results (saved runs remain). This allows you to change settings and start a new run.",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#understanding-the-interface-key-components",
    "href": "tms_llm.html#understanding-the-interface-key-components",
    "title": "TMS LLM Task",
    "section": "Understanding the Interface (Key Components)",
    "text": "Understanding the Interface (Key Components)\n\nSettings\n\nAgent Mode:\n\nDirect Interactions Only: Agents only remember outcomes of tasks they were assigned or performed. Simulates limited information sharing.\nObserve Everything: Agents see the outcome of all tasks in the relevant history. Simulates full transparency.\n\nOrganizational Prompt: Provides different high-level strategies added to the AI’s instructions:\n\nBaseline: Generic guidance.\nFocus on Best History: Prioritize delegation based on past success.\nEncourage Delegation: Nudge towards delegation if others seem better.\nSelf-Reliant Bias: Nudge towards performing tasks oneself.\n\nTotal Rounds / Prompt History: Control simulation length and the amount of past context influencing decisions.\nCustomize Agent Prompts: Allows advanced users to edit the base system prompt for each agent (takes effect after Reset).\n\n\n\nSimulation Control\n\nStatus/Score Display: Shows the simulation’s current state and the team’s cumulative score for the ongoing run.\nInspect Buttons (A1, A2, A3): Opens a pop-up showing the selected agent’s observed history and AI interactions (for the current run).\nRun/Stop/Reset Buttons: Control the execution flow (step-by-step, full auto-run, stopping, clearing the current run).\nSave Simulation Results: (Enabled when complete) Stores the current run’s configuration and results for later comparison.\n\n\n\nResults Area\n\nDisplays a log of the current simulation’s progress, round by round. Each entry shows:\n\nWho was assigned which task.\nThe agent’s decision (Self or Delegate Agent X). May indicate if corrected/forced.\nThe simulated outcome (Success/Failure).\nWhether the task was performed by the agent with the best actual skill ([Optimal Solver] or [Suboptimal Solver (Best: Agent Y)]).\n\n\n\n\nComparison Area\n\nCompare Stored Runs: Lists simulations you’ve saved.\n\nCheckboxes: Select runs to display on the charts and matrices.\nColor Box: Indicates the color used for that run in the charts.\nLabel: Summarizes the run’s key settings and final score.\nView Log: Opens a detailed pop-up for the saved run (settings, prompts, full results).\nDelete: Removes a single saved run.\n\nAction Buttons (Select All, etc.): Manage the selection and deletion of multiple saved runs.\n\n\n\nCharts\n\nCumulative Score Chart: Tracks the total team score over rounds for the current run (black) and selected saved runs.\nCorrect Assignment Rate Chart: Shows the percentage of tasks per round performed by the optimally skilled agent, comparing runs. Helps visualize if the team is learning to delegate effectively.\n\n\n\nAgent Choice Matrices\n\nProvides a breakdown of individual agent decision patterns, aggregated across selected saved runs (and the current run if active).\nFor each agent, a table shows:\n\nRows: The task type the agent was assigned (Math, Writing, Logic).\nColumns: The agent chosen to perform the task (Agent 1, Agent 2, Agent 3 - “Self” maps to the deciding agent’s ID).\nCells: The percentage (%) of time that agent made that specific choice for that assigned task.\nGreen Highlight: Indicates the cell corresponding to the theoretically optimal agent for that task, helping to see if agents are learning correct delegation patterns.\n\n\n\n\nKey Pop-ups (Modals)\n\nInspect Agent: Opened via A1/A2/A3 buttons. Shows the agent’s ground-truth skills, its observed task history (based on Agent Mode), and allows viewing the detailed LLM interaction history (prompts sent, raw AI response) for debugging decisions in the current run.\nCustomize Agent Prompts: Opened via the “Customize…” button in Settings. Allows editing the base system prompt text for each agent (requires Reset Run to apply changes).\nDetailed Simulation Log: Opened via the “View Log” button next to a saved run. Provides a comprehensive view of a completed, saved simulation, including:\n\nOverview Tab: Settings used, final score, average performance metrics.\nSystem Prompts Tab: The exact system prompts used for each agent during that run.\nResult Logs Tab: The full round-by-round results for that saved run.\n\n\nThis simulation provides a visual and interactive way to explore the dynamics of team knowledge sharing and task delegation, offering insights into how different factors can influence the development of an effective Transactive Memory System.",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#tms-llm-simulation",
    "href": "tms_llm.html#tms-llm-simulation",
    "title": "TMS LLM Task",
    "section": "TMS-LLM Simulation",
    "text": "TMS-LLM Simulation\nenter for key below: sk-4597455063b242e9a43187a32115c04a\n\nOpen task in separate window (provides more space)",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#tms-llm-simulation-background-instructions",
    "href": "tms_llm.html#tms-llm-simulation-background-instructions",
    "title": "TMS LLM Task",
    "section": "TMS-LLM Simulation Background & Instructions",
    "text": "TMS-LLM Simulation Background & Instructions\n\nWhat This Simulation Demonstrates\nThis web page provides a simplified, interactive simulation exploring how a TMS might emerge and function within a small team. It uses AI agents (powered by a Large Language Model like Llama 3.2 via an API) to represent team members.\nThe Core Idea:\n\nThe Team: Three simulated “Agents” (Agent 1, Agent 2, Agent 3). Each has predefined, hidden skill levels (Good or Poor) in three domains: Math, Writing, and Logic. For instance, Agent 1 might be Good at Math but Poor at Writing and Logic.\nThe Tasks: In each “round,” every agent is assigned one task type (Math, Writing, or Logic).\nThe Decision: This is the heart of the TMS simulation. Based on the information available to it (controlled by settings like “Agent Mode” and “Prompt History”), each agent (the LLM) decides whether to:\n\nPerform the task itself (“Self”).\nDelegate the task to another agent it believes is better suited (“Agent X”).\n\nLearning (Implicitly): Agents don’t have explicit memory structures that get updated like a database. Instead, they “learn” by observing the outcomes of past tasks. A summary of relevant past rounds (the “history”) is included in the prompt sent to the LLM for its next decision. This history influences its future choices. The amount and type of history observed depend on the Agent Mode setting.\nSuccess/Failure Simulation: After the decision, the simulation determines the outcome based on the actual skill level of the agent who ended up performing the task.\n\nGood Skill: 90% chance of success.\nPoor Skill: 10% chance of success.\nThe outcome (Success/Failure) is determined randomly based on these probabilities.\n\nTeam Scoring: The team earns points (+3) for a round only if all three tasks assigned in that round are completed successfully. This incentivizes effective delegation across the entire team.\nExperimentation & Comparison: You can run multiple simulations using different settings (e.g., how agents observe history, what general instructions they receive). By saving and comparing the results using charts and data tables, you can explore which conditions foster a more effective TMS, leading to better scores and more accurate task assignments.\n\nEssentially, this demo is a sandbox for observing how communication patterns (Agent Mode) and high-level guidance (Organizational Prompts) impact the ability of AI agents to develop and utilize a TMS to leverage their collective expertise.\n\n\nRunning a Simulation: Basic Workflow\n\nSetup (API Key): Enter your Purdue API key in the “API Key Setup” section and click “Save”. This is essential for the simulation to communicate with the AI model. The main simulation area will appear after saving.\nConfigure Settings: Before starting, adjust the simulation parameters:\n\nAgent Mode: Controls how much task history agents observe (Direct Interactions Only vs. Observe Everything).\nOrganizational Prompt: Selects high-level guidance given to agents (e.g., Baseline, Focus on Best History, Encourage Delegation, Self-Reliant Bias).\nTotal Rounds: Sets the number of rounds for the simulation (e.g., 15).\nPrompt History: Determines how many past rounds are included in the context given to the AI for decision-making (0 means no history).\n(Optional) Customize Agent Prompts: Click the button to modify the underlying base instructions for each agent for more fine-grained control (requires Reset Run to apply).\n\nRun the Simulation:\n\nClick Run next step to execute one round at a time.\nClick Run full simulation to automatically run all remaining rounds. Use Stop Auto Run to halt mid-way.\n\nObserve Results:\n\nThe Results Area displays the detailed outcome of each round for the current run (who was assigned what, who performed it, success/failure, optimality).\nThe Charts (Cumulative Score, Correct Assignment Rate) update automatically, showing the current run’s performance (in black) alongside any selected saved runs.\n\nInspect & Debug (Optional):\n\nClick the A1, A2, A3 buttons under “Inspect” to see what an agent has observed (based on Agent Mode) and its detailed AI interaction history (prompts & responses) for the current run.\n\nSave & Compare:\n\nOnce a simulation completes, click Save Simulation Results to store its settings and outcomes.\nIn the Compare Stored Runs section, check the boxes next to saved runs to compare their performance on the charts and in the Agent Choice Matrices.\nUse View Log to examine the detailed configuration and results of a specific saved run. Manage the list using Select All, Deselect All, Delete Selected.\n\nReset: Click Reset Current Run to clear the current simulation’s progress and results (saved runs remain). This allows you to change settings and start a new run.\n\n\n\nUnderstanding the Interface (Key Components)\n\nSettings\n\nAgent Mode:\n\nDirect Interactions Only: Agents only remember outcomes of tasks they were assigned or performed. Simulates limited information sharing.\nObserve Everything: Agents see the outcome of all tasks in the relevant history. Simulates full transparency.\n\nOrganizational Prompt: Provides different high-level strategies added to the AI’s instructions:\n\nBaseline: Generic guidance.\nFocus on Best History: Prioritize delegation based on past success.\nEncourage Delegation: Nudge towards delegation if others seem better.\nSelf-Reliant Bias: Nudge towards performing tasks oneself.\n\nTotal Rounds / Prompt History: Control simulation length and the amount of past context influencing decisions.\nCustomize Agent Prompts: Allows advanced users to edit the base system prompt for each agent (takes effect after Reset).\n\n\n\nSimulation Control\n\nStatus/Score Display: Shows the simulation’s current state and the team’s cumulative score for the ongoing run.\nInspect Buttons (A1, A2, A3): Opens a pop-up showing the selected agent’s observed history and AI interactions (for the current run).\nRun/Stop/Reset Buttons: Control the execution flow (step-by-step, full auto-run, stopping, clearing the current run).\nSave Simulation Results: (Enabled when complete) Stores the current run’s configuration and results for later comparison.\n\n\n\nResults Area\n\nDisplays a log of the current simulation’s progress, round by round. Each entry shows:\n\nWho was assigned which task.\nThe agent’s decision (Self or Delegate Agent X). May indicate if corrected/forced.\nThe simulated outcome (Success/Failure).\nWhether the task was performed by the agent with the best actual skill ([Optimal Solver] or [Suboptimal Solver (Best: Agent Y)]).\n\n\n\n\n\nComparison Area\n\nCompare Stored Runs: Lists simulations you’ve saved.\n\nCheckboxes: Select runs to display on the charts and matrices.\nColor Box: Indicates the color used for that run in the charts.\nLabel: Summarizes the run’s key settings and final score.\nView Log: Opens a detailed pop-up for the saved run (settings, prompts, full results).\nDelete: Removes a single saved run.\n\nAction Buttons (Select All, etc.): Manage the selection and deletion of multiple saved runs.\n\n\nCharts\n\nCumulative Score Chart: Tracks the total team score over rounds for the current run (black) and selected saved runs.\nCorrect Assignment Rate Chart: Shows the percentage of tasks per round performed by the optimally skilled agent, comparing runs. Helps visualize if the team is learning to delegate effectively.\n\n\n\nAgent Choice Matrices\n\nProvides a breakdown of individual agent decision patterns, aggregated across selected saved runs (and the current run if active).\nFor each agent, a table shows:\n\nRows: The task type the agent was assigned (Math, Writing, Logic).\nColumns: The agent chosen to perform the task (Agent 1, Agent 2, Agent 3 - “Self” maps to the deciding agent’s ID).\nCells: The percentage (%) of time that agent made that specific choice for that assigned task.\nGreen Highlight: Indicates the cell corresponding to the theoretically optimal agent for that task, helping to see if agents are learning correct delegation patterns.\n\n\n\n\nKey Pop-ups (Modals)\n\nInspect Agent: Opened via A1/A2/A3 buttons. Shows the agent’s ground-truth skills, its observed task history (based on Agent Mode), and allows viewing the detailed LLM interaction history (prompts sent, raw AI response) for debugging decisions in the current run.\nCustomize Agent Prompts: Opened via the “Customize…” button in Settings. Allows editing the base system prompt text for each agent (requires Reset Run to apply changes).\nDetailed Simulation Log: Opened via the “View Log” button next to a saved run. Provides a comprehensive view of a completed, saved simulation, including:\n\nOverview Tab: Settings used, final score, average performance metrics.\nSystem Prompts Tab: The exact system prompts used for each agent during that run.\nResult Logs Tab: The full round-by-round results for that saved run.\n\n\nThis simulation provides a visual and interactive way to explore the dynamics of team knowledge sharing and task delegation, offering insights into how different factors can influence the development of an effective Transactive Memory System.",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  },
  {
    "objectID": "tms_llm.html#tms-llm-simulation---try-it-out",
    "href": "tms_llm.html#tms-llm-simulation---try-it-out",
    "title": "TMS LLM Task",
    "section": "TMS-LLM Simulation - Try it out!",
    "text": "TMS-LLM Simulation - Try it out!\nenter key below: sk-4597455063b242e9a43187a32115c04a\n\nThen press save key to start the simulation demo\nPress the Run full simulation button to see the simulation in action\n\n\nOpen task in separate window (optional - provides more space)",
    "crumbs": [
      "LLM Literature",
      "TMS LLM Task"
    ]
  }
]