<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.32">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Thomas E. Gorman">
<meta name="dcterms.date" content="2024-10-31">

<title>Individual decision lit – CCL Projects</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-2486e1f0a3ee9ee1fc393803a1361cdb.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-29f71986bfdb98847d3932762ebabacd.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="Assets/Style/calloutTG.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ai_gd.html">LLM Literature</a></li><li class="breadcrumb-item"><a href="./ai_decision.html">Individual decision lit</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">CCL Projects</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/tegorman13/ccl" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">LLM Literature</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_gd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Group Decision Lit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_decision.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Individual decision lit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./llm_energy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLM Energy</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_interaction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Interactive AI Lit</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Misc</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transactive Memory Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Samuel_Project.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Samuel’s Project</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Driving.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Driving Lit</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making" id="toc-to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making" class="nav-link active" data-scroll-target="#to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making">To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making</a></li>
  <li><a href="#irrationality-and-cognitive-biases-in-large-language-models." id="toc-irrationality-and-cognitive-biases-in-large-language-models." class="nav-link" data-scroll-target="#irrationality-and-cognitive-biases-in-large-language-models.">(Ir)rationality and cognitive biases in large language models.</a></li>
  <li><a href="#human-like-intuitive-behavior-and-reasoning-biases-emerged-in-large-language-models-but-disappeared-in-chatgpt" id="toc-human-like-intuitive-behavior-and-reasoning-biases-emerged-in-large-language-models-but-disappeared-in-chatgpt" class="nav-link" data-scroll-target="#human-like-intuitive-behavior-and-reasoning-biases-emerged-in-large-language-models-but-disappeared-in-chatgpt">Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT</a></li>
  <li><a href="#using-cognitive-psychology-to-understand-gpt-3." id="toc-using-cognitive-psychology-to-understand-gpt-3." class="nav-link" data-scroll-target="#using-cognitive-psychology-to-understand-gpt-3.">Using cognitive psychology to understand GPT-3.</a></li>
  <li><a href="#studying-and-improving-reasoning-in-humans-and-machines." id="toc-studying-and-improving-reasoning-in-humans-and-machines." class="nav-link" data-scroll-target="#studying-and-improving-reasoning-in-humans-and-machines.">Studying and improving reasoning in humans and machines.</a></li>
  <li><a href="#exploring-variability-in-risk-taking-with-large-language-models." id="toc-exploring-variability-in-risk-taking-with-large-language-models." class="nav-link" data-scroll-target="#exploring-variability-in-risk-taking-with-large-language-models.">Exploring variability in risk taking with large language models.</a></li>
  <li><a href="#human-bias-in-ai-models-anchoring-effects-and-mitigation-strategies-in-large-language-models" id="toc-human-bias-in-ai-models-anchoring-effects-and-mitigation-strategies-in-large-language-models" class="nav-link" data-scroll-target="#human-bias-in-ai-models-anchoring-effects-and-mitigation-strategies-in-large-language-models">Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models</a></li>
  <li><a href="#a-turing-test-of-whether-ai-chatbots-are-behaviorally-similar-to-humans" id="toc-a-turing-test-of-whether-ai-chatbots-are-behaviorally-similar-to-humans" class="nav-link" data-scroll-target="#a-turing-test-of-whether-ai-chatbots-are-behaviorally-similar-to-humans">A Turing test of whether AI chatbots are behaviorally similar to humans</a></li>
  <li><a href="#deciding-fast-and-slow-the-role-of-cognitive-biases-in-ai-assisted-decision-making" id="toc-deciding-fast-and-slow-the-role-of-cognitive-biases-in-ai-assisted-decision-making" class="nav-link" data-scroll-target="#deciding-fast-and-slow-the-role-of-cognitive-biases-in-ai-assisted-decision-making">Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making</a></li>
  <li><a href="#decision-control-and-explanations-in-human-ai-collaboration-improving-user-perceptions-and-compliance" id="toc-decision-control-and-explanations-in-human-ai-collaboration-improving-user-perceptions-and-compliance" class="nav-link" data-scroll-target="#decision-control-and-explanations-in-human-ai-collaboration-improving-user-perceptions-and-compliance">Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance</a></li>
  <li><a href="#risk-and-prosocial-behavioural-cues-elicit-human-like-response-patterns-from-ai-chatbots." id="toc-risk-and-prosocial-behavioural-cues-elicit-human-like-response-patterns-from-ai-chatbots." class="nav-link" data-scroll-target="#risk-and-prosocial-behavioural-cues-elicit-human-like-response-patterns-from-ai-chatbots.">Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots.</a></li>
  <li><a href="#do-large-language-models-show-decision-heuristics-similar-to-humans-a-case-study-using-gpt-3.5" id="toc-do-large-language-models-show-decision-heuristics-similar-to-humans-a-case-study-using-gpt-3.5" class="nav-link" data-scroll-target="#do-large-language-models-show-decision-heuristics-similar-to-humans-a-case-study-using-gpt-3.5">Do large language models show decision heuristics similar to humans? A case study using GPT-3.5</a></li>
  <li><a href="#can-large-language-models-capture-human-preferences" id="toc-can-large-language-models-capture-human-preferences" class="nav-link" data-scroll-target="#can-large-language-models-capture-human-preferences">Can Large Language Models Capture Human Preferences?</a></li>
  <li><a href="#language-models-like-humans-show-content-effects-on-reasoning-tasks" id="toc-language-models-like-humans-show-content-effects-on-reasoning-tasks" class="nav-link" data-scroll-target="#language-models-like-humans-show-content-effects-on-reasoning-tasks">Language models, like humans, show content effects on reasoning tasks</a></li>
  <li><a href="#the-emergence-of-economic-rationality-of-gpt" id="toc-the-emergence-of-economic-rationality-of-gpt" class="nav-link" data-scroll-target="#the-emergence-of-economic-rationality-of-gpt">The emergence of economic rationality of GPT</a></li>
  <li><a href="#the-potential-of-generative-ai-for-personalized-persuasion-at-scale." id="toc-the-potential-of-generative-ai-for-personalized-persuasion-at-scale." class="nav-link" data-scroll-target="#the-potential-of-generative-ai-for-personalized-persuasion-at-scale.">The potential of generative AI for personalized persuasion at scale.</a></li>
  <li><a href="#decision-making-paradoxes-in-humans-vs-machines-the-case-of-the-allais-and-ellsberg-paradoxes." id="toc-decision-making-paradoxes-in-humans-vs-machines-the-case-of-the-allais-and-ellsberg-paradoxes." class="nav-link" data-scroll-target="#decision-making-paradoxes-in-humans-vs-machines-the-case-of-the-allais-and-ellsberg-paradoxes.">Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes.</a></li>
  <li><a href="#do-llms-exhibit-human-like-response-biases-a-case-study-in-survey-design." id="toc-do-llms-exhibit-human-like-response-biases-a-case-study-in-survey-design." class="nav-link" data-scroll-target="#do-llms-exhibit-human-like-response-biases-a-case-study-in-survey-design.">Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design.</a></li>
  <li><a href="#cognitive-ease-at-a-cost-llms-reduce-mental-effort-but-compromise-depth-in-student-scientific-inquiry" id="toc-cognitive-ease-at-a-cost-llms-reduce-mental-effort-but-compromise-depth-in-student-scientific-inquiry" class="nav-link" data-scroll-target="#cognitive-ease-at-a-cost-llms-reduce-mental-effort-but-compromise-depth-in-student-scientific-inquiry">Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry</a></li>
  <li><a href="#cognitive-llms-towards-integrating-cognitive-architectures-and-large-language-models-for-manufacturing-decision-making" id="toc-cognitive-llms-towards-integrating-cognitive-architectures-and-large-language-models-for-manufacturing-decision-making" class="nav-link" data-scroll-target="#cognitive-llms-towards-integrating-cognitive-architectures-and-large-language-models-for-manufacturing-decision-making">Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making</a></li>
  <li><a href="#large-language-models-amplify-human-biases-in-moral-decision-making" id="toc-large-language-models-amplify-human-biases-in-moral-decision-making" class="nav-link" data-scroll-target="#large-language-models-amplify-human-biases-in-moral-decision-making">Large Language Models Amplify Human Biases in Moral Decision-Making</a></li>
  <li><a href="#large-language-model-recall-uncertainty-is-modulated-by-the-fan-effect." id="toc-large-language-model-recall-uncertainty-is-modulated-by-the-fan-effect." class="nav-link" data-scroll-target="#large-language-model-recall-uncertainty-is-modulated-by-the-fan-effect.">Large Language Model Recall Uncertainty is Modulated by the Fan Effect.</a></li>
  <li><a href="#accuracy-time-tradeoffs-in-ai-assisted-decision-making-under-time-pressure." id="toc-accuracy-time-tradeoffs-in-ai-assisted-decision-making-under-time-pressure." class="nav-link" data-scroll-target="#accuracy-time-tradeoffs-in-ai-assisted-decision-making-under-time-pressure.">Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure.</a></li>
  <li><a href="#the-llm-effect-are-humans-truly-using-llms-or-are-they-being-influenced-by-them-instead" id="toc-the-llm-effect-are-humans-truly-using-llms-or-are-they-being-influenced-by-them-instead" class="nav-link" data-scroll-target="#the-llm-effect-are-humans-truly-using-llms-or-are-they-being-influenced-by-them-instead">The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?</a></li>
  <li><a href="#mutual-theory-of-mind-in-human-ai-collaboration-an-empirical-study-with-llm-driven-ai-agents-in-a-real-time-shared-workspace-task" id="toc-mutual-theory-of-mind-in-human-ai-collaboration-an-empirical-study-with-llm-driven-ai-agents-in-a-real-time-shared-workspace-task" class="nav-link" data-scroll-target="#mutual-theory-of-mind-in-human-ai-collaboration-an-empirical-study-with-llm-driven-ai-agents-in-a-real-time-shared-workspace-task">Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task</a></li>
  <li><a href="#bridging-the-gulf-of-envisioning-cognitive-design-challenges-in-llm-interfaces" id="toc-bridging-the-gulf-of-envisioning-cognitive-design-challenges-in-llm-interfaces" class="nav-link" data-scroll-target="#bridging-the-gulf-of-envisioning-cognitive-design-challenges-in-llm-interfaces">Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces</a></li>
  <li><a href="#learning-to-guide-human-decision-makers-with-vision-language-models" id="toc-learning-to-guide-human-decision-makers-with-vision-language-models" class="nav-link" data-scroll-target="#learning-to-guide-human-decision-makers-with-vision-language-models">Learning To Guide Human Decision Makers With Vision-Language Models</a></li>
  <li><a href="#how-does-value-similarity-affect-human-reliance-in-ai-assisted-ethical-decision-making" id="toc-how-does-value-similarity-affect-human-reliance-in-ai-assisted-ethical-decision-making" class="nav-link" data-scroll-target="#how-does-value-similarity-affect-human-reliance-in-ai-assisted-ethical-decision-making">How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making?</a></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="ai_decision.md"><i class="bi bi-file-code"></i>Github (GFM)</a></li><li><a href="indiv_ai_lit_notes.docx"><i class="bi bi-file-word"></i>MS Word</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./ai_gd.html">LLM Literature</a></li><li class="breadcrumb-item"><a href="./ai_decision.html">Individual decision lit</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Individual decision lit</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://tegorman13.github.io/">Thomas E. Gorman</a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://web.ics.purdue.edu/~treimer/">
            Communication and Cognition Lab, Purdue University, USA
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">October 31, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<section id="to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making" class="level2">
<h2 class="anchored" data-anchor-id="to-trust-or-to-think-cognitive-forcing-functions-can-reduce-overreliance-on-ai-in-ai-assisted-decision-making">To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making</h2>
<p>Buçinca, Z., Malaya, M. B., &amp; Gajos, K. Z. (2021). <strong>To Trust or to Think: Cognitive Forcing Functions Can Reduce Overreliance on AI in AI-assisted Decision-making.</strong> Proceedings of the ACM on Human-Computer Interaction, 5(CSCW1), 1–21. https://doi.org/10.1145/3449287</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>People supported by AI-powered decision support tools frequently overrely on the AI: they accept an AI’s suggestion even when that suggestion is wrong. Adding explanations to the AI decisions does not appear to reduce the overreliance and some studies suggest that it might even increase it. Informed by the dual-process theory of cognition, we posit that people rarely engage analytically with each individual AI recommendation and explanation, and instead develop general heuristics about whether and when to follow the AI suggestions. Building on prior research on medical decision-making, we designed three cognitive forcing interventions to compel people to engage more thoughtfully with the AI-generated explanations. We conducted an experiment (N=199), in which we compared our three cognitive forcing designs to two simple explainable AI approaches and to a no-AI baseline. The results demonstrate that cognitive forcing significantly reduced overreliance compared to the simple explainable AI approaches. However, there was a trade-off: people assigned the least favorable subjective ratings to the designs that reduced the overreliance the most. To audit our work for intervention-generated inequalities, we investigated whether our interventions benefited equally people with different levels of Need for Cognition (i.e., motivation to engage in effortful mental activities). Our results show that, on average, cognitive forcing interventions benefited participants higher in Need for Cognition more. Our research suggests that human cognitive motivation moderates the effectiveness of explainable AI solutions.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Bucina1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure from @bucincaTrustThinkCognitive2021"><img src="images/Bucina1.png" class="img-fluid figure-img" alt="Figure from Buçinca et al. (2021)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="bucincaTrustThinkCognitive2021">Buçinca et al. (<a href="#ref-bucincaTrustThinkCognitive2021" role="doc-biblioref">2021</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="irrationality-and-cognitive-biases-in-large-language-models." class="level2">
<h2 class="anchored" data-anchor-id="irrationality-and-cognitive-biases-in-large-language-models.">(Ir)rationality and cognitive biases in large language models.</h2>
<p>Macmillan-Scott, O., &amp; Musolesi, M. (2024). <strong>(Ir)rationality and cognitive biases in large language models</strong> Royal Society Open Science, 11(6), 240255. https://doi.org/10.1098/rsos.240255</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.</p>
</div>
</details>
<div id="fig-macmillian" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-macmillian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Macmillian_24_img2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure&nbsp;1: Figures from @macmillan-scottIrrationalityCognitiveBiases2024"><img src="images/Macmillian_24_img2.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Macmillian_24_img1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure&nbsp;1: Figures from @macmillan-scottIrrationalityCognitiveBiases2024"><img src="images/Macmillian_24_img1.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-macmillian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Figures from <span class="citation" data-cites="macmillan-scottIrrationalityCognitiveBiases2024">Macmillan-Scott &amp; Musolesi (<a href="#ref-macmillan-scottIrrationalityCognitiveBiases2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="human-like-intuitive-behavior-and-reasoning-biases-emerged-in-large-language-models-but-disappeared-in-chatgpt" class="level2">
<h2 class="anchored" data-anchor-id="human-like-intuitive-behavior-and-reasoning-biases-emerged-in-large-language-models-but-disappeared-in-chatgpt">Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT</h2>
<p>Hagendorff, T., Fabi, S., &amp; Kosinski, M. (2023). <strong>Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in ChatGPT.</strong> Nature Computational Science, 3(10), 833–838. https://doi.org/10.1038/s43588-023-00527-x</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>We design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI’s generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input–output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics.</p>
</div>
</details>
<div id="fig-macmillian" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-macmillian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><a href="images/Hagendorff_23_img1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4" title="Figure&nbsp;2: Figures from @hagendorffHumanlikeIntuitiveBehavior2023"><img src="images/Hagendorff_23_img1.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><a href="images/Hagendorff_23_img2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;2: Figures from @hagendorffHumanlikeIntuitiveBehavior2023"><img src="images/Hagendorff_23_img2.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 33.3%;justify-content: center;">
<p><a href="images/Hagendorff_23_img3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure&nbsp;2: Figures from @hagendorffHumanlikeIntuitiveBehavior2023"><img src="images/Hagendorff_23_img3.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-macmillian-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Figures from <span class="citation" data-cites="hagendorffHumanlikeIntuitiveBehavior2023">Hagendorff et al. (<a href="#ref-hagendorffHumanlikeIntuitiveBehavior2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="using-cognitive-psychology-to-understand-gpt-3." class="level2">
<h2 class="anchored" data-anchor-id="using-cognitive-psychology-to-understand-gpt-3.">Using cognitive psychology to understand GPT-3.</h2>
<p>Binz, M., &amp; Schulz, E. (2023). <strong>Using cognitive psychology to understand GPT-3.</strong> Proceedings of the National Academy of Sciences, 120(6), e2218523120. https://doi.org/10.1073/pnas.2218523120</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3’s decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3’s behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Binz_23_img1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure from @binzUsingCognitivePsychology2023"><img src="images/Binz_23_img1.png" class="img-fluid figure-img" alt="Figure from Binz &amp; Schulz (2023)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="binzUsingCognitivePsychology2023">Binz &amp; Schulz (<a href="#ref-binzUsingCognitivePsychology2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="studying-and-improving-reasoning-in-humans-and-machines." class="level2">
<h2 class="anchored" data-anchor-id="studying-and-improving-reasoning-in-humans-and-machines.">Studying and improving reasoning in humans and machines.</h2>
<p>Yax, N., Anlló, H., &amp; Palminteri, S. (2024). <strong>Studying and improving reasoning in humans and machines.</strong> Communications Psychology, 2(1), 1–16. https://doi.org/10.1038/s44271-024-00091-8</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>In the present study, we investigate and compare reasoning in large language models (LLMs) and humans, using a selection of cognitive psychology tools traditionally dedicated to the study of (bounded) rationality. We presented to human participants and an array of pretrained LLMs new variants of classical cognitive experiments, and cross-compared their performances. Our results showed that most of the included models presented reasoning errors akin to those frequently ascribed to error-prone, heuristic-based human reasoning. Notwithstanding this superficial similarity, an in-depth comparison between humans and LLMs indicated important differences with human-like reasoning, with models’ limitations disappearing almost entirely in more recent LLMs’ releases. Moreover, we show that while it is possible to devise strategies to induce better performance, humans and machines are not equally responsive to the same prompting schemes. We conclude by discussing the epistemological implications and challenges of comparing human and machine behavior for both artificial intelligence and cognitive psychology.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Yax_24_img1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure from @yaxStudyingImprovingReasoning2024"><img src="images/Yax_24_img1.png" class="img-fluid figure-img" alt="Figure from Yax et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="yaxStudyingImprovingReasoning2024">Yax et al. (<a href="#ref-yaxStudyingImprovingReasoning2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="exploring-variability-in-risk-taking-with-large-language-models." class="level2">
<h2 class="anchored" data-anchor-id="exploring-variability-in-risk-taking-with-large-language-models.">Exploring variability in risk taking with large language models.</h2>
<p>Bhatia, S. (2024). <strong>Exploring variability in risk taking with large language models.</strong> Journal of Experimental Psychology: General, 153(7), 1838–1860. https://doi.org/10.1037/xge0001607</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>What are the sources of individual-level differences in risk taking, and how do they depend on the domain or situation in which the decision is being made? Psychologists currently answer such questions with psychometric methods, which analyze correlations across participant responses in survey data sets. In this article, we analyze the preferences that give rise to these correlations. Our approach uses (a) large language models (LLMs) to quantify everyday risky behaviors in terms of the attributes or reasons that may describe those behaviors, and (b) decision models to map these attributes and reasons onto participant responses. We show that LLM-based decision models can explain observed correlations between behaviors in terms of the reasons different behaviors elicit and explain observed correlations between individuals in terms of the weights different individuals place on reasons, thereby providing a decision theoretic foundation for psychometric findings. Since LLMs can generate quantitative representations for nearly any naturalistic decision, they can be used to make accurate out-of-sample predictions for hundreds of everyday behaviors, predict the reasons why people may or may not want to engage in these behaviors, and interpret these reasons in terms of core psychological constructs. Our approach has important theoretical and practical implications for the study of heterogeneity in everyday behavior.</p>
</div>
</details>
<p><span class="citation" data-cites="bhatiaExploringVariabilityRisk2024">Bhatia (<a href="#ref-bhatiaExploringVariabilityRisk2024" role="doc-biblioref">2024</a>)</span></p>
</section>
<section id="human-bias-in-ai-models-anchoring-effects-and-mitigation-strategies-in-large-language-models" class="level2">
<h2 class="anchored" data-anchor-id="human-bias-in-ai-models-anchoring-effects-and-mitigation-strategies-in-large-language-models">Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models</h2>
<p>Nguyen, J. (2024). <strong>Human Bias in AI Models? Anchoring Effects and Mitigation Strategies in Large Language Models.</strong> Journal of Behavioral and Experimental Finance, 100971. https://doi.org/10.1016/j.jbef.2024.100971</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>This study builds on the seminal work of Tversky and Kahneman (1974), exploring the presence and extent of anchoring bias in forecasts generated by four Large Language Models (LLMs): GPT-4, Claude 2, Gemini Pro and GPT-3.5. In contrast to recent findings of advanced reasoning capabilities in LLMs, our randomised controlled trials reveal the presence of anchoring bias across all models: forecasts are significantly influenced by prior mention of high or low values. We examine two mitigation prompting strategies, ‘Chain of Thought’ and ‘ignore previous’, finding limited and varying degrees of effectiveness. Our results extend the anchoring bias research in finance beyond human decision-making to encompass LLMs, highlighting the importance of deliberate and informed prompting in AI forecasting in both ad hoc LLM use and in crafting few-shot examples.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Nguyen_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure from @nguyenHumanBiasAI2024"><img src="images/Nguyen_24_img.png" class="img-fluid figure-img" alt="Figure from Nguyen (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="nguyenHumanBiasAI2024">Nguyen (<a href="#ref-nguyenHumanBiasAI2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="a-turing-test-of-whether-ai-chatbots-are-behaviorally-similar-to-humans" class="level2">
<h2 class="anchored" data-anchor-id="a-turing-test-of-whether-ai-chatbots-are-behaviorally-similar-to-humans">A Turing test of whether AI chatbots are behaviorally similar to humans</h2>
<p>Mei, Q., Xie, Y., Yuan, W., &amp; Jackson, M. O. (2024). <strong>A Turing test of whether AI chatbots are behaviorally similar to humans.</strong> Proceedings of the National Academy of Sciences, 121(9), e2313925121. https://doi.org/10.1073/pnas.2313925121</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>We administer a Turing test to AI chatbots. We examine how chatbots behave in a suite of classic behavioral games that are designed to elicit characteristics such as trust, fairness, risk-aversion, cooperation, etc., as well as how they respond to a traditional Big-5 psychological survey that measures personality traits. ChatGPT-4 exhibits behavioral and personality traits that are statistically indistinguishable from a random human from tens of thousands of human subjects from more than 50 countries. Chatbots also modify their behavior based on previous experience and contexts “as if” they were learning from the interactions and change their behavior in response to different framings of the same strategic situation. Their behaviors are often distinct from average and modal human behaviors, in which case they tend to behave on the more altruistic and cooperative end of the distribution. We estimate that they act as if they are maximizing an average of their own and partner’s payoffs.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Mei_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure from @meiTuringTestWhether2024"><img src="images/Mei_24_img.png" class="img-fluid figure-img" alt="Figure from Mei et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="meiTuringTestWhether2024">Mei et al. (<a href="#ref-meiTuringTestWhether2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="deciding-fast-and-slow-the-role-of-cognitive-biases-in-ai-assisted-decision-making" class="level2">
<h2 class="anchored" data-anchor-id="deciding-fast-and-slow-the-role-of-cognitive-biases-in-ai-assisted-decision-making">Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making</h2>
<p>Rastogi, C., Zhang, Y., Wei, D., Varshney, K. R., Dhurandhar, A., &amp; Tomsett, R. (2022). <strong>Deciding Fast and Slow: The Role of Cognitive Biases in AI-assisted Decision-making.</strong> Proceedings of the ACM on Human-Computer Interaction, 6(CSCW1), 1–22. https://doi.org/10.1145/3512930</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Several strands of research have aimed to bridge the gap between artificial intelligence (AI) and human decision-makers in AI-assisted decision-making, where humans are the consumers of AI model predictions and the ultimate decision-makers in high-stakes applications. However, people’s perception and understanding are often distorted by their cognitive biases, such as confirmation bias, anchoring bias, availability bias, to name a few. In this work, we use knowledge from the field of cognitive science to account for cognitive biases in the human-AI collaborative decision-making setting, and mitigate their negative effects on collaborative performance. To this end, we mathematically model cognitive biases and provide a general framework through which researchers and practitioners can understand the interplay between cognitive biases and human-AI accuracy. We then focus specifically on anchoring bias, a bias commonly encountered in human-AI collaboration. We implement a time-based de-anchoring strategy and conduct our first user experiment that validates its effectiveness in human-AI collaborative decision-making. With this result, we design a time allocation strategy for a resource-constrained setting that achieves optimal human-AI collaboration under some assumptions. We, then, conduct a second user experiment which shows that our time allocation strategy with explanation can effectively de-anchor the human and improve collaborative performance when the AI model has low confidence and is incorrect.</p>
</div>
</details>
<div id="fig-rastogi" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rastogi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Rastogi_24_img1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;3: Figures from @rastogiDecidingFastSlow2022"><img src="images/Rastogi_24_img1.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Rastogi_24_img2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;3: Figures from @rastogiDecidingFastSlow2022"><img src="images/Rastogi_24_img2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rastogi-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Figures from <span class="citation" data-cites="rastogiDecidingFastSlow2022">Rastogi et al. (<a href="#ref-rastogiDecidingFastSlow2022" role="doc-biblioref">2022</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="decision-control-and-explanations-in-human-ai-collaboration-improving-user-perceptions-and-compliance" class="level2">
<h2 class="anchored" data-anchor-id="decision-control-and-explanations-in-human-ai-collaboration-improving-user-perceptions-and-compliance">Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance</h2>
<p>Westphal, M., Vössing, M., Satzger, G., Yom-Tov, G. B., &amp; Rafaeli, A. (2023). <strong>Decision control and explanations in human-AI collaboration: Improving user perceptions and compliance.</strong> Computers in Human Behavior, 144, 107714. https://doi.org/10.1016/j.chb.2023.107714</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Human-AI collaboration has become common, integrating highly complex AI systems into the workplace. Still, it is often ineffective; impaired perceptions – such as low trust or limited understanding – reduce compliance with recommendations provided by the AI system. Drawing from cognitive load theory, we examine two techniques of human-AI collaboration as potential remedies. In three experimental studies, we grant users decision control by empowering them to adjust the system’s recommendations, and we offer explanations for the system’s reasoning. We find decision control positively affects user perceptions of trust and understanding, and improves user compliance with system recommendations. Next, we isolate different effects of providing explanations that may help explain inconsistent findings in recent literature: while explanations help reenact the system’s reasoning, they also increase task complexity. Further, the effectiveness of providing an explanation depends on the specific user’s cognitive ability to handle complex tasks. In summary, our study shows that users benefit from enhanced decision control, while explanations – unless appropriately designed for the specific user – may even harm user perceptions and compliance. This work bears both theoretical and practical implications for the management of human-AI collaboration.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Westphal_23_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure from @westphalDecisionControlExplanations2023"><img src="images/Westphal_23_img.png" class="img-fluid figure-img" alt="Figure from Westphal et al. (2023)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="westphalDecisionControlExplanations2023">Westphal et al. (<a href="#ref-westphalDecisionControlExplanations2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="risk-and-prosocial-behavioural-cues-elicit-human-like-response-patterns-from-ai-chatbots." class="level2">
<h2 class="anchored" data-anchor-id="risk-and-prosocial-behavioural-cues-elicit-human-like-response-patterns-from-ai-chatbots.">Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots.</h2>
<p>Zhao, Y., Huang, Z., Seligman, M., &amp; Peng, K. (2024). <strong>Risk and prosocial behavioural cues elicit human-like response patterns from AI chatbots.</strong> Scientific Reports, 14(1), 7095. https://doi.org/10.1038/s41598-024-55949-y</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Emotions, long deemed a distinctly human characteristic, guide a repertoire of behaviors, e.g., promoting risk-aversion under negative emotional states or generosity under positive ones. The question of whether Artificial Intelligence (AI) can possess emotions remains elusive, chiefly due to the absence of an operationalized consensus on what constitutes ‘emotion’ within AI. Adopting a pragmatic approach, this study investigated the response patterns of AI chatbots—specifically, large language models (LLMs)—to various emotional primes. We engaged AI chatbots as one would human participants, presenting scenarios designed to elicit positive, negative, or neutral emotional states. Multiple accounts of OpenAI’s ChatGPT Plus were then tasked with responding to inquiries concerning investment decisions and prosocial behaviors. Our analysis revealed that ChatGPT-4 bots, when primed with positive, negative, or neutral emotions, exhibited distinct response patterns in both risk-taking and prosocial decisions, a phenomenon less evident in the ChatGPT-3.5 iterations. This observation suggests an enhanced capacity for modulating responses based on emotional cues in more advanced LLMs. While these findings do not suggest the presence of emotions in AI, they underline the feasibility of swaying AI responses by leveraging emotional indicators.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Zhao_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="@zhaoRiskProsocialBehavioural2024"><img src="images/Zhao_24_img.png" class="img-fluid figure-img" alt="Zhao et al. (2024)"></a></p>
<figcaption><span class="citation" data-cites="zhaoRiskProsocialBehavioural2024">Zhao et al. (<a href="#ref-zhaoRiskProsocialBehavioural2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="do-large-language-models-show-decision-heuristics-similar-to-humans-a-case-study-using-gpt-3.5" class="level2">
<h2 class="anchored" data-anchor-id="do-large-language-models-show-decision-heuristics-similar-to-humans-a-case-study-using-gpt-3.5">Do large language models show decision heuristics similar to humans? A case study using GPT-3.5</h2>
<p>Suri, G., Slater, L. R., Ziaee, A., &amp; Nguyen, M. (2024). <strong>Do large language models show decision heuristics similar to humans? A case study using GPT-3.5.</strong> Journal of Experimental Psychology: General, 153(4), 1066–1075. https://doi.org/10.1037/xge0001547</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>A Large Language Model (LLM) is an artificial intelligence system trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. Generative Pre-Trained Transformer (GPT)-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics and other context-sensitive responses. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (anchoring, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was influenced by anecdotal information (representativeness and availability heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively—even though both presentations contained statistically equivalent information (framing effect, Study 3); and it valued an owned item more than a newly found item even though the two items were objectively identical (endowment effect, Study 4). In each study, human participants showed similar effects. Heuristics and context-sensitive responses in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM—which lacks these processes—also shows such responses invites consideration of the possibility that language is sufficiently rich to carry these effects and may play a role in generating these effects in humans.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Suri_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure from @suriLargeLanguageModels2024"><img src="images/Suri_24_img.png" class="img-fluid figure-img" alt="Figure from Suri et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="suriLargeLanguageModels2024">Suri et al. (<a href="#ref-suriLargeLanguageModels2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="can-large-language-models-capture-human-preferences" class="level2">
<h2 class="anchored" data-anchor-id="can-large-language-models-capture-human-preferences">Can Large Language Models Capture Human Preferences?</h2>
<p>Goli, A., &amp; Singh, A. (2024). <strong>Can Large Language Models Capture Human Preferences?</strong> Marketing Science. https://doi.org/10.1287/mksc.2023.0306</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>We explore the viability of large language models (LLMs), specifically OpenAI’s GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting preferences, with a focus on intertemporal choices. Leveraging the extensive literature on intertemporal discounting for benchmarking, we examine responses from LLMs across various languages and compare them with human responses, exploring preferences between smaller, sooner and larger, later rewards. Our findings reveal that both generative pretrained transformer (GPT) models demonstrate less patience than humans, with GPT-3.5 exhibiting a lexicographic preference for earlier rewards unlike human decision makers. Although GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans. Interestingly, GPT models show greater patience in languages with weak future tense references, such as German and Mandarin, aligning with the existing literature that suggests a correlation between language structure and intertemporal preferences. We demonstrate how prompting GPT to explain its decisions, a procedure we term “chain-of-thought conjoint,” can mitigate, but does not eliminate, discrepancies between LLM and human responses. Although directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings of preferences. Chain-of-thought conjoint provides a structured framework for marketers to use LLMs to identify potential attributes or factors that can explain preference heterogeneity across different customers and contexts.</p>
</div>
</details>
<div id="fig-goli" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-goli-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Goli_24_img1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;4: Figures from @goliCanLargeLanguage2024"><img src="images/Goli_24_img1.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Goli_24_img2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;4: Figures from @goliCanLargeLanguage2024"><img src="images/Goli_24_img2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-goli-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Figures from <span class="citation" data-cites="goliCanLargeLanguage2024">Goli &amp; Singh (<a href="#ref-goliCanLargeLanguage2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="language-models-like-humans-show-content-effects-on-reasoning-tasks" class="level2">
<h2 class="anchored" data-anchor-id="language-models-like-humans-show-content-effects-on-reasoning-tasks">Language models, like humans, show content effects on reasoning tasks</h2>
<p>Lampinen, A. K., Dasgupta, I., Chan, S. C. Y., Sheahan, H. R., Creswell, A., Kumaran, D., McClelland, J. L., &amp; Hill, F. (2024). <strong>Language models, like humans, show content effects on reasoning tasks.</strong> PNAS Nexus, 3(7), pgae233. https://doi.org/10.1093/pnasnexus/pgae233</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks but exhibit many imperfections. However, human abstract reasoning is also imperfect. Human reasoning is affected by our real-world knowledge and beliefs, and shows notable “content effects”; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns are central to debates about the fundamental nature of human intelligence. Here, we investigate whether language models—whose prior expectations capture some aspects of human knowledge—similarly mix content into their answers to logic problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art LMs, as well as humans, and find that the LMs reflect many of the same qualitative human patterns on these tasks—like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected in accuracy patterns, and in some lower-level features like the relationship between LM confidence over possible answers and human response times. However, in some cases the humans and models behave differently—particularly on the Wason task, where humans perform much worse than large models, and exhibit a distinct error pattern. Our findings have implications for understanding possible contributors to these human cognitive effects, as well as the factors that influence language model performance.</p>
</div>
</details>
<p><span class="citation" data-cites="lampinenLanguageModelsHumans2024">Lampinen et al. (<a href="#ref-lampinenLanguageModelsHumans2024" role="doc-biblioref">2024</a>)</span></p>
</section>
<section id="the-emergence-of-economic-rationality-of-gpt" class="level2">
<h2 class="anchored" data-anchor-id="the-emergence-of-economic-rationality-of-gpt">The emergence of economic rationality of GPT</h2>
<p>Chen, Y., Liu, T. X., Shan, Y., &amp; Zhong, S. (2023). <strong>The emergence of economic rationality of GPT.</strong> Proceedings of the National Academy of Sciences, 120(51), e2316205120. https://doi.org/10.1073/pnas.2316205120</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>As large language models (LLMs) like GPT become increasingly prevalent, it is essential that we assess their capabilities beyond language processing. This paper examines the economic rationality of GPT by instructing it to make budgetary decisions in four domains: risk, time, social, and food preferences. We measure economic rationality by assessing the consistency of GPT’s decisions with utility maximization in classic revealed preference theory. We find that GPT’s decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature. Moreover, the estimated preference parameters of GPT are slightly different from human subjects and exhibit a lower degree of heterogeneity. We also find that the rationality scores are robust to the degree of randomness and demographic settings such as age and gender but are sensitive to contexts based on the language frames of the choice situations. These results suggest the potential of LLMs to make good decisions and the need to further understand their capabilities, limitations, and underlying mechanisms.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Chen_23_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="Figure from @chenEmergenceEconomicRationality2023"><img src="images/Chen_23_img.png" class="img-fluid figure-img" alt="Figure from Chen et al. (2023)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="chenEmergenceEconomicRationality2023">Chen et al. (<a href="#ref-chenEmergenceEconomicRationality2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="the-potential-of-generative-ai-for-personalized-persuasion-at-scale." class="level2">
<h2 class="anchored" data-anchor-id="the-potential-of-generative-ai-for-personalized-persuasion-at-scale.">The potential of generative AI for personalized persuasion at scale.</h2>
<p>Matz, S. C., Teeny, J. D., Vaid, S. S., Peters, H., Harari, G. M., &amp; Cerf, M. (2024). <strong>The potential of generative AI for personalized persuasion at scale.</strong> Scientific Reports, 14(1), 4692. https://doi.org/10.1038/s41598-024-53755-0</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Matching the language or content of a message to the psychological profile of its recipient (known as “personalized persuasion”) is widely considered to be one of the most effective messaging strategies. We demonstrate that the rapid advances in large language models (LLMs), like ChatGPT, could accelerate this influence by making personalized persuasion scalable. Across four studies (consisting of seven sub-studies; total N = 1788), we show that personalized messages crafted by ChatGPT exhibit significantly more influence than non-personalized messages. This was true across different domains of persuasion (e.g., marketing of consumer products, political appeals for climate action), psychological profiles (e.g., personality traits, political ideology, moral foundations), and when only providing the LLM with a single, short prompt naming or describing the targeted psychological dimension. Thus, our findings are among the first to demonstrate the potential for LLMs to automate, and thereby scale, the use of personalized persuasion in ways that enhance its effectiveness and efficiency. We discuss the implications for researchers, practitioners, and the general public.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Matz_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="Figure from @matzPotentialGenerativeAI2024"><img src="images/Matz_24_img.png" class="img-fluid figure-img" alt="Figure from Matz et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="matzPotentialGenerativeAI2024">Matz et al. (<a href="#ref-matzPotentialGenerativeAI2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="decision-making-paradoxes-in-humans-vs-machines-the-case-of-the-allais-and-ellsberg-paradoxes." class="level2">
<h2 class="anchored" data-anchor-id="decision-making-paradoxes-in-humans-vs-machines-the-case-of-the-allais-and-ellsberg-paradoxes.">Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes.</h2>
<p>Nobandegani, A. S., Rish, I., &amp; Shultz, T. R. (2023). <strong>Decision-Making Paradoxes in Humans vs Machines: The case of the Allais and Ellsberg Paradoxes.</strong> Proceedings of the Annual Meeting of the Cognitive Science Society, 46. https://arxiv.org/abs/2406.11426</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Human decision-making is filled with a variety of paradoxes demonstrating deviations from rationality principles. Do state-of-the-art artificial intelligence (AI) models also manifest these paradoxes when making decisions? As a case study, in this work we investigate whether GPT-4, a recently released state-of-the-art language model, would show two well-known paradoxes in human decision-making: the Allais paradox and the Ellsberg paradox. We demonstrate that GPT-4 succeeds in the two variants of the Allais paradox (the common-consequence effect and the common-ratio effect) but fails in the case of the Ellsberg paradox. We also show that providing GPT-4 with high-level normative principles allows it to succeed in the Ellsberg paradox, thus elevating GPT-4’s decision-making rationality. We discuss the implications of our work for AI rationality enhancement and AI-assisted decision-making.</p>
</div>
</details>
<p><span class="citation" data-cites="nobandeganiDecisionMakingParadoxesHumans2023">Nobandegani et al. (<a href="#ref-nobandeganiDecisionMakingParadoxesHumans2023" role="doc-biblioref">2023</a>)</span></p>
</section>
<section id="do-llms-exhibit-human-like-response-biases-a-case-study-in-survey-design." class="level2">
<h2 class="anchored" data-anchor-id="do-llms-exhibit-human-like-response-biases-a-case-study-in-survey-design.">Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design.</h2>
<p>Tjuatja, L., Chen, V., Wu, T., Talwalkwar, A., &amp; Neubig, G. (2024). <strong>Do LLMs Exhibit Human-like Response Biases? A Case Study in Survey Design.</strong> Transactions of the Association for Computational Linguistics, 12, 1011–1026. https://doi.org/10.1162/tacl_a_00685</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>One widely cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording—but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of “prompts” have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Tjuatja_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20" title="Figure from @tjuatjaLLMsExhibitHumanlike2024"><img src="images/Tjuatja_24_img.png" class="img-fluid figure-img" alt="Figure from Tjuatja et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="tjuatjaLLMsExhibitHumanlike2024">Tjuatja et al. (<a href="#ref-tjuatjaLLMsExhibitHumanlike2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="cognitive-ease-at-a-cost-llms-reduce-mental-effort-but-compromise-depth-in-student-scientific-inquiry" class="level2">
<h2 class="anchored" data-anchor-id="cognitive-ease-at-a-cost-llms-reduce-mental-effort-but-compromise-depth-in-student-scientific-inquiry">Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry</h2>
<p>Stadler, M., Bannert, M., &amp; Sailer, M. (2024). <strong>Cognitive ease at a cost: LLMs reduce mental effort but compromise depth in student scientific inquiry.</strong> Computers in Human Behavior, 160, 108386. https://doi.org/10.1016/j.chb.2024.108386</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>This study explores the cognitive load and learning outcomes associated with using large language models (LLMs) versus traditional search engines for information gathering during learning. A total of 91 university students were randomly assigned to either use ChatGPT3.5 or Google to research the socio-scientific issue of nanoparticles in sunscreen to derive valid recommendations and justifications. The study aimed to investigate potential differences in cognitive load, as well as the quality and homogeneity of the students’ recommendations and justifications. Results indicated that students using LLMs experienced significantly lower cognitive load. However, despite this reduction, these students demonstrated lower-quality reasoning and argumentation in their final recommendations compared to those who used traditional search engines. Further, the homogeneity of the recommendations and justifications did not differ significantly between the two groups, suggesting that LLMs did not restrict the diversity of students’ perspectives. These findings highlight the nuanced implications of digital tools on learning, suggesting that while LLMs can decrease the cognitive burden associated with information gathering during a learning task, they may not promote deeper engagement with content necessary for high-quality learning per se.</p>
</div>
</details>
<p><span class="citation" data-cites="stadlerCognitiveEaseCost2024">Stadler et al. (<a href="#ref-stadlerCognitiveEaseCost2024" role="doc-biblioref">2024</a>)</span></p>
</section>
<section id="cognitive-llms-towards-integrating-cognitive-architectures-and-large-language-models-for-manufacturing-decision-making" class="level2">
<h2 class="anchored" data-anchor-id="cognitive-llms-towards-integrating-cognitive-architectures-and-large-language-models-for-manufacturing-decision-making">Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making</h2>
<p>Wu, S., Oltramari, A., Francis, J., Giles, C. L., &amp; Ritter, F. E. (2024). <strong>Cognitive LLMs: Towards Integrating Cognitive Architectures and Large Language Models for Manufacturing Decision-making</strong> (arXiv:2408.09176). arXiv. http://arxiv.org/abs/2408.09176</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Resolving the dichotomy between the human-like yet constrained reasoning processes of Cognitive Architectures and the broad but often noisy inference behavior of Large Language Models (LLMs) remains a challenging but exciting pursuit, for enabling reliable machine reasoning capabilities in production systems. Because Cognitive Architectures are famously developed for the purpose of modeling the internal mechanisms of human cognitive decision-making at a computational level, new investigations consider the goal of informing LLMs with the knowledge necessary for replicating such processes, e.g., guided perception, memory, goal-setting, and action. Previous approaches that use LLMs for grounded decision-making struggle with complex reasoning tasks that require slower, deliberate cognition over fast and intuitive inference—reporting issues related to the lack of sufficient grounding, as in hallucination. To resolve these challenges, we introduce LLM-ACTR, a novel neurosymbolic architecture that provides human-aligned and versatile decision-making by integrating the ACT-R Cognitive Architecture with LLMs. Our framework extracts and embeds knowledge of ACT-R’s internal decision-making process as latent neural representations, injects this information into trainable LLM adapter layers, and fine-tunes the LLMs for downstream prediction. Our experiments on novel Design for Manufacturing tasks show both improved task performance as well as improved grounded decision-making capability of our approach, compared to LLM-only baselines that leverage chain-of-thought reasoning strategies.</p>
</div>
</details>
</section>
<section id="large-language-models-amplify-human-biases-in-moral-decision-making" class="level2">
<h2 class="anchored" data-anchor-id="large-language-models-amplify-human-biases-in-moral-decision-making">Large Language Models Amplify Human Biases in Moral Decision-Making</h2>
<p>Cheung, V., Maier, M., &amp; Lieder, F. (2024). <strong>Large Language Models Amplify Human Biases in Moral Decision-Making</strong> (https://osf.io/3kvjd/). https://doi.org/10.31234/osf.io/aj46b</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>As large language models (LLMs) become more widely used, people increasingly rely on them to make or advise on moral decisions. Some researchers even propose using LLMs as participants in psychology experiments. It is therefore important to understand how well LLMs make moral decisions and how they compare to humans. We investigated this question in realistic moral dilemmas using prompts where GPT-4, Llama 3, and Claude 3 give advice and where they emulate a research participant. In Study 1, we compared responses from LLMs to a representative US sample (N = 285) for 22 dilemmas: social dilemmas that pitted self-interest against the greater good, and moral dilemmas that pitted utilitarian cost-benefit reasoning against deontological rules. In social dilemmas, LLMs were more altruistic than participants. In moral dilemmas, LLMs exhibited stronger omission bias than participants: they usually endorsed inaction over action. In Study 2 (N = 490, preregistered), we replicated this omission bias and document an additional bias: unlike humans, LLMs (except GPT-4o) tended to answer “no” in moral dilemmas, whereby the phrasing of the question influences the decision even when physical action remains the same. Our findings show that LLM moral decision-making amplifies human biases and introduces potentially problematic biases.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Cheung_24_img.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21" title="Figure from @cheungLargeLanguageModels2024"><img src="images/Cheung_24_img.png" class="img-fluid figure-img" alt="Figure from Cheung et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="cheungLargeLanguageModels2024">Cheung et al. (<a href="#ref-cheungLargeLanguageModels2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="large-language-model-recall-uncertainty-is-modulated-by-the-fan-effect." class="level2">
<h2 class="anchored" data-anchor-id="large-language-model-recall-uncertainty-is-modulated-by-the-fan-effect.">Large Language Model Recall Uncertainty is Modulated by the Fan Effect.</h2>
<p>Roberts, J., Moore, K., Pham, T., Ewaleifoh, O., &amp; Fisher, D. (2024). <strong>Large Language Model Recall Uncertainty is Modulated by the Fan Effect.</strong></p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Roberts1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-22" title="Figure from @robertsLargeLanguageModel2024"><img src="images/Roberts1.png" class="img-fluid figure-img" alt="Figure from Roberts et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="robertsLargeLanguageModel2024">Roberts et al. (<a href="#ref-robertsLargeLanguageModel2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="accuracy-time-tradeoffs-in-ai-assisted-decision-making-under-time-pressure." class="level2">
<h2 class="anchored" data-anchor-id="accuracy-time-tradeoffs-in-ai-assisted-decision-making-under-time-pressure.">Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure.</h2>
<p>Swaroop, S., Buçinca, Z., Gajos, K. Z., &amp; Doshi-Velez, F. (2024). <strong>Accuracy-Time Tradeoffs in AI-Assisted Decision Making under Time Pressure.</strong> Proceedings of the 29th International Conference on Intelligent User Interfaces, 138–154. https://doi.org/10.1145/3640543.3645206</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>In settings where users both need high accuracy and are timepressured, such as doctors working in emergency rooms, we want to provide AI assistance that both increases decision accuracy and reduces decision-making time. Current literature focusses on how users interact with AI assistance when there is no time pressure, finding that different AI assistances have different benefits: some can reduce time taken while increasing overreliance on AI, while others do the opposite. The precise benefit can depend on both the user and task. In time-pressured scenarios, adapting when we show AI assistance is especially important: relying on the AI assistance can save time, and can therefore be beneficial when the AI is likely to be right. We would ideally adapt what AI assistance we show depending on various properties (of the task and of the user) in order to best trade off accuracy and time. We introduce a study where users have to answer a series of logic puzzles. We find that time pressure affects how users use different AI assistances, making some assistances more beneficial than others when compared to notime-pressure settings. We also find that a user’s overreliance rate is a key predictor of their behaviour: overreliers and not-overreliers use different AI assistance types differently. We find marginal correlations between a user’s overreliance rate (which is related to the user’s trust in AI recommendations) and their personality traits (Big Five Personality traits). Overall, our work suggests that AI assistances have different accuracy-time tradeoffs when people are under time pressure compared to no time pressure, and we explore how we might adapt AI assistances in this setting.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Swaroop1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-23" title="Figure from @swaroopAccuracyTimeTradeoffsAIAssisted2024"><img src="images/Swaroop1.png" class="img-fluid figure-img" alt="Figure from Swaroop et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="swaroopAccuracyTimeTradeoffsAIAssisted2024">Swaroop et al. (<a href="#ref-swaroopAccuracyTimeTradeoffsAIAssisted2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="the-llm-effect-are-humans-truly-using-llms-or-are-they-being-influenced-by-them-instead" class="level2">
<h2 class="anchored" data-anchor-id="the-llm-effect-are-humans-truly-using-llms-or-are-they-being-influenced-by-them-instead">The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?</h2>
<p>Choi, A. S., Akter, S. S., Singh, J. P., &amp; Anastasopoulos, A. (2024). <strong>The LLM Effect: Are Humans Truly Using LLMs, or Are They Being Influenced By Them Instead?</strong> (arXiv:2410.04699). arXiv. http://arxiv.org/abs/2410.04699</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Large Language Models (LLMs) have shown capabilities close to human performance in various analytical tasks, leading researchers to use them for time and labor-intensive analyses. However, their capability to handle highly specialized and open-ended tasks in domains like policy studies remains in question. This paper investigates the efficiency and accuracy of LLMs in specialized tasks through a structured user study focusing on Human-LLM partnership. The study, conducted in two stages-Topic Discovery and Topic Assignment-integrates LLMs with expert annotators to observe the impact of LLM suggestions on what is usually human-only analysis. Results indicate that LLM-generated topic lists have significant overlap with human generated topic lists, with minor hiccups in missing document-specific topics. However, LLM suggestions may significantly improve task completion speed, but at the same time introduce anchoring bias, potentially affecting the depth and nuance of the analysis, raising a critical question about the trade-off between increased efficiency and the risk of biased analysis.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/choi1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-24" title="Figure from @choiLLMEffectAre2024"><img src="images/choi1.png" class="img-fluid figure-img" alt="Figure from Choi et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="choiLLMEffectAre2024">Choi et al. (<a href="#ref-choiLLMEffectAre2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="mutual-theory-of-mind-in-human-ai-collaboration-an-empirical-study-with-llm-driven-ai-agents-in-a-real-time-shared-workspace-task" class="level2">
<h2 class="anchored" data-anchor-id="mutual-theory-of-mind-in-human-ai-collaboration-an-empirical-study-with-llm-driven-ai-agents-in-a-real-time-shared-workspace-task">Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task</h2>
<p>Zhang, S., Wang, X., Zhang, W., Chen, Y., Gao, L., Wang, D., Zhang, W., Wang, X., &amp; Wen, Y. (2024). <strong>Mutual Theory of Mind in Human-AI Collaboration: An Empirical Study with LLM-driven AI Agents in a Real-time Shared Workspace Task</strong> (arXiv:2409.08811). arXiv. http://arxiv.org/abs/2409.08811</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Theory of Mind (ToM) significantly impacts human collaboration and communication as a crucial capability to understand others. When AI agents with ToM capability collaborate with humans, Mutual Theory of Mind (MToM) arises in such human-AI teams (HATs). The MToM process, which involves interactive communication and ToM-based strategy adjustment, affects the team’s performance and collaboration process. To explore the MToM process, we conducted a mixed-design experiment using a large language model-driven AI agent with ToM and communication modules in a real-time shared-workspace task. We find that the agent’s ToM capability does not significantly impact team performance but enhances human understanding of the agent and the feeling of being understood. Most participants in our study believe verbal communication increases human burden, and the results show that bidirectional communication leads to lower HAT performance. We discuss the results’ implications for designing AI agents that collaborate with humans in real-time shared workspace tasks.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/zhang_tom1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-25" title="Figure from @zhangMutualTheoryMind2024"><img src="images/zhang_tom1.png" class="img-fluid figure-img" alt="Figure from Zhang et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="zhangMutualTheoryMind2024">Zhang et al. (<a href="#ref-zhangMutualTheoryMind2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="bridging-the-gulf-of-envisioning-cognitive-design-challenges-in-llm-interfaces" class="level2">
<h2 class="anchored" data-anchor-id="bridging-the-gulf-of-envisioning-cognitive-design-challenges-in-llm-interfaces">Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces</h2>
<p>Subramonyam, H., Pea, R., Pondoc, C. L., Agrawala, M., &amp; Seifert, C. (2024). <strong>Bridging the Gulf of Envisioning: Cognitive Design Challenges in LLM Interfaces</strong> (arXiv:2309.14459; Version 2). arXiv. http://arxiv.org/abs/2309.14459</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Large language models (LLMs) exhibit dynamic capabilities and appear to comprehend complex and ambiguous natural language prompts. However, calibrating LLM interactions is challenging for interface designers and end-users alike. A central issue is our limited grasp of how human cognitive processes begin with a goal and form intentions for executing actions, a blindspot even in established interaction models such as Norman’s gulfs of execution and evaluation. To address this gap, we theorize how end-users ‘envision’ translating their goals into clear intentions and craft prompts to obtain the desired LLM response. We define a process of Envisioning by highlighting three misalignments: (1) knowing whether LLMs can accomplish the task, (2) how to instruct the LLM to do the task, and (3) how to evaluate the success of the LLM’s output in meeting the goal. Finally, we make recommendations to narrow the envisioning gulf in human-LLM interactions.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Subramonyam1.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-26" title="Figure from @subramonyamBridgingGulfEnvisioning2024"><img src="images/Subramonyam1.jpg" class="img-fluid figure-img" alt="Figure from Subramonyam et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="subramonyamBridgingGulfEnvisioning2024">Subramonyam et al. (<a href="#ref-subramonyamBridgingGulfEnvisioning2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="learning-to-guide-human-decision-makers-with-vision-language-models" class="level2">
<h2 class="anchored" data-anchor-id="learning-to-guide-human-decision-makers-with-vision-language-models">Learning To Guide Human Decision Makers With Vision-Language Models</h2>
<p>Banerjee, D., Teso, S., Sayin, B., &amp; Passerini, A. (2024). <strong>Learning To Guide Human Decision Makers With Vision-Language Models</strong> (arXiv:2403.16501). arXiv. http://arxiv.org/abs/2403.16501</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>There is increasing interest in developing AIs for assisting human decision-making in high-stakes tasks, such as medical diagnosis, for the purpose of improving decision quality and reducing cognitive strain. Mainstream approaches team up an expert with a machine learning model to which safer decisions are offloaded, thus letting the former focus on cases that demand their attention. his separation of responsibilities setup, however, is inadequate for high-stakes scenarios. On the one hand, the expert may end up over-relying on the machine’s decisions due to anchoring bias, thus losing the human oversight that is increasingly being required by regulatory agencies to ensure trustworthy AI. On the other hand, the expert is left entirely unassisted on the (typically hardest) decisions on which the model abstained. As a remedy, we introduce learning to guide (LTG), an alternative framework in which - rather than taking control from the human expert - the machine provides guidance useful for decision making, and the human is entirely responsible for coming up with a decision. In order to ensure guidance is interpretable} and task-specific, we develop SLOG, an approach for turning any vision-language model into a capable generator of textual guidance by leveraging a modicum of human feedback. Our empirical evaluation highlights the promise of on a challenging, real-world medical diagnosis task.</p>
</div>
</details>
<div id="fig-banerjee" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-banerjee-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Banerjee1.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-27" title="Figure&nbsp;5: Figures from @banerjeeLearningGuideHuman2024"><img src="images/Banerjee1.jpg" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Banerjee2.jpg" class="lightbox" data-gallery="quarto-lightbox-gallery-28" title="Figure&nbsp;5: Figures from @banerjeeLearningGuideHuman2024"><img src="images/Banerjee2.jpg" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-banerjee-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Figures from <span class="citation" data-cites="banerjeeLearningGuideHuman2024">Banerjee et al. (<a href="#ref-banerjeeLearningGuideHuman2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
</section>
<section id="how-does-value-similarity-affect-human-reliance-in-ai-assisted-ethical-decision-making" class="level2">
<h2 class="anchored" data-anchor-id="how-does-value-similarity-affect-human-reliance-in-ai-assisted-ethical-decision-making">How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making?</h2>
<p>Narayanan, S., Yu, G., Ho, C.-J., &amp; Yin, M. (2023). How does Value Similarity affect Human Reliance in AI-Assisted Ethical Decision Making? Proceedings of the 2023 AAAI/ACM Conference on AI, Ethics, and Society, 49–57. https://doi.org/10.1145/3600211.3604709</p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>This paper explores the impact of value similarity between humans and AI on human reliance in the context of AI-assisted ethical decision-making. Using kidney allocation as a case study, we conducted a randomized human-subject experiment where workers were presented with ethical dilemmas in various conditions, including no AI recommendations, recommendations from a similar AI, and recommendations from a dissimilar AI. We found that recommendations provided by a dissimilar AI had a higher overall effect on human decisions than recommendations from a similar AI. However, when humans and AI disagreed, participants were more likely to change their decisions when provided with recommendations from a similar AI. The effect was not due to humans’ perceptions of the AI being similar, but rather due to the AI displaying similar ethical values through its recommendations. We also conduct a preliminary analysis on the relationship between value similarity and trust, and potential shifts in ethical preferences at the population-level.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Narayanan1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-29" title="Figure from @narayananHowDoesValue2023"><img src="images/Narayanan1.png" class="img-fluid figure-img" alt="Figure from Narayanan et al. (2023)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="narayananHowDoesValue2023">Narayanan et al. (<a href="#ref-narayananHowDoesValue2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>



</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-banerjeeLearningGuideHuman2024" class="csl-entry" role="listitem">
Banerjee, D., Teso, S., Sayin, B., &amp; Passerini, A. (2024). <em>Learning <span>To Guide Human Decision Makers With Vision-Language Models</span></em> (arXiv:2403.16501). arXiv. <a href="https://arxiv.org/abs/2403.16501">https://arxiv.org/abs/2403.16501</a>
</div>
<div id="ref-bhatiaExploringVariabilityRisk2024" class="csl-entry" role="listitem">
Bhatia, S. (2024). Exploring variability in risk taking with large language models. <em>Journal of Experimental Psychology: General</em>, <em>153</em>(7), 1838–1860. <a href="https://doi.org/10.1037/xge0001607">https://doi.org/10.1037/xge0001607</a>
</div>
<div id="ref-binzUsingCognitivePsychology2023" class="csl-entry" role="listitem">
Binz, M., &amp; Schulz, E. (2023). Using cognitive psychology to understand <span>GPT-3</span>. <em>Proceedings of the National Academy of Sciences</em>, <em>120</em>(6), e2218523120. <a href="https://doi.org/10.1073/pnas.2218523120">https://doi.org/10.1073/pnas.2218523120</a>
</div>
<div id="ref-bucincaTrustThinkCognitive2021" class="csl-entry" role="listitem">
Buçinca, Z., Malaya, M. B., &amp; Gajos, K. Z. (2021). To <span>Trust</span> or to <span>Think</span>: <span>Cognitive Forcing Functions Can Reduce Overreliance</span> on <span>AI</span> in <span class="nocase">AI-assisted Decision-making</span>. <em>Proceedings of the ACM on Human-Computer Interaction</em>, <em>5</em>(CSCW1), 1–21. <a href="https://doi.org/10.1145/3449287">https://doi.org/10.1145/3449287</a>
</div>
<div id="ref-chenEmergenceEconomicRationality2023" class="csl-entry" role="listitem">
Chen, Y., Liu, T. X., Shan, Y., &amp; Zhong, S. (2023). The emergence of economic rationality of <span>GPT</span>. <em>Proceedings of the National Academy of Sciences</em>, <em>120</em>(51), e2316205120. <a href="https://doi.org/10.1073/pnas.2316205120">https://doi.org/10.1073/pnas.2316205120</a>
</div>
<div id="ref-cheungLargeLanguageModels2024" class="csl-entry" role="listitem">
Cheung, V., Maier, M., &amp; Lieder, F. (2024). <em>Large <span>Language Models Amplify Human Biases</span> in <span>Moral Decision-Making</span></em>. <a href="https://doi.org/10.31234/osf.io/aj46b">https://doi.org/10.31234/osf.io/aj46b</a>
</div>
<div id="ref-choiLLMEffectAre2024" class="csl-entry" role="listitem">
Choi, A. S., Akter, S. S., Singh, J. P., &amp; Anastasopoulos, A. (2024). <em>The <span>LLM Effect</span>: <span>Are Humans Truly Using LLMs</span>, or <span>Are They Being Influenced By Them Instead</span>?</em> (arXiv:2410.04699). arXiv. <a href="https://arxiv.org/abs/2410.04699">https://arxiv.org/abs/2410.04699</a>
</div>
<div id="ref-goliCanLargeLanguage2024" class="csl-entry" role="listitem">
Goli, A., &amp; Singh, A. (2024). Can <span>Large Language Models Capture Human Preferences</span>? <em>Marketing Science</em>. <a href="https://doi.org/10.1287/mksc.2023.0306">https://doi.org/10.1287/mksc.2023.0306</a>
</div>
<div id="ref-hagendorffHumanlikeIntuitiveBehavior2023" class="csl-entry" role="listitem">
Hagendorff, T., Fabi, S., &amp; Kosinski, M. (2023). Human-like intuitive behavior and reasoning biases emerged in large language models but disappeared in <span>ChatGPT</span>. <em>Nature Computational Science</em>, <em>3</em>(10), 833–838. <a href="https://doi.org/10.1038/s43588-023-00527-x">https://doi.org/10.1038/s43588-023-00527-x</a>
</div>
<div id="ref-lampinenLanguageModelsHumans2024" class="csl-entry" role="listitem">
Lampinen, A. K., Dasgupta, I., Chan, S. C. Y., Sheahan, H. R., Creswell, A., Kumaran, D., McClelland, J. L., &amp; Hill, F. (2024). Language models, like humans, show content effects on reasoning tasks. <em>PNAS Nexus</em>, <em>3</em>(7), pgae233. <a href="https://doi.org/10.1093/pnasnexus/pgae233">https://doi.org/10.1093/pnasnexus/pgae233</a>
</div>
<div id="ref-macmillan-scottIrrationalityCognitiveBiases2024" class="csl-entry" role="listitem">
Macmillan-Scott, O., &amp; Musolesi, M. (2024). (<span>Ir</span>)rationality and cognitive biases in large language models. <em>Royal Society Open Science</em>, <em>11</em>(6), 240255. <a href="https://doi.org/10.1098/rsos.240255">https://doi.org/10.1098/rsos.240255</a>
</div>
<div id="ref-matzPotentialGenerativeAI2024" class="csl-entry" role="listitem">
Matz, S. C., Teeny, J. D., Vaid, S. S., Peters, H., Harari, G. M., &amp; Cerf, M. (2024). The potential of generative <span>AI</span> for personalized persuasion at scale. <em>Scientific Reports</em>, <em>14</em>(1), 4692. <a href="https://doi.org/10.1038/s41598-024-53755-0">https://doi.org/10.1038/s41598-024-53755-0</a>
</div>
<div id="ref-meiTuringTestWhether2024" class="csl-entry" role="listitem">
Mei, Q., Xie, Y., Yuan, W., &amp; Jackson, M. O. (2024). A <span>Turing</span> test of whether <span>AI</span> chatbots are behaviorally similar to humans. <em>Proceedings of the National Academy of Sciences</em>, <em>121</em>(9), e2313925121. <a href="https://doi.org/10.1073/pnas.2313925121">https://doi.org/10.1073/pnas.2313925121</a>
</div>
<div id="ref-narayananHowDoesValue2023" class="csl-entry" role="listitem">
Narayanan, S., Yu, G., Ho, C.-J., &amp; Yin, M. (2023). How does <span>Value Similarity</span> affect <span>Human Reliance</span> in <span>AI-Assisted Ethical Decision Making</span>? <em>Proceedings of the 2023 <span>AAAI</span>/<span>ACM Conference</span> on <span>AI</span>, <span>Ethics</span>, and <span>Society</span></em>, 49–57. <a href="https://doi.org/10.1145/3600211.3604709">https://doi.org/10.1145/3600211.3604709</a>
</div>
<div id="ref-nguyenHumanBiasAI2024" class="csl-entry" role="listitem">
Nguyen, J. (2024). Human <span>Bias</span> in <span>AI Models</span>? <span>Anchoring Effects</span> and <span>Mitigation Strategies</span> in <span>Large Language Models</span>. <em>Journal of Behavioral and Experimental Finance</em>, 100971. <a href="https://doi.org/10.1016/j.jbef.2024.100971">https://doi.org/10.1016/j.jbef.2024.100971</a>
</div>
<div id="ref-nobandeganiDecisionMakingParadoxesHumans2023" class="csl-entry" role="listitem">
Nobandegani, A. S., Rish, I., &amp; Shultz, T. R. (2023). Decision-<span>Making Paradoxes</span> in <span>Humans</span> vs <span>Machines</span>: <span>The</span> case of the <span>Allais</span> and <span>Ellsberg Paradoxes</span>. <em>Proceedings of the <span>Annual Meeting</span> of the <span>Cognitive Science Society</span></em>, <em>46</em>.
</div>
<div id="ref-rastogiDecidingFastSlow2022" class="csl-entry" role="listitem">
Rastogi, C., Zhang, Y., Wei, D., Varshney, K. R., Dhurandhar, A., &amp; Tomsett, R. (2022). Deciding <span>Fast</span> and <span>Slow</span>: <span>The Role</span> of <span>Cognitive Biases</span> in <span class="nocase">AI-assisted Decision-making</span>. <em>Proceedings of the ACM on Human-Computer Interaction</em>, <em>6</em>(CSCW1), 1–22. <a href="https://doi.org/10.1145/3512930">https://doi.org/10.1145/3512930</a>
</div>
<div id="ref-robertsLargeLanguageModel2024" class="csl-entry" role="listitem">
Roberts, J., Moore, K., Pham, T., Ewaleifoh, O., &amp; Fisher, D. (2024). <em>Large <span>Language Model Recall Uncertainty</span> is <span>Modulated</span> by the <span>Fan Effect</span></em>.
</div>
<div id="ref-stadlerCognitiveEaseCost2024" class="csl-entry" role="listitem">
Stadler, M., Bannert, M., &amp; Sailer, M. (2024). Cognitive ease at a cost: <span>LLMs</span> reduce mental effort but compromise depth in student scientific inquiry. <em>Computers in Human Behavior</em>, <em>160</em>, 108386. <a href="https://doi.org/10.1016/j.chb.2024.108386">https://doi.org/10.1016/j.chb.2024.108386</a>
</div>
<div id="ref-subramonyamBridgingGulfEnvisioning2024" class="csl-entry" role="listitem">
Subramonyam, H., Pea, R., Pondoc, C. L., Agrawala, M., &amp; Seifert, C. (2024). <em>Bridging the <span>Gulf</span> of <span>Envisioning</span>: <span>Cognitive Design Challenges</span> in <span>LLM Interfaces</span></em> (arXiv:2309.14459). arXiv. <a href="https://arxiv.org/abs/2309.14459">https://arxiv.org/abs/2309.14459</a>
</div>
<div id="ref-suriLargeLanguageModels2024" class="csl-entry" role="listitem">
Suri, G., Slater, L. R., Ziaee, A., &amp; Nguyen, M. (2024). Do large language models show decision heuristics similar to humans? <span>A</span> case study using <span>GPT-35</span>. <em>Journal of Experimental Psychology: General</em>, <em>153</em>(4), 1066–1075. <a href="https://doi.org/10.1037/xge0001547">https://doi.org/10.1037/xge0001547</a>
</div>
<div id="ref-swaroopAccuracyTimeTradeoffsAIAssisted2024" class="csl-entry" role="listitem">
Swaroop, S., Buçinca, Z., Gajos, K. Z., &amp; Doshi-Velez, F. (2024). Accuracy-<span>Time Tradeoffs</span> in <span>AI-Assisted Decision Making</span> under <span>Time Pressure</span>. <em>Proceedings of the 29th <span>International Conference</span> on <span>Intelligent User Interfaces</span></em>, 138–154. <a href="https://doi.org/10.1145/3640543.3645206">https://doi.org/10.1145/3640543.3645206</a>
</div>
<div id="ref-tjuatjaLLMsExhibitHumanlike2024" class="csl-entry" role="listitem">
Tjuatja, L., Chen, V., Wu, T., Talwalkwar, A., &amp; Neubig, G. (2024). Do <span class="nocase">LLMs Exhibit Human-like Response Biases</span>? <span>A Case Study</span> in <span>Survey Design</span>. <em>Transactions of the Association for Computational Linguistics</em>, <em>12</em>, 1011–1026. <a href="https://doi.org/10.1162/tacl_a_00685">https://doi.org/10.1162/tacl_a_00685</a>
</div>
<div id="ref-westphalDecisionControlExplanations2023" class="csl-entry" role="listitem">
Westphal, M., Vössing, M., Satzger, G., Yom-Tov, G. B., &amp; Rafaeli, A. (2023). Decision control and explanations in human-<span>AI</span> collaboration: <span>Improving</span> user perceptions and compliance. <em>Computers in Human Behavior</em>, <em>144</em>, 107714. <a href="https://doi.org/10.1016/j.chb.2023.107714">https://doi.org/10.1016/j.chb.2023.107714</a>
</div>
<div id="ref-yaxStudyingImprovingReasoning2024" class="csl-entry" role="listitem">
Yax, N., Anlló, H., &amp; Palminteri, S. (2024). Studying and improving reasoning in humans and machines. <em>Communications Psychology</em>, <em>2</em>(1), 1–16. <a href="https://doi.org/10.1038/s44271-024-00091-8">https://doi.org/10.1038/s44271-024-00091-8</a>
</div>
<div id="ref-zhangMutualTheoryMind2024" class="csl-entry" role="listitem">
Zhang, S., Wang, X., Zhang, W., Chen, Y., Gao, L., Wang, D., Zhang, W., Wang, X., &amp; Wen, Y. (2024). <em>Mutual <span>Theory</span> of <span>Mind</span> in <span>Human-AI Collaboration</span>: <span>An Empirical Study</span> with <span class="nocase">LLM-driven AI Agents</span> in a <span class="nocase">Real-time Shared Workspace Task</span></em> (arXiv:2409.08811). arXiv. <a href="https://arxiv.org/abs/2409.08811">https://arxiv.org/abs/2409.08811</a>
</div>
<div id="ref-zhaoRiskProsocialBehavioural2024" class="csl-entry" role="listitem">
Zhao, Y., Huang, Z., Seligman, M., &amp; Peng, K. (2024). Risk and prosocial behavioural cues elicit human-like response patterns from <span>AI</span> chatbots. <em>Scientific Reports</em>, <em>14</em>(1), 7095. <a href="https://doi.org/10.1038/s41598-024-55949-y">https://doi.org/10.1038/s41598-024-55949-y</a>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/tegorman13\.github\.io\/ccl");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>