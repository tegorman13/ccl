<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.24">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Thomas E. Gorman">

<title>Samuel’s Project – CCL Projects</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-018089954d508eae8a473f0b7f0491f0.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-8b2d1bdf29de7083a8e5e4bc1bbbef90.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="Assets/Style/calloutTG.css">
</head>

<body class="nav-sidebar docked fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./tms.html">Misc</a></li><li class="breadcrumb-item"><a href="./Samuel_Project.html">Samuel’s Project</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">CCL Projects</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/tegorman13/ccl" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">LLM Literature</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_gd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Group Decision Lit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Individual decision lit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./llm_energy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLM Energy</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Misc</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transactive Memory Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Samuel_Project.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Samuel’s Project</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Driving.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Driving Lit</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./tms.html">Misc</a></li><li class="breadcrumb-item"><a href="./Samuel_Project.html">Samuel’s Project</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Samuel’s Project</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://tegorman13.github.io/">Thomas E. Gorman</a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://web.ics.purdue.edu/~treimer/">
            Communication and Cognition Lab, Purdue University, USA
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
  
    
  </div>
  


</header>


<section id="task-brainstorming" class="level1 page-columns page-full">
<h1>Task Brainstorming</h1>
<p>Click on one of the tasks to activate, then use the arrow keys to control the car</p>
<section id="original" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="original">Original</h2>
<div class="column-page-right">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="task1.html" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Sam’s original mockup"><embed src="task1.html" width="600" height="550"></a></p>
<figcaption>Sam’s original mockup</figcaption>
</figure>
</div>
</div>
</section>
<section id="alt-2" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-2">Alt 2</h2>
<div class="column-page-right">
<p><a href="task2.html" class="lightbox" data-gallery="quarto-lightbox-gallery-2"><embed src="task2.html" width="800" height="700"></a></p>
</div>
</section>
<section id="alt-2---larger" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-2---larger">Alt 2 - larger</h2>
<div class="column-page-right">
<p><a href="task4.html" class="lightbox" data-gallery="quarto-lightbox-gallery-3"><embed src="task4.html" width="800" height="800"></a></p>
</div>
</section>
<section id="alt-2b" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-2b">Alt 2b</h2>
<div class="column-page-right">
<p><a href="task2b.html" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><embed src="task2b.html" width="850" height="800"></a></p>
</div>
</section>
<section id="alt-2c" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-2c">Alt 2c</h2>
<div class="column-page-right">
<p><a href="task2c.html" class="lightbox" data-gallery="quarto-lightbox-gallery-5"><embed src="task2c.html" width="800" height="700"></a></p>
</div>
</section>
<section id="alt-3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-3">Alt 3</h2>
<div class="column-page-right">
<p><a href="task3.html" class="lightbox" data-gallery="quarto-lightbox-gallery-6"><embed src="task3.html" width="800" height="700"></a></p>
</div>
</section>
<section id="alt-3b" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-3b">Alt 3b</h2>
<div class="column-page-right">
<p><a href="task3b.html" class="lightbox" data-gallery="quarto-lightbox-gallery-7"><embed src="task3b.html" width="800" height="700"></a></p>
</div>
</section>
<section id="alt-3c" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-3c">Alt 3c</h2>
<div class="column-page-right">
<p><a href="task3c.html" class="lightbox" data-gallery="quarto-lightbox-gallery-8"><embed src="task3c.html" width="800" height="700"></a></p>
</div>
<hr>
</section>
</section>
<section id="literature" class="level1">
<h1>Literature</h1>
<section id="tasks-used-in-other-studes" class="level2">
<h2 class="anchored" data-anchor-id="tasks-used-in-other-studes">Tasks used in Other Studes</h2>
<section id="structuring-knowledge-with-cognitive-maps-and-cognitive-graphs." class="level3">
<h3 class="anchored" data-anchor-id="structuring-knowledge-with-cognitive-maps-and-cognitive-graphs.">Structuring Knowledge with Cognitive Maps and Cognitive Graphs.</h3>
<p>Peer, M., Brunec, I. K., Newcombe, N. S., &amp; Epstein, R. A. (2021). <strong>Structuring Knowledge with Cognitive Maps and Cognitive Graphs.</strong> Trends in Cognitive Sciences, 25(1), 37–54. https://doi.org/10.1016/j.tics.2020.10.004</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Humans and animals use mental representations of the spatial structure of the world to navigate. The classical view is that these representations take the form of Euclidean cognitive maps, but alternative theories suggest that they are cognitive graphs consisting of locations connected by paths. We review evidence suggesting that both map-like and graph-like representations exist in the mind/brain that rely on partially overlapping neural systems. Maps and graphs can operate simultaneously or separately, and they may be applied to both spatial and nonspatial knowledge. By providing structural frameworks for complex information, cognitive maps and cognitive graphs may provide fundamental organizing schemata that allow us to navigate in physical, social, and conceptual spaces.</p>
</div>
</details>
<div id="fig-peer" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-peer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/peer1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure&nbsp;1: Figures from @peerStructuringKnowledgeCognitive2021"><img src="images/peer1.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/peer2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;1: Figures from @peerStructuringKnowledgeCognitive2021"><img src="images/peer2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/peer3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;1: Figures from @peerStructuringKnowledgeCognitive2021"><img src="images/peer3.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-peer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Figures from <span class="citation" data-cites="peerStructuringKnowledgeCognitive2021">Peer et al. (<a href="#ref-peerStructuringKnowledgeCognitive2021" role="doc-biblioref">2021</a>)</span>
</figcaption>
</figure>
</div>
<hr>
</section>
<section id="wormholes-in-virtual-space-from-cognitive-maps-to-cognitive-graphs" class="level3">
<h3 class="anchored" data-anchor-id="wormholes-in-virtual-space-from-cognitive-maps-to-cognitive-graphs">Wormholes in virtual space: From cognitive maps to cognitive graphs</h3>
<p>Warren, W. H., Rothman, D. B., Schnapp, B. H., &amp; Ericson, J. D. (2017). <strong>Wormholes in virtual space: From cognitive maps to cognitive graphs.</strong> Cognition, 166, 152–163. https://doi.org/10.1016/j.cognition.2017.05.020</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Humans and other animals build up spatial knowledge of the environment on the basis of visual information and path integration. We compare three hypotheses about the geometry of this knowledge of navigation space: (a) ‘cognitive map’ with metric Euclidean structure and a consistent coordinate system, (b) ‘topological graph’ or network of paths between places, and (c) ‘labelled graph’ incorporating local metric information about path lengths and junction angles. In two experiments, participants walked in a non-Euclidean environment, a virtual hedge maze containing two ‘wormholes’ that visually rotated and teleported them between locations. During training, they learned the metric locations of eight target objects from a ‘home’ location, which were visible individually. During testing, shorter wormhole routes to a target were preferred, and novel shortcuts were directional, contrary to the topological hypothesis. Shortcuts were strongly biased by the wormholes, with mean constant errors of 37° and 41° (45° expected), revealing violations of the metric postulates in spatial knowledge. In addition, shortcuts to targets near wormholes shifted relative to flanking targets, revealing ‘rips’ (86% of cases), ‘folds’ (91%), and ordinal reversals (66%) in spatial knowledge. Moreover, participants were completely unaware of these geometric inconsistencies, reflecting a surprising insensitivity to Euclidean structure. The probability of the shortcut data under the Euclidean map model and labelled graph model indicated decisive support for the latter (BFGM&gt;100). We conclude that knowledge of navigation space is best characterized by a labelled graph, in which local metric information is approximate, geometrically inconsistent, and not embedded in a common coordinate system. This class of ‘cognitive graph’ models supports route finding, novel detours, and rough shortcuts, and has the potential to unify a range of data on spatial navigation.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Warren_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure from Warren et al.&nbsp;2017"><img src="images/Warren_drive1.png" class="img-fluid figure-img" alt="Figure from Warren et al.&nbsp;2017"></a></p>
<figcaption>Figure from Warren et al.&nbsp;2017</figcaption>
</figure>
</div>
<hr>
</section>
<section id="route-effects-in-city-based-survey-knowledge-estimates" class="level3">
<h3 class="anchored" data-anchor-id="route-effects-in-city-based-survey-knowledge-estimates">Route effects in city-based survey knowledge estimates</h3>
<p>Krukar, J., Navas Medrano, S., &amp; Schwering, A. (2023). <strong>Route effects in city-based survey knowledge estimates.</strong> Cognitive Processing, 24(2), 213–231. https://doi.org/10.1007/s10339-022-01122-0</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>When studying wayfinding in urban environments, researchers are often interested in obtaining measures of participants’ survey knowledge, i.e., their estimate of distant locations relative to other places. Previous work showed that distance estimations are consistently biased when no direct route is available to the queried target or when participants follow a detour. Here we investigated whether a corresponding bias is manifested in two other popular measures of survey knowledge: a pointing task and a sketchmapping task. The aim of this study was to investigate whether there is a systematic bias in pointing/sketchmapping performance associated with the preferred route choice in an applied urban setting. The results were mixed. We found moderate evidence for the presence of a systematic bias, but only for a subset of urban locations. When two plausible routes to the target were available, survey knowledge estimates were significantly biased in the direction of the route chosen by the participant. When only one plausible route was available, we did not find a statistically significant pattern. The results may have methodological implications for spatial cognition studies in applied urban settings that might be obtaining systematically biased survey knowledge estimates at some urban locations. Researchers should be aware that the choice of urban locations from which pointing and sketchmapping are performed might systematically distort the results, in particular when two plausible but diverging routes to the target are visible from the location.</p>
</div>
</details>
<div id="fig-krukar" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-krukar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Krukar1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure&nbsp;2: Figures from @krukarRouteEffectsCitybased2023"><img src="images/Krukar1.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Krukar2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;2: Figures from @krukarRouteEffectsCitybased2023"><img src="images/Krukar2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-krukar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Figures from <span class="citation" data-cites="krukarRouteEffectsCitybased2023">Krukar et al. (<a href="#ref-krukarRouteEffectsCitybased2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<hr>
</section>
<section id="spatial-decision-dynamics-during-wayfinding-intersections-prompt-the-decision-making-process." class="level3">
<h3 class="anchored" data-anchor-id="spatial-decision-dynamics-during-wayfinding-intersections-prompt-the-decision-making-process.">Spatial decision dynamics during wayfinding: Intersections prompt the decision-making process.</h3>
<p>Brunyé, T. T., Gardony, A. L., Holmes, A., &amp; Taylor, H. A. (2018). <strong>Spatial decision dynamics during wayfinding: Intersections prompt the decision-making process.</strong> Cognitive Research: Principles and Implications, 3(1), 13. https://doi.org/10.1186/s41235-018-0098-3</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Intersections are critical decision points for wayfinders, but it is unknown how decision dynamics unfold during pedestrian wayfinding. Some research implies that pedestrians leverage available visual cues to actively compare options while in an intersection, whereas other research suggests that people strive to make decisions long before overt responses are required. Two experiments examined these possibilities while participants navigated virtual desktop environments, assessing information-seeking behavior (Experiment 1) and movement dynamics (Experiment 2) while approaching intersections. In Experiment 1, we found that participants requested navigation guidance while in path segments approaching an intersection and the guidance facilitated choice behavior. In Experiment 2, we found that participants tended to orient themselves toward an upcoming turn direction before entering an intersection, particularly as they became more familiar with the environment. Some of these patterns were modulated by individual differences in spatial ability, sense of direction, spatial strategies, and gender. Together, we provide novel evidence that deciding whether to continue straight or turn involves a dynamic, distributed decision-making process that is prompted by upcoming intersections and modulated by individual differences and environmental experience. We discuss implications of these results for spatial decision-making theory and the development of innovative adaptive, beacon-based navigation guidance systems.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Brunye_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure from @brunyeSpatialDecisionDynamics2018"><img src="images/Brunye_drive1.png" class="img-fluid figure-img" alt="Figure from Brunyé et al. (2018)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="brunyeSpatialDecisionDynamics2018">Brunyé et al. (<a href="#ref-brunyeSpatialDecisionDynamics2018" role="doc-biblioref">2018</a>)</span></figcaption>
</figure>
</div>
<hr>
<p>Ericson, J. D., &amp; Warren, W. H. (2020). <strong>Probing the invariant structure of spatial knowledge: Support for the cognitive graph hypothesis.</strong> Cognition, 200, 104276. https://doi.org/10.1016/j.cognition.2020.104276</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>We tested four hypotheses about the structure of spatial knowledge used for navigation: (1) the Euclidean hypothesis, a geometrically consistent map; (2) the Neighborhood hypothesis, adjacency relations between spatial regions, based on visible boundaries; (3) the Cognitive Graph hypothesis, a network of paths between places, labeled with approximate local distances and angles; and (4) the Constancy hypothesis, whatever geometric properties are invariant during learning. In two experiments, different groups of participants learned three virtual hedge mazes, which varied specific geometric properties (Euclidean Control Maze, Elastic Maze with stretching paths, Swap Maze with alternating paths to the same place). Spatial knowledge was then tested using three navigation tasks (metric shortcuts on empty ground plane, neighborhood shortcuts with visible boundaries, route task in corridors). They yielded the following results: (a) Metric shortcuts were insensitive to detectable shifts in target location, inconsistent with the Euclidean hypothesis. (b) Neighborhood shortcuts were constrained by visible boundaries in the Elastic Maze, but not in the Swap Maze, contrary to the Neighborhood and Constancy hypotheses. (c) The route task indicated that a graph of the maze was acquired in all environments, including knowledge of local path lengths. We conclude that primary spatial knowledge is consistent with the Cognitive Graph hypothesis. Neighborhoods are derived from the graph, and local distance and angle information is not embedded in a geometrically consistent map.</p>
</div>
</details>
<div id="fig-ericson" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ericson-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Ericson1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure&nbsp;3: Figures from @ericsonProbingInvariantStructure2020"><img src="images/Ericson1.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Ericson2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;3: Figures from @ericsonProbingInvariantStructure2020"><img src="images/Ericson2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ericson-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Figures from <span class="citation" data-cites="ericsonProbingInvariantStructure2020">Ericson &amp; Warren (<a href="#ref-ericsonProbingInvariantStructure2020" role="doc-biblioref">2020</a>)</span>
</figcaption>
</figure>
</div>
<hr>
</section>
<section id="rational-use-of-cognitive-resources-in-human-planning" class="level3">
<h3 class="anchored" data-anchor-id="rational-use-of-cognitive-resources-in-human-planning">Rational use of cognitive resources in human planning</h3>
<p>Callaway, F., Van Opheusden, B., Gul, S., Das, P., Krueger, P. M., Griffiths, T. L., &amp; Lieder, F. (2022). Rational use of cognitive resources in human planning. Nature Human Behaviour, 6(8), 1112–1125. https://doi.org/10.1038/s41562-022-01332-8</p>
<p><a href="https://osf.io/6venh/">link to code</a> &nbsp; <a href="https://cocosci.princeton.edu/papers/callawayrationaluse.pdf">link to paper</a> &nbsp; <a href="https://roadtrip-task-demo.netlify.app/">link to live task demo</a> &nbsp;</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/callaway_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="Figure from @callawayRationalUseCognitive2022"><img src="images/callaway_drive1.png" class="img-fluid figure-img" alt="Figure from Callaway et al. (2022)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="callawayRationalUseCognitive2022">Callaway et al. (<a href="#ref-callawayRationalUseCognitive2022" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="people-construct-simplified-mental-representations-to-plan." class="level3">
<h3 class="anchored" data-anchor-id="people-construct-simplified-mental-representations-to-plan.">People construct simplified mental representations to plan.</h3>
<p>Ho, M. K., Abel, D., Correa, C. G., Littman, M. L., Cohen, J. D., &amp; Griffiths, T. L. (2022). <strong>People construct simplified mental representations to plan.</strong> Nature, 1–8. https://doi.org/10.1038/s41586-022-04743-9</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/ho_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="Figure from Ho et al.&nbsp;2022"><img src="images/ho_drive1.png" class="img-fluid figure-img" alt="Figure from Ho et al.&nbsp;2022"></a></p>
<figcaption>Figure from Ho et al.&nbsp;2022</figcaption>
</figure>
</div>
<p>Ho, M. K., Abel, D., Correa, C. G., Littman, M. L., Cohen, J. D., &amp; Griffiths, T. L. (2021). <strong>Control of mental representations in human planning.</strong> arXiv:2105.06948 [Cs]. http://arxiv.org/abs/2105.06948</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/ho_drive2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20" title="Figure from @hoControlMentalRepresentations2021"><img src="images/ho_drive2.png" class="img-fluid figure-img" alt="Figure from Ho et al. (2021)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="hoControlMentalRepresentations2021">Ho et al. (<a href="#ref-hoControlMentalRepresentations2021" role="doc-biblioref">2021</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="emergent-collective-sensing-in-human-groups." class="level3">
<h3 class="anchored" data-anchor-id="emergent-collective-sensing-in-human-groups.">Emergent Collective Sensing in Human Groups.</h3>
<p>Krafft, P. M., Hawkins, R. X., Pentland, A., Goodman, N. D., &amp; Tenenbaum, J. B. (2015). <strong>Emergent Collective Sensing in Human Groups.</strong> In CogSci. https://people.csail.mit.edu/pkrafft/papers/krafft-et-al-2015-emergent.pdf</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Kraft_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21" title="Figure from @krafftEmergentCollectiveSensing2015"><img src="images/Kraft_drive1.png" class="img-fluid figure-img" alt="Figure from Krafft (2015)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="krafftEmergentCollectiveSensing2015">Krafft (<a href="#ref-krafftEmergentCollectiveSensing2015" role="doc-biblioref">2015</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="towards-a-computational-model-of-responsibility-judgments-in-sequential-human-ai-collaboration" class="level3">
<h3 class="anchored" data-anchor-id="towards-a-computational-model-of-responsibility-judgments-in-sequential-human-ai-collaboration">Towards a computational model of responsibility judgments in sequential human-AI collaboration</h3>
<p>Tsirtsis, S., Gomez Rodriguez, M., &amp; Gerstenberg, T. (2024). <strong>Towards a computational model of responsibility judgments in sequential human-AI collaboration.</strong> In Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 46). https://osf.io/preprints/psyarxiv/m4yad</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Tsirtsis_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-22" title="Figure from @tsirtsisComputationalModelResponsibility2024"><img src="images/Tsirtsis_drive1.png" class="img-fluid figure-img" alt="Figure from Tsirtsis et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="tsirtsisComputationalModelResponsibility2024">Tsirtsis et al. (<a href="#ref-tsirtsisComputationalModelResponsibility2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="spatial-planning-with-long-visual-range-benefits-escape-from-visual-predators-in-complex-naturalistic-environments." class="level3">
<h3 class="anchored" data-anchor-id="spatial-planning-with-long-visual-range-benefits-escape-from-visual-predators-in-complex-naturalistic-environments.">Spatial planning with long visual range benefits escape from visual predators in complex naturalistic environments.</h3>
<p>Mugan, U., &amp; MacIver, M. A. (2020). <strong>Spatial planning with long visual range benefits escape from visual predators in complex naturalistic environments.</strong> Nature Communications, 11(1), 3057. https://doi.org/10.1038/s41467-020-16102-1</p>
<p><a href="https://maciver-lab.github.io/plangame/">live task demo</a><br>
<a href="https://github.com/MacIver-Lab/plangame">code repository</a><br>
</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Mugan_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-23" title="Figure from @muganSpatialPlanningLong2020"><img src="images/Mugan_drive1.png" class="img-fluid figure-img" alt="Figure from Mugan &amp; MacIver (2020)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="muganSpatialPlanningLong2020">Mugan &amp; MacIver (<a href="#ref-muganSpatialPlanningLong2020" role="doc-biblioref">2020</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="pattern-driven-navigation-in-2d-multiscale-visualizations-with-scalable-insets." class="level3">
<h3 class="anchored" data-anchor-id="pattern-driven-navigation-in-2d-multiscale-visualizations-with-scalable-insets.">Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets.</h3>
<p>Lekschas, F., Behrisch, M., Bach, B., Kerpedjiev, P., Gehlenborg, N., &amp; Pfister, H. (2020). <strong>Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets.</strong> IEEE Transactions on Visualization and Computer Graphics, 26(1), 611–621. IEEE Transactions on Visualization and Computer Graphics. https://doi.org/10.1109/TVCG.2019.2934555</p>
<p><a href="https://github.com/flekschas/higlass-scalable-insets">link to code on github</a></p>
<p><a href="https://scalable-insets.lekschas.de/">project page</a></p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>We present Scalable Insets, a technique for interactively exploring and navigating large numbers of annotated patterns in multiscale visualizations such as gigapixel images, matrices, or maps. Exploration of many but sparsely-distributed patterns in multiscale visualizations is challenging as visual representations change across zoom levels, context and navigational cues get lost upon zooming, and navigation is time consuming. Our technique visualizes annotated patterns too small to be identifiable at certain zoom levels using insets, i.e., magnified thumbnail views of the annotated patterns. Insets support users in searching, comparing, and contextualizing patterns while reducing the amount of navigation needed. They are dynamically placed either within the viewport or along the boundary of the viewport to offer a compromise between locality and context preservation. Annotated patterns are interactively clustered by location and type. They are visually represented as an aggregated inset to provide scalable exploration within a single viewport. In a controlled user study with 18 participants, we found that Scalable Insets can speed up visual search and improve the accuracy of pattern comparison at the cost of slower frequency estimation compared to a baseline technique. A second study with 6 experts in the field of genomics showed that Scalable Insets is easy to learn and provides first insights into how Scalable Insets can be applied in an open-ended data exploration scenario.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Lekschas.png" class="lightbox" data-gallery="quarto-lightbox-gallery-24" title="Figure from @lekschasPatternDrivenNavigation2D2020"><img src="images/Lekschas.png" class="img-fluid figure-img" alt="Figure from Lekschas et al. (2020)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="lekschasPatternDrivenNavigation2D2020">Lekschas et al. (<a href="#ref-lekschasPatternDrivenNavigation2D2020" role="doc-biblioref">2020</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="personality-traits-and-spatial-skills-are-related-to-group-dynamics-and-success-during-collective-wayfinding" class="level3">
<h3 class="anchored" data-anchor-id="personality-traits-and-spatial-skills-are-related-to-group-dynamics-and-success-during-collective-wayfinding">Personality Traits and Spatial Skills Are Related to Group Dynamics and Success During Collective Wayfinding</h3>
<p>Brunyé, T. T., Hendel, D., Gardony, A. L., Hussey, E. K., &amp; Taylor, H. A. (2024). <strong>Personality traits and spatial skills are related to group dynamics and success during collective wayfinding.</strong> Collective Spatial Cognition, 60-99.</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>This chapter reviews and identifies gaps in research examining collective navigation and describes the results of a small study aimed at better elucidating the independent and interactive roles of personality and spatial skill in guiding group wayfinding dynamics, wayfinding performance, and spatial memory. In this study, individuals, dyads, and triads completed a series of individual differences tasks and questionnaires, and an individual or shared (dyads and triads) virtual wayfinding experience involving planning and executing routes between origin and destination pairs. Navigators were provided with a single digital map that they could share during the task; patterns of map sharing, virtual navigation, and wayfinding performance were logged. Higher spatial anxiety was associated with more map viewing among group members, higher scores on questionnaires assessing autism-type traits were associated with lower group cohesion, higher group heterogeneity was associated with lower group cohesion and lower path efficiency, and triads tended to have poorer memory for the location of goal locations relative to individuals and dyads. Results speak to the inherent complexity and dynamics of collective navigation, the need for understanding individual differences in guiding group behavior, and the value of continuing research in this domain.</p>
</div>
</details>
<p><span class="citation" data-cites="brunyePersonalityTraitsSpatial2023">Brunyé et al. (<a href="#ref-brunyePersonalityTraitsSpatial2023" role="doc-biblioref">2023</a>)</span></p>
<hr>
</section>
<section id="wayfinding-in-pairs-comparing-the-planning-and-navigation-performance-of-dyads-and-individuals-in-a-real-world-environment." class="level3">
<h3 class="anchored" data-anchor-id="wayfinding-in-pairs-comparing-the-planning-and-navigation-performance-of-dyads-and-individuals-in-a-real-world-environment.">Wayfinding in pairs: Comparing the planning and navigation performance of dyads and individuals in a real-world environment.</h3>
<p>Bae, C., Montello, D., &amp; Hegarty, M. (2024). <strong>Wayfinding in pairs: Comparing the planning and navigation performance of dyads and individuals in a real-world environment.</strong> Cognitive Research: Principles and Implications, 9(1), 40. https://doi.org/10.1186/s41235-024-00563-9</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Navigation is essential to life, and it is cognitively complex, drawing on abilities such as prospective and situated planning, spatial memory, location recognition, and real-time decision-making. In many cases, day-to-day navigation is embedded in a social context where cognition and behavior are shaped by others, but the great majority of existing research in spatial cognition has focused on individuals. The two studies we report here contribute to our understanding of social wayfinding, assessing the performance of paired and individual navigators on a real-world wayfinding task in which they were instructed to minimize time and distance traveled. In the first study, we recruited 30 pairs of friends (familiar dyads); in the second, we recruited 30 solo participants (individuals). We compare the two studies to the results of an earlier study of 30 pairs of strangers (unfamiliar dyads). We draw out differences in performance with respect to spatial, social, and cognitive considerations. Of the three conditions, solo participants were least successful in reaching the destination accurately on their initial attempt. Friends traveled more efficiently than either strangers or individuals. Working with a partner also appeared to lend confidence to wayfinders: dyads of either familiarity type were more persistent than individuals in the navigation task, even after encountering challenges or making incorrect attempts. Route selection was additionally impacted by route complexity and unfamiliarity with the study area. Navigators explicitly used ease of remembering as a planning criterion, and the resulting differences in route complexity likely influenced success during enacted navigation.</p>
</div>
</details>
<div id="fig-bae" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/bae1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-25" title="Figure&nbsp;4: Figures from @baeWayfindingPairsComparing2024"><img src="images/bae1.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/bae2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-26" title="Figure&nbsp;4: Figures from @baeWayfindingPairsComparing2024"><img src="images/bae2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/bae3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-27" title="Figure&nbsp;4: Figures from @baeWayfindingPairsComparing2024"><img src="images/bae3.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/bae4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-28" title="Figure&nbsp;4: Figures from @baeWayfindingPairsComparing2024"><img src="images/bae4.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Figures from <span class="citation" data-cites="baeWayfindingPairsComparing2024">Bae et al. (<a href="#ref-baeWayfindingPairsComparing2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
<hr>
</section>
<section id="individual-and-collective-foraging-in-autonomous-search-agents-with-human-intervention" class="level3">
<h3 class="anchored" data-anchor-id="individual-and-collective-foraging-in-autonomous-search-agents-with-human-intervention">Individual and collective foraging in autonomous search agents with human intervention</h3>
<p>Schloesser, D. S., Hollenbeck, D., &amp; Kello, C. T. (2021). <strong>Individual and collective foraging in autonomous search agents with human intervention.</strong> Scientific Reports, 11(1), Article 1. https://doi.org/10.1038/s41598-021-87717-7</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Humans and other complex organisms exhibit intelligent behaviors as individual agents and as groups of coordinated agents. They can switch between independent and collective modes of behavior, and flexible switching can be advantageous for adapting to ongoing changes in conditions. In the present study, we investigated the flexibility between independent and collective modes of behavior in a simulated social foraging task designed to benefit from both modes: distancing among ten foraging agents promoted faster detection of resources, whereas flocking promoted faster consumption. There was a tradeoff between faster detection versus faster consumption, but both factors contributed to foraging success. Results showed that group foraging performance among simulated agents was enhanced by loose coupling that balanced distancing and flocking among agents and enabled them to fluidly switch among a variety of groupings. We also examined the effects of more sophisticated cognitive capacities by studying how human players improve performance when they control one of the search agents. Results showed that human intervention further enhanced group performance with loosely coupled agents, and human foragers performed better when coordinating with loosely coupled agents. Humans players adapted their balance of independent versus collective search modes in response to the dynamics of simulated agents, thereby demonstrating the importance of adaptive flexibility in social foraging.</p>
</div>
</details>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Kello1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-29"><img src="images/Kello1.png" class="img-fluid"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Kello2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-30"><img src="images/Kello2.png" class="img-fluid"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Figures from <span class="citation" data-cites="schloesserIndividualCollectiveForaging2021">Schloesser et al. (<a href="#ref-schloesserIndividualCollectiveForaging2021" role="doc-biblioref">2021</a>)</span></p>
</div>
</div>
</div>
<hr>
<p><br>
</p>
<p>Alharbi, A. H., Khafaga, D. S., El-kenawy, E.-S. M., Eid, M. M., Ibrahim, A., Abualigah, L., Khodadadi, N., &amp; Abdelhamid, A. A. (2024). <strong>Optimizing electric vehicle paths to charging stations using parallel greylag goose algorithm and Restricted Boltzmann Machines.</strong> Frontiers in Energy Research, 12. https://doi.org/10.3389/fenrg.2024.1401330</p>
<p>Garg, K., Kello, C. T., &amp; Smaldino, P. E. (2022). <strong>Individual exploration and selective social learning: Balancing exploration–exploitation trade-offs in collective foraging.</strong> Journal of The Royal Society Interface, 19(189), 20210915. https://doi.org/10.1098/rsif.2021.0915</p>
<p>Mezey, D., Deffner, D., Kurvers, R. H. J. M., &amp; Romanczuk, P. (2024). <strong>Visual social information use in collective foraging.</strong> PLOS Computational Biology, 20(5), e1012087. https://doi.org/10.1371/journal.pcbi.1012087</p>



</section>
</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-baeWayfindingPairsComparing2024" class="csl-entry" role="listitem">
Bae, C., Montello, D., &amp; Hegarty, M. (2024). Wayfinding in pairs: Comparing the planning and navigation performance of dyads and individuals in a real-world environment. <em>Cognitive Research: Principles and Implications</em>, <em>9</em>(1), 40. <a href="https://doi.org/10.1186/s41235-024-00563-9">https://doi.org/10.1186/s41235-024-00563-9</a>
</div>
<div id="ref-brunyeSpatialDecisionDynamics2018" class="csl-entry" role="listitem">
Brunyé, T. T., Gardony, A. L., Holmes, A., &amp; Taylor, H. A. (2018). Spatial decision dynamics during wayfinding: Intersections prompt the decision-making process. <em>Cognitive Research: Principles and Implications</em>, <em>3</em>(1), 13. <a href="https://doi.org/10.1186/s41235-018-0098-3">https://doi.org/10.1186/s41235-018-0098-3</a>
</div>
<div id="ref-brunyePersonalityTraitsSpatial2023" class="csl-entry" role="listitem">
Brunyé, T. T., Hendel, D., Gardony, A. L., Hussey, E. K., &amp; Taylor, H. A. (2023). Personality <span>Traits</span> and <span>Spatial Skills Are Related</span> to <span>Group Dynamics</span> and <span>Success During Collective Wayfinding</span>. In <em>Collective <span>Spatial Cognition</span></em>. Routledge.
</div>
<div id="ref-callawayRationalUseCognitive2022" class="csl-entry" role="listitem">
Callaway, F., Van Opheusden, B., Gul, S., Das, P., Krueger, P. M., Griffiths, T. L., &amp; Lieder, F. (2022). Rational use of cognitive resources in human planning. <em>Nature Human Behaviour</em>, <em>6</em>(8), 1112–1125. <a href="https://doi.org/10.1038/s41562-022-01332-8">https://doi.org/10.1038/s41562-022-01332-8</a>
</div>
<div id="ref-ericsonProbingInvariantStructure2020" class="csl-entry" role="listitem">
Ericson, J. D., &amp; Warren, W. H. (2020). Probing the invariant structure of spatial knowledge: <span>Support</span> for the cognitive graph hypothesis. <em>Cognition</em>, <em>200</em>, 104276. <a href="https://doi.org/10.1016/j.cognition.2020.104276">https://doi.org/10.1016/j.cognition.2020.104276</a>
</div>
<div id="ref-hoControlMentalRepresentations2021" class="csl-entry" role="listitem">
Ho, M. K., Abel, D., Correa, C. G., Littman, M. L., Cohen, J. D., &amp; Griffiths, T. L. (2021). Control of mental representations in human planning. <em>arXiv:2105.06948 [Cs]</em>. <a href="https://arxiv.org/abs/2105.06948">https://arxiv.org/abs/2105.06948</a>
</div>
<div id="ref-krafftEmergentCollectiveSensing2015" class="csl-entry" role="listitem">
Krafft, P. M. (2015). <em>Emergent <span>Collective Sensing</span> in <span>Human Groups</span></em>. 6.
</div>
<div id="ref-krukarRouteEffectsCitybased2023" class="csl-entry" role="listitem">
Krukar, J., Navas Medrano, S., &amp; Schwering, A. (2023). Route effects in city-based survey knowledge estimates. <em>Cognitive Processing</em>, <em>24</em>(2), 213–231. <a href="https://doi.org/10.1007/s10339-022-01122-0">https://doi.org/10.1007/s10339-022-01122-0</a>
</div>
<div id="ref-lekschasPatternDrivenNavigation2D2020" class="csl-entry" role="listitem">
Lekschas, F., Behrisch, M., Bach, B., Kerpedjiev, P., Gehlenborg, N., &amp; Pfister, H. (2020). Pattern-<span>Driven Navigation</span> in <span>2D Multiscale Visualizations</span> with <span>Scalable Insets</span>. <em>IEEE Transactions on Visualization and Computer Graphics</em>, <em>26</em>(1), 611–621. <a href="https://doi.org/10.1109/TVCG.2019.2934555">https://doi.org/10.1109/TVCG.2019.2934555</a>
</div>
<div id="ref-muganSpatialPlanningLong2020" class="csl-entry" role="listitem">
Mugan, U., &amp; MacIver, M. A. (2020). Spatial planning with long visual range benefits escape from visual predators in complex naturalistic environments. <em>Nature Communications</em>, <em>11</em>(1), 3057. <a href="https://doi.org/10.1038/s41467-020-16102-1">https://doi.org/10.1038/s41467-020-16102-1</a>
</div>
<div id="ref-peerStructuringKnowledgeCognitive2021" class="csl-entry" role="listitem">
Peer, M., Brunec, I. K., Newcombe, N. S., &amp; Epstein, R. A. (2021). Structuring <span>Knowledge</span> with <span>Cognitive Maps</span> and <span>Cognitive Graphs</span>. <em>Trends in Cognitive Sciences</em>, <em>25</em>(1), 37–54. <a href="https://doi.org/10.1016/j.tics.2020.10.004">https://doi.org/10.1016/j.tics.2020.10.004</a>
</div>
<div id="ref-schloesserIndividualCollectiveForaging2021" class="csl-entry" role="listitem">
Schloesser, D. S., Hollenbeck, D., &amp; Kello, C. T. (2021). Individual and collective foraging in autonomous search agents with human intervention. <em>Scientific Reports</em>, <em>11</em>(1), 8492. <a href="https://doi.org/10.1038/s41598-021-87717-7">https://doi.org/10.1038/s41598-021-87717-7</a>
</div>
<div id="ref-tsirtsisComputationalModelResponsibility2024" class="csl-entry" role="listitem">
Tsirtsis, S., Rodriguez, M. G., &amp; Gerstenberg, T. (2024). <em>Towards a computational model of responsibility judgments in sequential human-<span>AI</span> collaboration</em>. <a href="https://doi.org/10.31234/osf.io/m4yad">https://doi.org/10.31234/osf.io/m4yad</a>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/tegorman13\.github\.io\/ccl");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"descPosition":"bottom","loop":false,"closeEffect":"zoom","openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>