<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.6.35">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Thomas E. Gorman">
<meta name="dcterms.date" content="2024-11-18">

<title>Samuel’s Project – CCL Projects</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="site_libs/quarto-nav/quarto-nav.js"></script>
<script src="site_libs/quarto-nav/headroom.min.js"></script>
<script src="site_libs/clipboard/clipboard.min.js"></script>
<script src="site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="site_libs/quarto-search/fuse.min.js"></script>
<script src="site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="./">
<script src="site_libs/quarto-html/quarto.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="site_libs/quarto-html/anchor.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-syntax-highlighting-383d4a065b21370043bc4709e21900a7.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/bootstrap/bootstrap.min.js"></script>
<link href="site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="site_libs/bootstrap/bootstrap-7fa2055fb7403aac80cb702c524d448d.min.css" rel="stylesheet" append-hash="true" id="quarto-bootstrap" data-mode="light">
<script src="site_libs/quarto-contrib/glightbox/glightbox.min.js"></script>
<link href="site_libs/quarto-contrib/glightbox/glightbox.min.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/glightbox/lightbox.css" rel="stylesheet">
<script id="quarto-search-options" type="application/json">{
  "location": "sidebar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "start",
  "type": "textbox",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>


<link rel="stylesheet" href="Assets/Style/calloutTG.css">
</head>

<body class="nav-sidebar docked">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
  <nav class="quarto-secondary-nav">
    <div class="container-fluid d-flex">
      <button type="button" class="quarto-btn-toggle btn" data-bs-toggle="collapse" role="button" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
        <i class="bi bi-layout-text-sidebar-reverse"></i>
      </button>
        <nav class="quarto-page-breadcrumbs" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./tms.html">Misc</a></li><li class="breadcrumb-item"><a href="./Samuel_Project.html">Samuel’s Project</a></li></ol></nav>
        <a class="flex-grow-1" role="navigation" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item" aria-controls="quarto-sidebar" aria-expanded="false" aria-label="Toggle sidebar navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">      
        </a>
      <button type="button" class="btn quarto-search-button" aria-label="Search" onclick="window.quartoOpenSearch();">
        <i class="bi bi-search"></i>
      </button>
    </div>
  </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article">
<!-- sidebar -->
  <nav id="quarto-sidebar" class="sidebar collapse collapse-horizontal quarto-sidebar-collapse-item sidebar-navigation docked overflow-auto">
    <div class="pt-lg-2 mt-2 text-left sidebar-header">
    <div class="sidebar-title mb-0 py-0">
      <a href="./">CCL Projects</a> 
        <div class="sidebar-tools-main">
    <a href="https://github.com/tegorman13/ccl" title="" class="quarto-navigation-tool px-1" aria-label=""><i class="bi bi-github"></i></a>
  <a href="" class="quarto-reader-toggle quarto-navigation-tool px-1" onclick="window.quartoToggleReader(); return false;" title="Toggle reader mode">
  <div class="quarto-reader-toggle-btn">
  <i class="bi"></i>
  </div>
</a>
</div>
    </div>
      </div>
        <div class="mt-2 flex-shrink-0 align-items-center">
        <div class="sidebar-search">
        <div id="quarto-search" class="" title="Search"></div>
        </div>
        </div>
    <div class="sidebar-menu-container"> 
    <ul class="list-unstyled mt-1">
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true">
 <span class="menu-text">LLM Literature</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-1" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-1" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_gd.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Group Decision Lit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_decision.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Individual decision lit</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./llm_energy.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">LLM Energy Lit Highlights</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./ai_interaction.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Interactive AI Lit</span></a>
  </div>
</li>
      </ul>
  </li>
        <li class="px-0"><hr class="sidebar-divider hi "></li>
        <li class="sidebar-item sidebar-item-section">
      <div class="sidebar-item-container"> 
            <a class="sidebar-item-text sidebar-link text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true">
 <span class="menu-text">Misc</span></a>
          <a class="sidebar-item-toggle text-start" data-bs-toggle="collapse" data-bs-target="#quarto-sidebar-section-2" role="navigation" aria-expanded="true" aria-label="Toggle section">
            <i class="bi bi-chevron-right ms-2"></i>
          </a> 
      </div>
      <ul id="quarto-sidebar-section-2" class="collapse list-unstyled sidebar-section depth1 show">  
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./tms.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Transactive Memory Systems</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Samuel_Project.html" class="sidebar-item-text sidebar-link active">
 <span class="menu-text">Samuel’s Project</span></a>
  </div>
</li>
          <li class="sidebar-item">
  <div class="sidebar-item-container"> 
  <a href="./Driving.html" class="sidebar-item-text sidebar-link">
 <span class="menu-text">Driving Lit</span></a>
  </div>
</li>
      </ul>
  </li>
    </ul>
    </div>
</nav>
<div id="quarto-sidebar-glass" class="quarto-sidebar-collapse-item" data-bs-toggle="collapse" data-bs-target=".quarto-sidebar-collapse-item"></div>
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#literature" id="toc-literature" class="nav-link active" data-scroll-target="#literature">Literature</a>
  <ul class="collapse">
  <li><a href="#social-influence" id="toc-social-influence" class="nav-link" data-scroll-target="#social-influence">Social Influence</a>
  <ul class="collapse">
  <li><a href="#visual-spatial-dynamics-drive-adaptive-social-learning-in-immersive-environments" id="toc-visual-spatial-dynamics-drive-adaptive-social-learning-in-immersive-environments" class="nav-link" data-scroll-target="#visual-spatial-dynamics-drive-adaptive-social-learning-in-immersive-environments">Visual-spatial dynamics drive adaptive social learning in immersive environments</a></li>
  <li><a href="#specialization-and-selective-social-attention-establishes-the-balance-between-individual-and-social-learning." id="toc-specialization-and-selective-social-attention-establishes-the-balance-between-individual-and-social-learning." class="nav-link" data-scroll-target="#specialization-and-selective-social-attention-establishes-the-balance-between-individual-and-social-learning.">Specialization and selective social attention establishes the balance between individual and social learning.</a></li>
  <li><a href="#collective-incentives-reduce-over-exploitation-of-social-information-in-unconstrained-human-groups." id="toc-collective-incentives-reduce-over-exploitation-of-social-information-in-unconstrained-human-groups." class="nav-link" data-scroll-target="#collective-incentives-reduce-over-exploitation-of-social-information-in-unconstrained-human-groups.">Collective incentives reduce over-exploitation of social information in unconstrained human groups.</a></li>
  <li><a href="#insights-about-the-common-generative-rule-underlying-an-information-foraging-task-can-be-facilitated-via-collective-search." id="toc-insights-about-the-common-generative-rule-underlying-an-information-foraging-task-can-be-facilitated-via-collective-search." class="nav-link" data-scroll-target="#insights-about-the-common-generative-rule-underlying-an-information-foraging-task-can-be-facilitated-via-collective-search.">Insights about the common generative rule underlying an information foraging task can be facilitated via collective search.</a></li>
  <li><a href="#individualism-versus-collective-movement-during-travel." id="toc-individualism-versus-collective-movement-during-travel." class="nav-link" data-scroll-target="#individualism-versus-collective-movement-during-travel.">Individualism versus collective movement during travel.</a></li>
  <li><a href="#beyond-the-individual-a-social-foraging-framework-to-study-decisions-in-groups." id="toc-beyond-the-individual-a-social-foraging-framework-to-study-decisions-in-groups." class="nav-link" data-scroll-target="#beyond-the-individual-a-social-foraging-framework-to-study-decisions-in-groups.">Beyond the individual: A social foraging framework to study decisions in groups.</a></li>
  </ul></li>
  <li><a href="#complexity" id="toc-complexity" class="nav-link" data-scroll-target="#complexity">Complexity</a>
  <ul class="collapse">
  <li><a href="#task-complexity-moderates-group-synergy." id="toc-task-complexity-moderates-group-synergy." class="nav-link" data-scroll-target="#task-complexity-moderates-group-synergy.">Task complexity moderates group synergy.</a></li>
  <li><a href="#the-interaction-between-map-complexity-and-crowd-movement-on-navigation-decisions-in-virtual-reality." id="toc-the-interaction-between-map-complexity-and-crowd-movement-on-navigation-decisions-in-virtual-reality." class="nav-link" data-scroll-target="#the-interaction-between-map-complexity-and-crowd-movement-on-navigation-decisions-in-virtual-reality.">The interaction between map complexity and crowd movement on navigation decisions in virtual reality.</a></li>
  <li><a href="#task-complexity-and-performance-in-individuals-and-groups-without-communication." id="toc-task-complexity-and-performance-in-individuals-and-groups-without-communication." class="nav-link" data-scroll-target="#task-complexity-and-performance-in-individuals-and-groups-without-communication.">Task Complexity and Performance in Individuals and Groups Without Communication.</a></li>
  <li><a href="#environmental-memory-boosts-group-formation-of-clueless-individuals." id="toc-environmental-memory-boosts-group-formation-of-clueless-individuals." class="nav-link" data-scroll-target="#environmental-memory-boosts-group-formation-of-clueless-individuals.">Environmental memory boosts group formation of clueless individuals.</a></li>
  </ul></li>
  <li><a href="#uncertainty" id="toc-uncertainty" class="nav-link" data-scroll-target="#uncertainty">Uncertainty</a>
  <ul class="collapse">
  <li><a href="#flexible-social-inference-facilitates-targeted-social-learning-when-rewards-are-not-observable." id="toc-flexible-social-inference-facilitates-targeted-social-learning-when-rewards-are-not-observable." class="nav-link" data-scroll-target="#flexible-social-inference-facilitates-targeted-social-learning-when-rewards-are-not-observable.">Flexible social inference facilitates targeted social learning when rewards are not observable.</a></li>
  <li><a href="#spatial-planning-with-long-visual-range-benefits-escape-from-visual-predators-in-complex-naturalistic-environments." id="toc-spatial-planning-with-long-visual-range-benefits-escape-from-visual-predators-in-complex-naturalistic-environments." class="nav-link" data-scroll-target="#spatial-planning-with-long-visual-range-benefits-escape-from-visual-predators-in-complex-naturalistic-environments.">Spatial planning with long visual range benefits escape from visual predators in complex naturalistic environments.</a></li>
  <li><a href="#the-form-of-uncertainty-affects-selection-for-social-learning." id="toc-the-form-of-uncertainty-affects-selection-for-social-learning." class="nav-link" data-scroll-target="#the-form-of-uncertainty-affects-selection-for-social-learning.">The form of uncertainty affects selection for social learning.</a></li>
  </ul></li>
  <li><a href="#peters-talks-on-foraging" id="toc-peters-talks-on-foraging" class="nav-link" data-scroll-target="#peters-talks-on-foraging">Peter’s Talks on Foraging</a></li>
  </ul></li>
  <li><a href="#tasks-used-in-other-studes" id="toc-tasks-used-in-other-studes" class="nav-link" data-scroll-target="#tasks-used-in-other-studes">Tasks used in Other Studes</a>
  <ul class="collapse">
  <li><a href="#structuring-knowledge-with-cognitive-maps-and-cognitive-graphs." id="toc-structuring-knowledge-with-cognitive-maps-and-cognitive-graphs." class="nav-link" data-scroll-target="#structuring-knowledge-with-cognitive-maps-and-cognitive-graphs.">Structuring Knowledge with Cognitive Maps and Cognitive Graphs.</a></li>
  <li><a href="#wormholes-in-virtual-space-from-cognitive-maps-to-cognitive-graphs" id="toc-wormholes-in-virtual-space-from-cognitive-maps-to-cognitive-graphs" class="nav-link" data-scroll-target="#wormholes-in-virtual-space-from-cognitive-maps-to-cognitive-graphs">Wormholes in virtual space: From cognitive maps to cognitive graphs</a></li>
  <li><a href="#route-effects-in-city-based-survey-knowledge-estimates" id="toc-route-effects-in-city-based-survey-knowledge-estimates" class="nav-link" data-scroll-target="#route-effects-in-city-based-survey-knowledge-estimates">Route effects in city-based survey knowledge estimates</a></li>
  <li><a href="#spatial-decision-dynamics-during-wayfinding-intersections-prompt-the-decision-making-process." id="toc-spatial-decision-dynamics-during-wayfinding-intersections-prompt-the-decision-making-process." class="nav-link" data-scroll-target="#spatial-decision-dynamics-during-wayfinding-intersections-prompt-the-decision-making-process.">Spatial decision dynamics during wayfinding: Intersections prompt the decision-making process.</a></li>
  <li><a href="#rational-use-of-cognitive-resources-in-human-planning" id="toc-rational-use-of-cognitive-resources-in-human-planning" class="nav-link" data-scroll-target="#rational-use-of-cognitive-resources-in-human-planning">Rational use of cognitive resources in human planning</a></li>
  <li><a href="#people-construct-simplified-mental-representations-to-plan." id="toc-people-construct-simplified-mental-representations-to-plan." class="nav-link" data-scroll-target="#people-construct-simplified-mental-representations-to-plan.">People construct simplified mental representations to plan.</a></li>
  <li><a href="#emergent-collective-sensing-in-human-groups." id="toc-emergent-collective-sensing-in-human-groups." class="nav-link" data-scroll-target="#emergent-collective-sensing-in-human-groups.">Emergent Collective Sensing in Human Groups.</a></li>
  <li><a href="#towards-a-computational-model-of-responsibility-judgments-in-sequential-human-ai-collaboration" id="toc-towards-a-computational-model-of-responsibility-judgments-in-sequential-human-ai-collaboration" class="nav-link" data-scroll-target="#towards-a-computational-model-of-responsibility-judgments-in-sequential-human-ai-collaboration">Towards a computational model of responsibility judgments in sequential human-AI collaboration</a></li>
  <li><a href="#pattern-driven-navigation-in-2d-multiscale-visualizations-with-scalable-insets." id="toc-pattern-driven-navigation-in-2d-multiscale-visualizations-with-scalable-insets." class="nav-link" data-scroll-target="#pattern-driven-navigation-in-2d-multiscale-visualizations-with-scalable-insets.">Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets.</a></li>
  <li><a href="#personality-traits-and-spatial-skills-are-related-to-group-dynamics-and-success-during-collective-wayfinding" id="toc-personality-traits-and-spatial-skills-are-related-to-group-dynamics-and-success-during-collective-wayfinding" class="nav-link" data-scroll-target="#personality-traits-and-spatial-skills-are-related-to-group-dynamics-and-success-during-collective-wayfinding">Personality Traits and Spatial Skills Are Related to Group Dynamics and Success During Collective Wayfinding</a></li>
  <li><a href="#wayfinding-in-pairs-comparing-the-planning-and-navigation-performance-of-dyads-and-individuals-in-a-real-world-environment." id="toc-wayfinding-in-pairs-comparing-the-planning-and-navigation-performance-of-dyads-and-individuals-in-a-real-world-environment." class="nav-link" data-scroll-target="#wayfinding-in-pairs-comparing-the-planning-and-navigation-performance-of-dyads-and-individuals-in-a-real-world-environment.">Wayfinding in pairs: Comparing the planning and navigation performance of dyads and individuals in a real-world environment.</a></li>
  <li><a href="#individual-and-collective-foraging-in-autonomous-search-agents-with-human-intervention" id="toc-individual-and-collective-foraging-in-autonomous-search-agents-with-human-intervention" class="nav-link" data-scroll-target="#individual-and-collective-foraging-in-autonomous-search-agents-with-human-intervention">Individual and collective foraging in autonomous search agents with human intervention</a></li>
  </ul></li>
  <li><a href="#task-brainstorming" id="toc-task-brainstorming" class="nav-link" data-scroll-target="#task-brainstorming">Task Brainstorming</a>
  <ul class="collapse">
  <li><a href="#original" id="toc-original" class="nav-link" data-scroll-target="#original">Original</a></li>
  <li><a href="#alt-2" id="toc-alt-2" class="nav-link" data-scroll-target="#alt-2">Alt 2</a></li>
  <li><a href="#alt-2---larger" id="toc-alt-2---larger" class="nav-link" data-scroll-target="#alt-2---larger">Alt 2 - larger</a></li>
  <li><a href="#alt-2b" id="toc-alt-2b" class="nav-link" data-scroll-target="#alt-2b">Alt 2b</a></li>
  <li><a href="#alt-2c" id="toc-alt-2c" class="nav-link" data-scroll-target="#alt-2c">Alt 2c</a></li>
  <li><a href="#alt-3" id="toc-alt-3" class="nav-link" data-scroll-target="#alt-3">Alt 3</a></li>
  <li><a href="#alt-3b" id="toc-alt-3b" class="nav-link" data-scroll-target="#alt-3b">Alt 3b</a></li>
  <li><a href="#alt-3c" id="toc-alt-3c" class="nav-link" data-scroll-target="#alt-3c">Alt 3c</a></li>
  </ul></li>
  </ul>
<div class="quarto-alternate-formats"><h2>Other Formats</h2><ul><li><a href="Samuel_Project.md"><i class="bi bi-file-code"></i>Github (GFM)</a></li><li><a href="Samuel_Project.pdf"><i class="bi bi-file-pdf"></i>PDF</a></li><li><a href="samuel_project.docx"><i class="bi bi-file-word"></i>MS Word</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default"><nav class="quarto-page-breadcrumbs quarto-title-breadcrumbs d-none d-lg-block" aria-label="breadcrumb"><ol class="breadcrumb"><li class="breadcrumb-item"><a href="./tms.html">Misc</a></li><li class="breadcrumb-item"><a href="./Samuel_Project.html">Samuel’s Project</a></li></ol></nav>
<div class="quarto-title">
<h1 class="title">Samuel’s Project</h1>
</div>


<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://tegorman13.github.io/">Thomas E. Gorman</a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://web.ics.purdue.edu/~treimer/">
            Communication and Cognition Lab, Purdue University, USA
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 18, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<hr>
<section id="literature" class="level1">
<h1>Literature</h1>
<section id="social-influence" class="level2">
<h2 class="anchored" data-anchor-id="social-influence">Social Influence</h2>
<section id="visual-spatial-dynamics-drive-adaptive-social-learning-in-immersive-environments" class="level3">
<h3 class="anchored" data-anchor-id="visual-spatial-dynamics-drive-adaptive-social-learning-in-immersive-environments">Visual-spatial dynamics drive adaptive social learning in immersive environments</h3>
<p>Wu, C. M., Deffner, D., Kahl, B., Meder, B., Ho, M. H., &amp; Kurvers, R. H. J. M. (2023). <strong>Visual-spatial dynamics drive adaptive social learning in immersive environments</strong> [Preprint]. https://doi.org/10.1101/2023.06.28.546887</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Humans are uniquely capable social learners. Our capacity to learn from others across short and long timescales is a driving force behind the success of our species. Yet there are seemingly maladaptive patterns of human social learning, characterized by both overreliance and underreliance on social information. Recent advances in animal research have incorporated rich visual and spatial dynamics to study social learning in ecological contexts, showing how simple mechanisms can give rise to intelligent group dynamics. However, similar techniques have yet to be translated into human research, which additionally requires integrating the sophistication of human individual and social learning mechanisms. Thus, it is still largely unknown how humans dynamically adapt social learning strategies to different environments and how group dynamics emerge under realistic conditions. Here, we use a collective foraging experiment in an immersive Minecraft environment to provide unique insights into how visual-spatial interactions give rise to adaptive, specialized, and selective social learning. Our analyses show how groups adapt to the demands of the environment through specialization of learning strategies rather than homogeneity and through the adaptive deployment of selective imitation rather than indiscriminate copying. We test these mechanisms using computational modeling, providing a deeper understanding of the cognitive mechanisms that dynamically inﬂuence social decision-making in ecological contexts. All results are compared against an asocial baseline, allowing us to specify specialization and selective attention as uniquely social phenomena, which provide the adaptive foundations of human social learning.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/2024-11-18-18-45-33.png" class="lightbox" data-gallery="quarto-lightbox-gallery-1" title="Figure from @wuVisualspatialDynamicsDrive2023"><img src="images/2024-11-18-18-45-33.png" class="img-fluid figure-img" alt="Figure from Wu et al. (2023)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="wuVisualspatialDynamicsDrive2023">Wu et al. (<a href="#ref-wuVisualspatialDynamicsDrive2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="specialization-and-selective-social-attention-establishes-the-balance-between-individual-and-social-learning." class="level3">
<h3 class="anchored" data-anchor-id="specialization-and-selective-social-attention-establishes-the-balance-between-individual-and-social-learning.">Specialization and selective social attention establishes the balance between individual and social learning.</h3>
<p>Wu, C. M., Ho, M. K., Kahl, B., Leuker, C., Meder, B., &amp; Kurvers, R. H. J. M. (2021). <strong>Specialization and selective social attention establishes the balance between individual and social learning.</strong> Proceedings of the 43rd Annual Conference of the Cognitive Science Society, 1921–1927. https://doi.org/10.1101/2021.02.03.429553</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>A key question individuals face in any social learning environment is when to innovate alone and when to imitate others. Previous simulation results have found that the best performing groups exhibit an intermediate balance, yet it is still largely unknown how individuals collectively negotiate this balance. We use an immersive collective foraging experiment, implemented in the Minecraft game engine, facilitating unprecedented access to spatial trajectories and visual ﬁeld data. The virtual environment imposes a limited ﬁeld of view, creating a natural trade-off between allocating visual attention towards individual innovation or to look towards peers for social imitation. By analyzing foraging patterns, social interactions (visual and spatial), and social inﬂuence, we shine new light on how groups collectively adapt to the ﬂuctuating demands of the environment through specialization and selective imitation, rather than homogeneity and indiscriminate copying of others.</p>
</div>
</details>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/_rDE49k1ENM" width="700" height="550" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
</section>
<section id="collective-incentives-reduce-over-exploitation-of-social-information-in-unconstrained-human-groups." class="level3">
<h3 class="anchored" data-anchor-id="collective-incentives-reduce-over-exploitation-of-social-information-in-unconstrained-human-groups.">Collective incentives reduce over-exploitation of social information in unconstrained human groups.</h3>
<p>Deffner, D., Mezey, D., Kahl, B., Schakowski, A., Romanczuk, P., Wu, C. M., &amp; Kurvers, R. H. J. M. (2024). <strong>Collective incentives reduce over-exploitation of social information in unconstrained human groups.</strong> Nature Communications, 15(1), 2683. https://doi.org/10.1038/s41467-024-47010-3</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Collective dynamics emerge from countless individual decisions. Yet, we poorly understand the processes governing dynamically-interacting individuals in human collectives under realistic conditions. We present a naturalistic immersive-reality experiment where groups of participants searched for rewards in different environments, studying how individuals weigh personal and social information and how this shapes individual and collective outcomes. Capturing high-resolution visual-spatial data, behavioral analyses revealed individual-level gains—but group-level losses—of high social information use and spatial proximity in environments with concentrated (vs.&nbsp;distributed) resources. Incentivizing participants at the group (vs.&nbsp;individual) level facilitated adaptation to concentrated environments, buffering apparently excessive scrounging. To infer discrete choices from unconstrained interactions and uncover the underlying decision mechanisms, we developed an unsupervised Social Hidden Markov Decision model. Computational results showed that participants were more sensitive to social information in concentrated environments frequently switching to a social relocation state where they approach successful group members. Group-level incentives reduced participants’ overall responsiveness to social information and promoted higher selectivity over time. Finally, mapping group-level spatio-temporal dynamics through time-lagged regressions revealed a collective exploration-exploitation trade-off across different timescales. Our study unravels the processes linking individual-level strategies to emerging collective dynamics, and provides tools to investigate decision-making in freely-interacting collectives.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/2024-11-18-18-42-07.png" class="lightbox" data-gallery="quarto-lightbox-gallery-2" title="Figure from @deffnerCollectiveIncentivesReduce2024"><img src="images/2024-11-18-18-42-07.png" class="img-fluid figure-img" alt="Figure from Deffner et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="deffnerCollectiveIncentivesReduce2024">Deffner et al. (<a href="#ref-deffnerCollectiveIncentivesReduce2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="insights-about-the-common-generative-rule-underlying-an-information-foraging-task-can-be-facilitated-via-collective-search." class="level3">
<h3 class="anchored" data-anchor-id="insights-about-the-common-generative-rule-underlying-an-information-foraging-task-can-be-facilitated-via-collective-search.">Insights about the common generative rule underlying an information foraging task can be facilitated via collective search.</h3>
<p>Naito, A., Katahira, K., &amp; Kameda, T. (2022). <strong>Insights about the common generative rule underlying an information foraging task can be facilitated via collective search.</strong> Scientific Reports, 12(1), 8047.</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Social learning is beneficial for efficient information search in unfamiliar environments (“within-task” learning). In the real world, however, possible search spaces are often so large that decision makers are incapable of covering all options, even if they pool their information collectively. One strategy to handle such overload is developing generalizable knowledge that extends to multiple related environments (“across-task” learning). However, it is unknown whether and how social information may facilitate such across-task learning. Here, we investigated participants’ social learning processes across multiple laboratory foraging sessions in spatially correlated reward landscapes that were generated according to a common rule. The results showed that paired participants were able to improve efficiency in information search across sessions more than solo participants. Computational analysis of participants’ choice-behaviors revealed that such improvement across sessions was related to better understanding of the common generative rule. Rule understanding was correlated within a pair, suggesting that social interaction is a key to the improvement of across-task learning.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/2024-11-18-19-03-56.png" class="lightbox" data-gallery="quarto-lightbox-gallery-3" title="Figure from @naitoInsightsCommonGenerative2022a"><img src="images/2024-11-18-19-03-56.png" class="img-fluid figure-img" alt="Figure from Naito et al. (2022)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="naitoInsightsCommonGenerative2022a">Naito et al. (<a href="#ref-naitoInsightsCommonGenerative2022a" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="individualism-versus-collective-movement-during-travel." class="level3">
<h3 class="anchored" data-anchor-id="individualism-versus-collective-movement-during-travel.">Individualism versus collective movement during travel.</h3>
<p>Doherty, C. T. M., &amp; Laidre, M. E. (2022). <strong>Individualism versus collective movement during travel.</strong> Scientific Reports, 12(1), 7508. https://doi.org/10.1038/s41598-022-11469-1</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Collective movement may emerge if coordinating one’s movement with others produces a greater benefit to oneself than can be achieved alone. Experimentally, the capacity to manoeuvre simulated groups in the wild could enable powerful tests of the impact of collective movement on individual decisions. Yet such experiments are currently lacking due to the inherent difficulty of controlling whole collectives. Here we used a novel technique of experimentally simulating the movement of collectives of social hermit crabs (Coenobita compressus) in the wild. Using large architectural arrays of shells dragged across the beach, we generated synchronous collective movement and systematically varied the simulated collective’s travel direction as well as the context (i.e., danger level). With drone video from above, we then tested whether focal individuals were biased in their movement by the collective. We found that, despite considerable engagement with the collective, individuals’ direction was not significantly biased. Instead, individuals expressed substantial variability across all stimulus directions and contexts. Notably, individuals typically achieved shorter displacements in the presence of the collective versus in the presence of the control stimulus, suggesting an impact of traffic. The absence of a directional bias in individual movement due to the collective suggests that social hermit crabs are individualists, which move with a high level of opportunistic independence, likely thanks to the personal architecture and armour they carry in the form of a protective shell. Future studies can manipulate this level of armour to test its role in autonomy of movement, including the consequences of shell architecture for social decisions. Our novel experimental approach can be used to ask many further questions about how and why collective and individual movement interact.</p>
</div>
</details>
<hr>
</section>
<section id="beyond-the-individual-a-social-foraging-framework-to-study-decisions-in-groups." class="level3">
<h3 class="anchored" data-anchor-id="beyond-the-individual-a-social-foraging-framework-to-study-decisions-in-groups.">Beyond the individual: A social foraging framework to study decisions in groups.</h3>
<p>Garg, K., Deng, W., &amp; Mobbs, D. (2024). <strong>Beyond the individual: A social foraging framework to study decisions in groups.</strong> OSF. https://doi.org/10.31219/osf.io/rmqyb</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>A key goal of the behavioral sciences is to understand how agents decide between rewarding, hazardous, and conflicting options. Foraging theory, which is rooted in ecology and evolutionary theory, has helped advance this pursuit but has largely been limited to the study of the individual. In this Perspective, we extend beyond an individual. We propose social foraging as a promising avenue to study social decisions, or decisions within a social context. Recent research has already applied similar paradigms to study social behavior in naturalistic conditions. We synthesize the key socio-cognitive elements involved in social foraging that can be further studied through foraging paradigms. We then propose a social foraging framework that distinguishes between the asocial and social components involved in the decision-making process and describes their integration. Our framework bridges research across disciplines to provide a promising new avenue for the study of social behavior by linking decisions across different scales, from individuals to collectives.</p>
</div>
</details>
<p><a href="images/2024-11-18-18-15-52.png" class="lightbox" data-gallery="quarto-lightbox-gallery-4"><img src="images/2024-11-18-18-15-52.png" class="img-fluid"></a></p>
<div id="fig-garg" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-garg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: flex-start;">
<a href="images/2024-11-18-18-19-19.png" class="lightbox" data-gallery="quarto-lightbox-gallery-5" title="Figure&nbsp;1: Figures from @gargIndividualSocialForaging2024"><img src="images/2024-11-18-18-19-19.png" class="img-fluid figure-img"></a>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-garg-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Figures from <span class="citation" data-cites="gargIndividualSocialForaging2024">Garg et al. (<a href="#ref-gargIndividualSocialForaging2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
<hr>
</section>
</section>
<section id="complexity" class="level2">
<h2 class="anchored" data-anchor-id="complexity">Complexity</h2>
<section id="task-complexity-moderates-group-synergy." class="level3">
<h3 class="anchored" data-anchor-id="task-complexity-moderates-group-synergy.">Task complexity moderates group synergy.</h3>
<p>Almaatouq, A., Alsobay, M., Yin, M., &amp; Watts, D. J. (2021). <strong>Task complexity moderates group synergy.</strong> Proceedings of the National Academy of Sciences, 118(36). https://doi.org/10.1073/pnas.2101062118</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Complexity—defined in terms of the number of components and the nature of the interdependencies between them—is clearly a relevant feature of all tasks that groups perform. Yet the role that task complexity plays in determining group performance remains poorly understood, in part because no clear language exists to express complexity in a way that allows for straightforward comparisons across tasks. Here we avoid this analytical difficulty by identifying a class of tasks for which complexity can be varied systematically while keeping all other elements of the task unchanged. We then test the effects of task complexity in a preregistered two-phase experiment in which 1,200 individuals were evaluated on a series of tasks of varying complexity (phase 1) and then randomly assigned to solve similar tasks either in interacting groups or as independent individuals (phase 2). We find that interacting groups are as fast as the fastest individual and more efficient than the most efficient individual for complex tasks but not for simpler ones. Leveraging our highly granular digital data, we define and precisely measure group process losses and synergistic gains and show that the balance between the two switches signs at intermediate values of task complexity. Finally, we find that interacting groups generate more solutions more rapidly and explore the solution space more broadly than independent problem solvers, finding higher-quality solutions than all but the highest-scoring individuals.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/2024-11-18-18-02-52.png" class="lightbox" data-gallery="quarto-lightbox-gallery-6" title="Figure from @almaatouqTaskComplexityModerates2021"><img src="images/2024-11-18-18-02-52.png" class="img-fluid figure-img" alt="Figure from Almaatouq et al. (2021)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="almaatouqTaskComplexityModerates2021">Almaatouq et al. (<a href="#ref-almaatouqTaskComplexityModerates2021" role="doc-biblioref">2021</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="the-interaction-between-map-complexity-and-crowd-movement-on-navigation-decisions-in-virtual-reality." class="level3">
<h3 class="anchored" data-anchor-id="the-interaction-between-map-complexity-and-crowd-movement-on-navigation-decisions-in-virtual-reality.">The interaction between map complexity and crowd movement on navigation decisions in virtual reality.</h3>
<p>Zhao, H., Thrash, T., Grossrieder, A., Kapadia, M., Moussaïd, M., Hölscher, C., &amp; Schinazi, V. R. (2020). <strong>The interaction between map complexity and crowd movement on navigation decisions in virtual reality.</strong> Royal Society Open Science, 7(3), 191523. https://doi.org/10.1098/rsos.191523</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>A carefully designed map can reduce pedestrians’ cognitive load during wayfinding and may be an especially useful navigation aid in crowded public environments. In the present paper, we report three studies that investigated the effects of map complexity and crowd movement on wayfinding time, accuracy and hesitation using both online and laboratory-based networked virtual reality (VR) platforms. In the online study, we found that simple map designs led to shorter decision times and higher accuracy compared to complex map designs. In the networked VR set-up, we found that co-present participants made very few errors. In the final VR study, we replayed the traces of participants’ avatars from the second study so that they indicated a different direction than the maps. In this scenario, we found an interaction between map design and crowd movement in terms of decision time and the distributions of locations at which participants hesitated. Together, these findings can help the designers of maps for public spaces account for the movements of real crowds.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/2024-11-18-18-10-44.png" class="lightbox" data-gallery="quarto-lightbox-gallery-7" title="Figure from @zhaoInteractionMapComplexity2020"><img src="images/2024-11-18-18-10-44.png" class="img-fluid figure-img" alt="Figure from Zhao et al. (2020)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="zhaoInteractionMapComplexity2020">Zhao et al. (<a href="#ref-zhaoInteractionMapComplexity2020" role="doc-biblioref">2020</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="task-complexity-and-performance-in-individuals-and-groups-without-communication." class="level3">
<h3 class="anchored" data-anchor-id="task-complexity-and-performance-in-individuals-and-groups-without-communication.">Task Complexity and Performance in Individuals and Groups Without Communication.</h3>
<p>Gulati, A., Nguyen, T. N., &amp; Gonzalez, C. (2021). <strong>Task Complexity and Performance in Individuals and Groups Without Communication.</strong> AAAI Fall Symposium. Cham: Springer Nature Switzerland, 8.</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>While groups where members communicate with each other may perform better than groups without communication, there are multiple scenarios where communication between group members is not possible. Our work analyses the impact of task complexity on individuals and groups of different sizes while solving a goal-seeking navigation task without communication. Our major goal is to determine the effect of task complexity on performance and whether agents in a group are able to coordinate to perform the task more effectively despite the lack of communication. We developed a cognitive model of each individual agent that performs the task. We compare the performance of this agent with individual human performance, who worked on the same task. We observe that the cognitive agent is able to replicate the general behavioral trends observed in humans. Using this cognitive model, we generate groups of different sizes where individual agents work in the same goal-seeking task independently and without communication. First, we observe that increasing task complexity by design does not necessarily lead to worse performance in individuals and groups. We also observe that larger groups perform better than smaller groups and individuals alone. However, individual agents within a group perform worse than an agent working on the task alone. This effect is not the result of agents within a group covering less ground in the task compared to individuals alone. Rather, it is an effect resulting from the overlap of the agents within a group. Importantly, agents learn to reduce their overlap and improve their performance without explicit communication. These results can inform the design of AI agents in humanmachine teams.</p>
</div>
</details>
<p><span class="citation" data-cites="gulatiTaskComplexityPerformance2021">Gulati et al. (<a href="#ref-gulatiTaskComplexityPerformance2021" role="doc-biblioref">2021</a>)</span></p>
<hr>
</section>
<section id="environmental-memory-boosts-group-formation-of-clueless-individuals." class="level3">
<h3 class="anchored" data-anchor-id="environmental-memory-boosts-group-formation-of-clueless-individuals.">Environmental memory boosts group formation of clueless individuals.</h3>
<p>Dias, C. S., Trivedi, M., Volpe, G., Araújo, N. A. M., &amp; Volpe, G. (2023). <strong>Environmental memory boosts group formation of clueless individuals.</strong> Nature Communications, 14(1), 7324. https://doi.org/10.1038/s41467-023-43099-0</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>The formation of groups of interacting individuals improves performance and fitness in many decentralised systems, from micro-organisms to social insects, from robotic swarms to artificial intelligence algorithms. Often, group formation and high-level coordination in these systems emerge from individuals with limited information-processing capabilities implementing low-level rules of communication to signal to each other. Here, we show that, even in a community of clueless individuals incapable of processing information and communicating, a dynamic environment can coordinate group formation by transiently storing memory of the earlier passage of individuals. Our results identify a new mechanism of indirect coordination via shared memory that is primarily promoted and reinforced by dynamic environmental factors, thus overshadowing the need for any form of explicit signalling between individuals. We expect this pathway to group formation to be relevant for understanding and controlling self-organisation and collective decision making in both living and artificial active matter in real-life environments.</p>
</div>
</details>
</section>
</section>
<section id="uncertainty" class="level2">
<h2 class="anchored" data-anchor-id="uncertainty">Uncertainty</h2>
<hr>
<section id="flexible-social-inference-facilitates-targeted-social-learning-when-rewards-are-not-observable." class="level3">
<h3 class="anchored" data-anchor-id="flexible-social-inference-facilitates-targeted-social-learning-when-rewards-are-not-observable.">Flexible social inference facilitates targeted social learning when rewards are not observable.</h3>
<p>Hawkins, R. D., Berdahl, A. M., Pentland, A. ‘Sandy,’ Tenenbaum, J. B., Goodman, N. D., &amp; Krafft, P. M. (2023). <strong>Flexible social inference facilitates targeted social learning when rewards are not observable.</strong> Nature Human Behaviour, 7(10), 1767–1776. https://doi.org/10.1038/s41562-023-01682-x</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Groups coordinate more effectively when individuals are able to learn from others’ successes. But acquiring such knowledge is not always easy, especially in real-world environments where success is hidden from public view. We suggest that social inference capacities may help bridge this gap, allowing individuals to update their beliefs about others’ underlying knowledge and success from observable trajectories of behaviour. We compared our social inference model against simpler heuristics in three studies of human behaviour in a collective-sensing task. Experiment 1 demonstrated that average performance improved as a function of group size at a rate greater than predicted by heuristic models. Experiment 2 introduced artificial agents to evaluate how individuals selectively rely on social information. Experiment 3 generalized these findings to a more complex reward landscape. Taken together, our findings provide insight into the relationship between individual social cognition and the flexibility of collective behaviour.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/2024-11-18-19-01-02.png" class="lightbox" data-gallery="quarto-lightbox-gallery-8" title="Figure from @hawkinsFlexibleSocialInference2023"><img src="images/2024-11-18-19-01-02.png" class="img-fluid figure-img" alt="Figure from Hawkins et al. (2023)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="hawkinsFlexibleSocialInference2023">Hawkins et al. (<a href="#ref-hawkinsFlexibleSocialInference2023" role="doc-biblioref">2023</a>)</span></figcaption>
</figure>
</div>
<hr>
<p>Tump, A. N., Wu, C. M., Bouhlel, I., &amp; Goldstone, R. L. (2019). <strong>The Evolutionary Dynamics of Cooperation in Collective Search</strong> [Preprint]. http://biorxiv.org/lookup/doi/10.1101/538447</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>How does cooperation arise in an evolutionary context? We approach this problem using a collective search paradigm where interactions are dynamic and there is competition for rewards. Using evolutionary simulations, we ﬁnd that the unconditional sharing of information can be an evolutionary advantageous strategy without the need for conditional strategies or explicit reciprocation. Shared information acts as a recruitment signal and facilitates the formation of a self-organized group. Thus, the improved search efﬁciency of the collective bestows byproduct beneﬁts onto the original sharer. A key mechanism is a visibility radius, where individuals have unconditional access to information about neighbors within a limited distance. Our results show that for a variety of initial conditions—including populations initially devoid of prosocial individuals—and across both static and dynamic ﬁtness landscapes, we ﬁnd strong selection pressure to evolve unconditional sharing.</p>
</div>
</details>
<hr>
</section>
<section id="spatial-planning-with-long-visual-range-benefits-escape-from-visual-predators-in-complex-naturalistic-environments." class="level3">
<h3 class="anchored" data-anchor-id="spatial-planning-with-long-visual-range-benefits-escape-from-visual-predators-in-complex-naturalistic-environments.">Spatial planning with long visual range benefits escape from visual predators in complex naturalistic environments.</h3>
<p>Mugan, U., &amp; MacIver, M. A. (2020). <strong>Spatial planning with long visual range benefits escape from visual predators in complex naturalistic environments.</strong> Nature Communications, 11(1), 3057. https://doi.org/10.1038/s41467-020-16102-1</p>
<p><a href="https://maciver-lab.github.io/plangame/">live task demo</a><br>
<a href="https://github.com/MacIver-Lab/plangame">code repository</a><br>
</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>It is uncontroversial that land animals have more elaborated cognitive abilities than their aquatic counterparts such as fish. Yet there is no apparent a-priori reason for this. A key cognitive faculty is planning. We show that in visually guided predator-prey interactions, planning provides a significant advantage, but only on land. During animal evolution, the water-to-land transition resulted in a massive increase in visual range. Simulations of behavior identify a specific type of terrestrial habitat, clustered open and closed areas (savanna-like), where the advantage of planning peaks. Our computational experiments demonstrate how this patchy terrestrial structure, in combination with enhanced visual range, can reveal and hide agents as a function of their movement and create a selective benefit for imagining, evaluating, and selecting among possible future scenarios—in short, for planning. The vertebrate invasion of land may have been an important step in their cognitive evolution.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Mugan_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-9" title="Figure from @muganSpatialPlanningLong2020"><img src="images/Mugan_drive1.png" class="img-fluid figure-img" alt="Figure from Mugan &amp; MacIver (2020)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="muganSpatialPlanningLong2020">Mugan &amp; MacIver (<a href="#ref-muganSpatialPlanningLong2020" role="doc-biblioref">2020</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="the-form-of-uncertainty-affects-selection-for-social-learning." class="level3">
<h3 class="anchored" data-anchor-id="the-form-of-uncertainty-affects-selection-for-social-learning.">The form of uncertainty affects selection for social learning.</h3>
<p>Turner, M. A., Moya, C., Smaldino, P. E., &amp; Jones, J. H. (2023). <strong>The form of uncertainty affects selection for social learning.</strong> Evolutionary Human Sciences, 5, e20. https://doi.org/10.1017/ehs.2023.11</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Social learning is a critical adaptation for dealing with different forms of variability. Uncertainty is a severe form of variability where the space of possible decisions or probabilities of associated outcomes are unknown. We identified four theoretically important sources of uncertainty: temporal environmental variability; payoff ambiguity; selection-set size; and effective lifespan. When these combine, it is nearly impossible to fully learn about the environment. We develop an evolutionary agent-based model to test how each form of uncertainty affects the evolution of social learning. Agents perform one of several behaviours, modelled as a multi-armed bandit, to acquire payoffs. All agents learn about behavioural payoffs individually through an adaptive behaviour-choice model that uses a softmax decision rule. Use of vertical and oblique payoff-biased social learning evolved to serve as a scaffold for adaptive individual learning – they are not opposite strategies. Different types of uncertainty had varying effects. Temporal environmental variability suppressed social learning, whereas larger selection-set size promoted social learning, even when the environment changed frequently. Payoff ambiguity and lifespan interacted with other uncertainty parameters. This study begins to explain how social learning can predominate despite highly variable real-world environments when effective individual learning helps individuals recover from learning outdated social information.</p>
</div>
</details>
<hr>
</section>
</section>
<section id="peters-talks-on-foraging" class="level2">
<h2 class="anchored" data-anchor-id="peters-talks-on-foraging">Peter’s Talks on Foraging</h2>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/McNrmZdEO0w" width="700" height="550" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/H8WLS3S1rAo" width="700" height="550" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/yL-kf2IGbxk" width="700" height="550" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<div class="quarto-video"><iframe data-external="1" src="https://www.youtube.com/embed/Ay9ydPF4UAg" width="700" height="550" title="" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen=""></iframe></div>
<hr>
</section>
</section>
<section id="tasks-used-in-other-studes" class="level1">
<h1>Tasks used in Other Studes</h1>
<section id="structuring-knowledge-with-cognitive-maps-and-cognitive-graphs." class="level3">
<h3 class="anchored" data-anchor-id="structuring-knowledge-with-cognitive-maps-and-cognitive-graphs.">Structuring Knowledge with Cognitive Maps and Cognitive Graphs.</h3>
<p>Peer, M., Brunec, I. K., Newcombe, N. S., &amp; Epstein, R. A. (2021). <strong>Structuring Knowledge with Cognitive Maps and Cognitive Graphs.</strong> Trends in Cognitive Sciences, 25(1), 37–54. https://doi.org/10.1016/j.tics.2020.10.004</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Humans and animals use mental representations of the spatial structure of the world to navigate. The classical view is that these representations take the form of Euclidean cognitive maps, but alternative theories suggest that they are cognitive graphs consisting of locations connected by paths. We review evidence suggesting that both map-like and graph-like representations exist in the mind/brain that rely on partially overlapping neural systems. Maps and graphs can operate simultaneously or separately, and they may be applied to both spatial and nonspatial knowledge. By providing structural frameworks for complex information, cognitive maps and cognitive graphs may provide fundamental organizing schemata that allow us to navigate in physical, social, and conceptual spaces.</p>
</div>
</details>
<div id="fig-peer" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-peer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/peer1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-10" title="Figure&nbsp;2: Figures from @peerStructuringKnowledgeCognitive2021"><img src="images/peer1.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/peer2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-11" title="Figure&nbsp;2: Figures from @peerStructuringKnowledgeCognitive2021"><img src="images/peer2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/peer3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-12" title="Figure&nbsp;2: Figures from @peerStructuringKnowledgeCognitive2021"><img src="images/peer3.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-peer-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Figures from <span class="citation" data-cites="peerStructuringKnowledgeCognitive2021">Peer et al. (<a href="#ref-peerStructuringKnowledgeCognitive2021" role="doc-biblioref">2021</a>)</span>
</figcaption>
</figure>
</div>
<hr>
</section>
<section id="wormholes-in-virtual-space-from-cognitive-maps-to-cognitive-graphs" class="level3">
<h3 class="anchored" data-anchor-id="wormholes-in-virtual-space-from-cognitive-maps-to-cognitive-graphs">Wormholes in virtual space: From cognitive maps to cognitive graphs</h3>
<p>Warren, W. H., Rothman, D. B., Schnapp, B. H., &amp; Ericson, J. D. (2017). <strong>Wormholes in virtual space: From cognitive maps to cognitive graphs.</strong> Cognition, 166, 152–163. https://doi.org/10.1016/j.cognition.2017.05.020</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Humans and other animals build up spatial knowledge of the environment on the basis of visual information and path integration. We compare three hypotheses about the geometry of this knowledge of navigation space: (a) ‘cognitive map’ with metric Euclidean structure and a consistent coordinate system, (b) ‘topological graph’ or network of paths between places, and (c) ‘labelled graph’ incorporating local metric information about path lengths and junction angles. In two experiments, participants walked in a non-Euclidean environment, a virtual hedge maze containing two ‘wormholes’ that visually rotated and teleported them between locations. During training, they learned the metric locations of eight target objects from a ‘home’ location, which were visible individually. During testing, shorter wormhole routes to a target were preferred, and novel shortcuts were directional, contrary to the topological hypothesis. Shortcuts were strongly biased by the wormholes, with mean constant errors of 37° and 41° (45° expected), revealing violations of the metric postulates in spatial knowledge. In addition, shortcuts to targets near wormholes shifted relative to flanking targets, revealing ‘rips’ (86% of cases), ‘folds’ (91%), and ordinal reversals (66%) in spatial knowledge. Moreover, participants were completely unaware of these geometric inconsistencies, reflecting a surprising insensitivity to Euclidean structure. The probability of the shortcut data under the Euclidean map model and labelled graph model indicated decisive support for the latter (BFGM&gt;100). We conclude that knowledge of navigation space is best characterized by a labelled graph, in which local metric information is approximate, geometrically inconsistent, and not embedded in a common coordinate system. This class of ‘cognitive graph’ models supports route finding, novel detours, and rough shortcuts, and has the potential to unify a range of data on spatial navigation.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Warren_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-13" title="Figure from Warren et al.&nbsp;2017"><img src="images/Warren_drive1.png" class="img-fluid figure-img" alt="Figure from Warren et al.&nbsp;2017"></a></p>
<figcaption>Figure from Warren et al.&nbsp;2017</figcaption>
</figure>
</div>
<hr>
</section>
<section id="route-effects-in-city-based-survey-knowledge-estimates" class="level3">
<h3 class="anchored" data-anchor-id="route-effects-in-city-based-survey-knowledge-estimates">Route effects in city-based survey knowledge estimates</h3>
<p>Krukar, J., Navas Medrano, S., &amp; Schwering, A. (2023). <strong>Route effects in city-based survey knowledge estimates.</strong> Cognitive Processing, 24(2), 213–231. https://doi.org/10.1007/s10339-022-01122-0</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>When studying wayfinding in urban environments, researchers are often interested in obtaining measures of participants’ survey knowledge, i.e., their estimate of distant locations relative to other places. Previous work showed that distance estimations are consistently biased when no direct route is available to the queried target or when participants follow a detour. Here we investigated whether a corresponding bias is manifested in two other popular measures of survey knowledge: a pointing task and a sketchmapping task. The aim of this study was to investigate whether there is a systematic bias in pointing/sketchmapping performance associated with the preferred route choice in an applied urban setting. The results were mixed. We found moderate evidence for the presence of a systematic bias, but only for a subset of urban locations. When two plausible routes to the target were available, survey knowledge estimates were significantly biased in the direction of the route chosen by the participant. When only one plausible route was available, we did not find a statistically significant pattern. The results may have methodological implications for spatial cognition studies in applied urban settings that might be obtaining systematically biased survey knowledge estimates at some urban locations. Researchers should be aware that the choice of urban locations from which pointing and sketchmapping are performed might systematically distort the results, in particular when two plausible but diverging routes to the target are visible from the location.</p>
</div>
</details>
<div id="fig-krukar" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-krukar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Krukar1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-14" title="Figure&nbsp;3: Figures from @krukarRouteEffectsCitybased2023"><img src="images/Krukar1.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Krukar2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-15" title="Figure&nbsp;3: Figures from @krukarRouteEffectsCitybased2023"><img src="images/Krukar2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-krukar-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Figures from <span class="citation" data-cites="krukarRouteEffectsCitybased2023">Krukar et al. (<a href="#ref-krukarRouteEffectsCitybased2023" role="doc-biblioref">2023</a>)</span>
</figcaption>
</figure>
</div>
<hr>
</section>
<section id="spatial-decision-dynamics-during-wayfinding-intersections-prompt-the-decision-making-process." class="level3">
<h3 class="anchored" data-anchor-id="spatial-decision-dynamics-during-wayfinding-intersections-prompt-the-decision-making-process.">Spatial decision dynamics during wayfinding: Intersections prompt the decision-making process.</h3>
<p>Brunyé, T. T., Gardony, A. L., Holmes, A., &amp; Taylor, H. A. (2018). <strong>Spatial decision dynamics during wayfinding: Intersections prompt the decision-making process.</strong> Cognitive Research: Principles and Implications, 3(1), 13. https://doi.org/10.1186/s41235-018-0098-3</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Intersections are critical decision points for wayfinders, but it is unknown how decision dynamics unfold during pedestrian wayfinding. Some research implies that pedestrians leverage available visual cues to actively compare options while in an intersection, whereas other research suggests that people strive to make decisions long before overt responses are required. Two experiments examined these possibilities while participants navigated virtual desktop environments, assessing information-seeking behavior (Experiment 1) and movement dynamics (Experiment 2) while approaching intersections. In Experiment 1, we found that participants requested navigation guidance while in path segments approaching an intersection and the guidance facilitated choice behavior. In Experiment 2, we found that participants tended to orient themselves toward an upcoming turn direction before entering an intersection, particularly as they became more familiar with the environment. Some of these patterns were modulated by individual differences in spatial ability, sense of direction, spatial strategies, and gender. Together, we provide novel evidence that deciding whether to continue straight or turn involves a dynamic, distributed decision-making process that is prompted by upcoming intersections and modulated by individual differences and environmental experience. We discuss implications of these results for spatial decision-making theory and the development of innovative adaptive, beacon-based navigation guidance systems.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Brunye_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-16" title="Figure from @brunyeSpatialDecisionDynamics2018"><img src="images/Brunye_drive1.png" class="img-fluid figure-img" alt="Figure from Brunyé et al. (2018)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="brunyeSpatialDecisionDynamics2018">Brunyé et al. (<a href="#ref-brunyeSpatialDecisionDynamics2018" role="doc-biblioref">2018</a>)</span></figcaption>
</figure>
</div>
<hr>
<p>Ericson, J. D., &amp; Warren, W. H. (2020). <strong>Probing the invariant structure of spatial knowledge: Support for the cognitive graph hypothesis.</strong> Cognition, 200, 104276. https://doi.org/10.1016/j.cognition.2020.104276</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>We tested four hypotheses about the structure of spatial knowledge used for navigation: (1) the Euclidean hypothesis, a geometrically consistent map; (2) the Neighborhood hypothesis, adjacency relations between spatial regions, based on visible boundaries; (3) the Cognitive Graph hypothesis, a network of paths between places, labeled with approximate local distances and angles; and (4) the Constancy hypothesis, whatever geometric properties are invariant during learning. In two experiments, different groups of participants learned three virtual hedge mazes, which varied specific geometric properties (Euclidean Control Maze, Elastic Maze with stretching paths, Swap Maze with alternating paths to the same place). Spatial knowledge was then tested using three navigation tasks (metric shortcuts on empty ground plane, neighborhood shortcuts with visible boundaries, route task in corridors). They yielded the following results: (a) Metric shortcuts were insensitive to detectable shifts in target location, inconsistent with the Euclidean hypothesis. (b) Neighborhood shortcuts were constrained by visible boundaries in the Elastic Maze, but not in the Swap Maze, contrary to the Neighborhood and Constancy hypotheses. (c) The route task indicated that a graph of the maze was acquired in all environments, including knowledge of local path lengths. We conclude that primary spatial knowledge is consistent with the Cognitive Graph hypothesis. Neighborhoods are derived from the graph, and local distance and angle information is not embedded in a geometrically consistent map.</p>
</div>
</details>
<div id="fig-ericson" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-ericson-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Ericson1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-17" title="Figure&nbsp;4: Figures from @ericsonProbingInvariantStructure2020"><img src="images/Ericson1.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 100.0%;justify-content: center;">
<p><a href="images/Ericson2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-18" title="Figure&nbsp;4: Figures from @ericsonProbingInvariantStructure2020"><img src="images/Ericson2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-ericson-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;4: Figures from <span class="citation" data-cites="ericsonProbingInvariantStructure2020">Ericson &amp; Warren (<a href="#ref-ericsonProbingInvariantStructure2020" role="doc-biblioref">2020</a>)</span>
</figcaption>
</figure>
</div>
<hr>
</section>
<section id="rational-use-of-cognitive-resources-in-human-planning" class="level3">
<h3 class="anchored" data-anchor-id="rational-use-of-cognitive-resources-in-human-planning">Rational use of cognitive resources in human planning</h3>
<p>Callaway, F., Van Opheusden, B., Gul, S., Das, P., Krueger, P. M., Griffiths, T. L., &amp; Lieder, F. (2022). Rational use of cognitive resources in human planning. Nature Human Behaviour, 6(8), 1112–1125. https://doi.org/10.1038/s41562-022-01332-8</p>
<p><a href="https://osf.io/6venh/">link to code</a> &nbsp; <a href="https://cocosci.princeton.edu/papers/callawayrationaluse.pdf">link to paper</a> &nbsp; <a href="https://roadtrip-task-demo.netlify.app/">link to live task demo</a> &nbsp;</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/callaway_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-19" title="Figure from @callawayRationalUseCognitive2022"><img src="images/callaway_drive1.png" class="img-fluid figure-img" alt="Figure from Callaway et al. (2022)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="callawayRationalUseCognitive2022">Callaway et al. (<a href="#ref-callawayRationalUseCognitive2022" role="doc-biblioref">2022</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="people-construct-simplified-mental-representations-to-plan." class="level3">
<h3 class="anchored" data-anchor-id="people-construct-simplified-mental-representations-to-plan.">People construct simplified mental representations to plan.</h3>
<p>Ho, M. K., Abel, D., Correa, C. G., Littman, M. L., Cohen, J. D., &amp; Griffiths, T. L. (2022). <strong>People construct simplified mental representations to plan.</strong> Nature, 1–8. https://doi.org/10.1038/s41586-022-04743-9</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/ho_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-20" title="Figure from Ho et al.&nbsp;2022"><img src="images/ho_drive1.png" class="img-fluid figure-img" alt="Figure from Ho et al.&nbsp;2022"></a></p>
<figcaption>Figure from Ho et al.&nbsp;2022</figcaption>
</figure>
</div>
<p>Ho, M. K., Abel, D., Correa, C. G., Littman, M. L., Cohen, J. D., &amp; Griffiths, T. L. (2021). <strong>Control of mental representations in human planning.</strong> arXiv:2105.06948 [Cs]. http://arxiv.org/abs/2105.06948</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/ho_drive2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-21" title="Figure from @hoControlMentalRepresentations2021"><img src="images/ho_drive2.png" class="img-fluid figure-img" alt="Figure from Ho et al. (2021)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="hoControlMentalRepresentations2021">Ho et al. (<a href="#ref-hoControlMentalRepresentations2021" role="doc-biblioref">2021</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="emergent-collective-sensing-in-human-groups." class="level3">
<h3 class="anchored" data-anchor-id="emergent-collective-sensing-in-human-groups.">Emergent Collective Sensing in Human Groups.</h3>
<p>Krafft, P. M., Hawkins, R. X., Pentland, A., Goodman, N. D., &amp; Tenenbaum, J. B. (2015). <strong>Emergent Collective Sensing in Human Groups.</strong> In CogSci. https://people.csail.mit.edu/pkrafft/papers/krafft-et-al-2015-emergent.pdf</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Kraft_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-22" title="Figure from @krafftEmergentCollectiveSensing2015"><img src="images/Kraft_drive1.png" class="img-fluid figure-img" alt="Figure from Krafft (2015)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="krafftEmergentCollectiveSensing2015">Krafft (<a href="#ref-krafftEmergentCollectiveSensing2015" role="doc-biblioref">2015</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="towards-a-computational-model-of-responsibility-judgments-in-sequential-human-ai-collaboration" class="level3">
<h3 class="anchored" data-anchor-id="towards-a-computational-model-of-responsibility-judgments-in-sequential-human-ai-collaboration">Towards a computational model of responsibility judgments in sequential human-AI collaboration</h3>
<p>Tsirtsis, S., Gomez Rodriguez, M., &amp; Gerstenberg, T. (2024). <strong>Towards a computational model of responsibility judgments in sequential human-AI collaboration.</strong> In Proceedings of the Annual Meeting of the Cognitive Science Society (Vol. 46). https://osf.io/preprints/psyarxiv/m4yad</p>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Tsirtsis_drive1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-23" title="Figure from @tsirtsisComputationalModelResponsibility2024"><img src="images/Tsirtsis_drive1.png" class="img-fluid figure-img" alt="Figure from Tsirtsis et al. (2024)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="tsirtsisComputationalModelResponsibility2024">Tsirtsis et al. (<a href="#ref-tsirtsisComputationalModelResponsibility2024" role="doc-biblioref">2024</a>)</span></figcaption>
</figure>
</div>
<hr>
</section>
<section id="pattern-driven-navigation-in-2d-multiscale-visualizations-with-scalable-insets." class="level3">
<h3 class="anchored" data-anchor-id="pattern-driven-navigation-in-2d-multiscale-visualizations-with-scalable-insets.">Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets.</h3>
<p>Lekschas, F., Behrisch, M., Bach, B., Kerpedjiev, P., Gehlenborg, N., &amp; Pfister, H. (2020). <strong>Pattern-Driven Navigation in 2D Multiscale Visualizations with Scalable Insets.</strong> IEEE Transactions on Visualization and Computer Graphics, 26(1), 611–621. IEEE Transactions on Visualization and Computer Graphics. https://doi.org/10.1109/TVCG.2019.2934555</p>
<p><a href="https://github.com/flekschas/higlass-scalable-insets">link to code on github</a></p>
<p><a href="https://scalable-insets.lekschas.de/">project page</a></p>
<details class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>We present Scalable Insets, a technique for interactively exploring and navigating large numbers of annotated patterns in multiscale visualizations such as gigapixel images, matrices, or maps. Exploration of many but sparsely-distributed patterns in multiscale visualizations is challenging as visual representations change across zoom levels, context and navigational cues get lost upon zooming, and navigation is time consuming. Our technique visualizes annotated patterns too small to be identifiable at certain zoom levels using insets, i.e., magnified thumbnail views of the annotated patterns. Insets support users in searching, comparing, and contextualizing patterns while reducing the amount of navigation needed. They are dynamically placed either within the viewport or along the boundary of the viewport to offer a compromise between locality and context preservation. Annotated patterns are interactively clustered by location and type. They are visually represented as an aggregated inset to provide scalable exploration within a single viewport. In a controlled user study with 18 participants, we found that Scalable Insets can speed up visual search and improve the accuracy of pattern comparison at the cost of slower frequency estimation compared to a baseline technique. A second study with 6 experts in the field of genomics showed that Scalable Insets is easy to learn and provides first insights into how Scalable Insets can be applied in an open-ended data exploration scenario.</p>
</div>
</details>
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="images/Lekschas.png" class="lightbox" data-gallery="quarto-lightbox-gallery-24" title="Figure from @lekschasPatternDrivenNavigation2D2020"><img src="images/Lekschas.png" class="img-fluid figure-img" alt="Figure from Lekschas et al. (2020)"></a></p>
<figcaption>Figure from <span class="citation" data-cites="lekschasPatternDrivenNavigation2D2020">Lekschas et al. (<a href="#ref-lekschasPatternDrivenNavigation2D2020" role="doc-biblioref">2020</a>)</span></figcaption>
</figure>
</div>
</section>
<section id="personality-traits-and-spatial-skills-are-related-to-group-dynamics-and-success-during-collective-wayfinding" class="level3">
<h3 class="anchored" data-anchor-id="personality-traits-and-spatial-skills-are-related-to-group-dynamics-and-success-during-collective-wayfinding">Personality Traits and Spatial Skills Are Related to Group Dynamics and Success During Collective Wayfinding</h3>
<p>Brunyé, T. T., Hendel, D., Gardony, A. L., Hussey, E. K., &amp; Taylor, H. A. (2024). <strong>Personality traits and spatial skills are related to group dynamics and success during collective wayfinding.</strong> Collective Spatial Cognition, 60-99.</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>This chapter reviews and identifies gaps in research examining collective navigation and describes the results of a small study aimed at better elucidating the independent and interactive roles of personality and spatial skill in guiding group wayfinding dynamics, wayfinding performance, and spatial memory. In this study, individuals, dyads, and triads completed a series of individual differences tasks and questionnaires, and an individual or shared (dyads and triads) virtual wayfinding experience involving planning and executing routes between origin and destination pairs. Navigators were provided with a single digital map that they could share during the task; patterns of map sharing, virtual navigation, and wayfinding performance were logged. Higher spatial anxiety was associated with more map viewing among group members, higher scores on questionnaires assessing autism-type traits were associated with lower group cohesion, higher group heterogeneity was associated with lower group cohesion and lower path efficiency, and triads tended to have poorer memory for the location of goal locations relative to individuals and dyads. Results speak to the inherent complexity and dynamics of collective navigation, the need for understanding individual differences in guiding group behavior, and the value of continuing research in this domain.</p>
</div>
</details>
<p><span class="citation" data-cites="brunyePersonalityTraitsSpatial2023">Brunyé et al. (<a href="#ref-brunyePersonalityTraitsSpatial2023" role="doc-biblioref">2023</a>)</span></p>
<hr>
</section>
<section id="wayfinding-in-pairs-comparing-the-planning-and-navigation-performance-of-dyads-and-individuals-in-a-real-world-environment." class="level3">
<h3 class="anchored" data-anchor-id="wayfinding-in-pairs-comparing-the-planning-and-navigation-performance-of-dyads-and-individuals-in-a-real-world-environment.">Wayfinding in pairs: Comparing the planning and navigation performance of dyads and individuals in a real-world environment.</h3>
<p>Bae, C., Montello, D., &amp; Hegarty, M. (2024). <strong>Wayfinding in pairs: Comparing the planning and navigation performance of dyads and individuals in a real-world environment.</strong> Cognitive Research: Principles and Implications, 9(1), 40. https://doi.org/10.1186/s41235-024-00563-9</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Navigation is essential to life, and it is cognitively complex, drawing on abilities such as prospective and situated planning, spatial memory, location recognition, and real-time decision-making. In many cases, day-to-day navigation is embedded in a social context where cognition and behavior are shaped by others, but the great majority of existing research in spatial cognition has focused on individuals. The two studies we report here contribute to our understanding of social wayfinding, assessing the performance of paired and individual navigators on a real-world wayfinding task in which they were instructed to minimize time and distance traveled. In the first study, we recruited 30 pairs of friends (familiar dyads); in the second, we recruited 30 solo participants (individuals). We compare the two studies to the results of an earlier study of 30 pairs of strangers (unfamiliar dyads). We draw out differences in performance with respect to spatial, social, and cognitive considerations. Of the three conditions, solo participants were least successful in reaching the destination accurately on their initial attempt. Friends traveled more efficiently than either strangers or individuals. Working with a partner also appeared to lend confidence to wayfinders: dyads of either familiarity type were more persistent than individuals in the navigation task, even after encountering challenges or making incorrect attempts. Route selection was additionally impacted by route complexity and unfamiliarity with the study area. Navigators explicitly used ease of remembering as a planning criterion, and the resulting differences in route complexity likely influenced success during enacted navigation.</p>
</div>
</details>
<div id="fig-bae" class="quarto-layout-panel">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-bae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/bae1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-25" title="Figure&nbsp;5: Figures from @baeWayfindingPairsComparing2024"><img src="images/bae1.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/bae2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-26" title="Figure&nbsp;5: Figures from @baeWayfindingPairsComparing2024"><img src="images/bae2.png" class="img-fluid figure-img"></a></p>
</div>
</div>
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/bae3.png" class="lightbox" data-gallery="quarto-lightbox-gallery-27" title="Figure&nbsp;5: Figures from @baeWayfindingPairsComparing2024"><img src="images/bae3.png" class="img-fluid figure-img"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/bae4.png" class="lightbox" data-gallery="quarto-lightbox-gallery-28" title="Figure&nbsp;5: Figures from @baeWayfindingPairsComparing2024"><img src="images/bae4.png" class="img-fluid figure-img"></a></p>
</div>
</div>
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-bae-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;5: Figures from <span class="citation" data-cites="baeWayfindingPairsComparing2024">Bae et al. (<a href="#ref-baeWayfindingPairsComparing2024" role="doc-biblioref">2024</a>)</span>
</figcaption>
</figure>
</div>
<hr>
</section>
<section id="individual-and-collective-foraging-in-autonomous-search-agents-with-human-intervention" class="level3">
<h3 class="anchored" data-anchor-id="individual-and-collective-foraging-in-autonomous-search-agents-with-human-intervention">Individual and collective foraging in autonomous search agents with human intervention</h3>
<p>Schloesser, D. S., Hollenbeck, D., &amp; Kello, C. T. (2021). <strong>Individual and collective foraging in autonomous search agents with human intervention.</strong> Scientific Reports, 11(1), Article 1. https://doi.org/10.1038/s41598-021-87717-7</p>
<details open="" class="relevant-callout">
<summary>
Abstract
</summary>
<div>
<p>Humans and other complex organisms exhibit intelligent behaviors as individual agents and as groups of coordinated agents. They can switch between independent and collective modes of behavior, and flexible switching can be advantageous for adapting to ongoing changes in conditions. In the present study, we investigated the flexibility between independent and collective modes of behavior in a simulated social foraging task designed to benefit from both modes: distancing among ten foraging agents promoted faster detection of resources, whereas flocking promoted faster consumption. There was a tradeoff between faster detection versus faster consumption, but both factors contributed to foraging success. Results showed that group foraging performance among simulated agents was enhanced by loose coupling that balanced distancing and flocking among agents and enabled them to fluidly switch among a variety of groupings. We also examined the effects of more sophisticated cognitive capacities by studying how human players improve performance when they control one of the search agents. Results showed that human intervention further enhanced group performance with loosely coupled agents, and human foragers performed better when coordinating with loosely coupled agents. Humans players adapted their balance of independent versus collective search modes in response to the dynamics of simulated agents, thereby demonstrating the importance of adaptive flexibility in social foraging.</p>
</div>
</details>
<p><a href="images/Kello1.png" class="lightbox" data-gallery="quarto-lightbox-gallery-29"><img src="images/Kello1.png" class="img-fluid"></a></p>
<div>

</div>
<div class="quarto-layout-panel" data-layout-ncol="2">
<div class="quarto-layout-row">
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: center;">
<p><a href="images/Kello2.png" class="lightbox" data-gallery="quarto-lightbox-gallery-30"><img src="images/Kello2.png" class="img-fluid"></a></p>
</div>
<div class="quarto-layout-cell" style="flex-basis: 50.0%;justify-content: flex-start;">
<p>Figures from <span class="citation" data-cites="schloesserIndividualCollectiveForaging2021">Schloesser et al. (<a href="#ref-schloesserIndividualCollectiveForaging2021" role="doc-biblioref">2021</a>)</span></p>
</div>
</div>
</div>
<hr>
<p><br>
</p>
<p>Alharbi, A. H., Khafaga, D. S., El-kenawy, E.-S. M., Eid, M. M., Ibrahim, A., Abualigah, L., Khodadadi, N., &amp; Abdelhamid, A. A. (2024). <strong>Optimizing electric vehicle paths to charging stations using parallel greylag goose algorithm and Restricted Boltzmann Machines.</strong> Frontiers in Energy Research, 12. https://doi.org/10.3389/fenrg.2024.1401330</p>
<p>Garg, K., Kello, C. T., &amp; Smaldino, P. E. (2022). <strong>Individual exploration and selective social learning: Balancing exploration–exploitation trade-offs in collective foraging.</strong> Journal of The Royal Society Interface, 19(189), 20210915. https://doi.org/10.1098/rsif.2021.0915</p>
<p>Mezey, D., Deffner, D., Kurvers, R. H. J. M., &amp; Romanczuk, P. (2024). <strong>Visual social information use in collective foraging.</strong> PLOS Computational Biology, 20(5), e1012087. https://doi.org/10.1371/journal.pcbi.1012087</p>
<hr>
</section>
</section>
<section id="task-brainstorming" class="level1 page-columns page-full">
<h1>Task Brainstorming</h1>
<p>Click on one of the tasks to activate, then use the arrow keys to control the car</p>
<section id="original" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="original">Original</h2>
<div class="column-page-right">
<div class="quarto-figure quarto-figure-center">
<figure class="figure">
<p><a href="task1.html" class="lightbox" data-gallery="quarto-lightbox-gallery-31" title="Sam’s original mockup"><embed src="task1.html" width="600" height="550"></a></p>
<figcaption>Sam’s original mockup</figcaption>
</figure>
</div>
</div>
</section>
<section id="alt-2" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-2">Alt 2</h2>
<div class="column-page-right">
<p><a href="task2.html" class="lightbox" data-gallery="quarto-lightbox-gallery-32"><embed src="task2.html" width="800" height="700"></a></p>
</div>
</section>
<section id="alt-2---larger" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-2---larger">Alt 2 - larger</h2>
<div class="column-page-right">
<p><a href="task4.html" class="lightbox" data-gallery="quarto-lightbox-gallery-33"><embed src="task4.html" width="800" height="800"></a></p>
</div>
</section>
<section id="alt-2b" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-2b">Alt 2b</h2>
<div class="column-page-right">
<p><a href="task2b.html" class="lightbox" data-gallery="quarto-lightbox-gallery-34"><embed src="task2b.html" width="850" height="800"></a></p>
</div>
</section>
<section id="alt-2c" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-2c">Alt 2c</h2>
<div class="column-page-right">
<p><a href="task2c.html" class="lightbox" data-gallery="quarto-lightbox-gallery-35"><embed src="task2c.html" width="800" height="700"></a></p>
</div>
</section>
<section id="alt-3" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-3">Alt 3</h2>
<div class="column-page-right">
<p><a href="task3.html" class="lightbox" data-gallery="quarto-lightbox-gallery-36"><embed src="task3.html" width="800" height="700"></a></p>
</div>
</section>
<section id="alt-3b" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-3b">Alt 3b</h2>
<div class="column-page-right">
<p><a href="task3b.html" class="lightbox" data-gallery="quarto-lightbox-gallery-37"><embed src="task3b.html" width="800" height="700"></a></p>
</div>
</section>
<section id="alt-3c" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="alt-3c">Alt 3c</h2>
<div class="column-page-right">
<p><a href="task3c.html" class="lightbox" data-gallery="quarto-lightbox-gallery-38"><embed src="task3c.html" width="800" height="700"></a></p>
</div>



</section>
</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" data-line-spacing="2" role="list">
<div id="ref-almaatouqTaskComplexityModerates2021" class="csl-entry" role="listitem">
Almaatouq, A., Alsobay, M., Yin, M., &amp; Watts, D. J. (2021). Task complexity moderates group synergy. <em>Proceedings of the National Academy of Sciences</em>, <em>118</em>(36). <a href="https://doi.org/10.1073/pnas.2101062118">https://doi.org/10.1073/pnas.2101062118</a>
</div>
<div id="ref-baeWayfindingPairsComparing2024" class="csl-entry" role="listitem">
Bae, C., Montello, D., &amp; Hegarty, M. (2024). Wayfinding in pairs: Comparing the planning and navigation performance of dyads and individuals in a real-world environment. <em>Cognitive Research: Principles and Implications</em>, <em>9</em>(1), 40. <a href="https://doi.org/10.1186/s41235-024-00563-9">https://doi.org/10.1186/s41235-024-00563-9</a>
</div>
<div id="ref-brunyeSpatialDecisionDynamics2018" class="csl-entry" role="listitem">
Brunyé, T. T., Gardony, A. L., Holmes, A., &amp; Taylor, H. A. (2018). Spatial decision dynamics during wayfinding: Intersections prompt the decision-making process. <em>Cognitive Research: Principles and Implications</em>, <em>3</em>(1), 13. <a href="https://doi.org/10.1186/s41235-018-0098-3">https://doi.org/10.1186/s41235-018-0098-3</a>
</div>
<div id="ref-brunyePersonalityTraitsSpatial2023" class="csl-entry" role="listitem">
Brunyé, T. T., Hendel, D., Gardony, A. L., Hussey, E. K., &amp; Taylor, H. A. (2023). Personality <span>Traits</span> and <span>Spatial Skills Are Related</span> to <span>Group Dynamics</span> and <span>Success During Collective Wayfinding</span>. In <em>Collective <span>Spatial Cognition</span></em>. Routledge.
</div>
<div id="ref-callawayRationalUseCognitive2022" class="csl-entry" role="listitem">
Callaway, F., Van Opheusden, B., Gul, S., Das, P., Krueger, P. M., Griffiths, T. L., &amp; Lieder, F. (2022). Rational use of cognitive resources in human planning. <em>Nature Human Behaviour</em>, <em>6</em>(8), 1112–1125. <a href="https://doi.org/10.1038/s41562-022-01332-8">https://doi.org/10.1038/s41562-022-01332-8</a>
</div>
<div id="ref-deffnerCollectiveIncentivesReduce2024" class="csl-entry" role="listitem">
Deffner, D., Mezey, D., Kahl, B., Schakowski, A., Romanczuk, P., Wu, C. M., &amp; Kurvers, R. H. J. M. (2024). Collective incentives reduce over-exploitation of social information in unconstrained human groups. <em>Nature Communications</em>, <em>15</em>(1), 2683. <a href="https://doi.org/10.1038/s41467-024-47010-3">https://doi.org/10.1038/s41467-024-47010-3</a>
</div>
<div id="ref-ericsonProbingInvariantStructure2020" class="csl-entry" role="listitem">
Ericson, J. D., &amp; Warren, W. H. (2020). Probing the invariant structure of spatial knowledge: <span>Support</span> for the cognitive graph hypothesis. <em>Cognition</em>, <em>200</em>, 104276. <a href="https://doi.org/10.1016/j.cognition.2020.104276">https://doi.org/10.1016/j.cognition.2020.104276</a>
</div>
<div id="ref-gargIndividualSocialForaging2024" class="csl-entry" role="listitem">
Garg, K., Deng, W., &amp; Mobbs, D. (2024). <em>Beyond the individual: <span>A</span> social foraging framework to study decisions in groups</em>. OSF. <a href="https://doi.org/10.31219/osf.io/rmqyb">https://doi.org/10.31219/osf.io/rmqyb</a>
</div>
<div id="ref-gulatiTaskComplexityPerformance2021" class="csl-entry" role="listitem">
Gulati, A., Nguyen, T. N., &amp; Gonzalez, C. (2021). Task <span>Complexity</span> and <span>Performance</span> in <span>Individuals</span> and <span>Groups Without Communication</span>. <em><span>AAAI Fall Symposium</span>. <span>Cham</span>: <span>Springer Nature Switzerland</span></em>, 8.
</div>
<div id="ref-hawkinsFlexibleSocialInference2023" class="csl-entry" role="listitem">
Hawkins, R. D., Berdahl, A. M., Pentland, A. ‘Sandy’., Tenenbaum, J. B., Goodman, N. D., &amp; Krafft, P. M. (2023). Flexible social inference facilitates targeted social learning when rewards are not observable. <em>Nature Human Behaviour</em>, <em>7</em>(10), 1767–1776. <a href="https://doi.org/10.1038/s41562-023-01682-x">https://doi.org/10.1038/s41562-023-01682-x</a>
</div>
<div id="ref-hoControlMentalRepresentations2021" class="csl-entry" role="listitem">
Ho, M. K., Abel, D., Correa, C. G., Littman, M. L., Cohen, J. D., &amp; Griffiths, T. L. (2021). Control of mental representations in human planning. <em>arXiv:2105.06948 [Cs]</em>. <a href="https://arxiv.org/abs/2105.06948">https://arxiv.org/abs/2105.06948</a>
</div>
<div id="ref-krafftEmergentCollectiveSensing2015" class="csl-entry" role="listitem">
Krafft, P. M. (2015). <em>Emergent <span>Collective Sensing</span> in <span>Human Groups</span></em>. 6.
</div>
<div id="ref-krukarRouteEffectsCitybased2023" class="csl-entry" role="listitem">
Krukar, J., Navas Medrano, S., &amp; Schwering, A. (2023). Route effects in city-based survey knowledge estimates. <em>Cognitive Processing</em>, <em>24</em>(2), 213–231. <a href="https://doi.org/10.1007/s10339-022-01122-0">https://doi.org/10.1007/s10339-022-01122-0</a>
</div>
<div id="ref-lekschasPatternDrivenNavigation2D2020" class="csl-entry" role="listitem">
Lekschas, F., Behrisch, M., Bach, B., Kerpedjiev, P., Gehlenborg, N., &amp; Pfister, H. (2020). Pattern-<span>Driven Navigation</span> in <span>2D Multiscale Visualizations</span> with <span>Scalable Insets</span>. <em>IEEE Transactions on Visualization and Computer Graphics</em>, <em>26</em>(1), 611–621. <a href="https://doi.org/10.1109/TVCG.2019.2934555">https://doi.org/10.1109/TVCG.2019.2934555</a>
</div>
<div id="ref-muganSpatialPlanningLong2020" class="csl-entry" role="listitem">
Mugan, U., &amp; MacIver, M. A. (2020). Spatial planning with long visual range benefits escape from visual predators in complex naturalistic environments. <em>Nature Communications</em>, <em>11</em>(1), 3057. <a href="https://doi.org/10.1038/s41467-020-16102-1">https://doi.org/10.1038/s41467-020-16102-1</a>
</div>
<div id="ref-naitoInsightsCommonGenerative2022a" class="csl-entry" role="listitem">
Naito, A., Katahira, K., &amp; Kameda, T. (2022). Insights about the common generative rule underlying an information foraging task can be facilitated via collective search. <em>Scientific Reports</em>, <em>12</em>(1), 8047.
</div>
<div id="ref-peerStructuringKnowledgeCognitive2021" class="csl-entry" role="listitem">
Peer, M., Brunec, I. K., Newcombe, N. S., &amp; Epstein, R. A. (2021). Structuring <span>Knowledge</span> with <span>Cognitive Maps</span> and <span>Cognitive Graphs</span>. <em>Trends in Cognitive Sciences</em>, <em>25</em>(1), 37–54. <a href="https://doi.org/10.1016/j.tics.2020.10.004">https://doi.org/10.1016/j.tics.2020.10.004</a>
</div>
<div id="ref-schloesserIndividualCollectiveForaging2021" class="csl-entry" role="listitem">
Schloesser, D. S., Hollenbeck, D., &amp; Kello, C. T. (2021). Individual and collective foraging in autonomous search agents with human intervention. <em>Scientific Reports</em>, <em>11</em>(1), 8492. <a href="https://doi.org/10.1038/s41598-021-87717-7">https://doi.org/10.1038/s41598-021-87717-7</a>
</div>
<div id="ref-tsirtsisComputationalModelResponsibility2024" class="csl-entry" role="listitem">
Tsirtsis, S., Rodriguez, M. G., &amp; Gerstenberg, T. (2024). <em>Towards a computational model of responsibility judgments in sequential human-<span>AI</span> collaboration</em>. <a href="https://doi.org/10.31234/osf.io/m4yad">https://doi.org/10.31234/osf.io/m4yad</a>
</div>
<div id="ref-wuVisualspatialDynamicsDrive2023" class="csl-entry" role="listitem">
Wu, C. M., Deffner, D., Kahl, B., Meder, B., Ho, M. H., &amp; Kurvers, R. H. J. M. (2023). <em>Visual-spatial dynamics drive adaptive social learning in immersive environments</em> [Preprint]. <a href="https://doi.org/10.1101/2023.06.28.546887">https://doi.org/10.1101/2023.06.28.546887</a>
</div>
<div id="ref-zhaoInteractionMapComplexity2020" class="csl-entry" role="listitem">
Zhao, H., Thrash, T., Grossrieder, A., Kapadia, M., Moussaïd, M., Hölscher, C., &amp; Schinazi, V. R. (2020). The interaction between map complexity and crowd movement on navigation decisions in virtual reality. <em>Royal Society Open Science</em>, <em>7</em>(3), 191523. <a href="https://doi.org/10.1098/rsos.191523">https://doi.org/10.1098/rsos.191523</a>
</div>
</div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp("https:\/\/tegorman13\.github\.io\/ccl");
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
          // target, if specified
          link.setAttribute("target", "_blank");
          if (link.getAttribute("rel") === null) {
            link.setAttribute("rel", "noopener");
          }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->
<script>var lightboxQuarto = GLightbox({"closeEffect":"zoom","descPosition":"bottom","loop":false,"openEffect":"zoom","selector":".lightbox"});
(function() {
  let previousOnload = window.onload;
  window.onload = () => {
    if (previousOnload) {
      previousOnload();
    }
    lightboxQuarto.on('slide_before_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      const href = trigger.getAttribute('href');
      if (href !== null) {
        const imgEl = window.document.querySelector(`a[href="${href}"] img`);
        if (imgEl !== null) {
          const srcAttr = imgEl.getAttribute("src");
          if (srcAttr && srcAttr.startsWith("data:")) {
            slideConfig.href = srcAttr;
          }
        }
      } 
    });
  
    lightboxQuarto.on('slide_after_load', (data) => {
      const { slideIndex, slideNode, slideConfig, player, trigger } = data;
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(slideNode);
      }
    });
  
  };
  
})();
          </script>




</body></html>