@article{abdelnabiLLMDeliberationEvaluatingLLMs2023,
  title = {{{LLM-Deliberation}}: {{Evaluating LLMs}} with {{Interactive Multi-Agent Negotiation Games}}.},
  shorttitle = {{{LLM-Deliberation}}},
  author = {Abdelnabi, Sahar and Gomaa, Amr and Sivaprasad, Sarath and Sch{\"o}nherr, Lea and Fritz, Mario},
  year = {2023},
  month = sep,
  publisher = {CISPA},
  doi = {10.60882/cispa.25233028.v1},
  urldate = {2024-09-30},
  abstract = {There is a growing interest in using Large Language Models (LLMs) as agents to tackle real-world tasks that may require assessing complex situations. Yet, we have a limited understanding of LLMs' reasoning and decision-making capabilities, partly stemming from a lack of dedicated evaluation benchmarks. As negotiating and compromising are key aspects of our everyday communication and collaboration, we propose using scorable negotiation games as a new evaluation framework for LLMs. We create a testbed of diverse text-based, multi-agent, multi-issue, semantically rich negotiation games, with easily tunable difficulty. To solve the challenge, agents need to have strong arithmetic, inference, exploration, and planning capabilities, while seamlessly integrating them. Via a systematic zero-shot Chain-of-Thought prompting (CoT), we show that agents can negotiate and consistently reach successful deals. We quantify the performance with multiple metrics and observe a large gap between GPT-4 and earlier models. Importantly, we test the generalization to new games and setups. Finally, we show that these games can help evaluate other critical aspects, such as the interaction dynamics between agents in the presence of greedy and adversarial players.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Abdelnabi et al_2023_LLM-Deliberation.pdf}
}

@misc{abdouCanLanguageModels2021,
  title = {Can {{Language Models Encode Perceptual Structure Without Grounding}}? {{A Case Study}} in {{Color}}},
  shorttitle = {Can {{Language Models Encode Perceptual Structure Without Grounding}}?},
  author = {Abdou, Mostafa and Kulmizev, Artur and Hershcovich, Daniel and Frank, Stella and Pavlick, Ellie and S{\o}gaard, Anders},
  year = {2021},
  month = sep,
  number = {arXiv:2109.06129},
  eprint = {2109.06129},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2109.06129},
  urldate = {2023-02-13},
  abstract = {Pretrained language models have been shown to encode relational information, such as the relations between entities or concepts in knowledge-bases -- (Paris, Capital, France). However, simple relations of this type can often be recovered heuristically and the extent to which models implicitly reflect topological structure that is grounded in world, such as perceptual structure, is unknown. To explore this question, we conduct a thorough case study on color. Namely, we employ a dataset of monolexemic color terms and color chips represented in CIELAB, a color space with a perceptually meaningful distance metric. Using two methods of evaluating the structural alignment of colors in this space with text-derived color term representations, we find significant correspondence. Analyzing the differences in alignment across the color spectrum, we find that warmer colors are, on average, better aligned to the perceptual color space than cooler ones, suggesting an intriguing connection to findings from recent work on efficient communication in color naming. Further analysis suggests that differences in alignment are, in part, mediated by collocationality and differences in syntactic usage, posing questions as to the relationship between color perception and usage and context.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/abdouCanLanguageModels2021-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Abdou et al_2021_Can Language Models Encode Perceptual Structure Without Grounding.pdf}
}

@misc{abdurahmanEvaluatingLargeLanguage2024,
  title = {Evaluating {{Large Language Models}} in {{Psychological Research}}: {{A Guide}} for {{Reviewers}}},
  shorttitle = {Evaluating {{Large Language Models}} in {{Psychological Research}}},
  author = {Abdurahman, Suhaib and Ziabari, Alireza S. and Moore, Alexander and Bartels, Daniel and Dehghani, Morteza},
  year = {2024},
  month = apr,
  doi = {10.31234/osf.io/ag7hy},
  urldate = {2024-09-23},
  abstract = {Large Language Models (LLMs) are being increasingly used in scientific research, be it to analyze data, generate synthetic data, or even to write scientific papers. This trend necessitates that journal reviewers are able to evaluate the quality of works that utilize LLMs. We provide reviewers of psychological research with a comprehensive guide on evaluating research that uses LLMs, examining their dual roles of automating data processing and simulating human data. Essential considerations for reviewers are highlighted, focusing on the evaluation of methodological rigor, the importance of replicability, and the validity of results when employing LLMs. We offer practical advice on assessing the appropriateness of LLM applications in submitted studies, emphasizing the need for transparency in methodological reporting and the challenges posed by the non-deterministic and continuously evolving nature of these models. By providing a framework for critical review, this guide aims to ensure high-quality, innovative research within the evolving landscape of psychological studies utilizing LLMs.},
  copyright = {https://creativecommons.org/publicdomain/zero/1.0/legalcode},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Abdurahman et al_2024_Evaluating Large Language Models in Psychological Research.pdf}
}

@article{abdurahmanPerilsOpportunitiesUsing2024,
  title = {Perils and Opportunities in Using Large Language Models in Psychological Research},
  author = {Abdurahman, Suhaib and Atari, Mohammad and {Karimi-Malekabadi}, Farzan and Xue, Mona J and Trager, Jackson and Park, Peter S and Golazizian, Preni and Omrani, Ali and Dehghani, Morteza},
  year = {2024},
  month = jul,
  journal = {PNAS Nexus},
  volume = {3},
  number = {7},
  pages = {pgae245},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgae245},
  urldate = {2024-09-22},
  abstract = {The emergence of large language models (LLMs) has sparked considerable interest in their potential application in psychological research, mainly as a model of the human psyche or as a general text-analysis tool. However, the trend of using LLMs without sufficient attention to their limitations and risks, which we rhetorically refer to as ``GPTology'', can be detrimental given the easy access to models such as ChatGPT. Beyond existing general guidelines, we investigate the current limitations, ethical implications, and potential of LLMs specifically for psychological research, and show their concrete impact in various empirical studies. Our results highlight the importance of recognizing global psychological diversity, cautioning against treating LLMs (especially in zero-shot settings) as universal solutions for text analysis, and developing transparent, open methods to address LLMs' opaque nature for reliable, reproducible, and robust inference from AI-generated data. Acknowledging LLMs' utility for task automation, such as text annotation, or to expand our understanding of human psychology, we argue for diversifying human samples and expanding psychology's methodological toolbox to promote an inclusive, generalizable science, countering homogenization, and over-reliance on LLMs.},
  annotation = {https://github.com/goytoom/LLMs\_perils\_opportunities\\
\\
https://osf.io/nafzy/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Abdurahman et al_2024_Perils and opportunities in using large language models in psychological.pdf;/Users/thomasgorman/Zotero/storage/46BGZT3T/7712371.html}
}

@article{abrahamseReviewInterventionStudies2005,
  title = {A Review of Intervention Studies Aimed at Household Energy Conservation},
  author = {Abrahamse, Wokje and Steg, Linda and Vlek, Charles and Rothengatter, Talib},
  year = {2005},
  month = sep,
  journal = {Journal of Environmental Psychology},
  volume = {25},
  number = {3},
  pages = {273--291},
  issn = {0272-4944},
  doi = {10.1016/j.jenvp.2005.08.002},
  urldate = {2024-07-02},
  abstract = {This article reviews and evaluates the effectiveness of interventions aiming to encourage households to reduce energy consumption. Thirty-eight studies performed within the field of (applied) social and environmental psychology are reviewed, and categorized as involving either antecedent strategies (i.e. commitment, goal setting, information, modeling) or consequence strategies (i.e. feedback, rewards). Particular attention is given to the following evaluation criteria: (1) to what extent did the intervention result in behavioral changes and/or reductions in energy use, (2) were underlying behavioral determinants examined (e.g. knowledge, attitudes), (3) to what extent could effects be attributed to the interventions and, (4) were effects maintained over longer periods of time? Interestingly, most studies focus on voluntary behavior change, by changing individual knowledge and/or perceptions rather than changing contextual factors (i.e. pay-off structure) which may determine households' behavioral decisions. Interventions have been employed with varying degrees of success. Information tends to result in higher knowledge levels, but not necessarily in behavioral changes or energy savings. Rewards have effectively encouraged energy conservation, but with rather short-lived effects. Feedback has also proven its merits, in particular when given frequently. Some important issues cloud these conclusions, such as methodological problems. Also, little attention is given to actual environmental impact of energy savings. Often, an intervention's effectiveness is studied without examining underlying psychological determinants of energy use and energy savings. Also, it is not always clear whether effects were maintained over a longer period of time. Recommendations are given to further improve intervention planning and to enhance the effectiveness of interventions.},
  keywords = {Household energy conservation,Interventions,Review},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Abrahamse et al_2005_A review of intervention studies aimed at household energy conservation.pdf}
}

@misc{aherUsingLargeLanguage2022,
  title = {Using {{Large Language Models}} to {{Simulate Multiple Humans}}},
  author = {Aher, Gati and Arriaga, Rosa I. and Kalai, Adam Tauman},
  year = {2022},
  month = sep,
  number = {arXiv:2208.10264},
  eprint = {2208.10264},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-01-12},
  abstract = {We propose a method for using a large language model, such as GPT-3, to simulate responses of different humans in a given context. We test our method by attempting to reproduce well-established economic, psycholinguistic, and social experiments. The method requires prompt templates for each experiment. Simulations are run by varying the (hypothetical) subject details, such as name, and analyzing the text generated by the language model. To validate our methodology, we use GPT-3 to simulate the Ultimatum Game, garden path sentences, risk aversion, and the Milgram Shock experiments. In order to address concerns of exposure to these studies in training data, we also evaluate simulations on novel variants of these studies. We show that it is possible to simulate responses of different people and that their responses are largely consistent with prior human studies from the literature. Using large language models as simulators offers advantages but also poses risks. Our use of a language model for simulation is contrasted with anthropomorphic views of a language model as having its own behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/aherUsingLargeLanguage2022-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Aher et al_2022_Using Large Language Models to Simulate Multiple Humans.pdf}
}

@misc{ahrabianCuriousCaseNonverbal2024,
  title = {The {{Curious Case}} of {{Nonverbal Abstract Reasoning}} with {{Multi-Modal Large Language Models}}},
  author = {Ahrabian, Kian and Sourati, Zhivar and Sun, Kexuan and Zhang, Jiarui and Jiang, Yifan and Morstatter, Fred and Pujara, Jay},
  year = {2024},
  month = feb,
  number = {arXiv:2401.12117},
  eprint = {2401.12117},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-05},
  abstract = {While large language models (LLMs) are still being adopted to new domains and utilized in novel applications, we are experiencing an influx of the new generation of foundation models, namely multi-modal large language models (MLLMs). These models integrate verbal and visual information, opening new possibilities to demonstrate more complex reasoning abilities at the intersection of the two modalities. However, despite the revolutionizing prospect of MLLMs, our understanding of their reasoning abilities is limited. In this study, we assess the nonverbal abstract reasoning abilities of open-source and closed-source MLLMs using variations of Raven's Progressive Matrices. Our experiments expose the difficulty of solving such problems while showcasing the immense gap between open-source and closed-source models. We also reveal critical shortcomings with individual visual and textual modules, subjecting the models to low-performance ceilings. Finally, to improve MLLMs' performance, we experiment with various methods, such as Chain-of-Thought prompting, resulting in a significant (up to 100\%) boost in performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Ahrabian et al_2024_The Curious Case of Nonverbal Abstract Reasoning with Multi-Modal Large.pdf;/Users/thomasgorman/Zotero/storage/TC2ZEYH7/2401.html}
}

@misc{akataPlayingRepeatedGames2023,
  title = {Playing Repeated Games with {{Large Language Models}}},
  author = {Akata, Elif and Schulz, Lion and {Coda-Forno}, Julian and Oh, Seong Joon and Bethge, Matthias and Schulz, Eric},
  year = {2023},
  month = may,
  number = {arXiv:2305.16867},
  eprint = {2305.16867},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-12-05},
  abstract = {Large Language Models (LLMs) are transforming society and permeating into diverse applications. As a result, LLMs will frequently interact with us and other agents. It is, therefore, of great societal value to understand how LLMs behave in interactive social settings. Here, we propose to use behavioral game theory to study LLM's cooperation and coordination behavior. To do so, we let different LLMs (GPT-3, GPT-3.5, and GPT-4) play finitely repeated games with each other and with other, human-like strategies. Our results show that LLMs generally perform well in such tasks and also uncover persistent behavioral signatures. In a large set of two players-two strategies games, we find that LLMs are particularly good at games where valuing their own self-interest pays off, like the iterated Prisoner's Dilemma family. However, they behave sub-optimally in games that require coordination. We, therefore, further focus on two games from these distinct families. In the canonical iterated Prisoner's Dilemma, we find that GPT-4 acts particularly unforgivingly, always defecting after another agent has defected only once. In the Battle of the Sexes, we find that GPT-4 cannot match the behavior of the simple convention to alternate between options. We verify that these behavioral signatures are stable across robustness checks. Finally, we show how GPT-4's behavior can be modified by providing further information about the other player as well as by asking it to predict the other player's actions before making a choice. These results enrich our understanding of LLM's social behavior and pave the way for a behavioral game theory for machines.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Akata et al_2023_Playing repeated games with Large Language Models.pdf;/Users/thomasgorman/Zotero/storage/UIZ6JIP9/2305.html}
}

@article{allawayExceptionsInstantiationsOvergeneralization2024,
  title = {Exceptions, {{Instantiations}}, and {{Overgeneralization}}: {{Insights}} into {{How Language Models Process Generics}}},
  shorttitle = {Exceptions, {{Instantiations}}, and {{Overgeneralization}}},
  author = {Allaway, Emily and Bhagavatula, Chandra and Hwang, Jena D. and McKeown, Kathleen and Leslie, Sarah-Jane},
  year = {2024},
  month = jul,
  journal = {Computational Linguistics},
  pages = {1--60},
  issn = {0891-2017},
  doi = {10.1162/coli_a_00530},
  urldate = {2024-08-11},
  abstract = {Large language models (LLMs) have garnered a great deal of attention for their exceptional generative performance on commonsense and reasoning tasks. In this work, we investigate LLMs' capabilities for generalization using a particularly challenging type of statement: generics. Generics express generalizations (e.g., birds can fly) but do so without explicit quantification. They are notable because they generalize over their instantiations (e.g., sparrows can fly) yet hold true even in the presence of exceptions (e.g., penguins do not). For humans, these generic generalization play a fundamental role in cognition, concept acquisition, and intuitive reasoning. We investigate how LLMs respond to and reason about generics. To this end, we first propose a framework grounded in pragmatics to automatically generate both exceptions and instantiations -- collectively exemplars. We make use of focus -- a pragmatic phenomenon that highlights meaning-bearing elements in a sentence -- to capture the full range of interpretations of generics across different contexts of use. This allows us to derive precise logical definitions for exemplars and operationalize them to automatically generate exemplars from LLMs. Using our system, we generate a dataset of {$\sim$}370k exemplars across {$\sim$}17k generics and conduct a human validation of a sample of the generated data. We use our final generated dataset to investigate how LLMs' reason about generics. Humans have a documented tendency to conflate universally quantified statements (e.g., all birds can fly) with generics. Therefore, we probe whether LLMs exhibit similar overgeneralization behavior in terms of quantification and in property inheritance. We find that LLMs do show evidence of overgeneralization, although they sometimes struggle to reason about exceptions. Furthermore, we find that LLMs may exhibit similar non-logical behavior to humans when considering property inheritance from generics.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Allaway et al_2024_Exceptions, Instantiations, and Overgeneralization.pdf;/Users/thomasgorman/Zotero/storage/N23IV52E/123791.html}
}

@article{almashorCanPrivateLLM2024,
  title = {Can {{Private LLM Agents Synthesize Household Energy Consumption Data}}?},
  author = {Almashor, Mahathir and Miyashita, Yusuke},
  year = {2024},
  abstract = {Reproducible science requires easy access to data, especially with the rise of data-driven and increasingly complex models used within energy research. Too often however, the data to reconstruct and verify purported solutions in publications is hidden due to some combination of commercial, legal, and sensitivity issues. This early work presents our initial efforts to leverage the recent advancements in Large Language Models (LLMs) to create usable and shareable energy datasets. In particular, we're utilising their mimicry of human behaviors, with the goal of extracting and exploring synthetic energy data through the simulation of LLM agents capable of interacting with and executing actions in controlled environments. We also analyse and visualise publicly available data in an attempt to create realistic but not quite exact copies of the originals. Our early results show some promise, with outputs that resemble the twin peak curves for household energy consumption. The hope is that our generalised approach can be used to easily replicate usable and realistic copies of otherwise secret or sensitive data.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Almashor_Miyashita_2024_Can Private LLM Agents Synthesize Household Energy Consumption Data.pdf}
}

@article{alshaterExploringRoleArtificial,
  title = {Exploring the {{Role}} of {{Artificial Intelligence}} in {{Enhancing Academic Performance}}: {{A Case Study}} of {{ChatGPT}}},
  author = {Alshater, Dr Muneer M},
  langid = {english},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/alshaterExploringRoleArtificial-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Alshater_Exploring the Role of Artificial Intelligence in Enhancing Academic Performance.pdf}
}

@article{aruFeasibilityArtificialConsciousness2023,
  title = {The Feasibility of Artificial Consciousness through the Lens of Neuroscience},
  author = {Aru, Jaan and Larkum, Matthew E. and Shine, James M.},
  year = {2023},
  month = dec,
  journal = {Trends in Neurosciences},
  volume = {46},
  number = {12},
  pages = {1008--1017},
  issn = {0166-2236},
  doi = {10.1016/j.tins.2023.09.009},
  urldate = {2024-05-19},
  abstract = {Interactions with large language models (LLMs) have led to the suggestion that these models may soon be conscious. From the perspective of neuroscience, this position is difficult to defend. For one, the inputs to LLMs lack the embodied, embedded information content characteristic of our sensory contact with the world around us. Secondly, the architectures of present-day artificial intelligence algorithms are missing key features of the thalamocortical system that have been linked to conscious awareness in mammals. Finally, the evolutionary and developmental trajectories that led to the emergence of living conscious organisms arguably have no parallels in artificial systems as envisioned today. The existence of living organisms depends on their actions and their survival is intricately linked to multi-level cellular, inter-cellular, and organismal processes culminating in agency and consciousness.},
  keywords = {agency,artificial intelligence,generative AI,large language models,machine learning,thalamus},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Aru et al_2023_The feasibility of artificial consciousness through the lens of neuroscience.pdf;/Users/thomasgorman/Zotero/storage/T6AYSBND/S0166223623002278.html}
}

@article{aujlaSemanticLibrarianSearch2019,
  title = {The {{Semantic Librarian}}: {{A}} Search Engine Built from Vector-Space Models of Semantics},
  shorttitle = {The {{Semantic Librarian}}},
  author = {Aujla, Harinder and Crump, Matthew J. C. and Cook, Matthew T. and Jamieson, Randall K.},
  year = {2019},
  month = dec,
  journal = {Behavior Research Methods},
  volume = {51},
  number = {6},
  pages = {2405--2418},
  issn = {1554-3528},
  doi = {10.3758/s13428-019-01268-4},
  urldate = {2022-04-19},
  abstract = {Psychologists have made substantial progress at developing empirically validated formal expressions of how people perceive, learn, remember, think, and know. In this article, we present an academic search engine for cognitive psychology that leverages computational expressions of human cognition (vector-space models of semantics) to represent and find articles in the psychological record. The method shows how psychological theory can be used to inform and aid the design of psychologically intuitive computer interfaces.},
  langid = {english},
  keywords = {BEAGLE,Cognitive computing,Computational linguistics,Document representation and retrieval,Search engine},
  annotation = {https://crumplab.shinyapps.io/athena/\\
\\
https://www.crumplab.com/RsemanticLibrarian/\\
\\
https://github.com/CrumpLab/RsemanticLibrarian},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/aujlaSemanticLibrarianSearch2019-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Aujla et al_2019_The Semantic Librarian.pdf}
}

@article{bailCanGenerativeAI2024,
  title = {Can {{Generative AI}} Improve Social Science?},
  author = {Bail, Christopher A.},
  year = {2024},
  month = may,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {121},
  number = {21},
  pages = {e2314021121},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2314021121},
  urldate = {2024-07-10},
  abstract = {Generative AI that can produce realistic text, images, and other human-like outputs is currently transforming many different industries. Yet it is not yet known how such tools might influence social science research. I argue Generative AI has the potential to improve survey research, online experiments, automated content analyses, agent-based models, and other techniques commonly used to study human behavior. In the second section of this article, I discuss the many limitations of Generative AI. I examine how bias in the data used to train these tools can negatively impact social science research---as well as a range of other challenges related to ethics, replication, environmental impact, and the proliferation of low-quality research. I conclude by arguing that social scientists can address many of these limitations by creating open-source infrastructure for research on human behavior. Such infrastructure is not only necessary to ensure broad access to high-quality research tools, I argue, but also because the progress of AI will require deeper understanding of the social forces that guide human behavior.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Bail_2024_Can Generative AI improve social science.pdf}
}

@article{banerjiStartupFoundersTheir2019,
  title = {Startup Founders and Their {{LinkedIn}} Connections: {{Are}} Well-Connected Entrepreneurs More Successful?},
  shorttitle = {Startup Founders and Their {{LinkedIn}} Connections},
  author = {Banerji, Devika and Reimer, Torsten},
  year = {2019},
  month = jan,
  journal = {Computers in Human Behavior},
  volume = {90},
  pages = {46--52},
  issn = {07475632},
  doi = {10.1016/j.chb.2018.08.033},
  urldate = {2024-06-13},
  abstract = {For the past 40 years, entrepreneurs and researchers have assumed that entrepreneur networks are important for startup ventures. This study takes this notion further by testing whether these benefits translate into tangible financial outcomes for a startup. For this purpose, the study integrates two extensive databases that have not been studied together in previous research: Crunchbase.com, which provides information on the financial success of startup companies, and LinkedIn, which provides social network information of founders. The analysis revealed that several variables in LinkedIn profiles were positively correlated with the amount of funds raised by startup companies establishing a link between social networks and entrepreneurial success. The average number of followers that the founders of a company had according to their LinkedIn profile was the strongest predictor of the amount of funds raised by companies.},
  langid = {english},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Banerji_Reimer_2019_Startup founders and their LinkedIn connections.pdf}
}

@article{bankerMachineassistedSocialPsychology2024,
  title = {Machine-Assisted Social Psychology Hypothesis Generation},
  author = {Banker, Sachin and Chatterjee, Promothesh and Mishra, Himanshu and Mishra, Arul},
  year = {2024},
  journal = {American Psychologist},
  volume = {79},
  number = {6},
  pages = {789--797},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1935-990X},
  doi = {10.1037/amp0001222},
  abstract = {Social psychology research projects begin with generating a testable idea that relies heavily on a researcher's ability to assimilate, recall, and accurately process available research findings. However, an exponential increase in new research findings is making the task of synthesizing ideas across the multitude of topics challenging, which could result in important overlooked research connections. In this research, we leverage the fact that social psychology research is based on verbal models and employ large natural language models to generate hypotheses that can aid social psychology researchers in developing new research hypotheses. We adopted two methodological approaches. In the first approach, we fine-tuned the third-generation generative pre-trained transformer (GPT-3) language model on thousands of abstracts published in more than 50 social psychology journals in the past 55 years as well as on preprint repositories (PsyArXiv). Social psychology experts rated model- and human-generated hypotheses similarly on the dimensions of clarity, originality, and impact. In the second approach, without fine-tuning, we generated hypotheses using GPT-4 and found that social psychology experts rated these generated hypotheses as higher in quality than human-generated hypotheses on dimensions of clarity, originality, impact, plausibility, and relevance. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
  keywords = {Creativity,Deep Neural Networks,Experience Level,Generative Artificial Intelligence,Hypothesis Testing,Language,Models,Social Psychology},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Banker et al_2024_Machine-assisted social psychology hypothesis generation.pdf;/Users/thomasgorman/Zotero/storage/UB5SK6KQ/2025-23943-001.html}
}

@article{baptisteHistTextApplicationLeveraging2023,
  title = {{{HistText}}: {{An Application}} for Leveraging Large-Scale Historical Textbases},
  shorttitle = {{{HistText}}},
  author = {Baptiste, Blouin and Armand, C{\'e}cile and Henriot, Christian},
  year = {2023},
  month = nov,
  journal = {Journal of Data Mining \& Digital Humanities},
  volume = {2023},
  number = {Project presentations},
  pages = {11756},
  issn = {2416-5999},
  doi = {10.46298/jdmdh.11756},
  urldate = {2024-05-24},
  abstract = {This paper introduces HistText, a pioneering tool devised to facilitate large-scale data mining in historical documents, specifically targeting Chinese sources. Developed in response to the challenges posed by the large-scale Modern China Textual Database, HistText emerged as a solution to efficiently extract and visualize valuable insights from billions of words spread across millions of documents. With a user-friendly interface, advanced text analysis techniques, and powerful data visualization capabilities, HistText offers a robust platform for computational humanities research. This paper explores the rationale behind HistText, underscores its key features, and provides a comprehensive guide for its effective utilization, thus highlighting its potential to substantially enhance the realm of computational humanities.},
  langid = {english},
  annotation = {https://github.com/carmand03/american-university-men-china\\
\\
https://bookdown.enpchina.eu/Histtext/HistText\_interface.html},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Baptiste et al_2023_HistText.pdf}
}

@article{barberAlexaHappyAngry2024,
  title = {Is {{Alexa Happy}} or {{Angry}}? {{Perceptions}} and {{Attributions}} of {{Emotional Displays}} of {{Smart Technologies}} in {{Residential Homes}}},
  shorttitle = {Is {{Alexa Happy}} or {{Angry}}?},
  author = {Barber, Hayden and Reimer, Torsten and Zhang, Damin and Rayz, Julia},
  year = {2024},
  month = jan,
  journal = {Sustainability},
  volume = {16},
  number = {7},
  pages = {2721},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2071-1050},
  doi = {10.3390/su16072721},
  urldate = {2024-06-22},
  abstract = {Digital assistants such as Alexa can provide feedback to residents that affect energy consumption. One important characteristic of feedback refers to the emotionality of the provided feedback. Research on social cognition and attribution theory suggests that effects of emotional messages on behavior are contingent on the inferred cause of the emotion (e.g., why a message was said in a happy or neutral voice). As a prerequisite, to have the intended effects on energy saving behaviors, Alexa's emotional messages have to trigger three basic social cognitions: (1) the emotional display has to be identified by residents; (2) residents have to correctly identify their behavior as a target of the emotional display; and (3) residents have to attribute the emotional display to that behavior. In two studies (N = 194 and N = 353), several conditions were identified that triggered these three basic social cognitions in a simulated environment.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {attributions,emotion and communication,human-computer interactions},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Barber et al_2024_Is Alexa Happy or Angry.pdf}
}

@incollection{baroniProperRoleLinguistically2022,
  title = {On the {{Proper Role}} of {{Linguistically Oriented Deep Net Analysis}} in {{Linguistic Theorising}}},
  booktitle = {Algebraic {{Structures}} in {{Natural Language}}},
  author = {Baroni, Marco},
  year = {2022},
  publisher = {CRC Press},
  abstract = {A lively research field has recently emerged that uses experimental methods to probe the linguistic behaviour of modern deep networks. While work in this tradition often reports intriguing results about the grammatical skills of deep nets, it is not clear what their implications for linguistic theorising should be. As a consequence, linguistically oriented deep net analysis has had very little impact on linguistics at large. In this chapter, I suggest that deep networks should be treated as theories making explicit predictions about the acceptability of linguistic utterances. I argue that if we overcome some obstacles standing in the way of seriously pursuing this idea, we will gain a powerful new theoretical tool, complementary to mainstream algebraic approaches.},
  isbn = {978-1-00-320538-8},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Baroni_2022_On the Proper Role of Linguistically Oriented Deep Net Analysis in Linguistic.pdf}
}

@misc{barritNeuraSpecializedLarge2024,
  title = {Neura: A Specialized Large Language Model Solution in Neurology},
  shorttitle = {Neura},
  author = {Barrit, Sami and Torcida, Nathan and Mazeraud, Aur{\'e}lien and Boulogne, S{\'e}bastien and Benoit, Jeanne and Carette, Timoth{\'e}e and Carron, Thibault and Delsaut, Bertil and Diab, Eva and Kermorvant, Hugo and Maarouf, Adil and Slootjes, Sofia Maldonado and Redon, Sylvain and Robin, Alexis and Hadidane, Sofi{\`e}ne and Harlay, Vincent and Tota, Vito and Madec, Tanguy and Niset, Alexandre and El Hadwe, Salim and Massager, Nicolas and Lagarde, Stanislas and Carron, Romain},
  year = {2024},
  month = feb,
  doi = {10.1101/2024.02.11.24302658},
  urldate = {2024-08-11},
  abstract = {Large language models' (LLM) ability in natural language processing holds promise for diverse applications, yet their deployment in fields such as neurology faces domain-specific challenges. Hence, we introduce Neura: a scalable, explainable solution to specialize LLM. Blindly evaluated on a select set of five complex clinical cases compared to a cohort of 13 neurologists, Neura achieved normalized scores of 86.17\% overall, 85\% for differential diagnoses, and 88.24\% for final diagnoses (55.11\%, 46.15\%, and 70.93\% for neurologists) with rapid response times of 28.8 and 19 seconds (9 minutes and 37.2 seconds and 8 minutes and 51 seconds for neurologists) while consistently providing relevant, accurately cited information. These findings support the emerging role of LLM-driven applications to articulate human-acquired and integrated data with a vast corpus of knowledge, augmenting human experiential reasoning for clinical and research purposes.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Barrit et al_2024_Neura.pdf}
}

@article{bartalAINarrativeEmbeddings2024,
  title = {{{AI}} and Narrative Embeddings Detect {{PTSD}} Following Childbirth via Birth Stories},
  author = {Bartal, Alon and Jagodnik, Kathleen M. and Chan, Sabrina J. and Dekel, Sharon},
  year = {2024},
  month = apr,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {8336},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-54242-2},
  urldate = {2024-06-30},
  abstract = {Free-text analysis using machine learning (ML)-based natural language processing (NLP) shows promise for diagnosing psychiatric conditions. Chat Generative Pre-trained Transformer (ChatGPT) has demonstrated preliminary initial feasibility for this purpose; however, whether it can accurately assess mental illness remains to be determined. This study evaluates the effectiveness of ChatGPT and the text-embedding-ada-002 (ADA) model in detecting post-traumatic stress disorder following childbirth (CB-PTSD), a maternal postpartum mental illness affecting millions of women annually, with no standard screening protocol. Using a sample of 1295 women who gave birth in the last six months and were 18+ years old, recruited through hospital announcements, social media, and professional organizations, we explore ChatGPT's and ADA's potential to screen for CB-PTSD by analyzing maternal childbirth narratives. The PTSD Checklist for DSM-5 (PCL-5; cutoff 31) was used to assess CB-PTSD. By developing an ML model that utilizes numerical vector representation of the ADA model, we identify CB-PTSD via narrative classification. Our model outperformed (F1 score: 0.81) ChatGPT and six previously published large text-embedding models trained on mental health or clinical domains data, suggesting that the ADA model can be harnessed to identify CB-PTSD. Our modeling approach could be generalized to assess other mental health disorders.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Post-traumatic stress disorder,Preclinical research,Translational research},
  annotation = {https://github.com/bartala/ChatCBPTSD},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Bartal et al_2024_AI and narrative embeddings detect PTSD following childbirth via birth stories.pdf}
}

@article{baruaConceptInductionUsing,
  title = {Concept {{Induction}} Using {{LLMs}}: A User Experiment for Assessment},
  author = {Barua, Adrita and Widmer, Cara and Hitzler, Pascal},
  abstract = {Explainable Artificial Intelligence (XAI) poses a significant challenge in providing transparent and understandable insights into complex AI models. Traditional post-hoc algorithms, while useful, often struggle to deliver interpretable explanations. Concept-based models offer a promising avenue by incorporating explicit representations of concepts to enhance interpretability. However, existing research on automatic concept discovery methods is often limited by lower-level concepts, costly human annotation requirements, and a restricted domain of background knowledge. In this study, we explore the potential of a Large Language Model (LLM), specifically GPT-4, by leveraging its domain knowledge and common-sense capability to generate high-level concepts that are meaningful as explanations for humans, for a specific setting of image classification. We use minimal textual object information available in the data via prompting to facilitate this process. To evaluate the output, we compare the concepts generated by the LLM with two other methods: concepts generated by humans and the ECII heuristic concept induction system. Since there is no established metric to determine the human understandability of concepts, we conducted a human study to assess the effectiveness of the LLM-generated concepts. Our findings indicate that while human-generated explanations remain superior, concepts derived from GPT-4 are more comprehensible to humans compared to those generated by ECII.},
  langid = {english},
  annotation = {https://github.com/AdritaBarua/Concept-Induction-using-LLMs-a-user-experiment-for-assessment},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Barua et al_Concept Induction using LLMs.pdf}
}

@article{basuTrustDynamicsHuman,
  title = {Trust {{Dynamics}} in {{Human Autonomous Vehicle Interaction}}: {{A Review}} of {{Trust Models}}},
  author = {Basu, Chandrayee and Singhal, Mukesh},
  abstract = {Several ongoing research projects in Human autonomous car interactions are addressing the problem of safe co-existence for human and robot drivers on road. Automation in cars can vary across a continuum of levels at which it can replace manual tasks. Social relationships like anthropomorphic behavior of owners towards their cars is also expected to vary according to this spectrum of autonomous decision making capacity. Some researchers have proposed a joint cognitive model of a human-car collaboration that can make the best of the respective strengths of humans and machines. For a successful collaboration, it is important that the members of this humancar team develop, maintain and update each others behavioral models. We consider mutual trust as an integral part of these models. In this paper, we present a review of the quantitative models of trust in automation. We found that only a few models of humans' trust on automation exist in literature that account for the dynamic nature of trust and may be leveraged in human car interaction. However, these models do not support mutual trust. Our review suggests that there is significant scope for future research in the domain of mutual trust modeling for human car interaction, especially, when considered over the lifetime of the vehicle. Hardware and computational framework (for sensing, data aggregation, processing and modeling) must be developed to support these adaptive models over the operational phase of autonomous vehicles. In order to further research in mutual human - automation trust, we propose a framework for integrating Mutual Trust computation into standard Human - Robot Interaction research platforms. This framework includes User trust and Agent trust, the two fundamental components of Mutual trust. It allows us to harness multi-modal sensor data from the car as well as from the user's wearable or handheld device. The proposed framework provides access to prior trust aggregate and other cars' experience data from the Cloud and to feature primitives like gaze, facial expression, etc. from a standard low-cost Human - Robot Interaction platform.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Basu_Singhal_Trust Dynamics in Human Autonomous Vehicle Interaction.pdf}
}

@article{bayneTestsConsciousnessHumans2024,
  title = {Tests for Consciousness in Humans and Beyond},
  author = {Bayne, Tim and Seth, Anil K. and Massimini, Marcello and Shepherd, Joshua and Cleeremans, Axel and Fleming, Stephen M. and Malach, Rafael and Mattingley, Jason B. and Menon, David K. and Owen, Adrian M. and Peters, Megan A.K. and Razi, Adeel and Mudrik, Liad},
  year = {2024},
  month = may,
  journal = {Trends in Cognitive Sciences},
  volume = {28},
  number = {5},
  pages = {454--466},
  issn = {13646613},
  doi = {10.1016/j.tics.2024.01.010},
  urldate = {2024-05-19},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Bayne et al_2024_Tests for consciousness in humans and beyond.pdf}
}

@misc{belemPerceptionsLinguisticUncertainty2024,
  title = {Perceptions of {{Linguistic Uncertainty}} by {{Language Models}} and {{Humans}}},
  author = {Belem, Catarina G. and Kelly, Markelle and Steyvers, Mark and Singh, Sameer and Smyth, Padhraic},
  year = {2024},
  month = jul,
  number = {arXiv:2407.15814},
  eprint = {2407.15814},
  publisher = {arXiv},
  urldate = {2024-07-26},
  abstract = {Uncertainty expressions such as ``probably'' or ``highly unlikely'' are pervasive in human language. While prior work has established that there is population-level agreement in terms of how humans interpret these expressions, there has been little inquiry into the abilities of language models to interpret such expressions. In this paper, we investigate how language models map linguistic expressions of uncertainty to numerical responses. Our approach assesses whether language models can employ theory of mind in this setting: understanding the uncertainty of another agent about a particular statement, independently of the model's own certainty about that statement. We evaluate both humans and 10 popular language models on a task created to assess these abilities. Unexpectedly, we find that 8 out of 10 models are able to map uncertainty expressions to probabilistic responses in a human-like manner. However, we observe systematically different behavior depending on whether a statement is actually true or false. This sensitivity indicates that language models are substantially more susceptible to bias based on their prior knowledge (as compared to humans). These findings raise important questions and have broad implications for human-AI alignment and AI-AI communication.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/UCIDataLab/llm-uncertainty-perceptions},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Belem et al_2024_Perceptions of Linguistic Uncertainty by Language Models and Humans.pdf}
}

@misc{belouadiDeTikZifySynthesizingGraphics2024,
  title = {{{DeTikZify}}: {{Synthesizing Graphics Programs}} for {{Scientific Figures}} and {{Sketches}} with {{TikZ}}},
  shorttitle = {{{DeTikZify}}},
  author = {Belouadi, Jonas and Ponzetto, Simone Paolo and Eger, Steffen},
  year = {2024},
  month = may,
  number = {arXiv:2405.15306},
  eprint = {2405.15306},
  publisher = {arXiv},
  urldate = {2024-06-03},
  abstract = {Creating high-quality scientific figures can be time-consuming and challenging, even though sketching ideas on paper is relatively easy. Furthermore, recreating existing figures that are not stored in formats preserving semantic information is equally complex. To tackle this problem, we introduce DeTikZify, a novel multimodal language model that automatically synthesizes scientific figures as semantics-preserving TikZ graphics programs based on sketches and existing figures. To achieve this, we create three new datasets: DaTikZv2, the largest TikZ dataset to date, containing over 360k human-created TikZ graphics; SketchFig, a dataset that pairs hand-drawn sketches with their corresponding scientific figures; and SciCap++, a collection of diverse scientific figures and associated metadata. We train DeTikZify on SciCap++ and DaTikZv2, along with synthetically generated sketches learned from SketchFig. We also introduce an MCTS-based inference algorithm that enables DeTikZify to iteratively refine its outputs without the need for additional training. Through both automatic and human evaluation, we demonstrate that DeTikZify outperforms commercial Claude 3 and GPT-4V in synthesizing TikZ programs, with the MCTS algorithm effectively boosting its performance. We make our code, models, and datasets publicly available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {https://github.com/potamides/DeTikZify},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Belouadi et al_2024_DeTikZify.pdf}
}

@article{bhatiaExploringVariabilityRisk2024,
  title = {Exploring Variability in Risk Taking with Large Language Models},
  author = {Bhatia, Sudeep},
  year = {2024},
  journal = {Journal of Experimental Psychology: General},
  volume = {153},
  number = {7},
  pages = {1838--1860},
  publisher = {American Psychological Association},
  address = {US},
  issn = {1939-2222},
  doi = {10.1037/xge0001607},
  abstract = {What are the sources of individual-level differences in risk taking, and how do they depend on the domain or situation in which the decision is being made? Psychologists currently answer such questions with psychometric methods, which analyze correlations across participant responses in survey data sets. In this article, we analyze the preferences that give rise to these correlations. Our approach uses (a) large language models (LLMs) to quantify everyday risky behaviors in terms of the attributes or reasons that may describe those behaviors, and (b) decision models to map these attributes and reasons onto participant responses. We show that LLM-based decision models can explain observed correlations between behaviors in terms of the reasons different behaviors elicit and explain observed correlations between individuals in terms of the weights different individuals place on reasons, thereby providing a decision theoretic foundation for psychometric findings. Since LLMs can generate quantitative representations for nearly any naturalistic decision, they can be used to make accurate out-of-sample predictions for hundreds of everyday behaviors, predict the reasons why people may or may not want to engage in these behaviors, and interpret these reasons in terms of core psychological constructs. Our approach has important theoretical and practical implications for the study of heterogeneity in everyday behavior. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
  keywords = {Decision Theory,Large Language Models,Psychometrics,Responses,Risk Taking},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Bhatia_2024_Exploring variability in risk taking with large language models.pdf;/Users/thomasgorman/Zotero/storage/XYHGFS3V/2024-79012-001.html}
}

@article{bhatiaInductiveReasoningMinds2023,
  title = {Inductive Reasoning in Minds and Machines.},
  author = {Bhatia, Sudeep},
  year = {2023},
  month = sep,
  journal = {Psychological Review},
  issn = {1939-1471, 0033-295X},
  doi = {10.1037/rev0000446},
  urldate = {2024-05-25},
  abstract = {Induction---the ability to generalize from existing knowledge---is the cornerstone of intelligence. Cognitive models of human induction are largely limited to toy problems and cannot make quantitative predictions for the thousands of different induction arguments that have been studied by researchers, or to the countless induction arguments that could be encountered in everyday life. Leading large language models (LLMs) go beyond toy problems but fail to mimic observed patterns of human induction. In this article, we combine rich knowledge representations obtained from LLMs with theories of human inductive reasoning developed by cognitive psychologists. We show that this integrative approach can capture several benchmark empirical findings on human induction and generate human-like responses to natural language arguments with thousands of common categories and properties. These findings shed light on the cognitive mechanisms at play in human induction and show how existing theories in psychology and cognitive science can be integrated with new methods in artificial intelligence, to successfully model highlevel human cognition.},
  copyright = {http://www.apa.org/pubs/journals/resources/open-access.aspx},
  langid = {english},
  annotation = {https://osf.io/gebqv/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Bhatia_2023_Inductive reasoning in minds and machines.pdf}
}

@techreport{bhatiaTransformerNetworksHuman2020,
  type = {Preprint},
  title = {Transformer {{Networks}} of {{Human Conceptual Knowledge}}},
  author = {Bhatia, Sudeep and Richie, Russell},
  year = {2020},
  month = nov,
  institution = {PsyArXiv},
  doi = {10.31234/osf.io/hs4ra},
  urldate = {2021-12-31},
  abstract = {We present a computational model capable of simulating aspects of human knowledge for thousands of real-world concepts. Our approach involves fine-tuning a transformer network for natural language processing on participant-generated feature norms. We show that such a model can successfully extrapolate from its training dataset, and predict human knowledge for novel concepts and features. We also apply our model to stimuli from twenty-three previous experiments in semantic cognition research, and show that it reproduces fifteen classic findings involving semantic verification, concept typicality, feature distribution, and semantic similarity. We interpret these results using established properties of classic connectionist networks. The success of our approach shows how the combination of natural language data and psychological data can be used to build cognitive models with rich world knowledge. Such models can be used in the service of new psychological applications, such as the cognitive process modeling of naturalistic semantic verification and knowledge retrieval, as well as the modeling of real-world categorization, decision making, and reasoning.},
  langid = {english},
  keywords = {Python code},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Bhatia_Richie_2020_Transformer Networks of Human Conceptual Knowledge.pdf}
}

@article{bienefeldHumanAITeamingLeveraging2023,
  title = {Human-{{AI}} Teaming: Leveraging Transactive Memory and Speaking up for Enhanced Team Effectiveness},
  shorttitle = {Human-{{AI}} Teaming},
  author = {Bienefeld, Nadine and Kolbe, Michaela and Camen, Giovanni and Huser, Dominic and Buehler, Philipp Karl},
  year = {2023},
  month = aug,
  journal = {Frontiers in Psychology},
  volume = {14},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2023.1208019},
  urldate = {2024-09-11},
  abstract = {In this prospective observational study, we investigate the role of transactive memory and speaking up in human-AI teams comprising 180 intensive care (ICU) physicians and nurses working with AI in a simulated clinical environment. Our findings indicate that interactions with AI agents differ significantly from human interactions, as accessing information from AI agents is positively linked to a team's ability to generate novel hypotheses and demonstrate speaking-up behavior, but only in higher-performing teams. Conversely, accessing information from human team members is negatively associated with these aspects, regardless of team performance. This study is a valuable contribution to the expanding field of research on human-AI teams and team science in general, as it emphasizes the necessity of incorporating AI agents as knowledge sources in a team's transactive memory system, as well as highlighting their role as catalysts for speaking up. Practical implications include suggestions for the design of future AI systems and human-AI team training in healthcare and beyond.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Behavioral Observation6,Explainable Artificial Intelligence / XAI4,Healthcare Teams5,Human-AI Teams1,Interaction Analysis7,Speaking up3,Team Performance8,Transactive Memory Systems2},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Bienefeld et al_2023_Human-AI teaming.pdf}
}

@article{binzTurningLargeLanguage,
  title = {Turning Large Language Models into Cognitive Models},
  author = {Binz, Marcel and Schulz, Eric},
  abstract = {Large language models are powerful systems that excel at many tasks, ranging from translation to mathematical reasoning. Yet, at the same time, these models often show unhuman-like characteristics. In the present paper, we address this gap and ask whether large language models can be turned into cognitive models. We find that -- after finetuning them on data from psychological experiments -- these models offer accurate representations of human behavior, even outperforming traditional cognitive models in two decision-making domains. In addition, we show that their representations contain the information necessary to model behavior on the level of individual subjects. Finally, we demonstrate that finetuning on multiple tasks enables large language models to predict human behavior in a previously unseen task. Taken together, these results suggest that large, pre-trained models can be adapted to become generalist cognitive models, thereby opening up new research directions that could transform cognitive psychology and the behavioral sciences as a whole.},
  langid = {english},
  annotation = {https://github.com/marcelbinz/CENTaUR},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Binz_Schulz_Turning large language models into cognitive models.pdf}
}

@techreport{binzUsingCognitivePsychology2022,
  type = {Preprint},
  title = {Using Cognitive Psychology to Understand {{GPT-3}}},
  author = {Binz, Marcel and Schulz, Eric},
  year = {2022},
  month = jun,
  institution = {PsyArXiv},
  doi = {10.31234/osf.io/6dfgk},
  urldate = {2023-01-12},
  abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning. Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. These results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
  langid = {english},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/binzUsingCognitivePsychology2022-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Binz_Schulz_2022_Using cognitive psychology to understand GPT-3.pdf}
}

@article{binzUsingCognitivePsychology2023,
  title = {Using Cognitive Psychology to Understand {{GPT-3}}},
  author = {Binz, Marcel and Schulz, Eric},
  year = {2023},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {6},
  pages = {e2218523120},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2218523120},
  urldate = {2024-06-19},
  abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: It solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multiarmed bandit task, and shows signatures of model-based reinforcement learning. Yet, we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. Taken together, these results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Binz_Schulz_2023_Using cognitive psychology to understand GPT-3.pdf}
}

@book{bober-irizarNeuralNetworksAbstraction2024,
  title = {Neural Networks for Abstraction and Reasoning: {{Towards}} Broad Generalization in Machines},
  shorttitle = {Neural Networks for Abstraction and Reasoning},
  author = {{Bober-Irizar}, Mikel and Banerjee, Soumya},
  year = {2024},
  month = jan,
  doi = {10.13140/RG.2.2.27041.79204},
  abstract = {For half a century, artificial intelligence research has attempted to reproduce the human qualities of abstraction and reasoning-creating computer systems that can learn new concepts from a minimal set of examples, in settings where humans find this easy. While specific neural networks are able to solve an impressive range of problems, broad generalisation to situations outside their training data has proved elusive. In this work, we look at several novel approaches for solving the Abstraction \& Reasoning Corpus (ARC). This is a dataset of abstract visual reasoning tasks introduced to test algorithms on broad generalization. Despite three international competitions with \$100,000 in prizes, the best algorithms still fail to solve a majority of ARC tasks. The best solvers today rely on complex hand-crafted rules, without using machine learning at all. We revisit whether recent advances in neural networks allow progress on this task, or whether an entirely different class of models are required. First, we adapt the DreamCoder Neurosymbolic reasoning solver to ARC. DreamCoder automatically writes programs in a bespoke domain-specific language to perform reasoning, using a neural network to mimic human intuition. We present the Perceptual Abstraction and Reasoning Language (PeARL) language, which allow DreamCoder to solve ARC tasks, and propose a new recognition model that allows us to significantly improve on the previous best implementation. We also propose a new encoding and augmentation scheme that allows large language models (LLMs) to solve ARC tasks, and find that the largest models can solve some ARC tasks. LLMs are able to solve a different group of problems to state-of-the-art solvers, and provide an interesting way to complement other approaches. We perform an ensemble analysis, combining models to achieve better results than any system alone. Finally, we publish the arckit Python library to make future research on ARC easier.},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Bober-Irizar_Banerjee_2024_Neural networks for abstraction and reasoning.pdf}
}

@article{botnarencoAutomatingScientificPaper2023,
  title = {Automating {{Scientific Paper Screening}} with {{ChatGPT}}: {{An Evaluation}} of {{E}}{\dbend}iciency and {{Accuracy}}},
  author = {Botnarenco, Daniel and Fleitas, Yeray Barrios and Ahmed, Faizan},
  year = {2023},
  abstract = {Goals: This study aims to evaluate the performance of di{\dbend}erent language models, including BERT and GPT, in scienti{\dbend}c paper screening. The primary research question is to assess their classi{\dbend}cation accuracy and language generation capabilities to gain insights into their potential and limitations. Method: The methodology involves evaluating the models for the speci{\dbend}c task of scienti{\dbend}c paper screening. The dataset comprises 6865 scienti{\dbend}c papers with screening decisions provided as ground truth labels. Evaluation metrics such as accuracy and F1 scores are used, along with confusion matrices, to assess the models' classi{\dbend}cation performance. Results: The results show that the BERT model achieved the highest accuracy and F1 score among the tested models, while GPT-3 Turbo and 4 exhibited lower classi{\dbend}cation accuracy and F1 score performance. The processing speeds varied, with BERT bene{\dbend}ting from the CUDA framework. Each model provides, at best, twice as fast as a human coder processing speed of documents. Implications: The {\dbend}ndings highlight the importance of prompt engineering and {\dbend}ne-tuning in improving language model performance for speci{\dbend}c tasks. The study contributes to developing and understanding large language models in natural language processing tasks, facilitating their e{\dbend}ective utilization in scienti{\dbend}c paper screening tasks.},
  langid = {english},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Botnarenco et al_2023_Automating Scientific Paper Screening with ChatGPT.pdf}
}

@article{brandEffectsMetacognitiveThinking2003,
  title = {Effects of Metacognitive Thinking and Knowledge Acquisition in Dyads on Individual Problem Solving and Transfer Performance},
  author = {Brand, Serge and Reimer, T. and Opwis, K.},
  year = {2003},
  month = dec,
  journal = {Swiss Journal of Psychology / Schweizerische Zeitschrift f{\"u}r Psychologie / Revue Suisse de Psychologie},
  volume = {62},
  number = {4},
  pages = {251--261},
  publisher = {Verlag Hans Huber},
  issn = {1421-0185},
  doi = {10.1024/1421-0185.62.4.251},
  urldate = {2024-07-03},
  abstract = {We investigated if metacognitive thinking and knowledge acquisition in dyads improve individual problem solving performance and transfer to new problems. In the learning phase, participants solved several Tower of Hanoi problems and half of them were stimulated to metacognitive thinking. A second variable studied was if the learning tasks were solved individually or in dyads. The subsequent individually completed test phase consisted of two structurally similar and of two dissimilar transfer tasks. Metacognitive stimulation enhanced performance in all cases. Those participants who had been stimulated to metacognitive thinking, whether individually or in dyads, performed better on every task than did the individuals in the control group. Dyads proved better at solving the learning tasks than did the individuals, although this advantage did not affect individual performance on the transfer tasks. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {dyads,Dyads,individuals,knowledge acquisition,Metacognition,metacognitive stimulation,metacognitive thinking,Performance,Problem Solving,problem solving performance,Stimulation,Thinking,Transfer (Learning),transfer performance},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Brand et al_2003_Effects of metacognitive thinking and knowledge acquisition in dyads on.pdf}
}

@article{brandHowWeLearn2007,
  title = {How Do We Learn in a Negative Mood? {{Effects}} of a Negative Mood on Transfer and Learning},
  shorttitle = {How Do We Learn in a Negative Mood?},
  author = {Brand, Serge and Reimer, Torsten and Opwis, Klaus},
  year = {2007},
  month = feb,
  journal = {Learning and Instruction},
  volume = {17},
  number = {1},
  pages = {1--16},
  issn = {09594752},
  doi = {10.1016/j.learninstruc.2006.11.002},
  urldate = {2024-06-22},
  abstract = {Findings show that both positive and negative mood may hinder or promote information processing. In two experiments, we show that negative mood impairs transfer effects and learning. In the first experiment, N {$\frac{1}{4}$} 54 participants drawn from a training course for the Swiss Corps of Fortification Guards first learned to solve the three- and four-disk Tower of Hanoi (ToH) problem to mastery level. After mood induction, they were asked to solve one proximal (five-disk ToH) and two distal transfer tasks (the Missionary and Cannibal Problem and the Katona Card Problem). Participants in a negative mood solved the transfer tasks less efficiently. In the second experiment, this result was replicated with a sample of N {$\frac{1}{4}$} 80 participants drawn from a training course for nurses. Additionally, mood affected performance if it was induced before the learning phase; participants in a negative mood needed more repetitions to reach the mastery level and also performed worse in the transfer tasks, although there were no greater mood differences in this problem-solving phase. The implications for the design of learning settings are discussed.},
  copyright = {https://www.elsevier.com/tdm/userlicense/1.0/},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Brand et al_2007_How do we learn in a negative mood.pdf}
}

@article{breithauptHumansCreateMore2024,
  title = {Humans Create More Novelty than {{ChatGPT}} When Asked to Retell a Story},
  author = {Breithaupt, Fritz and Otenen, Ege and Wright, Devin R. and Kruschke, John K. and Li, Ying and Tan, Yiyan},
  year = {2024},
  month = jan,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {875},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-50229-7},
  urldate = {2024-05-25},
  abstract = {Abstract             We compare how humans retell stories to how ChatGPT retells stories in chains of three retellings by different people or different accounts on ChatGPT. ChatGPT provides competent summaries of the original narrative texts in one step of retelling. In subsequent retellings few additional changes occur. Human retellers, by contrast, reduce the original text incrementally and by creating 55--60\% of novel words and concepts (synsets) at each iteration. The retellings by both ChatGPT and humans show very stable emotion ratings, which is a puzzle for human retellers given the high degree of novel inventions across retellings. ChatGPT maintains more nouns, adjectives, and prepositions and also uses language later acquired in life, while humans use more verbs, adverbs, and negations and use language acquired at a younger age. The results reveal that spontaneous retelling by humans involves ongoing creativity, anchored by emotions, beyond the default probabilistic wording of large language models such as ChatGPT.},
  langid = {english},
  annotation = {https://osf.io/jr2py/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Breithaupt et al_2024_Humans create more novelty than ChatGPT when asked to retell a story.pdf}
}

@article{brodyEvaluatingEffectivenessAI2024,
  title = {Evaluating the {{Effectiveness}} of {{AI Source Disclosure}} in {{Human}}--{{AI Communication}}},
  author = {Brody, Gabor and Rouillard, Vincent and Asherov, Daniel and Aravind, Athulya},
  year = {2024},
  month = mar,
  journal = {An MIT Exploration of Generative AI},
  doi = {10.21428/e4baedd9.3f5bb369},
  urldate = {2024-07-23},
  abstract = {Large language models can generate language that is often indistinguishable from language generated by humans, but they lack human motivations, beliefs, and accountability. These systems pose risks to our information ecosystem: bias amplification, fabrication, and misinformation are some of the forecasted negative consequences of mass adoption of the technology. The recurring regulatory proposal to mitigate such risks is obligatory source disclosure. The underlying assumption is that if people know the origin of AI-generated language, they can exercise appropriate caution when engaging with it. Here we apply concepts from linguistics and cognitive science to ask what appropriate caution means when engaging with AI-generated linguistic content. We discuss an idealized model of human communication as a motivated activity aimed at increasing the mutually shared beliefs among conversation partners. Building on this model, we develop a set of conceptual tools and empirical signatures to evaluate whether humans engage with AI-generated linguistic messages in the same way as they would with a fellow human or if they approach it differently. Our preliminary empirical investigation implies that even when humans know that some language was generated by AI, they nevertheless treat it on par with human language, albeit with somewhat diminished trust. The main implication of this finding is that source disclosure is not a sufficient regulatory strategy to manage risks that will arise with the proliferation of synthetic language.},
  langid = {english},
  annotation = {https://osf.io/rwj87/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Brody et al_2024_Evaluating the Effectiveness of AI Source Disclosure in Human–AI Communication.pdf}
}

@article{broschAffectiveInfluencesEnergyRelated2014,
  title = {Affective {{Influences}} on {{Energy-Related Decisions}} and {{Behaviors}}},
  author = {Brosch, Tobias and Patel, Martin K. and Sander, David},
  year = {2014},
  month = mar,
  journal = {Frontiers in Energy Research},
  volume = {2},
  publisher = {Frontiers},
  issn = {2296-598X},
  doi = {10.3389/fenrg.2014.00011},
  urldate = {2024-07-02},
  abstract = {A successful energy transition will depend not only on the development of new energy technologies, but also on changes in the patterns of individual energy-related decisions and behaviors resulting in substantial reductions in energy demand. Across scientific disciplines, most theoretical approaches that try to understand energy-related decisions and behaviors focus mainly on cognitive processes, such as computations of utility (typically economic), the impact of cognitive heuristics, or the role of individual beliefs. While these models already explain important aspects of human decisions and behavior in the energy domain, we argue that an additional consideration of the contributions of emotional processes may be very fruitful for a deeper understanding of the issue. In this contribution, we outline a theoretical perspective on energy-related decisions and behaviors that integrates emotions, elicited by a cognitive-affective appraisal of the relevance of a situation, into a response system driving adaptive decisions and behaviors. We empirically investigate the explanatory power of the model variables to predict intentions to reduce energy use demonstrating that the appraisal--emotion variables are able to account for additional variance that is not explained by two established models focused on cognitive processes (theory of planned behavior and value-belief-norm theory). Finally, we discuss how the appraisal--emotion approach may be fruitfully integrated with other existing approaches and outline some questions for future research.},
  langid = {english},
  keywords = {Affect,Behavior,Decision Making,emotion,Energy},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Brosch et al_2014_Affective Influences on Energy-Related Decisions and Behaviors.pdf}
}

@article{brueraModelingBrainRepresentations2023,
  title = {Modeling {{Brain Representations}} of {{Words}}' {{Concreteness}} in {{Context Using GPT-2}} and {{Human Ratings}}},
  author = {Bruera, Andrea and Tao, Yuan and Anderson, Andrew and {\c C}okal, Derya and Haber, Janosch and Poesio, Massimo},
  year = {2023},
  journal = {Cognitive Science},
  volume = {47},
  number = {12},
  pages = {e13388},
  issn = {1551-6709},
  doi = {10.1111/cogs.13388},
  urldate = {2023-12-22},
  abstract = {The meaning of most words in language depends on their context. Understanding how the human brain extracts contextualized meaning, and identifying where in the brain this takes place, remain important scientific challenges. But technological and computational advances in neuroscience and artificial intelligence now provide unprecedented opportunities to study the human brain in action as language is read and understood. Recent contextualized language models seem to be able to capture homonymic meaning variation (``bat'', in a baseball vs. a vampire context), as well as more nuanced differences of meaning---for example, polysemous words such as ``book'', which can be interpreted in distinct but related senses (``explain a book'', information, vs. ``open a book'', object) whose differences are fine-grained. We study these subtle differences in lexical meaning along the concrete/abstract dimension, as they are triggered by verb-noun semantic composition. We analyze functional magnetic resonance imaging (fMRI) activations elicited by Italian verb phrases containing nouns whose interpretation is affected by the verb to different degrees. By using a contextualized language model and human concreteness ratings, we shed light on where in the brain such fine-grained meaning variation takes place and how it is coded. Our results show that phrase concreteness judgments and the contextualized model can predict BOLD activation associated with semantic composition within the language network. Importantly, representations derived from a complex, nonlinear composition process consistently outperform simpler composition approaches. This is compatible with a holistic view of semantic composition in the brain, where semantic representations are modified by the process of composition itself. When looking at individual brain areas, we find that encoding performance is statistically significant, although with differing patterns of results, suggesting differential involvement, in the posterior superior temporal sulcus, inferior frontal gyrus and anterior temporal lobe, and in motor areas previously associated with processing of concreteness/abstractness.},
  copyright = {{\copyright} 2023 The Authors. Cognitive Science published by Wiley Periodicals LLC on behalf of Cognitive Science Society (CSS).},
  langid = {english},
  keywords = {Computational linguistics,Concreteness,fMRI,Language models,Machine learning,Polysemy,Semantic composition,Semantics},
  annotation = {https://osf.io/sphn4/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Bruera et al_2023_Modeling Brain Representations of Words' Concreteness in Context Using GPT-2.pdf;/Users/thomasgorman/Zotero/storage/FY67PRYP/cogs.html}
}

@article{brunyeGoingTownVisualized2012,
  title = {Going to Town: {{Visualized}} Perspectives and Navigation through Virtual Environments},
  shorttitle = {Going to Town},
  author = {Bruny{\'e}, Tad T. and Gardony, Aaron and Mahoney, Caroline R. and Taylor, Holly A.},
  year = {2012},
  month = jan,
  journal = {Computers in Human Behavior},
  volume = {28},
  number = {1},
  pages = {257--266},
  issn = {0747-5632},
  doi = {10.1016/j.chb.2011.09.008},
  urldate = {2020-08-28},
  abstract = {Two experiments examined how spatial learning perspectives support navigation through virtual urban environments. Participants briefly learned the overall layout of a virtual desktop environment, and then were taken on a simulated journey ending at a starting location within the environment. In Experiment 1, during the journey participants watched simulated video feeds either from the front of the vehicle (route perspective), above the vehicle (survey perspective), both feeds simultaneously, or no video at all. Participants then navigated between ten successive landmarks, and we measured indices of spatial and temporal efficiency, and heading error. Results indicated that the route perspective supported a restricted range of local navigation whereas the survey perspective better supported far-space navigation. Experiment 2 demonstrated that the survey perspective also better supports navigation around unexpected detours. Results are discussed with regard to theories of spatial memory and the design of computer-supported spatial visualization technologies.},
  langid = {english},
  keywords = {Navigation,Spatial cognition,Spatial perspectives,Spatial visualizations},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Brunyé et al_2012_Going to town.pdf;/Users/thomasgorman/Zotero/storage/C2A2SEJ4/S0747563211002019.html}
}

@article{brunyeRepresentationalFlexibilitySpecificity2008,
  title = {Representational Flexibility and Specificity Following Spatial Descriptions of Real-World Environments},
  author = {Bruny{\'e}, Tad T. and Rapp, David N. and Taylor, Holly A.},
  year = {2008},
  month = aug,
  journal = {Cognition},
  volume = {108},
  number = {2},
  pages = {418--443},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2008.03.005},
  urldate = {2020-08-28},
  abstract = {Current theories are mixed with regard to the nature of mental representations following spatial description reading. Whereas some findings argue that individuals' representations are invariant following text-based, map-based, or first-person experience, other studies have suggested that representations can also exhibit considerable flexibility. In the current project we investigated the influences of spatial description perspectives and depictions on the nature of mental representations. In Experiment 1, participants exhibited more flexibility following survey, compared to route, spatial descriptions. With extended study time, though, flexibility following route descriptions increased. In Experiment 2, complementary maps further enhanced flexibility for route-based descriptions. Interestingly, increased exposure to these maps actually reduced flexibility following survey descriptions. These results demonstrate that the nature of our spatial mental representations depends upon a variety of factors; delineating these factors is critical for resolving debates concerning the malleable and invariant characteristics of spatial memory.},
  langid = {english},
  keywords = {Maps,Memory,Mental models,Spatial cognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Brunyé et al_2008_Representational flexibility and specificity following spatial descriptions of.pdf;/Users/thomasgorman/Zotero/storage/EP4JX3NM/S0010027708000784.html}
}

@misc{burnellRevealingStructureLanguage2023,
  title = {Revealing the Structure of Language Model Capabilities},
  author = {Burnell, Ryan and Hao, Han and Conway, Andrew R. A. and Orallo, Jose Hernandez},
  year = {2023},
  month = jun,
  number = {arXiv:2306.10062},
  eprint = {2306.10062},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.10062},
  urldate = {2023-10-17},
  abstract = {Building a theoretical understanding of the capabilities of large language models (LLMs) is vital for our ability to predict and explain the behavior of these systems. Here, we investigate the structure of LLM capabilities by extracting latent capabilities from patterns of individual differences across a varied population of LLMs. Using a combination of Bayesian and frequentist factor analysis, we analyzed data from 29 different LLMs across 27 cognitive tasks. We found evidence that LLM capabilities are not monolithic. Instead, they are better explained by three well-delineated factors that represent reasoning, comprehension and core language modeling. Moreover, we found that these three factors can explain a high proportion of the variance in model performance. These results reveal a consistent structure in the capabilities of different LLMs and demonstrate the multifaceted nature of these capabilities. We also found that the three abilities show different relationships to model properties such as model size and instruction tuning. These patterns help refine our understanding of scaling laws and indicate that changes to a model that improve one ability might simultaneously impair others. Based on these findings, we suggest that benchmarks could be streamlined by focusing on tasks that tap into each broad model ability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Burnell et al_2023_Revealing the structure of language model capabilities.pdf;/Users/thomasgorman/Zotero/storage/NZYPJW3R/2306.html}
}

@article{burtonHowLargeLanguage2024,
  title = {How Large Language Models Can Reshape Collective Intelligence},
  author = {Burton, Jason W. and {Lopez-Lopez}, Ezequiel and Hechtlinger, Shahar and Rahwan, Zoe and Aeschbach, Samuel and Bakker, Michiel A. and Becker, Joshua A. and Berditchevskaia, Aleks and Berger, Julian and Brinkmann, Levin and Flek, Lucie and Herzog, Stefan M. and Huang, Saffron and Kapoor, Sayash and Narayanan, Arvind and Nussberger, Anne-Marie and Yasseri, Taha and Nickl, Pietro and Almaatouq, Abdullah and Hahn, Ulrike and Kurvers, Ralf H. J. M. and Leavy, Susan and Rahwan, Iyad and Siddarth, Divya and Siu, Alice and Woolley, Anita W. and Wulff, Dirk U. and Hertwig, Ralph},
  year = {2024},
  month = sep,
  journal = {Nature Human Behaviour},
  pages = {1--13},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-024-01959-9},
  urldate = {2024-09-23},
  abstract = {Collective intelligence underpins the success of groups, organizations, markets and societies. Through distributed cognition and coordination, collectives can achieve outcomes that exceed the capabilities of individuals---even experts---resulting in improved accuracy and novel capabilities. Often, collective intelligence is supported by information technology, such as online prediction markets that elicit the `wisdom of crowds', online forums that structure collective deliberation or digital platforms that crowdsource knowledge from the public. Large language models, however, are transforming how information is aggregated, accessed and transmitted online. Here we focus on the unique opportunities and challenges this transformation poses for collective intelligence. We bring together interdisciplinary perspectives from industry and academia to identify potential benefits, risks, policy-relevant considerations and open research questions, culminating in a call for a closer examination of how large language models affect humans' ability to collectively tackle complex problems.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  keywords = {Science,Society,technology and society},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Burton et al_2024_How large language models can reshape collective intelligence.pdf}
}

@misc{buschoffHaveWeBuilt2023,
  title = {Have We Built Machines That Think like People?},
  author = {Buschoff, Luca M. Schulze and Akata, Elif and Bethge, Matthias and Schulz, Eric},
  year = {2023},
  month = nov,
  number = {arXiv:2311.16093},
  eprint = {2311.16093},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {A chief goal of artificial intelligence is to build machines that think like people. Yet it has been argued that deep neural network architectures fail to accomplish this. Researchers have asserted these models' limitations in the domains of causal reasoning, intuitive physics, and intuitive psychology. Yet recent advancements, namely the rise of large language models, particularly those designed for visual processing, have rekindled interest in the potential to emulate human-like cognitive abilities. This paper evaluates the current state of vision-based large language models in the domains of intuitive physics, causal reasoning, and intuitive psychology. Through a series of controlled experiments, we investigate the extent to which these modern models grasp complex physical interactions, causal relationships, and intuitive understanding of others' preferences. Our findings reveal that, while these models demonstrate a notable proficiency in processing and interpreting visual data, they still fall short of human capabilities in these areas. The models exhibit a rudimentary understanding of physical laws and causal relationships, but their performance is hindered by a lack of deeper insights-a key aspect of human cognition. Furthermore, in tasks requiring an intuitive theory of mind, the models fail altogether. Our results emphasize the need for integrating more robust mechanisms for understanding causality, physical dynamics, and social cognition into modern-day, vision-based language models, and point out the importance of cognitively-inspired benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Buschoff et al_2023_Have we built machines that think like people.pdf;/Users/thomasgorman/Zotero/storage/7BFTHHRG/2311.html}
}

@misc{butlinConsciousnessArtificialIntelligence2023,
  title = {Consciousness in {{Artificial Intelligence}}: {{Insights}} from the {{Science}} of {{Consciousness}}},
  shorttitle = {Consciousness in {{Artificial Intelligence}}},
  author = {Butlin, Patrick and Long, Robert and Elmoznino, Eric and Bengio, Yoshua and Birch, Jonathan and Constant, Axel and Deane, George and Fleming, Stephen M. and Frith, Chris and Ji, Xu and Kanai, Ryota and Klein, Colin and Lindsay, Grace and Michel, Matthias and Mudrik, Liad and Peters, Megan A. K. and Schwitzgebel, Eric and Simon, Jonathan and VanRullen, Rufin},
  year = {2023},
  month = aug,
  number = {arXiv:2308.08708},
  eprint = {2308.08708},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-02-05},
  abstract = {Whether current or near-term AI systems could be conscious is a topic of scientific interest and increasing public concern. This report argues for, and exemplifies, a rigorous and empirically grounded approach to AI consciousness: assessing existing AI systems in detail, in light of our best-supported neuroscientific theories of consciousness. We survey several prominent scientific theories of consciousness, including recurrent processing theory, global workspace theory, higher-order theories, predictive processing, and attention schema theory. From these theories we derive "indicator properties" of consciousness, elucidated in computational terms that allow us to assess AI systems for these properties. We use these indicator properties to assess several recent AI systems, and we discuss how future systems might implement them. Our analysis suggests that no current AI systems are conscious, but also suggests that there are no obvious technical barriers to building AI systems which satisfy these indicators.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Butlin et al_2023_Consciousness in Artificial Intelligence.pdf;/Users/thomasgorman/Zotero/storage/6SDUUESH/2308.html}
}

@article{buttrickStudyingLargeLanguage2024,
  title = {Studying Large Language Models as Compression Algorithms for Human Culture},
  author = {Buttrick, Nicholas},
  year = {2024},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  volume = {28},
  number = {3},
  pages = {187--189},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2024.01.001},
  urldate = {2024-07-10},
  abstract = {Large language models (LLMs) extract and reproduce the statistical regularities in their training data. Researchers can use these models to study the conceptual relationships encoded in this training data (i.e., the open internet), providing a remarkable opportunity to understand the cultural distinctions embedded within much of recorded human communication.},
  keywords = {compression,cultural psychology,large language models},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Buttrick_2024_Studying large language models as compression algorithms for human culture.pdf;/Users/thomasgorman/Zotero/storage/UAHW9QEK/S1364661324000019.html}
}

@inproceedings{byunThisReferenceDoes2024,
  title = {This {{Reference Does Not Exist}}: {{An Exploration}} of {{LLM Citation Accuracy}} and {{Relevance}}},
  shorttitle = {This {{Reference Does Not Exist}}},
  booktitle = {Proceedings of the {{Third Workshop}} on {{Bridging Human}}--{{Computer Interaction}} and {{Natural Language Processing}}},
  author = {Byun, Courtni and Vasicek, Piper and Seppi, Kevin},
  editor = {Blodgett, Su Lin and Curry, Amanda Cercas and Dey, Sunipa and Madaio, Michael and Nenkova, Ani and Yang, Diyi and Xiao, Ziang},
  year = {2024},
  month = jun,
  pages = {28--39},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  doi = {10.18653/v1/2024.hcinlp-1.3},
  urldate = {2024-08-14},
  abstract = {Citations are a fundamental and indispensable part of research writing. They provide support and lend credibility to research findings. Recent GPT-fueled interest in large language models (LLMs) has shone a spotlight on the capabilities and limitations of these models when generating relevant citations for a document. Recent work has focused largely on title and author accuracy. We underline this effort and expand on it with a preliminary exploration in relevance of model-recommended citations. We define three citation-recommendation tasks. We also collect and annotate a dataset of model-recommended citations for those tasks. We find that GPT-4 largely outperforms earlier models on both author and title accuracy in two markedly different CS venues, but may not recommend references that are more relevant than those recommended by the earlier models. The two venues we compare are CHI and EMNLP. All models appear to perform better at recommending EMNLP papers than CHI papers.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Byun et al_2024_This Reference Does Not Exist.pdf}
}

@misc{canUsingLargeLanguage2023,
  title = {Using Large Language Models to Study Human Memory for Meaningful Narratives},
  author = {Can, Antonios Georgiou Tankut and Katkov, Mikhail and Tsodyks, Misha},
  year = {2023},
  month = nov,
  number = {arXiv:2311.04742},
  eprint = {2311.04742},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2023-11-15},
  abstract = {One of the most impressive achievements of the AI revolution is the development of large language models that can generate meaningful text and respond to instructions in plain English with no additional training necessary. Here we show that language models can be used as a scientific instrument for studying human memory for meaningful material. We developed a pipeline for designing large scale memory experiments and analyzing the obtained results. We performed online memory experiments with a large number of participants and collected recognition and recall data for narratives of different lengths. We found that both recall and recognition performance scale linearly with narrative length. Furthermore, in order to investigate the role of narrative comprehension in memory, we repeated these experiments using scrambled versions of the presented stories. We found that even though recall performance declined significantly, recognition remained largely unaffected. Interestingly, recalls in this condition seem to follow the original narrative order rather than the scrambled presentation, pointing to a contextual reconstruction of the story in memory.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Can et al_2023_Using large language models to study human memory for meaningful narratives.pdf;/Users/thomasgorman/Zotero/storage/J8V74DXC/2311.html}
}

@misc{caoWhatVisualCognition2024,
  title = {What Is the {{Visual Cognition Gap}} between {{Humans}} and {{Multimodal LLMs}}?},
  author = {Cao, Xu and Lai, Bolin and Ye, Wenqian and Ma, Yunsheng and Heintz, Joerg and Chen, Jintai and Cao, Jianguo and Rehg, James M.},
  year = {2024},
  month = jun,
  number = {arXiv:2406.10424},
  eprint = {2406.10424},
  publisher = {arXiv},
  urldate = {2024-07-03},
  abstract = {Visual Reasoning AVR is often used to determine human intelligence related to visual cognition and working memory [31, 32, 33]. Matrix reasoning and compositional visual relation reasoning are two of the most representative AVR problems that are widely used by RPM [7, 34], WISC [6, 35] to evaluate human's ability to detect the underlying conceptual relationship among visual objects and use reasoning to find visual cues. Early research indicated that deep learning models can be trained with large-scale AVR datasets to solve simple matrix reasoning [36, 13, 4, 37, 38] and compositional visual relation tasks [33, 20, 39, 40], achieving human-level accuracy. Several datasets and benchmarks are also proposed, such as PGM [9], RAVEN [10], RAVEN-I [12], RAVENFAIR [41], CVR [20]. However, these works have a key limitation. They ignore that humans can solve these problems by zero-shot reasoning without explicitly learning from large-scale data. After the blooming of LLMs, researchers are keen on testing whether LLMs reached the same abstract reasoning capabilities as humans. Webb et al. [24] encode matrix reasoning into a symbolic problem based on human's prior and validate LLM can understand this task. Recently, there are also some useful zero-shot visual reasoning inference datasets containing AVR samples have been proposed in the AI/ML community, such as RAVEN-IQ [16] containing 50 instances, Visual Reasoning Benchmark [42] containing 241 instances in total, and ConceptARC [43] containing 480 instances but all of them are limited by lacking rigorous human experiments as reference and conducting experiments on relatively small datasets without psychometrical validation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T01,Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {https://github.com/IrohXu/VCog-Bench},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Cao et al_2024_What is the Visual Cognition Gap between Humans and Multimodal LLMs.pdf}
}

@misc{carliniQuantifyingMemorizationNeural2023,
  title = {Quantifying {{Memorization Across Neural Language Models}}},
  author = {Carlini, Nicholas and Ippolito, Daphne and Jagielski, Matthew and Lee, Katherine and Tramer, Florian and Zhang, Chiyuan},
  year = {2023},
  month = mar,
  number = {arXiv:2202.07646},
  eprint = {2202.07646},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-22},
  abstract = {Large language models (LMs) have been shown to memorize parts of their training data, and when prompted appropriately, they will emit the memorized training data verbatim. This is undesirable because memorization violates privacy (exposing user data), degrades utility (repeated easy-to-memorize text is often low quality), and hurts fairness (some texts are memorized over others). We describe three log-linear relationships that quantify the degree to which LMs emit memorized training data. Memorization significantly grows as we increase (1) the capacity of a model, (2) the number of times an example has been duplicated, and (3) the number of tokens of context used to prompt the model. Surprisingly, we find the situation becomes more complicated when generalizing these results across model families. On the whole, we find that memorization in LMs is more prevalent than previously believed and will likely get worse as models continues to scale, at least without active mitigations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/carliniQuantifyingMemorizationNeural2023-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Carlini et al_2023_Quantifying Memorization Across Neural Language Models.pdf;/Users/thomasgorman/Zotero/storage/XY2PE3ER/2202.html}
}

@inproceedings{carpenterAssessingStudentExplanations2024,
  title = {Assessing {{Student Explanations}} with {{Large Language Models Using Fine-Tuning}} and {{Few-Shot Learning}}},
  booktitle = {Proceedings of the 19th {{Workshop}} on {{Innovative Use}} of {{NLP}} for {{Building Educational Applications}} ({{BEA}} 2024)},
  author = {Carpenter, Dan and Min, Wookhee and Lee, Seung and Ozogul, Gamze and Zheng, Xiaoying and Lester, James},
  editor = {Kochmar, Ekaterina and Bexte, Marie and Burstein, Jill and Horbach, Andrea and {Laarmann-Quante}, Ronja and Tack, Ana{\"i}s and Yaneva, Victoria and Yuan, Zheng},
  year = {2024},
  month = jun,
  pages = {403--413},
  publisher = {Association for Computational Linguistics},
  address = {Mexico City, Mexico},
  urldate = {2024-07-01},
  abstract = {The practice of soliciting self-explanations from students is widely recognized for its pedagogical benefits. However, the labor-intensive effort required to manually assess students' explanations makes it impractical for classroom settings. As a result, many current solutions to gauge students' understanding during class are often limited to multiple choice or fill-in-the-blank questions, which are less effective at exposing misconceptions or helping students to understand and integrate new concepts. Recent advances in large language models (LLMs) present an opportunity to assess student explanations in real-time, making explanation-based classroom response systems feasible for implementation. In this work, we investigate LLM-based approaches for assessing the correctness of students' explanations in response to undergraduate computer science questions. We investigate alternative prompting approaches for multiple LLMs (i.e., Llama 2, GPT-3.5, and GPT-4) and compare their performance to FLAN-T5 models trained in a fine-tuning manner. The results suggest that the highest accuracy and weighted F1 score were achieved by fine-tuning FLAN-T5, while an in-context learning approach with GPT-4 attains the highest macro F1 score.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Carpenter et al_2024_Assessing Student Explanations with Large Language Models Using Fine-Tuning and.pdf}
}

@techreport{cassaniMeaningModulationsStability2023,
  type = {Preprint},
  title = {Meaning {{Modulations}} and {{Stability}} in {{Large Language Models}}: {{An Analysis}} of {{BERT Embeddings}} for {{Psycholinguistic Research}}},
  shorttitle = {Meaning {{Modulations}} and {{Stability}} in {{Large Language Models}}},
  author = {Cassani, Giovanni and Guenther, Fritz and Attanasio, Giuseppe and Bianchi, Federico and Marelli, Marco},
  year = {2023},
  month = oct,
  institution = {PsyArXiv},
  urldate = {2023-11-19},
  abstract = {Computational models of semantic representations have long assumed and produced a single static representation for each word type, ignoring the influence of linguistic context on semantic representations. Recent Large Language Models (LLMs) introduced in Natural Language Processing, however, learn token-level contextualised representations, holding promise to study how semantic representations change in different contexts. In this study we probe type- and token-level representations learned using a prominent example of such models, Bidirectional Encoder Representations from Transformers (BERT), for their ability to i) explain semantic effects found for isolated words (semantic relatedness and similarity ratings, lexical decision, and semantic priming), but critically also to ii) exhibit systematic interactions between lexical semantics and context, and iii) explain meaning modulations in context. Across a wide range of empirical studies on each of these topics, we show that BERT representations satisfy two desiderata for psychologically valid semantic representations: i) they have a stable semantic core which allows people to interpret words in isolation and prevents words to be used arbitrarily and ii) they interact with sentence context in systematic ways, with representations shifting as a function of their semantic core and the context. This demonstrates that a single, comprehensive model which simultaneously learns abstract, type-level prototype representations as well as mechanisms of how these interact with context can explain both isolated word effects and context-dependent variations. Notably, these variations are not limited to discrete word senses, eschewing a strict dichotomy between exemplar and prototype models and re-framing traditional notions of polysemy.},
  langid = {english},
  annotation = {https://osf.io/94ena/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Cassani et al_2023_Meaning Modulations and Stability in Large Language Models.pdf}
}

@misc{cevoliShadesMeaningUncovering2023,
  title = {Shades of Meaning: {{Uncovering}} the Geometry of Ambiguous Word Representations through Contextualised Language Models},
  shorttitle = {Shades of Meaning},
  author = {Cevoli, Benedetta and Watkins, Chris and Gao, Yang and Rastle, Kathleen},
  year = {2023},
  month = apr,
  number = {arXiv:2304.13597},
  eprint = {2304.13597},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-04-29},
  abstract = {Lexical ambiguity presents a profound and enduring challenge to the language sciences. Researchers for decades have grappled with the problem of how language users learn, represent and process words with more than one meaning. Our work offers new insight into psychological understanding of lexical ambiguity through a series of simulations that capitalise on recent advances in contextual language models. These models have no grounded understanding of the meanings of words at all; they simply learn to predict words based on the surrounding context provided by other words. Yet, our analyses show that their representations capture fine-grained meaningful distinctions between unambiguous, homonymous, and polysemous words that align with lexicographic classifications and psychological theorising. These findings provide quantitative support for modern psychological conceptualisations of lexical ambiguity and raise new challenges for understanding of the way that contextual information shapes the meanings of words across different timescales.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/cevoliShadesMeaningUncovering2023-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Cevoli et al_2023_Shades of meaning.pdf;/Users/thomasgorman/Zotero/storage/E96FSE5X/2304.html}
}

@misc{changCharacterizingLearningCurves2023,
  title = {Characterizing {{Learning Curves During Language Model Pre-Training}}: {{Learning}}, {{Forgetting}}, and {{Stability}}},
  shorttitle = {Characterizing {{Learning Curves During Language Model Pre-Training}}},
  author = {Chang, Tyler A. and Tu, Zhuowen and Bergen, Benjamin K.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.15419},
  eprint = {2308.15419},
  publisher = {arXiv},
  urldate = {2023-09-02},
  abstract = {How do language models learn to make predictions during pre-training? To study this question, we extract learning curves from five autoregressive English language model pre-training runs, for 1M tokens in context. We observe that the language models generate short repetitive phrases before learning to generate longer and more coherent text. We quantify the final surprisal, within-run variability, age of acquisition, forgettability, and cross-run variability of learning curves for individual tokens in context. More frequent tokens reach lower final surprisals, exhibit less variability within and across pre-training runs, are learned earlier, and are less likely to be "forgotten" during pre-training. Higher n-gram probabilities further accentuate these effects. Independent of the target token, shorter and more frequent contexts correlate with marginally more stable and quickly acquired predictions. Effects of part-of-speech are also small, although nouns tend to be acquired later and less stably than verbs, adverbs, and adjectives. Our work contributes to a better understanding of language model pre-training dynamics and informs the deployment of stable language models in practice.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/tylerachang/lm-learning-curves},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chang et al_2023_Characterizing Learning Curves During Language Model Pre-Training.pdf;/Users/thomasgorman/Zotero/storage/88M6PRB6/2308.html}
}

@misc{chanScalingSyntheticData2024,
  title = {Scaling {{Synthetic Data Creation}} with 1,000,000,000 {{Personas}}},
  author = {Chan, Xin and Wang, Xiaoyang and Yu, Dian and Mi, Haitao and Yu, Dong},
  year = {2024},
  month = jun,
  number = {arXiv:2406.20094},
  eprint = {2406.20094},
  publisher = {arXiv},
  urldate = {2024-07-09},
  abstract = {We propose a novel persona-driven data synthesis methodology that leverages various perspectives within a large language model (LLM) to create diverse synthetic data. To fully exploit this methodology at scale, we introduce Persona Hub -- a collection of 1 billion diverse personas automatically curated from web data. These 1 billion personas ({$\sim$}13\% of the world's total population), acting as distributed carriers of world knowledge, can tap into almost every perspective encapsulated within the LLM, thereby facilitating the creation of diverse synthetic data at scale for various scenarios. By showcasing Persona Hub's use cases in synthesizing high-quality mathematical and logical reasoning problems, instructions (i.e., user prompts), knowledge-rich texts, game NPCs and tools (functions) at scale, we demonstrate persona-driven data synthesis is versatile, scalable, flexible, and easy to use, potentially driving a paradigm shift in synthetic data creation and applications in practice, which may have a profound impact on LLM research and development.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/tencent-ailab/persona-hub},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chan et al_2024_Scaling Synthetic Data Creation with 1,000,000,000 Personas.pdf}
}

@misc{chanTransformersGeneralizeDifferently2022,
  title = {Transformers Generalize Differently from Information Stored in Context vs in Weights},
  author = {Chan, Stephanie C. Y. and Dasgupta, Ishita and Kim, Junkyung and Kumaran, Dharshan and Lampinen, Andrew K. and Hill, Felix},
  year = {2022},
  month = oct,
  number = {arXiv:2210.05675},
  eprint = {2210.05675},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.05675},
  urldate = {2024-04-12},
  abstract = {Transformer models can use two fundamentally different kinds of information: information stored in weights during training, and information provided ``in-context'' at inference time. In this work, we show that transformers exhibit different inductive biases in how they represent and generalize from the information in these two sources. In particular, we characterize whether they generalize via parsimonious rules (rule-based generalization) or via direct comparison with observed examples (exemplar-based generalization). This is of important practical consequence, as it informs whether to encode information in weights or in context, depending on how we want models to use that information. In transformers trained on controlled stimuli, we find that generalization from weights is more rule-based whereas generalization from context is largely exemplar-based. In contrast, we find that in transformers pre-trained on natural language, in-context learning is significantly rule-based, with larger models showing more rule-basedness. We hypothesise that rule-based generalization from in-context information might be an emergent consequence of large-scale training on language, which has sparse rule-like structure. Using controlled stimuli, we verify that transformers pretrained on data containing sparse rule-like structure exhibit more rule-based generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning}
}

@misc{chenCanLLMGeneratedMisinformation2024,
  title = {Can {{LLM-Generated Misinformation Be Detected}}?},
  author = {Chen, Canyu and Shu, Kai},
  year = {2024},
  month = apr,
  number = {arXiv:2309.13788},
  eprint = {2309.13788},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.13788},
  urldate = {2024-07-23},
  abstract = {The advent of Large Language Models (LLMs) has made a transformative impact. However, the potential that LLMs such as ChatGPT can be exploited to generate misinformation has posed a serious concern to online safety and public trust. A fundamental research question is: will LLM-generated misinformation cause more harm than human-written misinformation? We propose to tackle this question from the perspective of detection difficulty. We first build a taxonomy of LLM-generated misinformation. Then we categorize and validate the potential real-world methods for generating misinformation with LLMs. Then, through extensive empirical investigation, we discover that LLM-generated misinformation can be harder to detect for humans and detectors compared to human-written misinformation with the same semantics, which suggests it can have more deceptive styles and potentially cause more harm. We also discuss the implications of our discovery on combating misinformation in the age of LLMs and the countermeasures.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chen_Shu_2024_Can LLM-Generated Misinformation Be Detected.pdf;/Users/thomasgorman/Zotero/storage/DR6F4LUW/2309.html}
}

@article{chenEmergenceEconomicRationality2023,
  title = {The Emergence of Economic Rationality of {{GPT}}},
  author = {Chen, Yiting and Liu, Tracy Xiao and Shan, You and Zhong, Songfa},
  year = {2023},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {51},
  pages = {e2316205120},
  doi = {10.1073/pnas.2316205120},
  urldate = {2024-07-10},
  abstract = {As large language models (LLMs) like GPT become increasingly prevalent, it is essential that we assess their capabilities beyond language processing. This paper examines the economic rationality of GPT by instructing it to make budgetary decisions in four domains: risk, time, social, and food preferences. We measure economic rationality by assessing the consistency of GPT's decisions with utility maximization in classic revealed preference theory. We find that GPT's decisions are largely rational in each domain and demonstrate higher rationality score than those of human subjects in a parallel experiment and in the literature. Moreover, the estimated preference parameters of GPT are slightly different from human subjects and exhibit a lower degree of heterogeneity. We also find that the rationality scores are robust to the degree of randomness and demographic settings such as age and gender but are sensitive to contexts based on the language frames of the choice situations. These results suggest the potential of LLMs to make good decisions and the need to further understand their capabilities, limitations, and underlying mechanisms.},
  annotation = {https://www.dropbox.com/scl/fo/572ptz57vjis5cqkczj9l/h?rlkey=hhpsgdb6ghdzsrvj35mafdnwc\&e=1\&dl=0},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chen et al_2023_The emergence of economic rationality of GPT.pdf}
}

@misc{chenHuatuoGPTVisionInjectingMedical2024,
  title = {{{HuatuoGPT-Vision}}, {{Towards Injecting Medical Visual Knowledge}} into {{Multimodal LLMs}} at {{Scale}}},
  author = {Chen, Junying and Ouyang, Ruyi and Gao, Anningzhe and Chen, Shunian and Chen, Guiming Hardy and Wang, Xidong and Zhang, Ruifei and Cai, Zhenyang and Ji, Ke and Yu, Guangjun and Wan, Xiang and Wang, Benyou},
  year = {2024},
  month = jun,
  number = {arXiv:2406.19280},
  eprint = {2406.19280},
  publisher = {arXiv},
  urldate = {2024-07-06},
  abstract = {The rapid development of multimodal large language models (MLLMs), such as GPT-4V, has led to significant advancements. However, these models still face challenges in medical multimodal capabilities due to limitations in the quantity and quality of medical vision-text data, stemming from data privacy concerns and high annotation costs. While pioneering approaches utilize PubMed's large-scale, de-identified medical image-text pairs to address these limitations, they still fall short due to inherent data noise. To tackle this, we refined medical image-text pairs from PubMed and employed MLLMs (GPT-4V) in an 'unblinded' capacity to denoise and reformat the data, resulting in the creation of the PubMedVision dataset with 1.3 million medical VQA samples. Our validation demonstrates that: (1) PubMedVision can significantly enhance the medical multimodal capabilities of current MLLMs, showing significant improvement in benchmarks including the MMMU Health \& Medicine track; (2) manual checks by medical experts and empirical results validate the superior data quality of our dataset compared to other data construction methods. Using PubMedVision, we train a 34B medical MLLM HuatuoGPT-Vision, which shows superior performance in medical multimodal scenarios among open-source MLLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {https://github.com/FreedomIntelligence/HuatuoGPT-Vision},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chen et al_2024_HuatuoGPT-Vision, Towards Injecting Medical Visual Knowledge into Multimodal.pdf}
}

@misc{chenMLLMJudgeAssessingMultimodal2024,
  title = {{{MLLM-as-a-Judge}}: {{Assessing Multimodal LLM-as-a-Judge}} with {{Vision-Language Benchmark}}},
  shorttitle = {{{MLLM-as-a-Judge}}},
  author = {Chen, Dongping and Chen, Ruoxi and Zhang, Shilin and Liu, Yinuo and Wang, Yaochen and Zhou, Huichi and Zhang, Qihui and Wan, Yao and Zhou, Pan and Sun, Lichao},
  year = {2024},
  month = jun,
  number = {arXiv:2402.04788},
  eprint = {2402.04788},
  publisher = {arXiv},
  urldate = {2024-06-16},
  abstract = {Multimodal Large Language Models (MLLMs) have gained significant attention recently, showing remarkable potential in artificial general intelligence. However, assessing the utility of MLLMs presents considerable challenges, primarily due to the absence of multimodal benchmarks that align with human preferences. Drawing inspiration from the concept of LLM-as-a-Judge within LLMs, this paper introduces a novel benchmark, termed MLLM-as-a-Judge, to assess the ability of MLLMs in assisting judges across diverse modalities, encompassing three distinct tasks: Scoring Evaluation, Pair Comparison, and Batch Ranking. Our study reveals that, while MLLMs demonstrate remarkable human-like discernment in Pair Comparison, there is a significant divergence from human preferences in Scoring Evaluation and Batch Ranking. Furthermore, a closer examination reveals persistent challenges in the judgment capacities of LLMs, including diverse biases, hallucinatory responses, and inconsistencies in judgment, even in advanced models such as GPT-4V. These findings emphasize the pressing need for enhancements and further research efforts to be undertaken before regarding MLLMs as fully reliable evaluators. In light of this, we advocate for additional efforts dedicated to supporting the continuous development within the domain of MLLM functioning as judges. The code and dataset are publicly available at our project homepage: https://mllm-judge. github.io/.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  annotation = {https://github.com/Dongping-Chen/MLLM-Judge},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chen et al_2024_MLLM-as-a-Judge.pdf}
}

@misc{chenSpatialVLMEndowingVisionLanguage2024,
  title = {{{SpatialVLM}}: {{Endowing Vision-Language Models}} with {{Spatial Reasoning Capabilities}}},
  shorttitle = {{{SpatialVLM}}},
  author = {Chen, Boyuan and Xu, Zhuo and Kirmani, Sean and Ichter, Brian and Driess, Danny and Florence, Pete and Sadigh, Dorsa and Guibas, Leonidas and Xia, Fei},
  year = {2024},
  month = jan,
  number = {arXiv:2401.12168},
  eprint = {2401.12168},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2401.12168},
  urldate = {2024-07-26},
  abstract = {Understanding and reasoning about spatial relationships is a fundamental capability for Visual Question Answering (VQA) and robotics. While Vision Language Models (VLM) have demonstrated remarkable performance in certain VQA benchmarks, they still lack capabilities in 3D spatial reasoning, such as recognizing quantitative relationships of physical objects like distances or size differences. We hypothesize that VLMs' limited spatial reasoning capability is due to the lack of 3D spatial knowledge in training data and aim to solve this problem by training VLMs with Internet-scale spatial reasoning data. To this end, we present a system to facilitate this approach. We first develop an automatic 3D spatial VQA data generation framework that scales up to 2 billion VQA examples on 10 million real-world images. We then investigate various factors in the training recipe, including data quality, training pipeline, and VLM architecture. Our work features the first internet-scale 3D spatial reasoning dataset in metric space. By training a VLM on such data, we significantly enhance its ability on both qualitative and quantitative spatial VQA. Finally, we demonstrate that this VLM unlocks novel downstream applications in chain-of-thought spatial reasoning and robotics due to its quantitative estimation capability. Project website: https://spatial-vlm.github.io/},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Robotics},
  annotation = {https://spatial-vlm.github.io/\\
\\
https://github.com/remyxai/VQASynth\\
\\
https://huggingface.co/remyxai/SpaceLLaVA\\
\\
https://www.youtube.com/watch?v=\_z9b5E\_obbE},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chen et al_2024_SpatialVLM.pdf;/Users/thomasgorman/Zotero/storage/R6UUQEUV/2401.html}
}

@misc{cheungLargeLanguageModels2024,
  title = {Large {{Language Models Amplify Human Biases}} in {{Moral Decision-Making}}},
  author = {Cheung, Vanessa and Maier, Maximilian and Lieder, Falk},
  year = {2024},
  month = jun,
  doi = {10.31234/osf.io/aj46b},
  urldate = {2024-06-19},
  abstract = {As large language models (LLMs) become more widely used, people increasingly rely on them to make or advise on moral decisions. Some researchers even propose using LLMs as participants in psychology experiments. It is therefore important to understand how well LLMs make moral decisions and how they compare to humans. We investigated this question in realistic moral dilemmas using prompts where GPT-4, Llama 3, and Claude 3 give advice and where they emulate a research participant. In Study 1, we compared responses from LLMs to a representative US sample (N = 285) for 22 dilemmas: social dilemmas that pitted self-interest against the greater good, and moral dilemmas that pitted utilitarian cost-benefit reasoning against deontological rules. In social dilemmas, LLMs were more altruistic than participants. In moral dilemmas, LLMs exhibited stronger omission bias than participants: they usually endorsed inaction over action. In Study 2 (N = 490, preregistered), we replicated this omission bias and document an additional bias: unlike humans, LLMs (except GPT-4o) tended to answer ``no'' in moral dilemmas, whereby the phrasing of the question influences the decision even when physical action remains the same. Our findings show that LLM moral decision-making amplifies human biases and introduces potentially problematic biases.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  annotation = {https://osf.io/3kvjd/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Cheung et al_2024_Large Language Models Amplify Human Biases in Moral Decision-Making.pdf}
}

@misc{chevalierLanguageModelsScience2024,
  title = {Language {{Models}} as {{Science Tutors}}},
  author = {Chevalier, Alexis and Geng, Jiayi and Wettig, Alexander and Chen, Howard and Mizera, Sebastian and Annala, Toni and Aragon, Max Jameson and Fanlo, Arturo Rodr{\'i}guez and Frieder, Simon and Machado, Simon and Prabhakar, Akshara and Thieu, Ellie and Wang, Jiachen T. and Wang, Zirui and Wu, Xindi and Xia, Mengzhou and Xia, Wenhan and Yu, Jiatong and Zhu, Jun-Jie and Ren, Zhiyong Jason and Arora, Sanjeev and Chen, Danqi},
  year = {2024},
  month = jul,
  number = {arXiv:2402.11111},
  eprint = {2402.11111},
  publisher = {arXiv},
  urldate = {2024-08-11},
  abstract = {NLP has recently made exciting progress toward training language models (LMs) with strong scientific problem-solving skills. However, model development has not focused on real-life usecases of LMs for science, including applications in education that require processing long scientific documents. To address this, we introduce TUTOREVAL and TUTORCHAT. TUTOREVAL is a diverse question-answering benchmark consisting of questions about long chapters from STEM textbooks, written by experts. TUTOREVAL helps measure real-life usability of LMs as scientific assistants, and it is the first benchmark combining long contexts, free-form generation, and multidisciplinary scientific knowledge. Moreover, we show that fine-tuning base models with existing dialogue datasets leads to poor performance on TUTOREVAL. Therefore, we create TUTORCHAT, a dataset of 80,000 long synthetic dialogues about textbooks. We use TUTORCHAT to fine-tune Llemma models with 7B and 34B parameters. These LM tutors specialized in math have a 32K-token context window, and they excel at TUTOREVAL while performing strongly on GSM8K and MATH. Our datasets build on opensource materials, and we release our models, data, and evaluations publicly.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/princeton-nlp/LM-Science-Tutor},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chevalier et al_2024_Language Models as Science Tutors.pdf}
}

@inproceedings{chiangEnhancingAIAssistedGroup2024,
  title = {Enhancing {{AI-Assisted Group Decision Making}} through {{LLM-Powered Devil}}'s {{Advocate}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Chiang, Chun-Wei and Lu, Zhuoran and Li, Zhuoyan and Yin, Ming},
  year = {2024},
  month = mar,
  pages = {103--119},
  publisher = {ACM},
  address = {Greenville SC USA},
  doi = {10.1145/3640543.3645199},
  urldate = {2024-07-03},
  abstract = {Group decision making plays a crucial role in our complex and interconnected world. The rise of AI technologies has the potential to provide data-driven insights to facilitate group decision making, although it is found that groups do not always utilize AI assistance appropriately. In this paper, we aim to examine whether and how the introduction of a devil's advocate in the AI-assisted group deci- sion making processes could help groups better utilize AI assistance and change the perceptions of group processes during decision making. Inspired by the exceptional conversational capabilities ex- hibited by modern large language models (LLMs), we design four different styles of devil's advocate powered by LLMs, varying their interactivity (i.e., interactive vs. non-interactive) and their target of objection (i.e., challenge the AI recommendation or the majority opinion within the group). Through a randomized human-subject experiment, we find evidence suggesting that LLM-powered devil's advocates that argue against the AI model's decision recommenda- tion have the potential to promote groups' appropriate reliance on AI. Meanwhile, the introduction of LLM-powered devil's advocate usually does not lead to substantial increases in people's perceived workload for completing the group decision making tasks, while interactive LLM-powered devil's advocates are perceived as more collaborating and of higher quality. We conclude by discussing the practical implications of our findings.},
  isbn = {9798400705083},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chiang et al_2024_Enhancing AI-Assisted Group Decision Making through LLM-Powered Devil's Advocate.pdf}
}

@misc{chobeyCanTrainingNeural2023,
  title = {Can Training Neural Language Models on a Curriculum with Developmentally Plausible Data Improve Alignment with Human Reading Behavior?},
  author = {Chobey, Aryaman and Smith, Oliver and Wang, Anzi and Prasad, Grusha},
  year = {2023},
  month = nov,
  number = {arXiv:2311.18761},
  eprint = {2311.18761},
  publisher = {arXiv},
  urldate = {2023-12-05},
  abstract = {The use of neural language models to model human behavior has met with mixed success. While some work has found that the surprisal estimates from these models can be used to predict a wide range of human neural and behavioral responses, other work studying more complex syntactic phenomena has found that these surprisal estimates generate incorrect behavioral predictions. This paper explores the extent to which the misalignment between empirical and model-predicted behavior can be minimized by training models on more developmentally plausible data, such as in the BabyLM Challenge. We trained teacher language models on the BabyLM "strict-small" dataset and used sentence level surprisal estimates from these teacher models to create a curriculum. We found tentative evidence that our curriculum made it easier for models to acquire linguistic knowledge from the training data: on the subset of tasks in the BabyLM challenge suite evaluating models' grammatical knowledge of English, models first trained on the BabyLM data curriculum and then on a few randomly ordered training epochs performed slightly better than models trained on randomly ordered epochs alone. This improved linguistic knowledge acquisition did not result in better alignment with human reading behavior, however: models trained on the BabyLM dataset (with or without a curriculum) generated predictions that were as misaligned with human behavior as models trained on larger less curated datasets. This suggests that training on developmentally plausible datasets alone is likely insufficient to generate language models capable of accurately predicting human language processing.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/vansky/neural-complexity},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chobey et al_2023_Can training neural language models on a curriculum with developmentally.pdf;/Users/thomasgorman/Zotero/storage/2YETKWHR/2311.html}
}

@misc{chuangDemographicsAligningRoleplaying2024,
  title = {Beyond {{Demographics}}: {{Aligning Role-playing LLM-based Agents Using Human Belief Networks}}},
  shorttitle = {Beyond {{Demographics}}},
  author = {Chuang, Yun-Shiuan and Studdiford, Zach and Nirunwiroj, Krirk and Goyal, Agam and Frigo, Vincent V. and Yang, Sijia and Shah, Dhavan and Hu, Junjie and Rogers, Timothy T.},
  year = {2024},
  month = jun,
  number = {arXiv:2406.17232},
  eprint = {2406.17232},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-30},
  abstract = {Creating human-like large language model (LLM) agents is crucial for faithful social simulation. Having LLMs role-play based on demographic information sometimes improves human likeness but often does not. This study assessed whether LLM alignment with human behavior can be improved by integrating information from empirically-derived human belief networks. Using data from a human survey, we estimated a belief network encompassing 18 topics loading on two non-overlapping latent factors. We then seeded LLM-based agents with an opinion on one topic, and assessed the alignment of its expressed opinions on remaining test topics with corresponding human data. Role-playing based on demographic information alone did not align LLM and human opinions, but seeding the agent with a single belief greatly improved alignment for topics related in the belief network, and not for topics outside the network. These results suggest a novel path for human-LLM belief alignment in work seeking to simulate and understand patterns of belief distributions in society.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chuang et al_2024_Beyond Demographics.pdf}
}

@misc{chuangEvaluatingLLMAgent2023,
  title = {Evaluating {{LLM Agent Group Dynamics}} against {{Human Group Dynamics}}: {{A Case Study}} on {{Wisdom}} of {{Partisan Crowds}}},
  shorttitle = {Evaluating {{LLM Agent Group Dynamics}} against {{Human Group Dynamics}}},
  author = {Chuang, Yun-Shiuan and Suresh, Siddharth and Harlalka, Nikunj and Goyal, Agam and Hawkins, Robert and Yang, Sijia and Shah, Dhavan and Hu, Junjie and Rogers, Timothy T.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.09665},
  eprint = {2311.09665},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {This study investigates the potential of Large Language Models (LLMs) to simulate human group dynamics, particularly within politically charged contexts. We replicate the Wisdom of Partisan Crowds phenomenon using LLMs to role-play as Democrat and Republican personas, engaging in a structured interaction akin to human group study. Our approach evaluates how agents' responses evolve through social influence. Our key findings indicate that LLM agents role-playing detailed personas and without Chain-of-Thought (CoT) reasoning closely align with human behaviors, while having CoT reasoning hurts the alignment. However, incorporating explicit biases into agent prompts does not necessarily enhance the wisdom of partisan crowds. Moreover, fine-tuning LLMs with human data shows promise in achieving human-like behavior but poses a risk of overfitting certain behaviors. These findings show the potential and limitations of using LLM agents in modeling human group phenomena.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chuang et al_2023_Evaluating LLM Agent Group Dynamics against Human Group Dynamics.pdf;/Users/thomasgorman/Zotero/storage/ZNNSG5KQ/2311.html}
}

@misc{chuangSimulatingOpinionDynamics2023,
  title = {Simulating {{Opinion Dynamics}} with {{Networks}} of {{LLM-based Agents}}},
  author = {Chuang, Yun-Shiuan and Goyal, Agam and Harlalka, Nikunj and Suresh, Siddharth and Hawkins, Robert and Yang, Sijia and Shah, Dhavan and Hu, Junjie and Rogers, Timothy T.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.09618},
  eprint = {2311.09618},
  primaryclass = {physics},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {Accurately simulating human opinion dynamics is crucial for understanding a variety of societal phenomena, including polarization and the spread of misinformation. However, the agent-based models (ABMs) commonly used for such simulations lack fidelity to human behavior. We propose a new approach to simulating opinion dynamics based on populations of Large Language Models (LLMs). Our findings reveal a strong inherent bias in LLM agents towards accurate information, leading to consensus in line with scientific reality. However, this bias limits the simulation of individuals with resistant views on issues like climate change. After inducing confirmation bias through prompt engineering, we observed opinion fragmentation in line with existing agent-based research. These insights highlight the promise and limitations of LLM agents in this domain and suggest a path forward: refining LLMs with real-world discourse to better simulate the evolution of human beliefs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Physics - Physics and Society},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chuang et al_2023_Simulating Opinion Dynamics with Networks of LLM-based Agents.pdf;/Users/thomasgorman/Zotero/storage/FMRAGVZL/2311.html}
}

@article{chuangWisdomPartisanCrowds2024,
  title = {The {{Wisdom}} of {{Partisan Crowds}}: {{Comparing Collective Intelligence}} in {{Humans}} and {{LLM-based Agents}}},
  author = {Chuang, Yun-Shiuan and Harlalka, Nikunj and Suresh, Siddharth and Goyal, Agam and Hawkins, Robert and Yang, Sijia and Shah, Dhavan and Hu, Junjie and Rogers, Timothy T},
  year = {2024},
  abstract = {Human groups are able to converge to more accurate beliefs through deliberation, even in the presence of polarization and partisan bias --- a phenomenon known as the ``wisdom of partisan crowds.'' Large Language Models (LLMs) are increasingly being used to simulate human collective behavior, yet few benchmarks exist for evaluating their dynamics against the behavior of human groups. In this paper, we examine the extent to which the wisdom of partisan crowds emerges in groups of LLM-based agents that are prompted to role-play as partisan personas (e.g., Democrat or Republican). We find that they not only display human-like partisan biases, but also converge to more accurate beliefs through deliberation, as humans do. We then identify several factors that interfere with convergence, including the use of chain-of-thought prompting and lack of details in personas. Conversely, fine-tuning on human data appears to enhance convergence. These findings show the potential and limitations of LLM-based agents as a model of human collective intelligence.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Chuang et al_The Wisdom of Partisan Crowds.pdf}
}

@article{cichyDeepNeuralNetworks2019,
  title = {Deep {{Neural Networks}} as {{Scientific Models}}},
  author = {Cichy, Radoslaw M. and Kaiser, Daniel},
  year = {2019},
  month = apr,
  journal = {Trends in Cognitive Sciences},
  volume = {23},
  number = {4},
  pages = {305--317},
  issn = {13646613},
  doi = {10.1016/j.tics.2019.01.009},
  urldate = {2022-01-15},
  langid = {english},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Cichy_Kaiser_2019_Deep Neural Networks as Scientific Models.pdf}
}

@article{cisneros-velardePrinciplesOpinionDynamics2024,
  title = {On the {{Principles}} behind {{Opinion Dynamics}} in {{Multi-Agent Systems}} of {{Large Language Models}}},
  author = {{Cisneros-Velarde}, Pedro},
  year = {2024},
  abstract = {We study the evolution of opinions inside a population of interacting large language models (LLMs). Every LLM needs to decide how much funding to allocate to an item with three initial possibilities: full, partial, or no funding. We identify biases that drive the exchange of opinions based on the LLM's tendency to (i) find consensus with the other LLM's opinion, (ii) display caution when specifying funding, and (iii) consider ethical concerns in its opinion. We find these biases are affected by the perceived absence of compelling reasons for opinion change, the perceived willingness to engage in discussion, and the distribution of allocation values. Moreover, tensions among biases can lead to the survival of funding for items with negative connotations. We also find that the final distribution of full, partial, and no funding opinions is more diverse when an LLM freely forms its opinion after an interaction than when its opinion is a multiple-choice selection among the three allocation options. In the latter case, consensus or polarization is generally attained. When agents are aware of past opinions, they seek to maintain consistency with them, and more diverse updating rules emerge. Our study is performed using a Llama 3 LLM.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Cisneros-Velarde_On the Principles behind Opinion Dynamics in Multi-Agent Systems of Large.pdf}
}

@misc{coda-fornoCogBenchLargeLanguage2024,
  title = {{{CogBench}}: A Large Language Model Walks into a Psychology Lab},
  shorttitle = {{{CogBench}}},
  author = {{Coda-Forno}, Julian and Binz, Marcel and Wang, Jane X. and Schulz, Eric},
  year = {2024},
  month = feb,
  number = {arXiv:2402.18225},
  eprint = {2402.18225},
  publisher = {arXiv},
  urldate = {2024-03-02},
  abstract = {Large language models (LLMs) have significantly advanced the field of artificial intelligence. Yet, evaluating them comprehensively remains challenging. We argue that this is partly due to the predominant focus on performance metrics in most benchmarks. This paper introduces CogBench, a benchmark that includes ten behavioral metrics derived from seven cognitive psychology experiments. This novel approach offers a toolkit for phenotyping LLMs' behavior. We apply CogBench to 35 LLMs, yielding a rich and diverse dataset. We analyze this data using statistical multilevel modeling techniques, accounting for the nested dependencies among fine-tuned versions of specific LLMs. Our study highlights the crucial role of model size and reinforcement learning from human feedback (RLHF) in improving performance and aligning with human behavior. Interestingly, we find that open-source models are less risk-prone than proprietary models and that fine-tuning on code does not necessarily enhance LLMs' behavior. Finally, we explore the effects of prompt-engineering techniques. We discover that chain-of-thought prompting improves probabilistic reasoning, while take-a-step-back prompting fosters model-based behaviors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/juliancodaforno/CogBench},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Coda-Forno et al_2024_CogBench.pdf;/Users/thomasgorman/Zotero/storage/2VYBI5YV/2402.html}
}

@misc{collinsBuildingMachinesThat2024,
  title = {Building {{Machines}} That {{Learn}} and {{Think}} with {{People}}},
  author = {Collins, Katherine M. and Sucholutsky, Ilia and Bhatt, Umang and Chandra, Kartik and Wong, Lionel and Lee, Mina and Zhang, Cedegao E. and {Zhi-Xuan}, Tan and Ho, Mark and Mansinghka, Vikash and Weller, Adrian and Tenenbaum, Joshua B. and Griffiths, Thomas L.},
  year = {2024},
  month = jul,
  number = {arXiv:2408.03943},
  eprint = {2408.03943},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-12},
  abstract = {What do we want from machine intelligence? We envision machines that are not just tools for thought, but partners in thought: reasonable, insightful, knowledgeable, reliable, and trustworthy systems that think with us. Current artificial intelligence (AI) systems satisfy some of these criteria, some of the time. In this Perspective, we show how the science of collaborative cognition can be put to work to engineer systems that really can be called ``thought partners,'' systems built to meet our expectations and complement our limitations. We lay out several modes of collaborative thought in which humans and AI thought partners can engage and propose desiderata for human-compatible thought partnerships. Drawing on motifs from computational cognitive science, we motivate an alternative scaling path for the design of thought partners and ecosystems around their use through a Bayesian lens, whereby the partners we construct actively build and reason over models of the human and world.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Collins et al_2024_Building Machines that Learn and Think with People.pdf}
}

@misc{collinsEvaluatingLanguageModels2023,
  title = {Evaluating {{Language Models}} for {{Mathematics}} through {{Interactions}}},
  author = {Collins, Katherine M. and Jiang, Albert Q. and Frieder, Simon and Wong, Lionel and Zilka, Miri and Bhatt, Umang and Lukasiewicz, Thomas and Wu, Yuhuai and Tenenbaum, Joshua B. and Hart, William and Gowers, Timothy and Li, Wenda and Weller, Adrian and Jamnik, Mateja},
  year = {2023},
  month = jun,
  number = {arXiv:2306.01694},
  eprint = {2306.01694},
  publisher = {arXiv},
  urldate = {2023-06-08},
  abstract = {The standard methodology of evaluating large language models (LLMs) based on static pairs of inputs and outputs is insufficient for developing assistants: this kind of assessments fails to take into account the essential interactive element in their deployment, and therefore limits how we understand language model capabilities. We introduce CheckMate, an adaptable prototype platform for humans to interact with and evaluate LLMs. We conduct a study with CheckMate to evaluate three language models{\textasciitilde}(InstructGPT, ChatGPT, and GPT-4) as assistants in proving undergraduate-level mathematics, with a mixed cohort of participants from undergraduate students to professors of mathematics. We release the resulting interaction and rating dataset, MathConverse. By analysing MathConverse, we derive a preliminary taxonomy of human behaviours and uncover that despite a generally positive correlation, there are notable instances of divergence between correctness and perceived helpfulness in LLM generations, amongst other findings. Further, we identify useful scenarios and existing issues of GPT-4 in mathematical reasoning through a series of case studies contributed by expert mathematicians. We conclude with actionable takeaways for ML practitioners and mathematicians: models which communicate uncertainty, respond well to user corrections, are more interpretable and concise may constitute better assistants; interactive evaluation is a promising way to continually navigate the capability of these models; humans should be aware of language models' algebraic fallibility, and for that reason discern where they should be used.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  annotation = {https://github.com/collinskatie/checkmate/tree/main},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Collins et al_2023_Evaluating Language Models for Mathematics through Interactions.pdf;/Users/thomasgorman/Zotero/storage/YBMW4HZM/2306.html}
}

@article{congClinicalEfficacyPretrained2024,
  title = {Clinical Efficacy of Pre-Trained Large Language Models through the Lens of Aphasia},
  author = {Cong, Yan and LaCroix, Arianna N. and Lee, Jiyeon},
  year = {2024},
  month = jul,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {15573},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-66576-y},
  urldate = {2024-07-09},
  abstract = {The rapid development of large language models (LLMs) motivates us to explore how such state-of-the-art natural language processing systems can inform aphasia research. What kind of language indices can we derive from a pre-trained LLM? How do they differ from or relate to the existing language features in aphasia? To what extent can LLMs serve as an interpretable and effective diagnostic and measurement tool in a clinical context? To investigate these questions, we constructed predictive and correlational models, which utilize mean surprisals from LLMs as predictor variables. Using AphasiaBank archived data, we validated our models' efficacy in aphasia diagnosis, measurement, and prediction. Our finding is that LLMs-surprisals can effectively detect the presence of aphasia and different natures of the disorder, LLMs in conjunction with the existing language indices improve models' efficacy in subtyping aphasia, and LLMs-surprisals can capture common agrammatic deficits at both word and sentence level. Overall, LLMs have potential to advance automatic and precise aphasia prediction. A natural language processing pipeline can be greatly benefitted from integrating LLMs, enabling us to refine models of existing language disorders, such as aphasia.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Predictive markers},
  annotation = {https://osf.io/ksv7p/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Cong et al_2024_Clinical efficacy of pre-trained large language models through the lens of.pdf}
}

@article{contreraskallensExploratoryMappingTheoretical2018,
  title = {Exploratory Mapping of Theoretical Landscapes through Word Use in Abstracts},
  author = {Contreras Kallens, Pablo and Dale, Rick},
  year = {2018},
  month = sep,
  journal = {Scientometrics},
  volume = {116},
  number = {3},
  pages = {1641--1674},
  issn = {1588-2861},
  doi = {10.1007/s11192-018-2811-x},
  urldate = {2022-07-11},
  abstract = {We present a case study of how scientometric tools can reveal the structure of scientific theory in a discipline. Specifically, we analyze the patterns of word use in the discipline of cognitive science using latent semantic analysis, a well-known semantic model, in the abstracts of over a thousand academic papers relevant to these theories. Our results show that it is possible to link these theories with specific statistical distributions of words in the abstracts of papers that espouse these theories. We show that theories have different patterns of word use, and that the similarity relationships with each other are intuitive and informative. Moreover, we show that it is possible to predict fairly accurately the theory of a paper by constructing a model of the theories based on their distribution of word use. These results may open new avenues for the application of scientometric tools on theoretical divides.},
  langid = {english},
  keywords = {Cognitive science,Latent semantic analysis,Text analysis,Theoretical issues},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Contreras Kallens_Dale_2018_Exploratory mapping of theoretical landscapes through word use in abstracts.pdf}
}

@article{contreraskallensLargeLanguageModels2023,
  title = {Large {{Language Models Demonstrate}} the {{Potential}} of {{Statistical Learning}} in {{Language}}},
  author = {Contreras Kallens, Pablo and Kristensen-McLachlan, Ross Deans and Christiansen, Morten H.},
  year = {2023},
  month = mar,
  journal = {Cognitive Science},
  volume = {47},
  number = {3},
  pages = {e13256},
  issn = {0364-0213, 1551-6709},
  doi = {10.1111/cogs.13256},
  urldate = {2024-01-05},
  abstract = {To what degree can language be acquired from linguistic input alone? This question has vexed scholars for millennia and is still a major focus of debate in the cognitive science of language. The complexity of human language has hampered progress because studies of language--especially those involving computational modeling--have only been able to deal with small fragments of our linguistic skills. We suggest that the most recent generation of Large Language Models (LLMs) might finally provide the computational tools to determine empirically how much of the human language ability can be acquired from linguistic experience. LLMs are sophisticated deep learning architectures trained on vast amounts of natural language data, enabling them to perform an impressive range of linguistic tasks. We argue that, despite their clear semantic and pragmatic limitations, LLMs have already demonstrated that human-like grammatical language can be acquired without the need for a built-in grammar. Thus, while there is still much to learn about how humans acquire and use language, LLMs provide fullfledged computational models for cognitive scientists to empirically evaluate just how far statistical learning might take us in explaining the full complexity of human language.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Contreras Kallens et al_2023_Large Language Models Demonstrate the Potential of Statistical Learning in.pdf}
}

@article{corlatescuAutomatedModelComprehension2024,
  title = {The Automated Model of Comprehension Version 4.0 -- {{Validation}} Studies and Integration of {{ChatGPT}}},
  author = {Corlatescu, Dragos-Georgian and Watanabe, Micah and Ruseti, Stefan and Dascalu, Mihai and McNamara, Danielle S.},
  year = {2024},
  month = may,
  journal = {Computers in Human Behavior},
  volume = {154},
  pages = {108154},
  issn = {0747-5632},
  doi = {10.1016/j.chb.2024.108154},
  urldate = {2024-05-25},
  abstract = {Modeling reading comprehension processes is a critical task for Learning Analytics, as accurate models of the reading process can be used to match students to texts, identify appropriate interventions, and predict learning outcomes. This paper introduces an improved version of the Automated Model of Comprehension, namely version 4.0. AMoC has its roots in two theoretical models of the comprehension process (i.e., the Construction-Integration model and the Landscape model), and the new version leverages state-of-the-art Large Language models, more specifically ChatGPT, to have a better contextualization of the text and a simplified construction of the underlying graph model. Besides showcasing the usage of the model, the study introduces three in-depth psychological validations that argue for the model's adequacy in modeling reading comprehension. In these studies, we demonstrated that AMoC is in line with the theoretical background proposed by the Construction-Integration and Landscape models, and it is better at replicating results from previous human psychological experiments than its predecessor. Thus, AMoC v4.0 can be further used as an educational tool to, for example, help teachers design better learning materials personalized for student profiles. Additionally, we release the code from AMoC v4.0 as open source in a Google Collab Notebook and a GitHub repository.},
  keywords = {Automated model of comprehension,ChatGPT,Large language models,Natural language processing,Reading comprehension},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Corlatescu et al_2024_The automated model of comprehension version 4.pdf;/Users/thomasgorman/Zotero/storage/N4AJWZ7E/S0747563224000219.html}
}

@article{costelloDurablyReducingConspiracy2024,
  title = {Durably Reducing Conspiracy Beliefs through Dialogues with {{AI}}},
  author = {Costello, Thomas H and Pennycook, Gordon and Rand, David G},
  year = {2024},
  abstract = {Conspiracy theory beliefs are notoriously persistent. Influential theories propose they fulfill important psychological needs, thus resisting counterevidence. Yet previous failures in correcting conspiracy beliefs may be due to counterevidence being insufficiently compelling and tailored. To evaluate this possibility, we leverage developments in generative artificial intelligence and engaged 2,190 conspiracy believers in personalized evidence-based dialogues with GPT-4 Turbo. The intervention reduced conspiracy belief by {\textasciitilde}20\%. The effect remained 2 months later, generalized across a wide range of conspiracy theories, and occurred even among participants with deeply entrenched beliefs. Although the dialogues focused on a single conspiracy, they nonetheless diminished belief in unrelated conspiracies and shifted conspiracy- related behavioral intentions. These findings suggest that many conspiracy theorybelievers can revise their views if presented with sufficiently compelling evidence.},
  langid = {english},
  annotation = {https://8cz637-thc.shinyapps.io/conversationshinyapp/\\
\\
https://8cz637-thc.shinyapps.io/ConspiracyDebunkingConversations/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Costello et al_Durably reducing conspiracy beliefs through dialogues with AI.pdf}
}

@misc{cuiAIenhancedCollectiveIntelligence2024,
  title = {{{AI-enhanced Collective Intelligence}}: {{The State}} of the {{Art}} and {{Prospects}}},
  shorttitle = {{{AI-enhanced Collective Intelligence}}},
  author = {Cui, Hao and Yasseri, Taha},
  year = {2024},
  month = mar,
  number = {arXiv:2403.10433},
  eprint = {2403.10433},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-03},
  abstract = {The current societal challenges exceed the capacity of human individual or collective effort alone. As AI evolves, its role within human collectives is poised to vary from an assistive tool to a participatory member. Humans and AI possess complementary capabilities that, when synergized, can achieve a level of collective intelligence that surpasses the collective capabilities of either humans or AI in isolation. However, the interactions in human-AI systems are inherently complex, involving intricate processes and interdependencies. This review incorporates perspectives from network science to conceptualize a multilayer representation of human-AI collective intelligence, comprising a cognition layer, a physical layer, and an information layer. Within this multilayer network, humans and AI agents exhibit varying characteristics; humans differ in diversity from surface-level to deep-level attributes, while AI agents range in degrees of functionality and anthropomorphism. The interplay among these agents shapes the overall structure and dynamics of the system. We explore how agents' diversity and interactions influence the system's collective intelligence. Furthermore, we present an analysis of real-world instances of AI-enhanced collective intelligence. We conclude by addressing the potential challenges in AI-enhanced collective intelligence and offer perspectives on future developments in this field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Cui_Yasseri_2024_AI-enhanced Collective Intelligence.pdf}
}

@article{cuiCanAIReplace,
  title = {Can {{AI Replace Human Subjects}}? {{A Large-Scale Replication}} of {{Psychological Experiments}}},
  author = {Cui, Ziyan and Li, Ning and Zhou, Huaikang},
  abstract = {Artificial Intelligence (AI) is increasingly being integrated into scientific research, particularly in the social sciences, where understanding human behavior is critical. Large Language Models (LLMs) such as GPT-4 have shown promise in replicating human-like responses in various psychological experiments. However, the extent to which LLMs can effectively replace human subjects across diverse experimental contexts remains unclear. Here, we conduct a large-scale study replicating 154 psychological experiments from top social science journals with 618 main effects and 138 interaction effects using GPT-4 as a simulated participant. We find that GPT-4 successfully replicates 76.0\% of main effects and 47.0\% of interaction effects observed in original studies, closely mirroring human responses in both direction and significance. However, only 19.44\% of GPT-4's replicated confidence intervals contain the original effect sizes, with the majority of replicated effect sizes exceeding the 95\% confidence interval of the original studies and showing a 71.6\% rate of unexpected significant results where the original studies reported null findings, suggesting potential overestimation or false positives. Our results demonstrate the potential of LLMs as powerful tools in psychological research but also emphasize the need for caution in interpreting AI-driven findings. While LLMs can complement human studies, they cannot yet fully replace the nuanced insights provided by human subjects.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Cui et al_Can AI Replace Human Subjects.pdf}
}

@article{cuiPromisesPitfallsUsing2024,
  title = {Promises and {{Pitfalls}}: {{Using Large Language Models}} to {{Generate Visualization Items}}},
  shorttitle = {Promises and {{Pitfalls}}},
  author = {Cui, Yuan and Ge, Lily W. and Ding, Yiren and Harrison, Lane and Yang, Fumeng and Kay, Matthew},
  year = {2024},
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  pages = {1--11},
  issn = {1941-0506},
  doi = {10.1109/TVCG.2024.3456309},
  urldate = {2024-09-23},
  abstract = {Visualization items---factual questions about visualizations that ask viewers to accomplish visualization tasks---are regularly used in the field of information visualization as educational and evaluative materials. For example, researchers of visualization literacy require large, diverse banks of items to conduct studies where the same skill is measured repeatedly on the same participants. Yet, generating a large number of high-quality, diverse items requires significant time and expertise. To address the critical need for a large number of diverse visualization items in education and research, this paper investigates the potential for large language models (LLMs) to automate the generation of multiple-choice visualization items. Through an iterative design process, we develop the VILA (Visualization Items Generated by Large LAnguage Models) pipeline, for efficiently generating visualization items that measure people's ability to accomplish visualization tasks. We use the VILA pipeline to generate 1,404 candidate items across 12 chart types and 13 visualization tasks. In collaboration with 11 visualization experts, we develop an evaluation rulebook which we then use to rate the quality of all candidate items. The result is the VILA bank of {$\sim$}1,100 items. From this evaluation, we also identify and classify current limitations of the VILA pipeline, and discuss the role of human oversight in ensuring quality. In addition, we demonstrate an application of our work by creating a visualization literacy test, VILA-VLAT, which measures people's ability to complete a diverse set of tasks on various types of visualizations; comparing it to the existing VLAT, VILA-VLAT shows moderate to high convergent validity (R = 0.70). Lastly, we discuss the application areas of the VILA pipeline and the VILA bank and provide practical recommendations for their use. All supplemental materials are available at https://osf.io/ysrhq/.},
  keywords = {Bars,Codes,Data visualization,Large Language Models,Pipelines,Taxonomy,Technological innovation,Visualization,Visualization Items,Visualization Literacy Assessment},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Cui et al_2024_Promises and Pitfalls.pdf;/Users/thomasgorman/Zotero/storage/TIYT8CD8/10670418.html}
}

@inproceedings{cuiSurveyMultimodalLarge2024,
  title = {A {{Survey}} on {{Multimodal Large Language Models}} for {{Autonomous Driving}}},
  booktitle = {2024 {{IEEE}}/{{CVF Winter Conference}} on {{Applications}} of {{Computer Vision Workshops}} ({{WACVW}})},
  author = {Cui, Can and Ma, Yunsheng and Cao, Xu and Ye, Wenqian and Zhou, Yang and Liang, Kaizhao and Chen, Jintai and Lu, Juanwu and Yang, Zichong and Liao, Kuei-Da and Gao, Tianren and Li, Erlong and Tang, Kun and Cao, Zhipeng and Zhou, Tong and Liu, Ao and Yan, Xinrui and Mei, Shuqi and Cao, Jianguo and Wang, Ziran and Zheng, Chao},
  year = {2024},
  month = jan,
  pages = {958--979},
  publisher = {IEEE},
  address = {Waikoloa, HI, USA},
  doi = {10.1109/WACVW60836.2024.00106},
  urldate = {2024-09-23},
  abstract = {With the emergence of Large Language Models (LLMs) and Vision Foundation Models (VFMs), multimodal AI systems benefiting from large models have the potential to equally perceive the real world, make decisions, and control tools as humans. In recent months, LLMs have shown widespread attention in autonomous driving and map systems. Despite its immense potential, there is still a lack of a comprehensive understanding of key challenges, opportunities, and future endeavors to apply in LLM driving systems. In this paper, we present a systematic investigation in this field. We first introduce the background of Multimodal Large Language Models (MLLMs), the multimodal models development using LLMs, and the history of autonomous driving. Then, we overview existing MLLM tools for driving, transportation, and map systems together with existing datasets and benchmarks. Moreover, we summarized the works in The 1st WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD), which is the first workshop of its kind regarding LLMs in autonomous driving. To further promote the development of this field, we also discuss several important problems regarding using MLLMs in autonomous driving systems that need to be solved by both academia and industry.},
  copyright = {https://doi.org/10.15223/policy-029},
  isbn = {9798350370287},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Cui et al_2024_A Survey on Multimodal Large Language Models for Autonomous Driving.pdf}
}

@article{cukurovaInterplayLearningAnalytics2024,
  title = {The {{Interplay}} of {{Learning}}, {{Analytics}}, and {{Artificial Intelligence}} in {{Education}}: {{A Vision}} for {{Hybrid Intelligence}}},
  author = {Cukurova, Mutlu},
  year = {2024},
  abstract = {This paper presents a multi-dimensional view of AI's role in learning and education, emphasizing the intricate interplay between AI, analytics, and the learning processes. Here, I challenge the prevalent narrow conceptualisation of AI as tools, as exemplified in generative AI tools, and argue for the importance of alternative conceptualisations of AI for achieving human-AI hybrid intelligence. I highlight the differences between human intelligence and artificial information processing, the importance of hybrid human-AI systems to extend human cognition, and posit that AI can also serve as an instrument for understanding human learning. Early learning sciences and AI in Education research (AIED), which saw AI as an analogy for human intelligence, have diverged from this perspective, prompting a need to rekindle this connection. The paper presents three unique conceptualisations of AI: the externalization of human cognition, the internalization of AI models to influence human mental models, and the extension of human cognition via tightly coupled human-AI hybrid intelligence systems. Examples from current research and practice are examined as instances of the three conceptualisations in education, highlighting the potential value and limitations of each conceptualisation for education, as well as the perils of overemphasis on externalising human cognition. The paper concludes with advocacy for a broader approach to AIED that goes beyond considerations on the design and development of AI, but also includes educating people about AI and innovating educational systems to remain relevant in an AI-ubiquitous world.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Cukurova_The Interplay of Learning, Analytics, and Artificial Intelligence in Education.pdf}
}

@article{cutlerSearchingSemanticKnowledge2019,
  title = {Searching for {{Semantic Knowledge}}: {{A Vector Space Semantic Analysis}} of the {{Feature Generation Task}}},
  shorttitle = {Searching for {{Semantic Knowledge}}},
  author = {Cutler, Rebecca A. and Duff, Melissa C. and Polyn, Sean M.},
  year = {2019},
  journal = {Frontiers in Human Neuroscience},
  volume = {13},
  issn = {1662-5161},
  urldate = {2023-08-15},
  abstract = {A recent neuropsychological study found that amnesic patients with hippocampal damage (HP) and severe declarative memory impairment produce markedly fewer responses than healthy comparison (CO) participants in a semantic feature generation task (Klooster and Duff, 2015), consistent with the idea that hippocampal damage is associated with semantic cognitive deficits. Participants were presented with a target word and asked to produce as many features of that word as possible (e.g., for target word ``book,'' ``read words on a page''). Here, we use the response sequences collected by Klooster and Duff (2015) to develop a vector space model of semantic search. We use this model to characterize the dynamics of semantic feature generation and consider the role of the hippocampus in this search process. Both HP and CO groups tended to initiate the search process with features close in semantic space to the target word, with a gradual decline in similarity to the target word over the first several responses. Adjacent features in the response sequence showed stronger similarity to each other than to non-adjacent features, suggesting that the search process follows a local trajectory in semantic space. Overall, HP patients generated features that were closer in semantic space to the representation of the target word, as compared to the features generated by the CO group, which ranged more widely in semantic space. These results are consistent with a model in which a compound retrieval cue (containing a representation of the target word and a representation of the previous response) is used to probe semantic memory. The model suggests that the HP group's search process is restricted from ranging as far in semantic space from the target word, relative to the CO group. These results place strong constraints on the structure of models of semantic memory search, and on the role of hippocampus in probing semantic memory.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Cutler et al_2019_Searching for Semantic Knowledge.pdf}
}

@article{dasguptaAnalyzingMachineLearnedRepresentations2020,
  title = {Analyzing {{Machine-Learned Representations}}: {{A Natural Language Case Study}}},
  shorttitle = {Analyzing {{Machine-Learned Representations}}},
  author = {Dasgupta, Ishita and Guo, Demi and Gershman, Samuel J. and Goodman, Noah D.},
  year = {2020},
  journal = {Cognitive Science},
  volume = {44},
  number = {12},
  pages = {e12925},
  issn = {1551-6709},
  doi = {10.1111/cogs.12925},
  urldate = {2021-10-19},
  abstract = {As modern deep networks become more complex, and get closer to human-like capabilities in certain domains, the question arises as to how the representations and decision rules they learn compare to the ones in humans. In this work, we study representations of sentences in one such artificial system for natural language processing. We first present a diagnostic test dataset to examine the degree of abstract composable structure represented. Analyzing performance on these diagnostic tests indicates a lack of systematicity in representations and decision rules, and reveals a set of heuristic strategies. We then investigate the effect of training distribution on learning these heuristic strategies, and we study changes in these representations with various augmentations to the training set. Our results reveal parallels to the analogous representations in people. We find that these systems can learn abstract rules and generalize them to new contexts under certain circumstances---similar to human zero-shot reasoning. However, we also note some shortcomings in this generalization behavior---similar to human judgment errors like belief bias. Studying these parallels suggests new ways to understand psychological phenomena in humans as well as informs best strategies for building artificial intelligence with human-like language understanding.},
  langid = {english},
  keywords = {Compositionality,Generalization,Heuristic,Natural language inference,Representation learning,Sentence embeddings,Strategies,Test datasets},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Dasgupta et al_2020_Analyzing Machine-Learned Representations.pdf;/Users/thomasgorman/Zotero/storage/GLJVKW5D/cogs.html}
}

@misc{dattaWhosThinkingPush2023,
  title = {Who's {{Thinking}}? {{A Push}} for {{Human-Centered Evaluation}} of {{LLMs}} Using the {{XAI Playbook}}},
  shorttitle = {Who's {{Thinking}}?},
  author = {Datta, Teresa and Dickerson, John P.},
  year = {2023},
  month = mar,
  number = {arXiv:2303.06223},
  eprint = {2303.06223},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-03-16},
  abstract = {Deployed artificial intelligence (AI) often impacts humans, and there is no one-size-fits-all metric to evaluate these tools. Human-centered evaluation of AI-based systems combines quantitative and qualitative analysis and human input. It has been explored to some depth in the explainable AI (XAI) and human-computer interaction (HCI) communities. Gaps remain, but the basic understanding that humans interact with AI and accompanying explanations, and that humans' needs -- complete with their cognitive biases and quirks -- should be held front and center, is accepted by the community. In this paper, we draw parallels between the relatively mature field of XAI and the rapidly evolving research boom around large language models (LLMs). Accepted evaluative metrics for LLMs are not human-centered. We argue that many of the same paths tread by the XAI community over the past decade will be retread when discussing LLMs. Specifically, we argue that humans' tendencies -- again, complete with their cognitive biases and quirks -- should rest front and center when evaluating deployed LLMs. We outline three developed focus areas of human-centered evaluation of XAI: mental models, use case utility, and cognitive engagement, and we highlight the importance of exploring each of these concepts for LLMs. Our goal is to jumpstart human-centered LLM evaluation.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/dattaWhoThinkingPush2023-zotero.md;/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Datta_Dickerson_2023_Who's Thinking.pdf;/Users/thomasgorman/Zotero/storage/KLQ6FK7J/2303.html}
}

@misc{davidsonGoalsRewardProducingPrograms2024,
  title = {Goals as {{Reward-Producing Programs}}},
  author = {Davidson, Guy and Todd, Graham and Togelius, Julian and Gureckis, Todd M. and Lake, Brenden M.},
  year = {2024},
  month = may,
  number = {arXiv:2405.13242},
  eprint = {2405.13242},
  publisher = {arXiv},
  urldate = {2024-06-05},
  abstract = {People are remarkably capable of generating their own goals, beginning with child's play and continuing into adulthood. Despite considerable empirical and computational work on goals and goal-oriented behavior, models are still far from capturing the richness of everyday human goals. Here, we bridge this gap by collecting a dataset of human-generated playful goals, modeling them as reward-producing programs, and generating novel human-like goals through program synthesis. Reward-producing programs capture the rich semantics of goals through symbolic operations that compose, add temporal constraints, and allow for program execution on behavioral traces to evaluate progress. To build a generative model of goals, we learn a fitness function over the infinite set of possible goal programs and sample novel goals with a quality-diversity algorithm. Human evaluators found that model-generated goals, when sampled from partitions of program space occupied by human examples, were indistinguishable from human-created games. We also discovered that our model's internal fitness scores predict games that are evaluated as more fun to play and more human-like.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {https://github.com/guydav/goals-as-reward-producing-programs/\\
\\
https://exps.gureckislab.org/guydav/goal\_programs\_viewer/main/\#/\\
\\
https://github.com/guydav/game-creation-behavioral-experiment},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Davidson et al_2024_Goals as Reward-Producing Programs.pdf}
}

@misc{debrayMappingModelingSemantic2024,
  title = {Mapping and Modeling the Semantic Space of Math Concepts},
  author = {Debray, Samuel and Dehaene, Stanislas},
  year = {2024},
  month = jun,
  doi = {10.1101/2024.05.27.596021},
  urldate = {2024-06-04},
  abstract = {Abstract           Mathematics is an underexplored domain of human cognition. While many studies have focused on subsets of math concepts such as numbers, fractions, or geometric shapes, few have ventured beyond these elementary domains. Here, we attempted to map out the full space of math concepts and to answer two specific questions: can distributed semantic models, such a GloVe, provide a satisfactory fit to human semantic judgments in mathematics? And how does this fit vary with education? We first analyzed all of the French and English Wikipedia pages with math contents, and used a semi-automatic procedure to extract the 1,000 most frequent math terms in both languages. In a second step, we collected extensive behavioral judgments of familiarity and semantic similarity between them. About half of the variance in human similarity judgments was explained by vector embeddings that attempt to capture latent semantic structures based on cooccurence statistics. Participants' self-reported level of education modulated familiarity and similarity, allowing us to create a partial hierarchy among high-level math concepts. Our results converge onto the proposal of a map of math space, organized as a database of math terms with information about their frequency, familiarity, grade of acquisition, and entanglement with other concepts.                        Author summary             Most studies in mathematical cognition focus on subdomains such as numbers, fractions, or geometric shapes. A broader picture of the full extent of mathematical cognition is lacking. Here, as a first step, we use behavioral and computational methods to create a comprehensive vocabulary of advanced math. We prove that statistical cooccurence vectors from large corpora (Wikipedia) provide an approximation of the meaning and organization of these concepts, as measured by human similarity ratings in participants of varying levels of education. Results are similar in French and in English, suggesting that our findings do not depend on the language. In future work, we plan to leverage this vocabulary to explore the brain mechanism of math cognition at various levels of expertise.},
  langid = {english},
  annotation = {https://osf.io/dxg2w/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Debray_Dehaene_2024_Mapping and modeling the semantic space of math concepts.pdf}
}

@misc{dedieuLearningCognitiveMaps2024,
  title = {Learning {{Cognitive Maps}} from {{Transformer Representations}} for {{Efficient Planning}} in {{Partially Observed Environments}}},
  author = {Dedieu, Antoine and Lehrach, Wolfgang and Zhou, Guangyao and George, Dileep and {L{\'a}zaro-Gredilla}, Miguel},
  year = {2024},
  month = jan,
  number = {arXiv:2401.05946},
  eprint = {2401.05946},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {Despite their stellar performance on a wide range of tasks, including in-context tasks only revealed during inference, vanilla transformers and variants trained for next-token predictions (a) do not learn an explicit world model of their environment which can be flexibly queried and (b) cannot be used for planning or navigation. In this paper, we consider partially observed environments (POEs), where an agent receives perceptually aliased observations as it navigates, which makes path planning hard. We introduce a transformer with (multiple) discrete bottleneck(s), TDB, whose latent codes learn a compressed representation of the history of observations and actions. After training a TDB to predict the future observation(s) given the history, we extract interpretable cognitive maps of the environment from its active bottleneck(s) indices. These maps are then paired with an external solver to solve (constrained) path planning problems. First, we show that a TDB trained on POEs (a) retains the near perfect predictive performance of a vanilla transformer or an LSTM while (b) solving shortest path problems exponentially faster. Second, a TDB extracts interpretable representations from text datasets, while reaching higher in-context accuracy than vanilla sequence models. Finally, in new POEs, a TDB (a) reaches near-perfect in-context accuracy, (b) learns accurate in-context cognitive maps (c) solves in-context path planning problems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Dedieu et al_2024_Learning Cognitive Maps from Transformer Representations for Efficient Planning.pdf}
}

@article{delgadoOpportunitiesGreaterEnergy2018,
  title = {Opportunities for Greater Energy Efficiency in Government Facilities by Aligning Decision Structures with Advances in Behavioral Science},
  author = {Delgado, Laura and Shealy, Tripp},
  year = {2018},
  month = feb,
  journal = {Renewable and Sustainable Energy Reviews},
  volume = {82},
  pages = {3952--3961},
  issn = {1364-0321},
  doi = {10.1016/j.rser.2017.10.078},
  urldate = {2024-07-02},
  abstract = {In 2007, Executive Order 13423 mandated 30\% energy and emission reductions for all government facilities by 2015. Unfortunately, the government fell short of their goal by 9\%. Their approach through mandates and federal legislation focused predominantly on new construction and major retrofits to existing facilities. To meet future energy and emission reduction goals, more emphasis on facility management is needed. The government manages over 370 million square feet of facilities each year. The daily decision process for government facility managers is full of competing interests, such as maintenance needs (preventative and corrective), limited operating budgets, time constraints to make decisions, and bounded rationality about energy consumption and savings. By understanding how these decisions are made and the cognitive bias that may occur, advances in facility management decision making can reduce energy consumption. Cognitive biases to the decision making process such as loss aversion, anchoring, and status quo bias are explored and an approach to overcome them is offered, a tactic called choice architecture, meaning restructuring decision environments to align with behavioral decision theory. Examples of choice architecture, such as, enabling procurement systems to query green products, changing default settings in mechanical systems, and requiring the use of pay back period calculators to account for cognitive limitations of the decision maker, are suggested and supported by behavioral science research to help direct facility managers towards energy efficient choices. This approach, through choice architecture, holds potential to yield relatively low-cost solutions (they do not require new mandates or laws) to support greater energy reduction in government facility management. This merging of literature from behavioral science to facility management is meant to open new avenues of interdisciplinary research.},
  keywords = {Choice Architecture,Cognitive bias,Energy reduction,Facility management,Government},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Delgado_Shealy_2018_Opportunities for greater energy efficiency in government facilities by.pdf;/Users/thomasgorman/Zotero/storage/RVVTDIUU/S1364032117314570.html}
}

@misc{demircanLanguageAlignedVisual2023,
  title = {Language {{Aligned Visual Representations Predict Human Behavior}} in {{Naturalistic Learning Tasks}}},
  author = {Demircan, Can and Saanum, Tankred and Pettini, Leonardo and Binz, Marcel and Baczkowski, Blazej M. and Kaanders, Paula and Doeller, Christian F. and Garvert, Mona M. and Schulz, Eric},
  year = {2023},
  month = jun,
  number = {arXiv:2306.09377},
  eprint = {2306.09377},
  publisher = {arXiv},
  urldate = {2023-06-22},
  abstract = {Humans possess the ability to identify and generalize relevant features of natural objects, which aids them in various situations. To investigate this phenomenon and determine the most effective representations for predicting human behavior, we conducted two experiments involving category learning and reward learning. Our experiments used realistic images as stimuli, and participants were tasked with making accurate decisions based on novel stimuli for all trials, thereby necessitating generalization. In both tasks, the underlying rules were generated as simple linear functions using stimulus dimensions extracted from human similarity judgments. Notably, participants successfully identified the relevant stimulus features within a few trials, demonstrating effective generalization. We performed an extensive model comparison, evaluating the trial-by-trial predictive accuracy of diverse deep learning models' representations of human choices. Intriguingly, representations from models trained on both text and image data consistently outperformed models trained solely on images, even surpassing models using the features that generated the task itself. These findings suggest that language-aligned visual representations possess sufficient richness to describe human generalization in naturalistic settings and emphasize the role of language in shaping human cognition.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  annotation = {https://github.com/candemircan/NaturalCogSci\\
\\
https://candemircan.github.io/NaturalCogSci/\\
\\
https://osf.io/h3t52/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Demircan et al_2023_Language Aligned Visual Representations Predict Human Behavior in Naturalistic.pdf}
}

@article{demszkyUsingLargeLanguage2023,
  title = {Using Large Language Models in Psychology},
  author = {Demszky, Dorottya and Yang, Diyi and Yeager, David S. and Bryan, Christopher J. and Clapper, Margarett and Chandhok, Susannah and Eichstaedt, Johannes C. and Hecht, Cameron and Jamieson, Jeremy and Johnson, Meghann and Jones, Michaela and {Krettek-Cobb}, Danielle and Lai, Leslie and JonesMitchell, Nirel and Ong, Desmond C. and Dweck, Carol S. and Gross, James J. and Pennebaker, James W.},
  year = {2023},
  month = oct,
  journal = {Nature Reviews Psychology},
  issn = {2731-0574},
  doi = {10.1038/s44159-023-00241-5},
  urldate = {2024-05-23},
  abstract = {Large language models (LLMs), such as OpenAI's GPT-4, Google's Bard or Meta's LLaMa, have created unprecedented opportunities for analysing and generating language data on a massive scale. Because language data have a central role in all areas of psychology, this new technology has the potential to transform the feld. In this Perspective, we review the foundations of LLMs. We then explain how the way that LLMs are constructed enables them to efectively generate human-like linguistic output without the ability to think or feel like a human. We argue that although LLMs have the potential to advance psychological measurement, experimentation and practice, they are not yet ready for many of the most transformative psychological applications --- but further research and development may enable such use. Next, we examine four major concerns about the application of LLMs to psychology, and how each might be overcome. Finally, we conclude with recommendations for investments that could help to address these concerns: feld-initiated `keystone' datasets; increased standardization of performance benchmarks; and shared computing and analysis infrastructure to ensure that the future of LLM-powered research is equitable.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Demszky et al_2023_Using large language models in psychology.pdf}
}

@article{dentellaSystematicTestingThree2023,
  title = {Systematic Testing of Three {{Language Models}} Reveals Low Language Accuracy, Absence of Response Stability, and a Yes-Response Bias},
  author = {Dentella, Vittoria and G{\"u}nther, Fritz and Leivada, Evelina},
  year = {2023},
  month = dec,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {51},
  pages = {e2309583120},
  issn = {0027-8424, 1091-6490},
  doi = {10.1073/pnas.2309583120},
  urldate = {2023-12-18},
  abstract = {Humans are universally good in providing stable and accurate judgments about what forms part of their language and what not. Large Language Models (LMs) are claimed to possess human-like language abilities; hence, they are expected to emulate this behavior by providing both stable and accurate answers, when asked whether a string of words complies with or deviates from their next-word predictions. This work tests whether stability and accuracy are showcased by GPT-3/text-davinci-002, GPT-3/text-davinci-003, and ChatGPT, using a series of judgment tasks that tap on 8 linguistic phenomena: plural attraction, anaphora, center embedding, comparatives, intrusive resumption, negative polarity items, order of adjectives, and order of adverbs. For every phenomenon, 10 sentences (5 grammatical and 5 ungrammatical) are tested, each randomly repeated 10 times, totaling 800 elicited judgments per LM (total n = 2,400). Our results reveal variable above-chance accuracy in the grammatical condition, below-chance accuracy in the ungrammatical condition, a significant instability of answers across phenomena, and a yes-response bias for all the tested LMs. Furthermore, we found no evidence that repetition aids the Models to converge on a processing strategy that culminates in stable answers, either accurate or inaccurate. We demonstrate that the LMs' performance in identifying (un)grammatical word patterns is in stark contrast to what is observed in humans (n = 80, tested on the same tasks) and argue that adopting LMs as theories of human language is not motivated at their current stage of development.},
  langid = {english},
  annotation = {https://osf.io/7ajxr/},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Dentella et al_2023_Systematic testing of three Language Models reveals low language accuracy,.pdf}
}

@inproceedings{deyneEvaluatingHumanlikeSimilarity2024,
  title = {Evaluating Human-like Similarity Biases at Every Scale in {{Large Language Models}}: {{Evidence}} from Remote and Basic-Level Triads.},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Cognitive Science Society}}, 46},
  author = {Deyne, Simon De},
  year = {2024},
  abstract = {n the remote triad task, participants judge the relatedness between randomly chosen words in a three-alternative choice triadic judgement task. While most word pairs in these triads are weakly related, humans agree on which to choose. This is theoretically interesting as it contradicts previous claims that suggest that the notion of similarity is unconstrained in principle (e.g., Goodman, 1972). Here, we present new evidence from GPT-4, showing that context-aware LLMs provide excellent predictions of this task. Moreover, the strength of this effect was even larger than that found for basic-level comparisons, which involve highly similar items. Together, this implies that the similarity of human representations is highly structured at every scale, even in tasks with limited context. Follow-up analysis provides insights into how LLMs are successful in this task. Further implications of the ability to compare words at every scale are discussed. Keywords: word meaning; concepts, remote triads, basiclevel comparisons, GPT-4},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Deyne_Evaluating human-like similarity biases at every scale in Large Language Models.pdf}
}

@article{dhamiPossibilitiesDecisionScience2024,
  title = {Possibilities for Decision Science in the Metaverse.},
  author = {Dhami, Mandeep K. and Zhu, Ying},
  year = {2024},
  month = aug,
  journal = {Decision},
  issn = {2325-9973, 2325-9965},
  doi = {10.1037/dec0000235},
  urldate = {2024-08-06},
  abstract = {As the next generation of the internet, the metaverse will be an immersive, interactive, and persistent three-dimensional world that merges both physical and virtual environments. Its associated advanced technologies (e.g., digital twins, avatar self-representation, virtual reality, augmented reality, extended reality, brain--computer interface, artificial intelligence, machine learning, Internet of Things, blockchain technology) are already functioning, and people are able to experience and make decisions in immersive virtual environments. In this article, we first briefly assess the similarities and differences in the judgment and decisionmaking (JDM) situations that people may face in the metaverse compared to their physical environments. Next, we discuss how human interaction with metaverse-related advanced technologies may affect the cognitive processes of perception, attention, memory, and reasoning that underlie human JDM, as well as the subsequent effects of these processes on JDM, in both the virtual and physical worlds. Finally, we highlight some opportunities that the metaverse may afford decision scientists, given the availability of integrated digitalized data collected from users (or research participants) as well as some of the challenges that lie therein. We structure these opportunities around the five key metaverse domains: digitalization, virtualization, social networks, virtual worlds, and data integration. As a research platform, the metaverse can help drive a paradigm shift away from traditional approaches to JDM research and enable decision scientists to test and consolidate existing theories as well as advance new ones, while conducting research that has social benefits.},
  copyright = {http://creativecommons.org/licenses/by/4.0},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Dhami_Zhu_2024_Possibilities for decision science in the metaverse.pdf}
}

@article{diaz-alvarezModellingHumanLanechange2018,
  title = {Modelling the Human Lane-Change Execution Behaviour through {{Multilayer Perceptrons}} and {{Convolutional Neural Networks}}},
  author = {{D{\'i}az-{\'A}lvarez}, Alberto and Clavijo, Miguel and Jim{\'e}nez, Felipe and Talavera, Edgar and Serradilla, Francisco},
  year = {2018},
  month = jul,
  journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
  volume = {56},
  pages = {134--148},
  issn = {1369-8478},
  doi = {10.1016/j.trf.2018.04.004},
  urldate = {2024-09-16},
  abstract = {Driving is a highly complex task that involves the execution of multiple cognitive tasks belonging to different levels of abstraction. Traffic emerges from the interaction of a big number of agents implementing those behaviours, but until recent years, modelling it by the interaction of these agents in the so called micro-simulators was a nearly impossible task as their number grows. However, with the growing computing power it is possible to model increasingly large quantities of individual vehicles according to their individual behaviours. These models are usually composed of two sub-models for two well-defined tasks: car-following and lane-change. In the case of lane-change the literature proposes many different models, but few of them use Computational Intelligence (CI) techniques, and much less use personalization for reaching individual granularity. This study explores one of the two aspects of the lane-change called lane-change acceptance, where the driver performs or not a lane-change given his intention and the vehicle environment. We demonstrate how the lane-change acceptance of a specific driver can be learned from his lane change intention and surrounding environment in an urban scenario using CI techniques such as feed-forward Artificial Neural Network (ANN). We work with Multilayer Perceptron (MLP) and Convolutional Neural Networks (CNN) architectures. How they perform one against the other and how the different topologies affect both to the generalization of the problem and the learning process are studied.},
  keywords = {Computational intelligence,Convolutional neural network,Driver model,Driving behaviour,Lane change,Multilayer perceptron},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Díaz-Álvarez et al_2018_Modelling the human lane-change execution behaviour through Multilayer.pdf;/Users/thomasgorman/Zotero/storage/WRU65AC4/S1369847817307040.html}
}

@misc{dicksonComparingPerceptualJudgments2024,
  title = {Comparing {{Perceptual Judgments}} in {{Large Multimodal Models}} and {{Humans}}},
  author = {Dickson, Billy and Maini, Sahaj Singh and Nosofsky, Robert and Tiganj, Zoran},
  year = {2024},
  month = jul,
  doi = {10.31234/osf.io/pcmrj},
  urldate = {2024-07-07},
  abstract = {Cognitive scientists commonly collect participants' judgments regarding perceptual characteristics of stimuli to develop and evaluate memory, learning, and decision-making models. For instance, to model human responses in tasks of category learning and item recognition, researchers often have to collect perceptual judgments of images to embed the images in multidimensional feature spaces. This process is time-consuming and costly. Recent advancements in Large Multimodal Models (LMMs) provide a potential alternative since such models can respond to prompts that include both text and images and could potentially replace human participants. To test whether the available LMMs can indeed be useful for this purpose, we evaluated their judgments on a dataset consisting of rock images that has been widely used by cognitive scientists. The dataset includes human perceptual judgments along ten dimensions considered important for classifying rock images. While the models exhibited a strong positive correlation with human responses, we found that they fell short in replacing an average of a set of judgments from human participants. The models provided correlations with these averaged data that were roughly the same magnitude as observed for individual participants, especially for dimensions that are relatively general (such as lightness and chromaticity) as opposed to domain-specific dimensions (such as pegmatitic structure), where they struggled more. We also found that modifying prompts and providing additional examples of images with corresponding ratings had a positive but relatively modest impact on model performance. Our study provides a benchmark for evaluating future LMMs on human perceptual judgment data.},
  copyright = {https://creativecommons.org/publicdomain/zero/1.0/legalcode},
  langid = {english},
  annotation = {https://osf.io/za847/\\
\\
https://cognlp.com/\\
\\
https://github.com/cogneuroai/multimodal-models-rock\\
\\
https://osf.io/cvwu9/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Dickson et al_2024_Comparing Perceptual Judgments in Large Multimodal Models and Humans.pdf}
}

@inproceedings{dingFluidTransformersCreative2023,
  title = {Fluid {{Transformers}} and {{Creative Analogies}}: {{Exploring Large Language Models}}' {{Capacity}} for {{Augmenting Cross-Domain Analogical Creativity}}},
  shorttitle = {Fluid {{Transformers}} and {{Creative Analogies}}},
  booktitle = {Creativity and {{Cognition}}},
  author = {Ding, Zijian and Srinivasan, Arvind and Macneil, Stephen and Chan, Joel},
  year = {2023},
  month = jun,
  pages = {489--505},
  publisher = {ACM},
  address = {Virtual Event USA},
  doi = {10.1145/3591196.3593516},
  urldate = {2024-02-03},
  isbn = {9798400701801},
  langid = {english},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Ding et al_2023_Fluid Transformers and Creative Analogies2.pdf}
}

@inproceedings{dingFluidTransformersCreative2023a,
  title = {Fluid {{Transformers}} and {{Creative Analogies}}: {{Exploring Large Language Models}}' {{Capacity}} for {{Augmenting Cross-Domain Analogical Creativity}}},
  shorttitle = {Fluid {{Transformers}} and {{Creative Analogies}}},
  booktitle = {Proceedings of the 15th {{Conference}} on {{Creativity}} and {{Cognition}}},
  author = {Ding, Zijian and Srinivasan, Arvind and Macneil, Stephen and Chan, Joel},
  year = {2023},
  month = jun,
  series = {C\&amp;{{C}} '23},
  pages = {489--505},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  doi = {10.1145/3591196.3593516},
  urldate = {2023-10-17},
  abstract = {Cross-domain analogical reasoning is a core creative ability that can be challenging for humans. Recent work has shown some proofs-of-concept of Large language Models' (LLMs) ability to generate cross-domain analogies. However, the reliability and potential usefulness of this capacity for augmenting human creative work has received little systematic exploration. In this paper, we systematically explore LLMs capacity to augment cross-domain analogical reasoning. Across three studies, we found: 1) LLM-generated cross-domain analogies were frequently judged as helpful in the context of a problem reformulation task (median 4 out of 5 helpfulness rating), and frequently ({$\sim$} 80\% of cases) led to observable changes in problem formulations, and 2) there was an upper bound of {$\sim$} 25\% of outputs being rated as potentially harmful, with a majority due to potentially upsetting content, rather than biased or toxic content. These results demonstrate the potential utility --- and risks --- of LLMs for augmenting cross-domain analogical creativity.},
  isbn = {9798400701801},
  keywords = {Analogy,Creativity Support Tools,Large Language Models},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ding et al_2023_Fluid Transformers and Creative Analogies.pdf}
}

@article{divyavenkateshComparingHumanText2024,
  title = {Comparing Human Text Classification Performance and Explainability with Large Language and Machine Learning Models Using Eye-Tracking},
  author = {Divya Venkatesh, Jeevithashree and Jaiswal, Aparajita and Nanda, Gaurav},
  year = {2024},
  month = jun,
  journal = {Scientific Reports},
  volume = {14},
  pages = {14295},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-65080-7},
  urldate = {2024-09-23},
  abstract = {To understand the alignment between reasonings of humans and artificial intelligence (AI) models, this empirical study compared the human text classification performance and explainability with a traditional machine learning (ML) model and large language model (LLM). A domain-specific noisy textual dataset of 204 injury narratives had to be classified into 6 cause-of-injury codes. The narratives varied in terms of complexity and ease of categorization based on the distinctive nature of cause-of-injury code. The user study involved 51 participants whose eye-tracking data was recorded while they performed the text classification task. While the ML model was trained on 120,000 pre-labelled injury narratives, LLM and humans did not receive any specialized training. The explainability of different approaches was compared based on the top words they used for making classification decision. These words were identified using eye-tracking for humans, explainable AI approach LIME for ML model, and prompts for LLM. The classification performance of ML model was observed to be relatively better than zero-shot LLM and non-expert humans, overall, and particularly for narratives with high complexity and difficult categorization. The top-3 predictive words used by ML and LLM for classification agreed with humans to a greater extent as compared to later predictive words.},
  pmcid = {PMC11192954},
  pmid = {38906943},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Divya Venkatesh et al_2024_Comparing human text classification performance and explainability with large.pdf}
}

@misc{doFacilitatingHumanLLMCollaboration2024,
  title = {Facilitating {{Human-LLM Collaboration}} through {{Factuality Scores}} and {{Source Attributions}}},
  author = {Do, Hyo Jin and Ostrand, Rachel and Weisz, Justin D. and Dugan, Casey and Sattigeri, Prasanna and Wei, Dennis and Murugesan, Keerthiram and Geyer, Werner},
  year = {2024},
  month = may,
  number = {arXiv:2405.20434},
  eprint = {2405.20434},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-14},
  abstract = {While humans increasingly rely on large language models (LLMs), they are susceptible to generating inaccurate or false information, also known as ``hallucinations''. Technical advancements have been made in algorithms that detect hallucinated content by assessing the factuality of the model's responses and attributing sections of those responses to specific source documents. However, there is limited research on how to effectively communicate this information to users in ways that will help them appropriately calibrate their trust toward LLMs. To address this issue, we conducted a scenario-based study (N=104) to systematically compare the impact of various design strategies for communicating factuality and source attribution on participants' ratings of trust, preferences, and ease in validating response accuracy. Our findings reveal that participants preferred a design in which phrases within a response were color-coded based on the computed factuality scores the most. Participants found it easy to validate the accuracy of an LLM's response and increased their trust in this style compared to a baseline in which no style was applied. Additionally, participants increased their trust ratings when relevant sections of the source material were highlighted or responses were annotated with reference numbers corresponding to those sources, compared to when they received no annotation in the source material. Our study offers practical design guidelines to facilitate human-LLM collaboration and it promotes a new human role to carefully evaluate and take responsibility for their use of LLM outputs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Do et al_2024_Facilitating Human-LLM Collaboration through Factuality Scores and Source.pdf}
}

@misc{dornerPersonalityTestsGeneralize2023,
  title = {Do Personality Tests Generalize to {{Large Language Models}}?},
  author = {Dorner, Florian E. and S{\"u}hr, Tom and Samadi, Samira and Kelava, Augustin},
  year = {2023},
  month = nov,
  number = {arXiv:2311.05297},
  eprint = {2311.05297},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-15},
  abstract = {With large language models (LLMs) appearing to behave increasingly human-like in text-based interactions, it has become popular to attempt to evaluate various properties of these models using tests originally designed for humans. While re-using existing tests is a resource-efficient way to evaluate LLMs, careful adjustments are usually required to ensure that test results are even valid across human sub-populations. Thus, it is not clear to what extent different tests' validity generalizes to LLMs. In this work, we provide evidence that LLMs' responses to personality tests systematically deviate from typical human responses, implying that these results cannot be interpreted in the same way as human test results. Concretely, reverse-coded items (e.g. "I am introverted" vs "I am extraverted") are often both answered affirmatively by LLMs. In addition, variation across different prompts designed to "steer" LLMs to simulate particular personality types does not follow the clear separation into five independent personality factors from human samples. In light of these results, we believe it is important to pay more attention to tests' validity for LLMs before drawing strong conclusions about potentially ill-defined concepts like LLMs' "personality".},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Dorner et al_2023_Do personality tests generalize to Large Language Models.pdf;/Users/thomasgorman/Zotero/storage/TESAUST3/2311.html}
}

@misc{duanExploringRelationshipContext2023,
  title = {Exploring the {{Relationship}} between {{In-Context Learning}} and {{Instruction Tuning}}},
  author = {Duan, Hanyu and Tang, Yixuan and Yang, Yi and Abbasi, Ahmed and Tam, Kar Yan},
  year = {2023},
  month = nov,
  number = {arXiv:2311.10367},
  eprint = {2311.10367},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-23},
  abstract = {In-Context Learning (ICL) and Instruction Tuning (IT) are two primary paradigms of adopting Large Language Models (LLMs) to downstream applications. However, they are significantly different. In ICL, a set of demonstrations are provided at inference time but the LLM's parameters are not updated. In IT, a set of demonstrations are used to tune LLM's parameters in training time but no demonstrations are used at inference time. Although a growing body of literature has explored ICL and IT, studies on these topics have largely been conducted in isolation, leading to a disconnect between these two paradigms. In this work, we explore the relationship between ICL and IT by examining how the hidden states of LLMs change in these two paradigms. Through carefully designed experiments conducted with LLaMA-2 (7B and 13B), we find that ICL is implicit IT. In other words, ICL changes an LLM's hidden states as if the demonstrations were used to instructionally tune the model. Furthermore, the convergence between ICL and IT is largely contingent upon several factors related to the provided demonstrations. Overall, this work offers a unique perspective to explore the connection between ICL and IT and sheds light on understanding the behaviors of LLM.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Duan et al_2023_Exploring the Relationship between In-Context Learning and Instruction Tuning.pdf;/Users/thomasgorman/Zotero/storage/MJZ8EC4R/2311.html}
}

@article{duanMacBehaviourPackageBehavioural2024,
  title = {{{MacBehaviour}}: {{An R}} Package for Behavioural Experimentation on Large Language Models},
  shorttitle = {{{MacBehaviour}}},
  author = {Duan, Xufeng and Li, Shixuan and Cai, Zhenguang Garry},
  year = {2024},
  month = feb,
  doi = {10.31234/osf.io/ywtfd},
  urldate = {2024-02-29},
  abstract = {There has been increasing interest in investigating the behaviours of large language models (LLMs) and LLM-powered chatbots. This paper presents the development of an R package called "MacBehaviour" that aims to streamline the experimental process when interfacing with LLMs for behavioural research. "MacBehaviour" offers a user-friendly package for conducting behavioural experimentation and for norming experimental stimuli with LLMs in the R environment. The package provides a suite of functions tailored for experiments on LLMs, including the design of experiment, the presentation of stimuli, and the manipulation of model behaviour. The package interfaces with APIs of an array of LLMs, including those from OpenAI's GPT series, Llama-2-chat-hf series in Hugging Face, and open-source models. Overall, "MacBehaviour" serves as a valuable resource for researchers, offering a user-friendly interface and comprehensive tools to streamline the experimental process and advance the study of machine behavior within the field of psychology.},
  langid = {american},
  annotation = {https://github.com/xufengduan/MacBehaviour},
  file = {/Users/thomasgorman/Zotero/storage/E9C34KFC/ywtfd.html}
}

@article{dubourgStepstepMethodCultural2024,
  title = {A Step-by-Step Method for Cultural Annotation by {{LLMs}}},
  author = {Dubourg, Edgar and Thouzeau, Valentin and Baumard, Nicolas},
  year = {2024},
  month = may,
  journal = {Frontiers in Artificial Intelligence},
  volume = {7},
  pages = {1365508},
  issn = {2624-8212},
  doi = {10.3389/frai.2024.1365508},
  urldate = {2024-05-24},
  abstract = {Building on the growing body of research highlighting the capabilities of Large Language Models (LLMs) like Generative Pre-trained Transformers (GPT), this paper presents a structured pipeline for the annotation of cultural (big) data through such LLMs, offering a detailed methodology for leveraging GPT's computational abilities. Our approach provides researchers across various fields with a method for efficient and scalable analysis of cultural phenomena, showcasing the potential of LLMs in the empirical study of human cultures. LLMs proficiency in processing and interpreting complex data finds relevance in tasks such as annotating descriptions of non-industrial societies, measuring the importance of specific themes in stories, or evaluating psychological constructs in texts across societies or historical periods. These applications demonstrate the model's versatility in serving disciplines like cultural anthropology, cultural psychology, cultural history, and cultural sciences at large.},
  langid = {english},
  annotation = {https://osf.io/3q6zb/\\
\\
https://rpubs.com/nirmal/setting\_chat\_gpt\_R},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Dubourg et al_2024_A step-by-step method for cultural annotation by LLMs.pdf}
}

@misc{duHumanlikeObjectConcept2024,
  title = {Human-like Object Concept Representations Emerge Naturally in Multimodal Large Language Models},
  author = {Du, Changde and Fu, Kaicheng and Wen, Bincheng and Sun, Yi and Peng, Jie and Wei, Wei and Gao, Ying and Wang, Shengpei and Zhang, Chuncheng and Li, Jinpeng and Qiu, Shuang and Chang, Le and He, Huiguang},
  year = {2024},
  month = jul,
  number = {arXiv:2407.01067},
  eprint = {2407.01067},
  publisher = {arXiv},
  urldate = {2024-07-04},
  abstract = {The conceptualization and categorization of natural objects in the human mind have long intrigued cognitive scientists and neuroscientists, offering crucial insights into human perception and cognition. Recently, the rapid development of Large Language Models (LLMs) has raised the attractive question of whether these models can also develop human-like object representations through exposure to vast amounts of linguistic and multimodal data. In this study, we combined behavioral and neuroimaging analysis methods to uncover how the object concept representations in LLMs correlate with those of humans. By collecting large-scale datasets of 4.7 million triplet judgments from LLM and Multimodal LLM (MLLM), we were able to derive low-dimensional embeddings that capture the underlying similarity structure of 1,854 natural objects. The resulting 66-dimensional embeddings were found to be highly stable and predictive, and exhibited semantic clustering akin to human mental representations. Interestingly, the interpretability of the dimensions underlying these embeddings suggests that LLM and MLLM have developed human-like conceptual representations of natural objects. Further analysis demonstrated strong alignment between the identified model embeddings and neural activity patterns in many functionally defined brain ROIs (e.g., EBA, PPA, RSC and FFA). This provides compelling evidence that the object representations in LLMs, while not identical to those in the human, share fundamental commonalities that reflect key schemas of human conceptual knowledge. This study advances our understanding of machine intelligence and informs the development of more human-like artificial cognitive systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  annotation = {https://github.com/ViCCo-Group/SPoSE},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Du et al_2024_Human-like object concept representations emerge naturally in multimodal large.pdf}
}

@article{duLargeLanguageModels2024,
  title = {Large {{Language Models}} for {{Collective Problem-Solving}}: {{Insights}} into {{Group Consensus Decision-Making}}},
  author = {Du, Yinuo and Rajivan, Prashanth and Gonzalez, Cleotilde C.},
  year = {2024},
  abstract = {Large Language models (LLM) exhibit human-like proficiency in various tasks such as translation, question answering, essay writing, and programming. Emerging research explores the use of LLMs in collective problem-solving endeavors, such as tasks where groups try to uncover clues through discussions. Although prior work has investigated individual problem-solving tasks, leveraging LLM-powered agents for group consensus and decision-making remains largely unexplored. This research addresses this gap by (1) proposing an algorithm to enable free-form conversation in groups of LLM agents, (2) creating metrics to evaluate the human-likeness of the generated dialogue and problem-solving performance, and (3) evaluating LLM agent groups against human groups using an open source dataset. Our results reveal that LLM groups outperform human groups in problem-solving tasks. LLM groups also show a greater improvement in scores after participating in free discussions. In particular, analyses indicate that LLM agent groups exhibit more disagreements, complex statements, and a propensity for positive statements compared to human groups. The results shed light on the potential of LLMs to facilitate collective reasoning and provide insight into the dynamics of group interactions involving synthetic LLM agents.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Du_Large Language Models for Collective Problem-Solving.pdf}
}

@inproceedings{echterhoffAvoidingDecisionFatigue2024,
  title = {Avoiding {{Decision Fatigue}} with {{AI-Assisted Decision-Making}}},
  booktitle = {Proceedings of the 32nd {{ACM Conference}} on {{User Modeling}}, {{Adaptation}} and {{Personalization}}},
  author = {Echterhoff, Jessica Maria and Melkote, Aditya and Kancherla, Sujen and McAuley, Julian},
  year = {2024},
  month = jun,
  pages = {1--11},
  publisher = {ACM},
  address = {Cagliari Italy},
  doi = {10.1145/3627043.3659569},
  urldate = {2024-08-11},
  isbn = {9798400704338},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Echterhoff et al_2024_Avoiding Decision Fatigue with AI-Assisted Decision-Making.pdf}
}

@misc{echterhoffCognitiveBiasHighStakes2024,
  title = {Cognitive {{Bias}} in {{High-Stakes Decision-Making}} with {{LLMs}}},
  author = {Echterhoff, Jessica and Liu, Yao and Alessa, Abeer and McAuley, Julian and He, Zexue},
  year = {2024},
  month = feb,
  number = {arXiv:2403.00811},
  eprint = {2403.00811},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-26},
  abstract = {Large language models (LLMs) offer significant potential as tools to support an expanding range of decision-making tasks. However, given their training on human (created) data, LLMs can inherit both societal biases against protected groups, as well as be subject to cognitive bias. Such human-like bias can impede fair and explainable decisions made with LLM assistance. Our work introduces BIASBUSTER, a framework designed to uncover, evaluate, and mitigate cognitive bias in LLMs, particularly in high-stakes decision-making tasks. Inspired by prior research in psychology and cognitive sciences, we develop a dataset containing 16,800 prompts to evaluate different cognitive biases (e.g., prompt-induced, sequential, inherent). We test various bias mitigation strategies, amidst proposing a novel method utilising LLMs to debias their own prompts. Our analysis provides a comprehensive picture on the presence and effects of cognitive bias across different commercial and open-source models. We demonstrate that our self-help debiasing effectively mitigate cognitive bias without having to manually craft examples for each bias type.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Echterhoff et al_2024_Cognitive Bias in High-Stakes Decision-Making with LLMs.pdf}
}

@article{edwardsFunctionalContextualAccount2022,
  title = {A {{Functional Contextual Account}} of {{Background Knowledge}} in {{Categorization}}: {{Implications}} for {{Artificial General Intelligence}} and {{Cognitive Accounts}} of {{General Knowledge}}},
  shorttitle = {A {{Functional Contextual Account}} of {{Background Knowledge}} in {{Categorization}}},
  author = {Edwards, Darren J. and McEnteggart, Ciara and {Barnes-Holmes}, Yvonne},
  year = {2022},
  month = mar,
  journal = {Frontiers in Psychology},
  volume = {13},
  pages = {745306},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2022.745306},
  urldate = {2024-05-25},
  abstract = {Psychology has benefited from an enormous wealth of knowledge about processes of cognition in relation to how the brain organizes information. Within the categorization literature, this behavior is often explained through theories of memory construction called exemplar theory and prototype theory which are typically based on similarity or rule functions as explanations of how categories emerge. Although these theories work well at modeling highly controlled stimuli in laboratory settings, they often perform less well outside of these settings, such as explaining the emergence of background knowledge processes. In order to explain background knowledge, we present a non-similaritybased post-Skinnerian theory of human language called Relational Frame Theory (RFT) which is rooted in a philosophical world view called functional contextualism (FC). This theory offers a very different interpretation of how categories emerge through the functions of behavior and through contextual cues, which may be of some benefit to existing categorization theories. Specifically, RFT may be able to offer a novel explanation of how background knowledge arises, and we provide some mathematical considerations in order to identify a formal model. Finally, we discuss much of this work within the broader context of general semantic knowledge and artificial intelligence research.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Edwards et al_2022_A Functional Contextual Account of Background Knowledge in Categorization2.pdf}
}

@article{edwardsFunctionalContextualAccount2022a,
  title = {A {{Functional Contextual Account}} of {{Background Knowledge}} in {{Categorization}}: {{Implications}} for {{Artificial General Intelligence}} and {{Cognitive Accounts}} of {{General Knowledge}}},
  shorttitle = {A {{Functional Contextual Account}} of {{Background Knowledge}} in {{Categorization}}},
  author = {Edwards, Darren J. and McEnteggart, Ciara and {Barnes-Holmes}, Yvonne},
  year = {2022},
  journal = {Frontiers in Psychology},
  volume = {13},
  issn = {1664-1078},
  urldate = {2023-02-28},
  abstract = {Psychology has benefited from an enormous wealth of knowledge about processes of cognition in relation to how the brain organizes information. Within the categorization literature, this behavior is often explained through theories of memory construction called exemplar theory and prototype theory which are typically based on similarity or rule functions as explanations of how categories emerge. Although these theories work well at modeling highly controlled stimuli in laboratory settings, they often perform less well outside of these settings, such as explaining the emergence of background knowledge processes. In order to explain background knowledge, we present a non-similarity-based post-Skinnerian theory of human language called Relational Frame Theory (RFT) which is rooted in a philosophical world view called functional contextualism (FC). This theory offers a very different interpretation of how categories emerge through the functions of behavior and through contextual cues, which may be of some benefit to existing categorization theories. Specifically, RFT may be able to offer a novel explanation of how background knowledge arises, and we provide some mathematical considerations in order to identify a formal model. Finally, we discuss much of this work within the broader context of general semantic knowledge and artificial intelligence research.},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/edwardsFunctionalContextualAccount2022-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Edwards et al_2022_A Functional Contextual Account of Background Knowledge in Categorization.pdf}
}

@misc{eignerDeterminantsLLMassistedDecisionMaking2024,
  title = {Determinants of {{LLM-assisted Decision-Making}}},
  author = {Eigner, Eva and H{\"a}ndler, Thorsten},
  year = {2024},
  month = feb,
  number = {arXiv:2402.17385},
  eprint = {2402.17385},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-26},
  abstract = {Decision-making is a fundamental capability in everyday life. Large Language Models (LLMs) provide multifaceted support in enhancing human decision-making processes. However, understanding the influencing factors of LLM-assisted decision-making is crucial for enabling individuals to utilize LLM-provided advantages and minimize associated risks in order to make more informed and better decisions. This study presents the results of a comprehensive literature analysis, providing a structural overview and detailed analysis of determinants impacting decision-making with LLM support. In particular, we explore the effects of technological aspects of LLMs, including transparency and prompt engineering, psychological factors such as emotions and decision-making styles, as well as decisionspecific determinants such as task difficulty and accountability. In addition, the impact of the determinants on the decision-making process is illustrated via multiple application scenarios. Drawing from our analysis, we develop a dependency framework that systematizes possible interactions in terms of reciprocal interdependencies between these determinants. Our research reveals that, due to the multifaceted interactions with various determinants, factors such as trust in or reliance on LLMs, the user's mental model, and the characteristics of information processing are identified as significant aspects influencing LLM-assisted decision-making processes. Our findings can be seen as crucial for improving decision quality in human-AI collaboration, empowering both users and organizations, and designing more effective LLM interfaces. Additionally, our work provides a foundation for future empirical investigations on the determinants of decision-making assisted by LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Eigner_Händler_2024_Determinants of LLM-assisted Decision-Making.pdf}
}

@article{einarssonApplicationChatGPTAutomated2024,
  title = {Application of {{ChatGPT}} for Automated Problem Reframing across Academic Domains},
  author = {Einarsson, Hafsteinn and Lund, Sigr{\'u}n Helga and J{\'o}nsd{\'o}ttir, Anna Helga},
  year = {2024},
  month = jun,
  journal = {Computers and Education: Artificial Intelligence},
  volume = {6},
  pages = {100194},
  issn = {2666-920X},
  doi = {10.1016/j.caeai.2023.100194},
  urldate = {2024-03-03},
  abstract = {This paper explores the potential of large language models, specifically ChatGPT, to reframe problems from probability theory and statistics, making them accessible to students across diverse academic fields including biology, economics, law, and engineering. The aim of this study is to enhance interdisciplinary learning by rendering complex concepts more accessible, relevant, and engaging. We conducted a pilot study using ChatGPT to adapt problems across 17 disciplines, evaluated through expert review. Our results demonstrate the significant potential of ChatGPT in reshaping problems for diverse settings, preserving theoretical meaning in 77.1\% of cases, and requiring no or only minor revisions in 74\% of cases. An evaluation performed by 23 domain experts revealed that in 73.6\% of cases the reframed problem was considered to add educational value compared to a corresponding abstract problem and to represent a real-world scenario in 57.0\% of cases. Furthermore, a survey involving 44 Computer Science students revealed a diverse range of preferences between original and reframed problems, underscoring the importance of considering student preferences and learning styles in the design of educational content. The study offers insights into the practicality and efficacy of employing large language models, like ChatGPT, to enhance interdisciplinary education and foster greater student engagement and understanding.},
  keywords = {Artificial intelligence,Interdisciplinary education,Large language models,Personalized learning,Problem reframing},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Einarsson et al_2024_Application of ChatGPT for automated problem reframing across academic domains.pdf;/Users/thomasgorman/Zotero/storage/QY62PJ4Z/S2666920X23000735.html}
}

@misc{eisapeSystematicComparisonSyllogistic2023,
  title = {A {{Systematic Comparison}} of {{Syllogistic Reasoning}} in {{Humans}} and {{Language Models}}},
  author = {Eisape, Tiwalayo and Tessler, M. H. and Dasgupta, Ishita and Sha, Fei and {van Steenkiste}, Sjoerd and Linzen, Tal},
  year = {2023},
  month = nov,
  number = {arXiv:2311.00445},
  eprint = {2311.00445},
  publisher = {arXiv},
  urldate = {2023-11-17},
  abstract = {A central component of rational behavior is logical inference: the process of determining which conclusions follow from a set of premises. Psychologists have documented several ways in which humans' inferences deviate from the rules of logic. Do language models, which are trained on text generated by humans, replicate these biases, or are they able to overcome them? Focusing on the case of syllogisms -- inferences from two simple premises, which have been studied extensively in psychology -- we show that larger models are more logical than smaller ones, and also more logical than humans. At the same time, even the largest models make systematic errors, some of which mirror human reasoning biases such as ordering effects and logical fallacies. Overall, we find that language models mimic the human biases included in their training data, but are able to overcome them in some cases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/skhemlani/mReasoner},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Eisape et al_2023_A Systematic Comparison of Syllogistic Reasoning in Humans and Language Models.pdf;/Users/thomasgorman/Zotero/storage/GFTKV8GI/2311.html}
}

@article{eismaWhatAttractsDrivers2022,
  title = {What {{Attracts}} the {{Driver}}'s {{Eye}}? {{Attention}} as a {{Function}} of {{Task}} and {{Events}}},
  shorttitle = {What {{Attracts}} the {{Driver}}'s {{Eye}}?},
  author = {Eisma, Yke Bauke and Eijssen, Dirk J. and {de Winter}, Joost C. F.},
  year = {2022},
  month = jul,
  journal = {Information},
  volume = {13},
  number = {7},
  pages = {333},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2078-2489},
  doi = {10.3390/info13070333},
  urldate = {2024-09-16},
  abstract = {This study explores how drivers of an automated vehicle distribute their attention as a function of environmental events and driving task instructions. Twenty participants were asked to monitor pre-recorded videos of a simulated driving trip while their eye movements were recorded using an eye-tracker. The results showed that eye movements are strongly situation-dependent, with areas of interest (windshield, mirrors, and dashboard) attracting attention when events (e.g., passing vehicles) occurred in those areas. Furthermore, the task instructions provided to participants (i.e., speed monitoring or hazard monitoring) affected their attention distribution in an interpretable manner. It is concluded that eye movements while supervising an automated vehicle are strongly `top-down', i.e., based on an expected value. The results are discussed in the context of the development of driver availability monitoring systems.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {attention,driving,eye-tracking},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Eisma et al_2022_What Attracts the Driver’s Eye.pdf}
}

@misc{ellisModelingHumanlikeConcept2023,
  title = {Modeling {{Human-like Concept Learning}} with {{Bayesian Inference}} over {{Natural Language}}},
  author = {Ellis, Kevin},
  year = {2023},
  month = jun,
  number = {arXiv:2306.02797},
  eprint = {2306.02797},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-09},
  abstract = {We model learning of abstract symbolic concepts by performing Bayesian inference over utterances in natural language. For efficient inference, we use a large language model as a proposal distribution. We fit a prior to human data to better model human learners, and evaluate on both generative and logical concepts.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/ellis_2023_modeling_human-like_concept_learning_with_bayesian.pdf;/Users/thomasgorman/Zotero/storage/K37W994P/2306.html}
}

@article{eltonApplyingDeutschsConcept2021,
  title = {Applying {{Deutsch}}'s Concept of Good Explanations to Artificial Intelligence and Neuroscience -- {{An}} Initial Exploration},
  author = {Elton, Daniel C.},
  year = {2021},
  month = jun,
  journal = {Cognitive Systems Research},
  volume = {67},
  pages = {9--17},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2020.12.002},
  urldate = {2022-03-05},
  abstract = {Artificial intelligence has made great strides since the deep learning revolution, but AI systems remain incapable of learning principles and rules which allow them to extrapolate outside of their training data to new situations. For inspiration we look to the domain of science, where scientists have been able to develop theories which show remarkable ability to extrapolate and sometimes even predict the existence of phenomena which have never been observed before. According to David Deutsch, this type of extrapolation, which he calls ``reach'', is due to scientific theories being hard to vary. In this work we investigate Deutsch's hard-to-vary principle and how it relates to more formalized principles in deep learning such as the bias-variance trade-off and Occam's razor. We distinguish internal variability, how much a model/theory can be varied internally while still yielding the same predictions, with external variability, which is how much a model must be varied to predict new, out-of-distribution data. We discuss how to measure internal variability using the notion of the Rashomon set and how to measure external variability using Kolmogorov complexity. We explore what role hard-to-vary explanations play in intelligence by looking at the human brain, the only example of highly general purpose intelligence known. We distinguish two learning systems in the brain -- the first operates similar to deep learning and likely underlies most of perception while the second is a more creative system capable of generating hard-to-vary models and explanations of the world. We make contact with Popperian epistemology which suggests that the generation of scientific theories is a not an inductive process but rather an evolutionary process which proceeds through conjecture and refutation. We argue that figuring out how replicate this second system, which is capable of generating hard-to-vary explanations, is a key challenge which needs to be solved in order to realize artificial general intelligence.},
  langid = {english},
  keywords = {Artificial intelligence,Critical rationalism,Deep learning,Extrapolation,Generalization,Induction,Intelligence,Interpolation,Karl Popper,Occam's razor,Simplicity},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Elton_2021_Applying Deutsch’s concept of good explanations to artificial intelligence and.pdf;/Users/thomasgorman/Zotero/storage/SH9JSG53/S138904172030108X.html}
}

@misc{evansonLanguageAcquisitionChildren2023,
  title = {Language Acquisition: Do Children and Language Models Follow Similar Learning Stages?},
  shorttitle = {Language Acquisition},
  author = {Evanson, Linnea and Lakretz, Yair and King, Jean-R{\'e}mi},
  year = {2023},
  month = jun,
  number = {arXiv:2306.03586},
  eprint = {2306.03586},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-11},
  abstract = {During language acquisition, children follow a typical sequence of learning stages, whereby they first learn to categorize phonemes before they develop their lexicon and eventually master increasingly complex syntactic structures. However, the computational principles that lead to this learning trajectory remain largely unknown. To investigate this, we here compare the learning trajectories of deep language models to those of children. Specifically, we test whether, during its training, GPT-2 exhibits stages of language acquisition comparable to those observed in children aged between 18 months and 6 years. For this, we train 48 GPT-2 models from scratch and evaluate their syntactic and semantic abilities at each training step, using 96 probes curated from the BLiMP, Zorro and BIG-Bench benchmarks. We then compare these evaluations with the behavior of 54 children during language production. Our analyses reveal three main findings. First, similarly to children, the language models tend to learn linguistic skills in a systematic order. Second, this learning scheme is parallel: the language tasks that are learned last improve from the very first training steps. Third, some - but not all - learning stages are shared between children and these language models. Overall, these results shed new light on the principles of language acquisition, and highlight important divergences in how humans and modern algorithms learn to process natural language.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Evanson et al_2023_Language acquisition.pdf;/Users/thomasgorman/Zotero/storage/EKE83WLY/2306.html}
}

@article{fajenScalingInformationAction2005,
  title = {The {{Scaling}} of {{Information}} to {{Action}} in {{Visually Guided Braking}}.},
  author = {Fajen, Brett R.},
  year = {2005},
  journal = {Journal of Experimental Psychology: Human Perception and Performance},
  volume = {31},
  number = {5},
  pages = {1107--1123},
  issn = {1939-1277, 0096-1523},
  doi = {10.1037/0096-1523.31.5.1107},
  urldate = {2024-09-16},
  abstract = {Braking to avoid a collision can be controlled by keeping the deceleration required to stop (i.e., ideal deceleration) in the ``safe'' region below maximum deceleration, but maximum deceleration is not optically specified and can vary as conditions change. When brake strength was manipulated between participants using a simulated braking task, the ratio of ideal to maximum deceleration at brake onset was invariant across groups, suggesting that calibration involves scaling information about ideal deceleration in intrinsic units of maximum deceleration. Evidence of rapid recalibration was found when brake strength was manipulated within participants, and the presence of external forces that affect brake dynamics resulted in biases in performance. Discussion focuses on the role of calibration, internal models, and affordance perception in visually guided action.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Fajen_2005_The Scaling of Information to Action in Visually Guided Braking.pdf}
}

@article{fanCanLargeLanguage2024,
  title = {Can {{Large Language Models Serve}} as {{Rational Players}} in {{Game Theory}}? {{A Systematic Analysis}}},
  shorttitle = {Can {{Large Language Models Serve}} as {{Rational Players}} in {{Game Theory}}?},
  author = {Fan, Caoyun and Chen, Jindou and Jin, Yaohui and He, Hao},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {16},
  pages = {17960--17967},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i16.29751},
  urldate = {2024-07-10},
  abstract = {Game theory, as an analytical tool, is frequently utilized to analyze human behavior in social science research. With the high alignment between the behavior of Large Language Models (LLMs) and humans, a promising research direction is to employ LLMs as substitutes for humans in game experiments, enabling social science research. However, despite numerous empirical researches on the combination of LLMs and game theory, the capability boundaries of LLMs in game theory remain unclear. In this research, we endeavor to systematically analyze LLMs in the context of game theory. Specifically, rationality, as the fundamental principle of game theory, serves as the metric for evaluating players' behavior --- building a clear desire, refining belief about uncertainty, and taking optimal actions. Accordingly, we select three classical games (dictator game, Rock-Paper-Scissors, and ringnetwork game) to analyze to what extent LLMs can achieve rationality in these three aspects. The experimental results indicate that even the current state-of-the-art LLM (GPT-4) exhibits substantial disparities compared to humans in game theory. For instance, LLMs struggle to build desires based on uncommon preferences, fail to refine belief from many simple patterns, and may overlook or modify refined belief when taking actions. Therefore, we consider that introducing LLMs into game experiments in the field of social science should be approached with greater caution.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Fan et al_2024_Can Large Language Models Serve as Rational Players in Game Theory.pdf}
}

@article{farquharDetectingHallucinationsLarge2024,
  title = {Detecting Hallucinations in Large Language Models Using Semantic Entropy},
  author = {Farquhar, Sebastian and Kossen, Jannik and Kuhn, Lorenz and Gal, Yarin},
  year = {2024},
  month = jun,
  journal = {Nature},
  volume = {630},
  number = {8017},
  pages = {625--630},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07421-0},
  urldate = {2024-08-02},
  abstract = {Large language model (LLM) systems, such as ChatGPT1 or Gemini2, can show impressive reasoning and question-answering capabilities but often `hallucinate' false outputs and unsubstantiated answers3,4. Answering unreliably or without the necessary information prevents adoption in diverse fields, with problems including fabrication of legal precedents5 or untrue facts in news articles6 and even posing a risk to human life in medical domains such as radiology7. Encouraging truthfulness through supervision or reinforcement has~been only partially successful8. Researchers need a general method for detecting hallucinations in LLMs that works even with new and unseen questions to which humans might not know the answer. Here we develop new methods grounded in statistics, proposing entropy-based uncertainty estimators for LLMs to detect a subset of hallucinations---confabulations---which are arbitrary and incorrect generations. Our method addresses the fact that one idea can be expressed in many ways by computing uncertainty at the level of meaning rather than specific sequences of words. Our method works across datasets and tasks without a priori knowledge of the task, requires no task-specific data and robustly generalizes to new tasks not seen before. By detecting when a prompt is likely to produce a confabulation, our method helps users understand when they must take extra care with LLMs and opens up new possibilities for using LLMs that are otherwise prevented by their unreliability.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computer science,Information technology},
  annotation = {https://github.com/jlko/semantic\_uncertainty},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Farquhar et al_2024_Detecting hallucinations in large language models using semantic entropy.pdf}
}

@misc{fastowskiUnderstandingKnowledgeDrift2024,
  title = {Understanding {{Knowledge Drift}} in {{LLMs}} through {{Misinformation}}},
  author = {Fastowski, Alina and Kasneci, Gjergji},
  year = {2024},
  month = sep,
  number = {arXiv:2409.07085},
  eprint = {2409.07085},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-15},
  abstract = {Large Language Models (LLMs) have revolutionized numerous applications, making them an integral part of our digital ecosystem. However, their reliability becomes critical, especially when these models are exposed to misinformation. We primarily analyze the susceptibility of state-of-the-art LLMs to factual inaccuracies when they encounter false information in a QnA scenario, an issue that can lead to a phenomenon we refer to as *knowledge drift*, which significantly undermines the trustworthiness of these models. We evaluate the factuality and the uncertainty of the models' responses relying on Entropy, Perplexity, and Token Probability metrics. Our experiments reveal that an LLM's uncertainty can increase up to 56.6\% when the question is answered incorrectly due to the exposure to false information. At the same time, repeated exposure to the same false information can decrease the models uncertainty again (-52.8\% w.r.t. the answers on the untainted prompts), potentially manipulating the underlying model's beliefs and introducing a drift from its original knowledge. These findings provide insights into LLMs' robustness and vulnerability to adversarial inputs, paving the way for developing more reliable LLM applications across various domains. The code is available at https://github.com/afastowski/knowledge\_drift.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Fastowski_Kasneci_2024_Understanding Knowledge Drift in LLMs through Misinformation.pdf}
}

@article{franceschelliCreativityMachineLearning2024,
  title = {Creativity and {{Machine Learning}}: {{A Survey}}},
  shorttitle = {Creativity and {{Machine Learning}}},
  author = {Franceschelli, Giorgio and Musolesi, Mirco},
  year = {2024},
  month = nov,
  journal = {ACM Computing Surveys},
  volume = {56},
  number = {11},
  pages = {1--41},
  issn = {0360-0300, 1557-7341},
  doi = {10.1145/3664595},
  urldate = {2024-08-11},
  abstract = {There is a growing interest in the area of machine learning and creativity. This survey presents an overview of the history and the state of the art of computational creativity theories, key machine learning techniques (including generative deep learning), and corresponding automatic evaluation methods. After presenting a critical discussion of the key contributions in this area, we outline the current research challenges and emerging opportunities in this field.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Franceschelli_Musolesi_2024_Creativity and Machine Learning.pdf}
}

@misc{frankBabyStepsEvaluating2017,
  title = {Baby Steps in Evaluating the Capacities of Large Language Models},
  author = {Frank, Michael C.},
  year = {2017},
  month = nov,
  doi = {10.1126/science.aag2132},
  urldate = {2023-07-01},
  abstract = {Large language models show remarkable capacities, but it is unclear what abstractions support their behavior. Methods from developmental psychology can help researchers understand the representations used by these models, complementing standard computational approaches---and perhaps leading to insights about the nature of mind.},
  langid = {english},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Frank_2017_Baby steps in evaluating the capacities of large language models.PDF}
}

@misc{frankeBayesianStatisticalModeling2024,
  title = {Bayesian {{Statistical Modeling}} with {{Predictors}} from {{LLMs}}},
  author = {Franke, Michael and Tsvilodub, Polina and Carcassi, Fausto},
  year = {2024},
  month = jun,
  number = {arXiv:2406.09012},
  eprint = {2406.09012},
  publisher = {arXiv},
  urldate = {2024-06-30},
  abstract = {State of the art large language models (LLMs) have shown impressive performance on a variety of benchmark tasks and are increasingly used as components in larger applications, where LLMbased predictions serve as proxies for human judgements or decision. This raises questions about the human-likeness of LLM-derived information, alignment with human intuition, and whether LLMs could possibly be considered (parts of) explanatory models of (aspects of) human cognition or language use. To shed more light on these issues, we here investigate the human-likeness of LLMs' predictions for multiple-choice decision tasks from the perspective of Bayesian statistical modeling. Using human data from a forced-choice experiment on pragmatic language use, we find that LLMs do not capture the variance in the human data at the item-level. We suggest different ways of deriving full distributional predictions from LLMs for aggregate, condition-level data, and find that some, but not all ways of obtaining condition-level predictions yield adequate fits to human data. These results suggests that assessment of LLM performance depends strongly on seemingly subtle choices in methodology, and that LLMs are at best predictors of human behavior at the aggregate, condition-level, for which they are, however, not designed to, or usually used to, make predictions in the first place.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://osf.io/f6j3a/?view\_only=5e820cc8bbee4549aed58dc252ba61b9},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Franke et al_2024_Bayesian Statistical Modeling with Predictors from LLMs.pdf}
}

@misc{frankLargeLanguageModels2023,
  title = {Large Language Models as Models of Human Cognition},
  author = {Frank, Michael C.},
  year = {2023},
  urldate = {2023-07-30},
  abstract = {Can a large language model be used as a `cognitive model', a scientific artifact that helps us understand the human mind? If LLMs can be made openly accessible to scientific investigation then they may provide a valuable model system for studying the emergence of language, reasoning, and other uniquely human behaviors.},
  langid = {english},
  annotation = {https://osf.io/preprints/psyarxiv/wxt69},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Frank_2023_Large language models as models of human cognition.pdf}
}

@article{frankOpenlyAccessibleLLMs2023,
  title = {Openly Accessible {{LLMs}} Can Help Us to Understand Human Cognition},
  author = {Frank, Michael C.},
  year = {2023},
  month = nov,
  journal = {Nature Human Behaviour},
  volume = {7},
  number = {11},
  pages = {1825--1827},
  publisher = {Nature Publishing Group},
  issn = {2397-3374},
  doi = {10.1038/s41562-023-01732-4},
  urldate = {2023-11-23},
  abstract = {Large language models can be construed as `cognitive models', scientific artefacts that help us to understand the human mind. If made openly accessible, they may provide a valuable model system for studying the emergence of language, reasoning and other uniquely human behaviours.},
  copyright = {2023 Springer Nature Limited},
  langid = {english},
  keywords = {Human behaviour,Language and linguistics},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Frank_2023_Openly accessible LLMs can help us to understand human cognition.pdf}
}

@misc{frisoniGenerateRetrieveEffectiveness2024,
  title = {To {{Generate}} or to {{Retrieve}}? {{On}} the {{Effectiveness}} of {{Artificial Contexts}} for {{Medical Open-Domain Question Answering}}},
  shorttitle = {To {{Generate}} or to {{Retrieve}}?},
  author = {Frisoni, Giacomo and Cocchieri, Alessio and Presepi, Alex and Moro, Gianluca and Meng, Zaiqiao},
  year = {2024},
  month = jun,
  number = {arXiv:2403.01924},
  eprint = {2403.01924},
  publisher = {arXiv},
  urldate = {2024-08-11},
  abstract = {Medical open-domain question answering demands substantial access to specialized knowledge. Recent efforts have sought to decouple knowledge from model parameters, counteracting architectural scaling and allowing for training on common low-resource hardware. The retrieve-then-read paradigm has become ubiquitous, with model predictions grounded on relevant knowledge pieces from external repositories such as PubMed, textbooks, and UMLS. An alternative path, still under-explored but made possible by the advent of domain-specific large language models, entails constructing artificial contexts through prompting. As a result, "to generate or to retrieve" is the modern equivalent of Hamlet's dilemma. This paper presents MedGENIE, the first generate-then-read framework for multiple-choice question answering in medicine. We conduct extensive experiments on MedQA-USMLE, MedMCQA, and MMLU, incorporating a practical perspective by assuming a maximum of 24GB VRAM. MedGENIE sets a new state-of-the-art in the open-book setting of each testbed, allowing a small-scale reader to outcompete zero-shot closed-book 175B baselines while using up to 706\${\textbackslash}times\$ fewer parameters. Our findings reveal that generated passages are more effective than retrieved ones in attaining higher accuracy.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {https://github.com/disi-unibo-nlp/medgenie},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Frisoni et al_2024_To Generate or to Retrieve.pdf}
}

@article{fuchsWhenWhereWhom2024,
  title = {When, Where, and with Whom during Crisis: {{The}} Effect of Risk Perceptions and Psychological Distance on Travel Intentions},
  shorttitle = {When, Where, and with Whom during Crisis},
  author = {Fuchs, Galia and {Efrat-Treister}, Dorit and Westphal, Monika},
  year = {2024},
  month = feb,
  journal = {Tourism Management},
  volume = {100},
  pages = {104809},
  issn = {0261-5177},
  doi = {10.1016/j.tourman.2023.104809},
  urldate = {2024-08-11},
  abstract = {We investigate how risk perceptions and psychological distance impacted people's travel intentions during Covid-19. Our findings reveal that traveling to a high-risk destination increased people's risk perceptions of Covid-19, and their risk perceptions at the destination, which, in turn, reduced people's travel intentions. We identify temporal, spatial, and social distance (the ``when, where, and with whom'' of traveling) as moderators of these effects; while social distance moderates the effect of risk, on risk perceptions, temporal and spatial distance moderate the effect of risk perceptions on travel intentions. We outline theoretical contributions and implications for tourism during crisis.},
  pmcid = {PMC10290809},
  pmid = {37387777},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Fuchs et al_2024_When, where, and with whom during crisis.pdf}
}

@misc{gaoAligningLLMAgents2024,
  title = {Aligning {{LLM Agents}} by {{Learning Latent Preference}} from {{User Edits}}},
  author = {Gao, Ge and Taymanov, Alexey and Salinas, Eduardo and Mineiro, Paul and Misra, Dipendra},
  year = {2024},
  month = jun,
  number = {arXiv:2404.15269},
  eprint = {2404.15269},
  publisher = {arXiv},
  urldate = {2024-08-11},
  abstract = {We study interactive learning of LLM-based language agents based on user edits made to the agent's output. In a typical setting such as writing assistants, the user interacts with a language agent to generate a response given a context, and may optionally edit the agent response to personalize it based on their latent preference, in addition to improving the correctness. The edit feedback is naturally generated, making it a suitable candidate for improving the agent's alignment with the user's preference, and for reducing the cost of user edits over time. We propose a learning framework, PRELUDE that infers a description of the user's latent preference based on historic edit data. The inferred user preference descriptions are used to define prompts for generating responses in the future. This avoids fine-tuning the agent, which is costly, challenging to scale with the number of users, and may even degrade its performance on other tasks. Furthermore, learning descriptive preference improves interpretability, allowing the user to view and modify the learned preference. However, user preference can be complex, subtle, and vary based on context, making it challenging to learn. To address this, we propose a simple yet effective algorithm named CIPHER that leverages the LLM to infer the user preference for a given context based on user edits. In the future, CIPHER retrieves inferred preferences from the k-closest contexts in the history, and forms an aggregate preference for response generation. We introduce two interactive environments -- summarization and email writing, and use a GPT-4 simulated user for evaluation. On both tasks, CIPHER outperforms several baselines by achieving the lowest edit distance cost while only having a small overhead in LLM query cost. Our analysis reports that user preferences learned by CIPHER show significant similarity to the ground truth latent preferences.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Information Retrieval,Computer Science - Machine Learning},
  annotation = {https://github.com/gao-g/prelude},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Gao et al_2024_Aligning LLM Agents by Learning Latent Preference from User Edits.pdf}
}

@article{gaoComparingScientificAbstracts2023,
  title = {Comparing Scientific Abstracts Generated by {{ChatGPT}} to Real Abstracts with Detectors and Blinded Human Reviewers},
  author = {Gao, Catherine A. and Howard, Frederick M. and Markov, Nikolay S. and Dyer, Emma C. and Ramesh, Siddhi and Luo, Yuan and Pearson, Alexander T.},
  year = {2023},
  month = apr,
  journal = {npj Digital Medicine},
  volume = {6},
  number = {1},
  pages = {1--5},
  publisher = {Nature Publishing Group},
  issn = {2398-6352},
  doi = {10.1038/s41746-023-00819-6},
  urldate = {2023-09-01},
  abstract = {Large language models such as ChatGPT can produce increasingly realistic text, with unknown information on the accuracy and integrity of using these models in scientific writing. We gathered fifth research abstracts from five high-impact factor medical journals and asked ChatGPT to generate research abstracts based on their titles and journals. Most generated abstracts were detected using an AI output detector, `GPT-2 Output Detector', with \% `fake' scores (higher meaning more likely to be generated) of median [interquartile range] of 99.98\% `fake' [12.73\%, 99.98\%] compared with median 0.02\% [IQR 0.02\%, 0.09\%] for the original abstracts. The AUROC of the AI output detector was 0.94. Generated abstracts scored lower than original abstracts when run through a plagiarism detector website and iThenticate (higher scores meaning more matching text found). When given a mixture of original and general abstracts, blinded human reviewers correctly identified 68\% of generated abstracts as being generated by ChatGPT, but incorrectly identified 14\% of original abstracts as being generated. Reviewers indicated that it was surprisingly difficult to differentiate between the two, though abstracts they suspected were generated were vaguer and more formulaic. ChatGPT writes believable scientific abstracts, though with completely generated data. Depending on publisher-specific guidelines, AI output detectors may serve as an editorial tool to help maintain scientific standards. The boundaries of ethical and acceptable use of large language models to help scientific writing are still being discussed, and different journals and conferences are adopting varying policies.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational biology and bioinformatics,Medical research,Publishing},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Gao et al_2023_Comparing scientific abstracts generated by ChatGPT to real abstracts with.pdf}
}

@misc{gaoMemorySharingLarge2024,
  title = {Memory {{Sharing}} for {{Large Language Model}} Based {{Agents}}},
  author = {Gao, Hang and Zhang, Yongfeng},
  year = {2024},
  month = jul,
  number = {arXiv:2404.09982},
  eprint = {2404.09982},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-11},
  abstract = {The adaptation of Large Language Model (LLM)-based agents to execute tasks via natural language prompts represents a significant advancement, notably eliminating the need for explicit retraining or fine tuning, but are constrained by the comprehensiveness and diversity of the provided examples, leading to outputs that often diverge significantly from expected results, especially when it comes to the open-ended questions. This paper introduces the Memory Sharing, a framework which integrates the real-time memory filter, storage and retrieval to enhance the In-Context Learning process. This framework allows for the sharing of memories among multiple agents, whereby the interactions and shared memories between different agents effectively enhance the diversity of the memories. The collective self-enhancement through interactive learning among multiple agents facilitates the evolution from individual intelligence to collective intelligence. Besides, the dynamically growing memory pool is utilized not only to improve the quality of responses but also to train and enhance the retriever. We evaluated our framework across three distinct domains involving specialized tasks of agents. The experimental results demonstrate that the MS framework significantly improves the agents' performance in addressing open-ended questions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Gao_Zhang_2024_Memory Sharing for Large Language Model based Agents.pdf}
}

@article{gargWhatCanTransformers,
  title = {What {{Can Transformers Learn In-Context}}? {{A Case Study}} of {{Simple Function Classes}}},
  author = {Garg, Shivam and Tsipras, Dimitris and Liang, Percy and Valiant, Gregory},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Garg et al_What Can Transformers Learn In-Context.pdf}
}

@article{giudiciDesigningHomeAutomation2024,
  title = {Designing {{Home Automation Routines Using}} an {{LLM-Based Chatbot}}},
  author = {Giudici, Mathyas and Padalino, Luca and Paolino, Giovanni and Paratici, Ilaria and Pascu, Alexandru Ionut and Garzotto, Franca},
  year = {2024},
  month = jun,
  journal = {Designs},
  volume = {8},
  number = {3},
  pages = {43},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2411-9660},
  doi = {10.3390/designs8030043},
  urldate = {2024-07-02},
  abstract = {Without any more delay, individuals are urged to adopt more sustainable behaviors to fight climate change. New digital systems mixed with engaging and gamification mechanisms could play an important role in achieving such an objective. In particular, Conversational Agents, like Smart Home Assistants, are a promising tool that encourage sustainable behaviors within household settings. In recent years, large language models (LLMs) have shown great potential in enhancing the capabilities of such assistants, making them more effective in interacting with users. We present the design and implementation of GreenIFTTT, an application empowered by GPT4 to create and control home automation routines. The agent helps users understand which energy consumption optimization routines could be created and applied to make their home appliances more environmentally sustainable. We performed an exploratory study (Italy, December 2023) with N = 13 participants to test our application's usability and UX. The results suggest that GreenIFTTT is a usable, engaging, easy, and supportive tool, providing insight into new perspectives and usage of LLMs to create more environmentally sustainable home automation.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {conversational agents,domestic sustainability,home automation,LLM,TAP},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Giudici et al_2024_Designing Home Automation Routines Using an LLM-Based Chatbot.pdf}
}

@article{goldsteinSharedComputationalPrinciples2022,
  title = {Shared Computational Principles for Language Processing in Humans and Deep Language Models},
  author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Catherine and Casto, Colton and Fanda, Lora and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Melloni, Lucia and Reichart, Roi and Devore, Sasha and Flinker, Adeen and Hasenfratz, Liat and Levy, Omer and Hassidim, Avinatan and Brenner, Michael and Matias, Yossi and Norman, Kenneth A. and Devinsky, Orrin and Hasson, Uri},
  year = {2022},
  month = mar,
  journal = {Nature Neuroscience},
  volume = {25},
  number = {3},
  pages = {369--380},
  issn = {1546-1726},
  doi = {10.1038/s41593-022-01026-4},
  urldate = {2023-11-24},
  abstract = {Departing from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregressive) deep language models (DLMs). Using a self-supervised next-word prediction task, these models generate appropriate linguistic responses in a given context. In the current study, nine participants listened to a 30-min podcast while their brain responses were recorded using electrocorticography (ECoG). We provide empirical evidence that the human brain and autoregressive DLMs share three fundamental computational principles as they process the same natural narrative: (1) both are engaged in continuous next-word prediction before word onset; (2) both match their pre-onset predictions to the incoming word to calculate post-onset surprise; (3) both rely on contextual embeddings to represent words in natural contexts. Together, our findings suggest that autoregressive DLMs provide a new and biologically feasible computational framework for studying the neural basis of language.},
  copyright = {2022 The Author(s)},
  langid = {english},
  keywords = {Electrophysiology,Language,Neural decoding,Neural encoding},
  annotation = {https://github.com/orgs/hassonlab/repositories/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Goldstein et al_2022_Shared computational principles for language processing in humans and deep.pdf}
}

@article{goliCanLargeLanguage2024,
  title = {Can {{Large Language Models Capture Human Preferences}}?},
  shorttitle = {Frontiers},
  author = {Goli, Ali and Singh, Amandeep},
  year = {2024},
  month = apr,
  journal = {Marketing Science},
  issn = {0732-2399},
  doi = {10.1287/mksc.2023.0306},
  urldate = {2024-06-16},
  abstract = {We explore the viability of large language models (LLMs), specifically OpenAI's GPT-3.5 and GPT-4, in emulating human survey respondents and eliciting preferences, with a focus on intertemporal choices. Leveraging the extensive literature on intertemporal discounting for benchmarking, we examine responses from LLMs across various languages and compare them with human responses, exploring preferences between smaller, sooner and larger, later rewards. Our findings reveal that both generative pretrained transformer (GPT) models demonstrate less patience than humans, with GPT-3.5 exhibiting a lexicographic preference for earlier rewards unlike human decision makers. Although GPT-4 does not display lexicographic preferences, its measured discount rates are still considerably larger than those found in humans. Interestingly, GPT models show greater patience in languages with weak future tense references, such as German and Mandarin, aligning with the existing literature that suggests a correlation between language structure and intertemporal preferences. We demonstrate how prompting GPT to explain its decisions, a procedure we term ``chain-of-thought conjoint,'' can mitigate, but does not eliminate, discrepancies between LLM and human responses. Although directly eliciting preferences using LLMs may yield misleading results, combining chain-of-thought conjoint with topic modeling aids in hypothesis generation, enabling researchers to explore the underpinnings of preferences. Chain-of-thought conjoint provides a structured framework for marketers to use LLMs to identify potential attributes or factors that can explain preference heterogeneity across different customers and contexts.History: Olivier Toubia served as the senior editor. This paper was accepted through the Marketing Science: Frontiers review process.Supplemental Material: The online appendix and data files are available at https://doi.org/10.1287/mksc.2023.0306.},
  keywords = {chain of thought,conjoint,decision making,intertemporal preferences,large language models},
  annotation = {https://pubsonline.informs.org/doi/suppl/10.1287/mksc.2023.0306},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Goli_Singh_2024_Frontiers.pdf}
}

@article{gotzLetAlgorithmSpeak2023,
  title = {Let the Algorithm Speak: {{How}} to Use Neural Networks for Automatic Item Generation in Psychological Scale Development},
  shorttitle = {Let the Algorithm Speak},
  author = {G{\"o}tz, Friedrich M. and Maertens, Rakoen and Loomba, Sahil and {van der Linden}, Sander},
  year = {2023},
  journal = {Psychological Methods},
  pages = {No Pagination Specified-No Pagination Specified},
  issn = {1939-1463},
  doi = {10.1037/met0000540},
  abstract = {Measurement is at the heart of scientific research. As many---perhaps most---psychological constructs cannot be directly observed, there is a steady demand for reliable self-report scales to assess latent constructs. However, scale development is a tedious process that requires researchers to produce good items in large quantities. In this tutorial, we introduce, explain, and apply the Psychometric Item Generator (PIG), an open-source, free-to-use, self-sufficient natural language processing algorithm that produces large-scale, human-like, customized text output within a few mouse clicks. The PIG is based on the GPT-2, a powerful generative language model, and runs on Google Colaboratory---an interactive virtual notebook environment that executes code on state-of-the-art virtual machines at no cost. Across two demonstrations and a preregistered five-pronged empirical validation with two Canadian samples (NSample 1 = 501, NSample 2 = 773), we show that the PIG is equally well-suited to generate large pools of face-valid items for novel constructs (i.e., wanderlust) and create parsimonious short scales of existing constructs (i.e., Big Five personality traits) that yield strong performances when tested in the wild and benchmarked against current gold standards for assessment. The PIG does not require any prior coding skills or access to computational resources and can easily be tailored to any desired context by simply switching out short linguistic prompts in a single line of code. In short, we present an effective, novel machine learning solution to an old psychological challenge. As such, the PIG will not require you to learn a new language---but instead, speak yours. (PsycInfo Database Record (c) 2023 APA, all rights reserved)},
  keywords = {Algorithms,Artificial Intelligence,Artificial Neural Networks,Natural Language Processing,Neural Networks,Pigs,Psychometrics,Test Construction,Test Reliability},
  annotation = {https://osf.io/fhpv6/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Götz et al_2023_Let the algorithm speak2.pdf;/Users/thomasgorman/Zotero/storage/FV3IB93Z/2023-47214-001.html}
}

@misc{GPT4TechnicalReport,
  title = {{{GPT-4 Technical Report}}},
  urldate = {2023-03-14},
  abstract = {We report the development of GPT-4, a large-scale, multimodal model which can accept image and text inputs and produce text outputs. While less capable than humans in many real-world scenarios, GPT-4 exhibits human-level performance on various professional and academic benchmarks, including passing a simulated bar exam with a score around the top 10\% of test takers. GPT-4 is a Transformerbased model pre-trained to predict the next token in a document. The post-training alignment process results in improved performance on measures of factuality and adherence to desired behavior. A core component of this project was developing infrastructure and optimization methods that behave predictably across a wide range of scales. This allowed us to accurately predict some aspects of GPT-4's performance based on models trained with no more than 1/1,000th the compute of GPT-4.},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/GPT4TechnicalReport-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/GPT-4 Technical Report.pdf}
}

@article{grandSemanticProjectionRecovers2022,
  title = {Semantic Projection Recovers Rich Human Knowledge of Multiple Object Features from Word Embeddings},
  author = {Grand, Gabriel and Blank, Idan Asher and Pereira, Francisco and Fedorenko, Evelina},
  year = {2022},
  month = apr,
  journal = {Nature Human Behaviour},
  volume = {6},
  number = {7},
  pages = {975--987},
  issn = {2397-3374},
  doi = {10.1038/s41562-022-01316-8},
  urldate = {2024-05-26},
  langid = {english},
  annotation = {https://osf.io/5r2sz/\\
\\
https://nlp.stanford.edu/projects/glove/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Grand et al_2022_Semantic projection recovers rich human knowledge of multiple object features.pdf}
}

@misc{griffithsBayesAgeIntelligent2023,
  title = {Bayes in the Age of Intelligent Machines},
  author = {Griffiths, Thomas L. and Zhu, Jian-Qiao and Grant, Erin and McCoy, R. Thomas},
  year = {2023},
  month = nov,
  number = {arXiv:2311.10206},
  eprint = {2311.10206},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-23},
  abstract = {The success of methods based on artificial neural networks in creating intelligent machines seems like it might pose a challenge to explanations of human cognition in terms of Bayesian inference. We argue that this is not the case, and that in fact these systems offer new opportunities for Bayesian modeling. Specifically, we argue that Bayesian models of cognition and artificial neural networks lie at different levels of analysis and are complementary modeling approaches, together offering a way to understand human cognition that spans these levels. We also argue that the same perspective can be applied to intelligent machines, where a Bayesian approach may be uniquely valuable in understanding the behavior of large, opaque artificial neural networks that are trained on proprietary data.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Griffiths et al_2023_Bayes in the age of intelligent machines.pdf;/Users/thomasgorman/Zotero/storage/E2BWGMFE/2311.html}
}

@misc{gruenMachineLearningAugmentation2023,
  title = {Machine Learning Augmentation Reduces Prediction Error in Collective Forecasting: Development and Validation across Prediction Markets},
  shorttitle = {Machine Learning Augmentation Reduces Prediction Error in Collective Forecasting},
  author = {Gruen, Alexander and Mattingly, Karl R. and Morwitch, Ellen and Bossaerts, Frederik and Clifford, Manning and Nash, Chad and Ioannidis, John P. A. and Ponsonby, Anne-Louise},
  year = {2023},
  month = jan,
  pages = {2023.01.19.23284578},
  publisher = {medRxiv},
  doi = {10.1101/2023.01.19.23284578},
  urldate = {2023-11-25},
  abstract = {The recent COVID-19 crisis highlighted the inadequacy of human forecasting. We aim to leverage human prediction markets with real-time machine weighting of likely higher accuracy trades to improve performance. The crowd sourced Almanis prediction market longitudinal platform (n=1822) and Next Generation Social Science (NGS2) platform (n=103) were utilised. A 43-feature model predicted top quintile relative Brier accuracy scores in two out-of-sample datasets (pboth{$<$}1{\texttimes}10-9). Trades graded as high machine accuracy quality vs. other trades had a greater AUC temporal gain from before to after trade. Hybrid human-machine forecasts had higher accuracy than human forecasts alone, particularly when the two systems disagreed by 5\% or more for binary event prediction: the hybrid system demonstrating substantial AUC gains of 13.2\%, p=1.35{\texttimes}10-14 and 13.8\%, p=0.003 in the out-of-sample Almanis B and NGS2 datasets respectively. When discordant, the hybrid model was correct for COVID-19 event occurrence 72.7\% of the time vs 27.3\% for human-only models, p=0.007. This net classification benefit was replicated in the separate Almanis B dataset, p=2.4{\texttimes}10-7. Real-time machine classification followed by weighting human trades according to likely accuracy improves collective forecasting performance. Implementation may allow improved anticipation of and response to emerging risks and improved human collective efforts generally. Significance Statement Human-machine hybrid approaches have been identified as a new frontier for event prediction and decision making in the artificial intelligence and collective human intelligence fields. For the first time, we present the successful development and validation of a human-machine hybrid prediction market approach and demonstrate its superior accuracy when compared to prediction markets based on human forecasting alone. The advantages of this new hybrid system are demonstrated in the context of COVID-19-related event prediction.},
  archiveprefix = {medRxiv},
  copyright = {{\copyright} 2023, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Gruen et al_2023_Machine learning augmentation reduces prediction error in collective forecasting.pdf}
}

@article{grzankowskiLLMsAreNot,
  title = {{{LLMs}} Are {{Not Just Next Token Predictors}}},
  author = {Grzankowski, Alex and Downes, Stephen M and Forber, Partick},
  abstract = {LLMs are statistical models of language learning through stochastic gradient descent with a next token prediction objective. Prompting a popular view among AI modelers: LLMs are just next token predictors. While LLMs are engineered using next token prediction, and trained based on their success at this task, our view is that a reduction to just next token predictor sells LLMs short. Moreover, there are important explanations of LLM behavior and capabilities that are lost when we engage in this kind of reduction. In order to draw this out, we will make an analogy with a once prominent research program in biology explaining evolution and development from the genes eye view.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Grzankowski et al_LLMs are Not Just Next Token Predictors.pdf}
}

@misc{guHowDataAnalysts2023,
  title = {How {{Do Data Analysts Respond}} to {{AI Assistance}}? {{A Wizard-of-Oz Study}}},
  shorttitle = {How {{Do Data Analysts Respond}} to {{AI Assistance}}?},
  author = {Gu, Ken and {Grunde-McLaughlin}, Madeleine and McNutt, Andrew M. and Heer, Jeffrey and Althoff, Tim},
  year = {2023},
  month = sep,
  number = {arXiv:2309.10108},
  eprint = {2309.10108},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-09-24},
  abstract = {Data analysis is challenging as analysts must navigate nuanced decisions that may yield divergent conclusions. AI assistants have the potential to support analysts in planning their analyses, enabling more robust decision-making. Though AI-based assistants that target code execution (e.g., Github Copilot) have received significant attention, limited research addresses assistance for both analysis execution and planning. In this work, we characterize helpful planning suggestions and their impacts on analysts' workflows. We first review the analysis planning literature and crowd-sourced analysis studies to categorize suggestion content. We then conduct a Wizard-of-Oz study (n=13) to observe analysts' preferences and reactions to planning assistance in a realistic scenario. Our findings highlight subtleties in contextual factors that impact suggestion helpfulness, emphasizing design implications for supporting different abstractions of assistance, forms of initiative, increased engagement, and alignment of goals between analysts and assistants.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Gu et al_2023_How Do Data Analysts Respond to AI Assistance.pdf;/Users/thomasgorman/Zotero/storage/Y6FSJ3XY/2309.html}
}

@article{guntherVectorSpaceModelsSemantic2019a,
  title = {Vector-{{Space Models}} of {{Semantic Representation From}} a {{Cognitive Perspective}}: {{A Discussion}} of {{Common Misconceptions}}},
  shorttitle = {Vector-{{Space Models}} of {{Semantic Representation From}} a {{Cognitive Perspective}}},
  author = {G{\"u}nther, Fritz and Rinaldi, Luca and Marelli, Marco},
  year = {2019},
  month = nov,
  journal = {Perspectives on Psychological Science},
  volume = {14},
  number = {6},
  pages = {1006--1033},
  publisher = {SAGE Publications Inc},
  issn = {1745-6916},
  doi = {10.1177/1745691619861372},
  urldate = {2024-07-12},
  abstract = {Models that represent meaning as high-dimensional numerical vectors---such as latent semantic analysis (LSA), hyperspace analogue to language (HAL), bound encoding of the aggregate language environment (BEAGLE), topic models, global vectors (GloVe), and word2vec---have been introduced as extremely powerful machine-learning proxies for human semantic representations and have seen an explosive rise in popularity over the past 2 decades. However, despite their considerable advancements and spread in the cognitive sciences, one can observe problems associated with the adequate presentation and understanding of some of their features. Indeed, when these models are examined from a cognitive perspective, a number of unfounded arguments tend to appear in the psychological literature. In this article, we review the most common of these arguments and discuss (a) what exactly these models represent at the implementational level and their plausibility as a cognitive theory, (b) how they deal with various aspects of meaning such as polysemy or compositionality, and (c) how they relate to the debate on embodied and grounded cognition. We identify common misconceptions that arise as a result of incomplete descriptions, outdated arguments, and unclear distinctions between theory and implementation of the models. We clarify and amend these points to provide a theoretical basis for future research and discussions on vector models of semantic representation.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Günther et al_2019_Vector-Space Models of Semantic Representation From a Cognitive Perspective.pdf}
}

@misc{guoEmbodiedLLMAgents2024,
  title = {Embodied {{LLM Agents Learn}} to {{Cooperate}} in {{Organized Teams}}},
  author = {Guo, Xudong and Huang, Kaixuan and Liu, Jiale and Fan, Wenhui and V{\'e}lez, Natalia and Wu, Qingyun and Wang, Huazheng and Griffiths, Thomas L. and Wang, Mengdi},
  year = {2024},
  month = may,
  number = {arXiv:2403.12482},
  eprint = {2403.12482},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-30},
  abstract = {Large Language Models (LLMs) have emerged as integral tools for reasoning, planning, and decision-making, drawing upon their extensive world knowledge and proficiency in language-related tasks. LLMs thus hold tremendous potential for natural language interaction within multi-agent systems to foster cooperation. However, LLM agents tend to over-report and comply with any instruction, which may result in information redundancy and confusion in multi-agent cooperation. Inspired by human organizations, this paper introduces a framework that imposes prompt-based organization structures on LLM agents to mitigate these problems. Through a series of experiments with embodied LLM agents and human-agent collaboration, our results highlight the impact of designated leadership on team efficiency, shedding light on the leadership qualities displayed by LLM agents and their spontaneous cooperative behaviors. Further, we harness the potential of LLMs to propose enhanced organizational prompts, via a Criticize-Reflect process, resulting in novel organization structures that reduce communication costs and enhance team efficiency1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Multiagent Systems},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Guo et al_2024_Embodied LLM Agents Learn to Cooperate in Organized Teams.pdf}
}

@article{haaseArtificialMusesGenerative2023,
  title = {Artificial Muses: {{Generative}} Artificial Intelligence Chatbots Have Risen to Human-Level Creativity},
  shorttitle = {Artificial Muses},
  author = {Haase, Jennifer and Hanel, Paul H. P.},
  year = {2023},
  month = dec,
  journal = {Journal of Creativity},
  volume = {33},
  number = {3},
  pages = {100066},
  issn = {2713-3745},
  doi = {10.1016/j.yjoc.2023.100066},
  urldate = {2024-05-25},
  abstract = {A widespread view is that Artificial Intelligence cannot be creative. We tested this assumption by comparing human-generated ideas with those generated by six Generative Artificial Intelligence (GAI) chatbots: alpa.ai, Copy.ai, ChatGPT (versions 3 and 4), Studio.ai, and YouChat. Humans and a specifically trained AI independently assessed the quality and quantity of ideas. We found no qualitative difference between AI and human-generated creativity, although there are differences in how ideas are generated. Interestingly, 9.4~\% of humans were more creative than the most creative GAI, GPT-4. Our findings suggest that GAIs are valuable assistants in the creative process. Continued research and development of GAI in creative tasks is crucial to fully understand this technology's potential benefits and drawbacks in shaping the future of creativity. Finally, we discuss the question of whether GAIs are capable of being ``truly'' creative.},
  keywords = {AI,Creativity,Generative artificial intelligence,Originality},
  annotation = {https://osf.io/9fctd/?view\_only=6c8f02c6972b49319c12f87cfb3f76db},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Haase_Hanel_2023_Artificial muses2.pdf;/Users/thomasgorman/Zotero/storage/BZ8EUPIK/S2713374523000250.html}
}

@article{hagendorffHumanlikeIntuitiveBehavior2023,
  title = {Human-like Intuitive Behavior and Reasoning Biases Emerged in Large Language Models but Disappeared in {{ChatGPT}}},
  author = {Hagendorff, Thilo and Fabi, Sarah and Kosinski, Michal},
  year = {2023},
  month = oct,
  journal = {Nature Computational Science},
  volume = {3},
  number = {10},
  pages = {833--838},
  issn = {2662-8457},
  doi = {10.1038/s43588-023-00527-x},
  urldate = {2024-06-29},
  abstract = {We design a battery of semantic illusions and cognitive reflection tests, aimed to elicit intuitive yet erroneous responses. We administer these tasks, traditionally used to study reasoning and decision-making in humans, to OpenAI's generative pre-trained transformer model family. The results show that as the models expand in size and linguistic proficiency they increasingly display human-like intuitive system 1 thinking and associated cognitive errors. This pattern shifts notably with the introduction of ChatGPT models, which tend to respond correctly, avoiding the traps embedded in the tasks. Both ChatGPT-3.5 and 4 utilize the input--output context window to engage in chain-of-thought reasoning, reminiscent of how people use notepads to support their system 2 thinking. Yet, they remain accurate even when prevented from engaging in chain-of-thought reasoning, indicating that their system-1-like next-word generation processes are more accurate than those of older models. Our findings highlight the value of applying psychological methodologies to study large language models, as this can uncover previously undetected emergent characteristics.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Computational science,Computer science,Psychology},
  annotation = {https://osf.io/w5vhp/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hagendorff et al_2023_Human-like intuitive behavior and reasoning biases emerged in large language.pdf}
}

@misc{hagendorffMachinePsychologyInvestigating2023,
  title = {Machine {{Psychology}}: {{Investigating Emergent Capabilities}} and {{Behavior}} in {{Large Language Models Using Psychological Methods}}},
  shorttitle = {Machine {{Psychology}}},
  author = {Hagendorff, Thilo},
  year = {2023},
  month = jul,
  number = {arXiv:2303.13988},
  eprint = {2303.13988},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2303.13988},
  urldate = {2023-10-17},
  abstract = {Large language models (LLMs) are currently at the forefront of intertwining AI systems with human communication and everyday life. Due to rapid technological advances and their extreme versatility, LLMs nowadays have millions of users and are at the cusp of being the main go-to technology for information retrieval, content generation, problem-solving, etc. Therefore, it is of great importance to thoroughly assess and scrutinize their capabilities. Due to increasingly complex and novel behavioral patterns in current LLMs, this can be done by treating them as participants in psychology experiments that were originally designed to test humans. For this purpose, the paper introduces a new field of research called "machine psychology". The paper outlines how different subfields of psychology can inform behavioral tests for LLMs. It defines methodological standards for machine psychology research, especially by focusing on policies for prompt designs. Additionally, it describes how behavioral patterns discovered in LLMs are to be interpreted. In sum, machine psychology aims to discover emergent abilities in LLMs that cannot be detected by most traditional natural language processing benchmarks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hagendorff_2023_Machine Psychology.pdf;/Users/thomasgorman/Zotero/storage/CLTQAFYN/2303.html}
}

@article{hahnResourcerationalModelHuman2022,
  title = {A Resource-Rational Model of Human Processing of Recursive Linguistic Structure},
  author = {Hahn, Michael and Futrell, Richard and Levy, Roger and Gibson, Edward},
  year = {2022},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {119},
  number = {43},
  pages = {e2122602119},
  doi = {10.1073/pnas.2122602119},
  urldate = {2023-08-11},
  abstract = {A major goal of psycholinguistic theory is to account for the cognitive constraints limiting the speed and ease of language comprehension and production. Wide-ranging evidence demonstrates a key role for linguistic expectations: A word's predictability, as measured by the information-theoretic quantity of surprisal, is a major determinant of processing difficulty. But surprisal, under standard theories, fails to predict the difficulty profile of an important class of linguistic patterns: the nested hierarchical structures made possible by recursion in human language. These nested structures are better accounted for by psycholinguistic theories of constrained working memory capacity. However, progress on theory unifying expectation-based and memory-based accounts has been limited. Here we present a unified theory of a rational trade-off between precision of memory representations with ease of prediction, a scaled-up computational implementation using contemporary machine learning methods, and experimental evidence in support of the theory's distinctive predictions. We show that the theory makes nuanced and distinctive predictions for difficulty patterns in nested recursive structures predicted by neither expectation-based nor memory-based theories alone. These predictions are confirmed 1) in two language comprehension experiments in English, and 2) in sentence completions in English, Spanish, and German. More generally, our framework offers computationally explicit theory and methods for understanding how memory constraints and prediction interact in human language comprehension and production.},
  annotation = {https://gitlab.com/m-hahn/resource-rational-surprisal/-/tree/main?ref\_type=heads\\
\\
https://zenodo.org/record/6988696},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hahn et al_2022_A resource-rational model of human processing of recursive linguistic structure.pdf}
}

@misc{hallerLanguageModelsEmulate2024,
  title = {Language Models Emulate Certain Cognitive Profiles: {{An}} Investigation of How Predictability Measures Interact with Individual Differences},
  shorttitle = {Language Models Emulate Certain Cognitive Profiles},
  author = {Haller, Patrick and Bolliger, Lena S. and J{\"a}ger, Lena A.},
  year = {2024},
  month = jun,
  number = {arXiv:2406.04988},
  eprint = {2406.04988},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.04988},
  urldate = {2024-06-18},
  abstract = {To date, most investigations on surprisal and entropy effects in reading have been conducted on the group level, disregarding individual differences. In this work, we revisit the predictive power of surprisal and entropy measures estimated from a range of language models (LMs) on data of human reading times as a measure of processing effort by incorporating information of language users' cognitive capacities. To do so, we assess the predictive power of surprisal and entropy estimated from generative LMs on reading data obtained from individuals who also completed a wide range of psychometric tests. Specifically, we investigate if modulating surprisal and entropy relative to cognitive scores increases prediction accuracy of reading times, and we examine whether LMs exhibit systematic biases in the prediction of reading times for cognitively high- or low-performing groups, revealing what type of psycholinguistic subject a given LM emulates. Our study finds that in most cases, incorporating cognitive capacities increases predictive power of surprisal and entropy on reading times, and that generally, high performance in the psychometric tests is associated with lower sensitivity to predictability effects. Finally, our results suggest that the analyzed LMs emulate readers with lower verbal intelligence, suggesting that for a given target group (i.e., individuals with high verbal intelligence), these LMs provide less accurate predictability estimates.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/DiLi-Lab/LM-cog-profiles},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Haller et al_2024_Language models emulate certain cognitive profiles.pdf;/Users/thomasgorman/Zotero/storage/H288BMZC/2406.html}
}

@article{hanInductiveReasoningHumans2024,
  title = {Inductive Reasoning in Humans and Large Language Models},
  author = {Han, Simon Jerome and Ransom, Keith J. and Perfors, Andrew and Kemp, Charles},
  year = {2024},
  month = jan,
  journal = {Cognitive Systems Research},
  volume = {83},
  pages = {101155},
  issn = {1389-0417},
  doi = {10.1016/j.cogsys.2023.101155},
  urldate = {2024-05-26},
  abstract = {The impressive recent performance of large language models has led many to wonder to what extent they can serve as models of general intelligence or are similar to human cognition. We address this issue by applying GPT-3.5 and GPT-4 to a classic problem in human inductive reasoning known as property induction. Over two experiments, we elicit human judgments on a range of property induction tasks spanning multiple domains. Although GPT-3.5 struggles to capture many aspects of human behavior, GPT-4 is much more successful: for the most part, its performance qualitatively matches that of humans, and the only notable exception is its failure to capture the phenomenon of premise non-monotonicity. Our work demonstrates that property induction allows for interesting comparisons between human and machine intelligence and provides two large datasets that can serve as benchmarks for future work in this vein.},
  keywords = {AI,Category-based induction,GPT-3.5,GPT-4,Large language models,Neural networks,Non-monotonicity,Property induction,Reasoning,Representation},
  annotation = {https://github.com/S-J-HAN/InductiveReasoningInLargeLanguageModels},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Han et al_2024_Inductive reasoning in humans and large language models.pdf;/Users/thomasgorman/Zotero/storage/YW23XGRV/S1389041723000839.html}
}

@misc{harel-canadaMeasuringPsychologicalDepth2024,
  title = {Measuring {{Psychological Depth}} in {{Language Models}}},
  author = {{Harel-Canada}, Fabrice and Zhou, Hanyu and Mupalla, Sreya and Yildiz, Zeynep and Sahai, Amit and Peng, Nanyun},
  year = {2024},
  month = jun,
  journal = {arXiv e-prints},
  doi = {10.48550/arXiv.2406.12680},
  urldate = {2024-07-14},
  abstract = {Evaluations of creative stories generated by large language models (LLMs) often focus on objective properties of the text, such as its style, coherence, and toxicity. While these metrics are indispensable, they do not speak to a story's subjective, psychological impact from a reader's perspective. We introduce the Psychological Depth Scale (PDS), a novel framework rooted in literary theory that measures an LLM's ability to produce authentic and narratively complex stories that provoke emotion, empathy, and engagement. We empirically validate our framework by showing that humans can consistently evaluate stories based on PDS (0.72 Krippendorff's alpha). We also explore techniques for automating the PDS to easily scale future analyses. GPT-4o, combined with a novel Mixture-of-Personas (MoP) prompting strategy, achieves an average Spearman correlation of \$0.51\$ with human judgment while Llama-3-70B scores as high as 0.68 for empathy. Finally, we compared the depth of stories authored by both humans and LLMs. Surprisingly, GPT-4 stories either surpassed or were statistically indistinguishable from highly-rated human-written stories sourced from Reddit. By shifting the focus from text to reader, the Psychological Depth Scale is a validated, automated, and systematic means of measuring the capacity of LLMs to connect with humans through the stories they tell.},
  keywords = {Computer Science - Computation and Language},
  annotation = {ADS Bibcode: 2024arXiv240612680H},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Harel-Canada et al_2024_Measuring Psychological Depth in Language Models.pdf}
}

@article{hasanBoostingWisdomCrowd2024,
  title = {Boosting Wisdom of the Crowd for Medical Image Annotation Using Training Performance and Task Features},
  author = {Hasan, Eeshan and Duhaime, Erik and Trueblood, Jennifer S.},
  year = {2024},
  month = may,
  journal = {Cognitive Research: Principles and Implications},
  volume = {9},
  number = {1},
  pages = {31},
  issn = {2365-7464},
  doi = {10.1186/s41235-024-00558-6},
  urldate = {2024-06-13},
  abstract = {A crucial bottleneck in medical artificial intelligence (AI) is high-quality labeled medical datasets. In this paper, we test a large variety of wisdom of the crowd algorithms to label medical images that were initially classified by individuals recruited through an app-based platform. Individuals classified skin lesions from the International Skin Lesion Challenge 2018 into 7 different categories. There was a large dispersion in the geographical location, experience, training, and performance of the recruited individuals. We tested several wisdom of the crowd algorithms of varying complexity from a simple unweighted average to more complex Bayesian models that account for individual patterns of errors. Using a switchboard analysis, we observe that the best-performing algorithms rely on selecting top performers, weighting decisions by training accuracy, and take into account the task environment. These algorithms far exceed expert performance. We conclude by discussing the implications of these approaches for the development of medical AI.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hasan et al_2024_Boosting wisdom of the crowd for medical image annotation using training.pdf}
}

@article{hasanHarnessingWisdomConfident2024,
  title = {Harnessing the Wisdom of the Confident Crowd in Medical Image Decision-Making},
  author = {Hasan, Eeshan and Eichbaum, Quentin and Seegmiller, Adam C. and Stratton, Charles and Trueblood, Jennifer S.},
  year = {2024},
  month = jan,
  journal = {Decision},
  series = {Judgment and {{Decision Research}} on the {{Wisdom}} of {{Crowds}}},
  volume = {11},
  number = {1},
  pages = {127--149},
  issn = {2325-9965},
  doi = {10.1037/dec0000210},
  urldate = {2024-06-13},
  abstract = {Improving the accuracy of medical image interpretation is critical to improving the diagnosis of many diseases. Using both novices (undergraduates) and experts (medical professionals), we investigated methods for improving the accuracy of a single decision maker and a group of decision makers by aggregating repeated decisions in different ways. Participants made classification decisions (cancerous vs. noncancerous) and confidence judgments on a series of cell images, viewing and classifying each image twice. We first examined whether it is possible to improve individual-level performance by using the maximum confidence slating (MCS) algorithm (Koriat, 2012b), which leverages metacognitive ability by using the most confident response for an image as the 'final response.' We find MCS improves individual classification accuracy for both novices and experts. Building on these results, we show that aggregation algorithms based on confidence weighting scale to larger groups of participants, dramatically improving diagnostic accuracy, with the performance of groups of novices reaching that of individual experts. In sum, we find that repeated decision-making and confidence weighting can be a valuable way to improve accuracy in medical image decision-making and that these techniques can be used in conjunction with each other. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
  keywords = {accuracy,College Students,confidence,decision making,Decision Making,Experience Level,experts,group decision makers,Medical Personnel,medical professionals,metacognition,Metacognition,novices,Pathology,pathology image interpretation,Self-Confidence,single decision maker,undergraduates,Wisdom,wisdom of the crowds},
  annotation = {https://osf.io/ckvxz/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hasan et al_2024_Harnessing the wisdom of the confident crowd in medical image decision-making.pdf}
}

@article{hasanImprovingMedicalImage2021,
  title = {Improving {{Medical Image Decision Making}} by {{Leveraging Metacognitive Processes}} and {{Representational Similarity}}},
  author = {Hasan, Eeshan and Trueblood, Jennifer and Eichbaum, Quentin and Seegmiller, Adam and Stratton, Charles William},
  year = {2021},
  journal = {Proceedings of the Annual Meeting of the Cognitive Science Society},
  volume = {43},
  number = {43},
  urldate = {2021-10-13},
  abstract = {Improving the accuracy of medical image interpretation is critical to improving the diagnosis of many diseases. Using both novices (undergraduates) and experts (medical professionals), we investigate methods for improving the accuracy of a single decision maker by aggregating repeated decisions from an individual in different ways. Our participants made classification decisions (cancerous versus non-cancerous) and confidence judgments on a series of cell images, viewing and classifying each image twice. We first applied the maximum confidence slating algorithm (Koriat, 2012), which leverages metacognitive ability by using the most confident response for an image as the `final response'. We also examined algorithms that aggregated decisions based on image similarity, leveraging neural network models to determine similarity. We found maximum confidence slating improves classification accuracy for both novices and experts. However, aggregating responses on similar images improves classification accuracy for novices and not experts, suggesting differences in the decision mechanisms of novices and experts.},
  langid = {english},
  annotation = {https://osf.io/3rmzy/},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Hasan et al_2021_Improving Medical Image Decision Making by Leveraging Metacognitive Processes.pdf;/Users/thomasgorman/Zotero/storage/FXG62JAL/7hk6r212.html}
}

@article{hasanImprovingMedicalImage2022,
  title = {Improving {{Medical Image Decision-Making}} by {{Leveraging Metacognitive Processes}} and {{Representational Similarity}}},
  author = {Hasan, Eeshan and Eichbaum, Quentin and Seegmiller, Adam C. and Stratton, Charles and Trueblood, Jennifer S.},
  year = {2022},
  journal = {Topics in Cognitive Science},
  volume = {14},
  number = {2},
  pages = {400--413},
  issn = {1756-8765},
  doi = {10.1111/tops.12588},
  urldate = {2024-06-13},
  abstract = {Improving the accuracy of medical image interpretation can improve the diagnosis of numerous diseases. We compared different approaches to aggregating repeated decisions about medical images to improve the accuracy of a single decision maker. We tested our algorithms on data from both novices (undergraduates) and experts (medical professionals). Participants viewed images of white blood cells and made decisions about whether the cells were cancerous or not. Each image was shown twice to the participants and their corresponding confidence judgments were collected. The maximum confidence slating (MCS) algorithm leverages metacognitive abilities to consider the more confident response in the pair of responses as the more accurate ``final response'' (Koriat, 2012), and it has previously been shown to improve accuracy on our task for both novices and experts (Hasan et al., 2021). We compared MCS to similarity-based aggregation (SBA) algorithms where the responses made by the same participant on similar images are pooled together to generate the ``final response.'' We determined similarity by using two different neural networks where one of the networks had been trained on white blood cells and the other had not. We show that SBA improves performance for novices even when the neural network had no specific training on white blood cell images. Using an informative representation (i.e., network trained on white blood cells) allowed one to aggregate over more neighbors and further boosted the performance of novices. However, SBA failed to improve the performance for experts even with the informative representation. This difference in efficacy of the SBA suggests different decision mechanisms for novices and experts.},
  langid = {english},
  keywords = {Computational modeling,Expertise,Medical image decision-making,Metacognition,Neural networks,Representation,Wisdom of the crowds},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hasan et al_2022_Improving Medical Image Decision-Making by Leveraging Metacognitive Processes2.pdf;/Users/thomasgorman/Zotero/storage/I9EYYPYB/tops.html}
}

@article{hasanRepresentationalSmoothingImprove,
  title = {Representational {{Smoothing}} to {{Improve Medical Image Decision Making}}},
  author = {Hasan, Eeshan and Trueblood, Jennifer S},
  abstract = {We demonstrate how medical-image classification decisions can be denoised by aggregating decisions on similar images. In our algorithm, the final decision on a target image is can- cerous if a percentage t of the k most similar images are can- cerous, else it is not cancerous. Similarity between images is calculated as the distance between representations from an artificial neural network. We vary k and t for novice and ex- pert participants using data from Trueblood et al. (2018) and Trueblood et al. (2021). We show that increasing k improves performance for novices, with their performance approaching that of experts. We also show that the algorithm is biased to- wards identifying cancerous cells, which is reflected in the rep- resentational space. The percentage t allows greater control over sensitivity and specificity and can be used to debias de- cisions. This algorithm is less effective for experts, partially explained by them giving similar responses on similar images. Keywords: Medical Image Decision Making; Computa- tional Modeling; Neural Networks; Representation; Con- cepts and Categories},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hasan_Trueblood_Representational Smoothing to Improve Medical Image Decision Making.pdf}
}

@article{haseUnreasonableEffectivenessEasy,
  title = {The {{Unreasonable Effectiveness}} of {{Easy Training Data}} for {{Hard Tasks}}},
  author = {Hase, Peter and Bansal, Mohit and Clark, Peter and Wiegreffe, Sarah},
  langid = {english},
  annotation = {https://github.com/allenai/easy-to-hard-generalization},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hase et al_The Unreasonable Effectiveness of Easy Training Data for Hard Tasks.pdf}
}

@article{hawkinsVisualResemblanceInteraction2023,
  title = {Visual Resemblance and Interaction History Jointly Constrain Pictorial Meaning},
  author = {Hawkins, Robert D. and Sano, Megumi and Goodman, Noah D. and Fan, Judith E.},
  year = {2023},
  month = apr,
  journal = {Nature Communications},
  volume = {14},
  number = {1},
  pages = {2199},
  issn = {2041-1723},
  doi = {10.1038/s41467-023-37737-w},
  urldate = {2023-06-01},
  abstract = {How do drawings---ranging from detailed illustrations to schematic diagrams---reliably convey meaning? Do viewers understand drawings based on how strongly they resemble an entity (i.e., as images) or based on socially mediated conventions (i.e., as symbols)? Here we evaluate a cognitive account of pictorial meaning in which visual and social information jointly support visual communication. Pairs of participants used drawings to repeatedly communicate the identity of a target object among multiple distractor objects. We manipulated social cues across three experiments and a full replication, finding that participants developed object-specific and interaction-specific strategies for communicating more efficiently over time, beyond what task practice or a resemblance-based account alone could explain. Leveraging model-based image analyses and crowdsourced annotations, we further determined that drawings did not drift toward ``arbitrariness,'' as predicted by a pure convention-based account, but preserved visually diagnostic features. Taken together, these findings advance psychological theories of how successful graphical conventions emerge.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Social behaviour},
  annotation = {https://github.com/hawkrobe/graphical\_conventions\\
\\
https://github.com/cogtoolslab/graphical\_conventions\_cogsci19},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hawkins et al_2023_Visual resemblance and interaction history jointly constrain pictorial meaning2.pdf}
}

@misc{hayesRelativeValueBiases2024,
  title = {Relative {{Value Biases}} in {{Large Language Models}}},
  author = {Hayes, William M. and Yax, Nicolas and Palminteri, Stefano},
  year = {2024},
  month = jan,
  number = {arXiv:2401.14530},
  eprint = {2401.14530},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-03},
  abstract = {Studies of reinforcement learning in humans and animals have demonstrated a preference for options that yielded relatively better outcomes in the past, even when those options are associated with lower absolute reward. The present study tested whether large language models would exhibit a similar bias. We had gpt-4-1106-preview (GPT-4 Turbo) and Llama-2-70B make repeated choices between pairs of options with the goal of maximizing payoffs. A complete record of previous outcomes was included in each prompt. Both models exhibited relative value decision biases similar to those observed in humans and animals. Making relative comparisons among outcomes more explicit magnified the bias, whereas prompting the models to estimate expected outcomes caused the bias to disappear. These results have implications for the potential mechanisms that contribute to context-dependent choice in human agents.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hayes et al_2024_Relative Value Biases in Large Language Models.pdf;/Users/thomasgorman/Zotero/storage/MSW322YJ/2401.html}
}

@inproceedings{heAIFutureCollaborative2024,
  title = {{{AI}} and the {{Future}} of {{Collaborative Work}}: {{Group Ideation}} with an {{LLM}} in a {{Virtual Canvas}}},
  shorttitle = {{{AI}} and the {{Future}} of {{Collaborative Work}}},
  booktitle = {Proceedings of the 3rd {{Annual Meeting}} of the {{Symposium}} on {{Human-Computer Interaction}} for {{Work}}},
  author = {He, Jessica and Houde, Stephanie and Gonzalez, Gabriel E. and Silva Moran, Dar{\'i}o Andr{\'e}s and Ross, Steven I. and Muller, Michael and Weisz, Justin D.},
  year = {2024},
  month = jun,
  pages = {1--14},
  publisher = {ACM},
  address = {Newcastle upon Tyne United Kingdom},
  doi = {10.1145/3663384.3663398},
  urldate = {2024-09-23},
  isbn = {9798400710179},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/He et al_2024_AI and the Future of Collaborative Work.pdf}
}

@article{heitmeierHowTrialtrialLearning2023,
  title = {How Trial-to-Trial Learning Shapes Mappings in the Mental Lexicon: {{Modelling}} Lexical Decision with Linear Discriminative Learning},
  shorttitle = {How Trial-to-Trial Learning Shapes Mappings in the Mental Lexicon},
  author = {Heitmeier, Maria and Chuang, Yu-Ying and Baayen, R. Harald},
  year = {2023},
  month = nov,
  journal = {Cognitive Psychology},
  volume = {146},
  pages = {101598},
  issn = {0010-0285},
  doi = {10.1016/j.cogpsych.2023.101598},
  urldate = {2024-05-07},
  abstract = {Trial-to-trial effects have been found in a number of studies, indicating that processing a stimulus influences responses in subsequent trials. A special case are priming effects which have been modelled successfully with error-driven learning (Marsolek, 2008), implying that participants are continuously learning during experiments. This study investigates whether trial-to-trial learning can be detected in an unprimed lexical decision experiment. We used the Discriminative Lexicon Model (DLM; Baayen et al., 2019), a model of the mental lexicon with meaning representations from distributional semantics, which models error-driven incremental learning with the Widrow-Hoff rule. We used data from the British Lexicon Project (BLP; Keuleers et al., 2012) and simulated the lexical decision experiment with the DLM on a trial-by-trial basis for each subject individually. Then, reaction times were predicted with Generalized Additive Models (GAMs), using measures derived from the DLM simulations as predictors. We extracted measures from two simulations per subject (one with learning updates between trials and one without), and used them as input to two GAMs. Learning-based models showed better model fit than the non-learning ones for the majority of subjects. Our measures also provide insights into lexical processing and individual differences. This demonstrates the potential of the DLM to model behavioural data and leads to the conclusion that trial-to-trial learning can indeed be detected in unprimed lexical decision. Our results support the possibility that our lexical knowledge is subject to continuous changes.},
  keywords = {Distributional semantics,Individual differences,Lexical decision,Linear discriminative learning,Mental lexicon,Trial-to-trial learning},
  annotation = {https://osf.io/bxmt2/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Heitmeier et al_2023_How trial-to-trial learning shapes mappings in the mental lexicon2.pdf;/Users/thomasgorman/Zotero/storage/9KR75BDB/S0010028523000567.html}
}

@article{heLaneKeepingCognitive2014,
  title = {Lane {{Keeping Under Cognitive Load}}: {{Performance Changes}} and {{Mechanisms}}},
  shorttitle = {Lane {{Keeping Under Cognitive Load}}},
  author = {He, Jibo and McCarley, Jason S. and Kramer, Arthur F.},
  year = {2014},
  month = mar,
  journal = {Human Factors},
  volume = {56},
  number = {2},
  pages = {414--426},
  publisher = {SAGE Publications Inc},
  issn = {0018-7208},
  doi = {10.1177/0018720813485978},
  urldate = {2024-09-16},
  abstract = {Objective:A pair of simulated driving experiments studied the effects of cognitive load on drivers' lane-keeping performance.Background:Cognitive load while driving often reduces the variability of lane position. However, there is no agreement as to whether this effect should be interpreted as a performance loss, consistent with other effects of distraction on driving, or as an anomalous performance gain.Method:Participants in a high-fidelity driving simulator performed a lane-keeping task in lateral wind, with instructions to keep a steady lane position. Under high load conditions, participants performed a concurrent working memory task with auditory stimuli. Cross-spectral analysis measured the relationship between wind force and steering inputs.Results:Cognitive load reduced the variability of lane position and increased the coupling between steering wheel position and crosswind strength.Conclusion:Although cognitive load disrupts driver performance in a variety of ways, it produces a performance gain in lane keeping. This effect appears to reflect drivers' efforts to protect lateral control against the risk of distraction, at the apparent neglect of other elements of driving performance.Application:Results may inform educational efforts to help drivers understand the risks of distraction and the inadequacies of compensatory driving strategies.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/He et al_2014_Lane Keeping Under Cognitive Load.pdf}
}

@misc{heMedEvalMultiLevelMultiTask2023,
  title = {{{MedEval}}: {{A Multi-Level}}, {{Multi-Task}}, and {{Multi-Domain Medical Benchmark}} for {{Language Model Evaluation}}},
  shorttitle = {{{MedEval}}},
  author = {He, Zexue and Wang, Yu and Yan, An and Liu, Yao and Chang, Eric Y. and Gentili, Amilcare and McAuley, Julian and Hsu, Chun-Nan},
  year = {2023},
  month = nov,
  number = {arXiv:2310.14088},
  eprint = {2310.14088},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-11},
  abstract = {Curated datasets for healthcare are often limited due to the need of human annotations from experts. In this paper, we present MEDEVAL, a multi-level, multi-task, and multi-domain medical benchmark to facilitate the development of language models for healthcare. MEDEVAL is comprehensive and consists of data from several healthcare systems and spans 35 human body regions from 8 examination modalities. With 22,779 collected sentences and 21,228 reports, we provide expert annotations at multiple levels, offering a granular potential usage of the data and supporting a wide range of tasks. Moreover, we systematically evaluated 10 generic and domain-specific language models under zero-shot and finetuning settings, from domain-adapted baselines in healthcare to general-purposed state-of-the-art large language models (e.g., ChatGPT). Our evaluations reveal varying effectiveness of the two categories of language models across different tasks, from which we notice the importance of instruction tuning for few-shot usage of large language models. Our investigation paves the way toward benchmarking language models for healthcare and provides valuable insights into the strengths and limitations of adopting large language models in medical domains, informing their practical applications and future advancements1.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/He et al_2023_MedEval.pdf}
}

@article{hertwigFluencyHeuristicModel2008,
  title = {Fluency {{Heuristic}}: {{A Model}} of {{How}} the {{Mind Exploits}} a {{By-Product}} of {{Information Retrieval}}},
  author = {Hertwig, Ralph and Herzog, Stefan M and Schooler, Lael J and Reimer, Torsten},
  year = {2008},
  journal = {Journal of Experimental Psychology: Learning, memory, and cognition},
  volume = {34},
  number = {5},
  abstract = {Boundedly rational heuristics for inference can be surprisingly accurate and frugal for several reasons. They can exploit environmental structures, co-opt complex capacities, and elude effortful search by exploiting information that automatically arrives on the mental stage. The fluency heuristic is a prime example of a heuristic that makes the most of an automatic by-product of retrieval from memory, namely, retrieval fluency. In 4 experiments, the authors show that retrieval fluency can be a proxy for real-world quantities, that people can discriminate between two objects' retrieval fluencies, and that people's inferences are in line with the fluency heuristic (in particular fast inferences) and with experimentally manipulated fluency. The authors conclude that the fluency heuristic may be one tool in the mind's repertoire of strategies that artfully probes memory for encapsulated frequency information that can veridically reflect statistical regularities in the world.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hertwig et al_Fluency Heuristic.pdf}
}

@article{hewittPredictingResultsSocial,
  title = {Predicting {{Results}} of {{Social Science Experiments Using Large Language Models}}},
  author = {Hewitt, Luke and Ashokkumar, Ashwini and Ghezae, Isaias and Willer, Robb},
  abstract = {To evaluate whether large language models (LLMs) can be leveraged to predict the results of social science experiments, we built an archive of 70 pre-registered, nationally representative, survey experiments conducted in the United States, involving 476 experimental treatment effects and 105,165 participants. We prompted an advanced, publicly-available LLM (GPT-4) to simulate how representative samples of Americans would respond to the stimuli from these experiments. Predictions derived from simulated responses correlate strikingly with actual treatment effects (r = 0.85), equaling or surpassing the predictive accuracy of human forecasters. Accuracy remained high for unpublished studies that could not appear in the model's training data (r = 0.90). We further assessed predictive accuracy across demographic subgroups, various disciplines, and in nine recent megastudies featuring an additional 346 treatment effects. Together, our results suggest LLMs can augment experimental methods in science and practice, but also highlight important limitations and risks of misuse.},
  langid = {english},
  annotation = {https://www.treatmenteffect.app/\\
\\
https://docsend.com/view/dmak8vsz4d2k6743},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hewitt et al_Predicting Results of Social Science Experiments Using Large Language Models.pdf}
}

@article{heymanImpactChatGPTHuman2023,
  title = {The Impact of {{ChatGPT}} on Human Data Collection: {{A}} Case Study Involving Typicality Norming Data},
  shorttitle = {The Impact of {{ChatGPT}} on Human Data Collection},
  author = {Heyman, Tom and Heyman, Geert},
  year = {2023},
  month = oct,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02235-w},
  urldate = {2023-12-31},
  abstract = {Tools like ChatGPT, which allow people to unlock the potential of large language models (LLMs), have taken the world by storm. ChatGPT's ability to produce written output of remarkable quality has inspired, or forced, academics to consider its consequences for both research and education. In particular, the question of what constitutes authorship, and how to evaluate (scientific) contributions has received a lot of attention. However, its impact on (online) human data collection has mostly flown under the radar. The current paper examines how ChatGPT can be (mis)used in the context of generating norming data. We found that ChatGPT is able to produce sensible output, resembling that of human participants, for a typicality rating task. Moreover, the test--retest reliability of ChatGPT's ratings was similar to that of human participants tested 1~day apart. We discuss the relevance of these findings in the context of (online) human data collection, focusing both on opportunities (e.g., (risk-)free pilot data) and challenges (e.g., data fabrication).},
  langid = {english},
  keywords = {ChatGPT,Human data collection,Large language models,Typicality},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Heyman_Heyman_2023_The impact of ChatGPT on human data collection.pdf}
}

@article{hillsMindNetworkMaps,
  title = {Is the {{Mind}} a {{Network}}? {{Maps}}, {{Vehicles}}, and {{Skyhooks}} in {{Cognitive Network Science}}},
  shorttitle = {Is the {{Mind}} a {{Network}}?},
  author = {Hills, Thomas T. and Kenett, Yoed N.},
  journal = {Topics in Cognitive Science},
  volume = {n/a},
  number = {n/a},
  issn = {1756-8765},
  doi = {10.1111/tops.12570},
  urldate = {2021-08-31},
  abstract = {Cognitive researchers often carve cognition up into structures and processes. Cognitive processes operate on structures, like vehicles driving over a map. Language alongside semantic and episodic memory are proposed to have structure, as are perceptual systems. Over these structures, processes operate to construct memory and solve problems by retrieving and manipulating information. Network science offers an approach to representing cognitive structures and has made tremendous inroads into understanding the nature of cognitive structure and process. But is the mind a network? If so, what kind? In this article, we briefly review the main metaphors, assumptions, and pitfalls prevalent in cognitive network science (maps and vehicles; one network/process to rule them all), highlight the need for new metaphors that elaborate on the map-and-vehicle framework (wormholes, skyhooks, and generators), and present open questions in studying the mind as a network (the challenge of capturing network change, what should the edges of cognitive networks be made of, and aggregated vs. individual-based networks). One critical lesson of this exercise is that the richness of the mind as network approach makes it a powerful tool in its own right; it has helped to make our assumptions more visible, generating new and fascinating questions, and enriching the prospects for future research. A second lesson is that the mind as a network--though useful--is incomplete. The mind is not a network, but it may contain them.},
  langid = {english},
  keywords = {Cognition,Cognitive networks,Language,Memory,Process,Representation},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Hills_Kenett_Is the Mind a Network.pdf;/Users/thomasgorman/Zotero/storage/ZEXBPFBP/tops.html}
}

@article{hoffrageModelsBoundedRationality2004,
  title = {Models of {{Bounded Rationality}}: {{The Approach}} of {{Fast}} and {{Frugal Heuristics}}},
  shorttitle = {Models of {{Bounded Rationality}}},
  author = {Hoffrage, Ulrich and Reimer, Torsten},
  year = {2004},
  journal = {management revu},
  volume = {15},
  number = {4},
  pages = {437--459},
  issn = {0935-9915},
  doi = {10.5771/0935-9915-2004-4-437},
  urldate = {2024-06-22},
  abstract = {In a complex and uncertain world, humans draw inferences and make decisions under the constraints of limited knowledge, resources, and time. Herbert Simon, with his call for models of bounded rationality, can be seen as one of the fathers of the recently initiated research program on ``simple heuristics that make us smart'' (Gigerenzer/Todd/the ABC Research Group, 1999). These heuristics perform well because they are ecologically rational: they explore the structure of environmental information and are adapted to this structure. The present review paper introduces the key concepts of this research tradition, and provides two examples: (1) The recognition heuristic, which exploits a partial lack of knowledge, and (2) Take The Best, a simple lexicographic strategy that deliberately ignores information although it is available. The paper explains their ecological rationality, provides empirical evidence of their use, and illustrates some of their applications in consumer behaviour and group decision making. Finally, this research program is related to various notions of rationality.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hoffrage_Reimer_2004_Models of Bounded Rationality.pdf}
}

@article{holmesJointDeepNeural2020,
  title = {A {{Joint Deep Neural Network}} and {{Evidence Accumulation Modeling Approach}} to {{Human Decision-Making}} with {{Naturalistic Images}}},
  author = {Holmes, William R. and O'Daniels, Payton and Trueblood, Jennifer S.},
  year = {2020},
  month = mar,
  journal = {Computational Brain \& Behavior},
  volume = {3},
  number = {1},
  pages = {1--12},
  issn = {2522-087X},
  doi = {10.1007/s42113-019-00042-1},
  urldate = {2021-10-13},
  abstract = {Evidence accumulation models (EAM) have proven to be an invaluable tool in probing the dynamical properties of decisions over recent decades. However, much of this literature has studied decisions utilizing simple stimuli where the experimenter has perfect knowledge and control over stimulus properties. Here, we develop and test a new method for studying decisions involving naturalistic stimuli (medical images in this case) where the experimenter has neither perfect knowledge nor control of the stimuli properties. The central challenge in studying such decisions is to extract useful representations of images that can be associated with accumulation or drift rates in EAMs. Here, we couple a deep convolutional neural network (CNN) with the diffusion decision model (DDM) to study how expert pathologists and novices make decisions involving the classification of digital images of blood cells as either normal (non-blast) or cancerous (blast). In our approach, the CNN is the basis of a function that translates each image into a drift rate for use in the DDM. Results of fitting the joint CNN-DDM model to choice and response time data demonstrates that (1) both novices and experts demonstrated substantial speed accuracy tradeoffs, (2) both were susceptible to biases introduced by the presentation of pre-stimulus probabilistic cues, and (3) experts were more adept at extracting useful information from images than novices. These results demonstrate that this is a fruitful approach to studying decisions involving complex stimuli that will open new avenues for studying questions not possible with existing methods. Furthermore, this approach is technically feasible and has the potential to be translated into other domains of decision-making research.},
  langid = {english},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Holmes et al_2020_A Joint Deep Neural Network and Evidence Accumulation Modeling Approach to.pdf}
}

@misc{hongAbstractionThoughtMakesLanguage2024,
  title = {Abstraction-of-{{Thought Makes Language Models Better Reasoners}}},
  author = {Hong, Ruixin and Zhang, Hongming and Pan, Xiaoman and Yu, Dong and Zhang, Changshui},
  year = {2024},
  month = jun,
  number = {arXiv:2406.12442},
  eprint = {2406.12442},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.12442},
  urldate = {2024-07-01},
  abstract = {Abstract reasoning, the ability to reason from the abstract essence of a problem, serves as a key to generalization in human reasoning. However, eliciting language models to perform reasoning with abstraction remains unexplored. This paper seeks to bridge this gap by introducing a novel structured reasoning format called Abstraction-of-Thought (AoT). The uniqueness of AoT lies in its explicit requirement for varying levels of abstraction within the reasoning process. This approach could elicit language models to first contemplate on the abstract level before incorporating concrete details, which is overlooked by the prevailing step-by-step Chain-of-Thought (CoT) method. To align models with the AoT format, we present AoT Collection, a generic finetuning dataset consisting of 348k high-quality samples with AoT reasoning processes, collected via an automated and scalable pipeline. We finetune a wide range of language models with AoT Collection and conduct extensive evaluations on 23 unseen tasks from the challenging benchmark Big-Bench Hard. Experimental results indicate that models aligned to AoT reasoning format substantially outperform those aligned to CoT in many reasoning tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hong et al_2024_Abstraction-of-Thought Makes Language Models Better Reasoners.pdf;/Users/thomasgorman/Zotero/storage/3DE5UIYP/2406.html}
}

@article{howellTestTaskInfluences1982,
  title = {A Test of Task Influences in Uncertainty Measurement},
  author = {Howell, William C. and Kerkar, Shanta P.},
  year = {1982},
  month = dec,
  journal = {Organizational Behavior and Human Performance},
  volume = {30},
  number = {3},
  pages = {365--390},
  issn = {0030-5073},
  doi = {10.1016/0030-5073(82)90226-4},
  urldate = {2020-08-30},
  abstract = {Three experiments were carried out to test the hypothesized effect of response mode and event-type factors on measured uncertainty for frequentistic events observed within a realistic task setting. Subjects served as emergency vehicle dispatchers for a hypothetical city and gained experience with events (emergency calls) generated by a stationary stochastic process over a number of sessions. Experiment I compared two forms of judgment (frequency and probability estimation) over eight different kinds of observed events. Subsequently it compared subjects' predictive choice performance on selected event pairs with and without the benefit of prior estimation requirement. The results demonstrated a superiority of frequency over probability judgment and a reliable event-type influence on the quality of estimations produced. Moreover, prior judgment (frequency or probability estimation) seemed to facilitate predictive choice performance. Experiment II sought to clarify the relation between judgment and choice by manipulating the quality of prior estimations via combinations of instructional set and response requirement. Despite the differences in quality of frequency and probability judgments, choices were not directly affected by this variable. Rather, estimations served to cue frequentistic information in storage that was accumulated similarly by all experimental groups. This cuing hypothesis was tested further in Experiment III by comparing an estimation group with a directly cued group on predictive choice accuracy. The similarity of performance by the two groups was considered evidence in support of this cuing hypothesis.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Howell_Kerkar_1982_A test of task influences in uncertainty measurement.pdf;/Users/thomasgorman/Zotero/storage/XX7W7ILL/0030507382902264.html}
}

@article{huangEnablingRobotsCommunicate2019,
  title = {Enabling Robots to Communicate Their Objectives},
  author = {Huang, Sandy H. and Held, David and Abbeel, Pieter and Dragan, Anca D.},
  year = {2019},
  month = feb,
  journal = {Autonomous Robots},
  volume = {43},
  number = {2},
  pages = {309--326},
  issn = {1573-7527},
  doi = {10.1007/s10514-018-9771-0},
  urldate = {2024-09-16},
  abstract = {The overarching goal of this work is to efficiently enable end-users to correctly anticipate a robot's behavior in novel situations. And since a robot's behavior is often a direct result of its underlying objective function, our insight is that end-users need to have an accurate mental model of this objective function in order to understand and predict what the robot will do. While people naturally develop such a mental model over time through observing the robot act, this familiarization process may be lengthy. Our approach reduces this time by having the robot model how people infer objectives from observed behavior, in order to then show those behaviors that are maximally informative. We introduce two factors to define candidate models of human inference, and show that certain models indeed produce example robot behaviors that better enable users to anticipate what it will do in novel situations. Our results also reveal that choosing the appropriate model is key, and suggest that our candidate models do not fully capture how humans extrapolate from examples of robot behavior. We leverage these findings to propose a stronger model of human learning in this setting, and conclude by analyzing the impact of different ways in which the assumed model of human learning may be incorrect.},
  langid = {english},
  keywords = {Artificial Intelligence,Explainable artificial intelligence,Human-robot interaction,Inverse reinforcement learning,Transparency},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Huang et al_2019_Enabling robots to communicate their objectives.pdf}
}

@misc{huangHowFarAre2024,
  title = {How {{Far Are We}} on the {{Decision-Making}} of {{LLMs}}? {{Evaluating LLMs}}' {{Gaming Ability}} in {{Multi-Agent Environments}}},
  shorttitle = {How {{Far Are We}} on the {{Decision-Making}} of {{LLMs}}?},
  author = {Huang, Jen-tse and Li, Eric John and Lam, Man Ho and Liang, Tian and Wang, Wenxuan and Yuan, Youliang and Jiao, Wenxiang and Wang, Xing and Tu, Zhaopeng and Lyu, Michael R.},
  year = {2024},
  month = sep,
  number = {arXiv:2403.11807},
  eprint = {2403.11807},
  publisher = {arXiv},
  urldate = {2024-09-23},
  abstract = {Decision-making, a complicated task requiring various types of abilities, presents an excellent framework for assessing Large Language Models (LLMs). Our research investigates decision-making capabilities of LLMs through the lens of Game Theory. We focus specifically on games that support the simultaneous participation of more than two agents. We introduce GAMA({$\gamma$})-Bench, which evaluates LLMs' Gaming Ability in Multi-Agent environments. {$\gamma$}-Bench includes eight classical multi-agent games and a scoring scheme specially designed to quantitatively assess LLMs' performance. Leveraging {$\gamma$}-Bench, we investigate LLMs' robustness, generalizability, and strategies for enhancement. Results reveal that while GPT-3.5 shows satisfying robustness, its generalizability is relatively limited. However, its performance can be improved through approaches such as Chain-of-Thought. Additionally, we evaluate twelve versions from six models, including GPT-3.5, GPT-4, Gemini, LLaMA-3.1, Mixtral, and Qwen-2. We find that Gemini-1.5-Pro outperforms other models with a score of 63.8 out of 100, followed by LLaMA-3.1-70B and GPT-4 with scores of 60.9 and 60.5, respectively. The code and experimental results are made publicly available via https://github.com/CUHK-ARISE/GAMABench.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {https://github.com/CUHK-ARISE/GAMABench},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Huang et al_2024_How Far Are We on the Decision-Making of LLMs.pdf}
}

@misc{huangLVLMsUnderstandCharts2023,
  title = {Do {{LVLMs Understand Charts}}? {{Analyzing}} and {{Correcting Factual Errors}} in {{Chart Captioning}}},
  shorttitle = {Do {{LVLMs Understand Charts}}?},
  author = {Huang, Kung-Hsiang and Zhou, Mingyang and Chan, Hou Pong and Fung, Yi R. and Wang, Zhenhailong and Zhang, Lingyu and Chang, Shih-Fu and Ji, Heng},
  year = {2023},
  month = dec,
  number = {arXiv:2312.10160},
  eprint = {2312.10160},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2312.10160},
  urldate = {2023-12-28},
  abstract = {Recent advancements in large vision-language models (LVLMs) have led to significant progress in generating natural language descriptions for visual content and thus enhancing various applications. One issue with these powerful models is that they sometimes produce texts that are factually inconsistent with the visual input. While there has been some effort to mitigate such inconsistencies in natural image captioning, the factuality of generated captions for structured document images, such as charts, has not received as much scrutiny, posing a potential threat to information reliability in critical applications. This work delves into the factuality aspect by introducing a comprehensive typology of factual errors in generated chart captions. A large-scale human annotation effort provides insight into the error patterns and frequencies in captions crafted by various chart captioning models, ultimately forming the foundation of a novel dataset, CHOCOLATE. Our analysis reveals that even state-of-the-art models, including GPT-4V, frequently produce captions laced with factual inaccuracies. In response to this challenge, we establish the new task of Chart Caption Factual Error Correction and introduce CHARTVE, a model for visual entailment that outperforms proprietary and open-source LVLMs in evaluating factual consistency. Furthermore, we propose C2TFEC, an interpretable two-stage framework that excels at correcting factual errors. This work inaugurates a new domain in factual error correction for chart captions, presenting a novel evaluation mechanism, and demonstrating an effective approach to ensuring the factuality of generated chart captions.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/khuangaf/CHOCOLATE},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Huang et al_2023_Do LVLMs Understand Charts.pdf;/Users/thomasgorman/Zotero/storage/98MDS2GY/2312.html}
}

@misc{huffPsychologyMachinesLarge2024,
  title = {Towards a {{Psychology}} of {{Machines}}: {{Large Language Models Predict Human Memory}}},
  shorttitle = {Towards a {{Psychology}} of {{Machines}}},
  author = {Huff, Markus and Ulak{\c c}{\i}, Elanur},
  year = {2024},
  month = mar,
  number = {arXiv:2403.05152},
  eprint = {2403.05152},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.05152},
  urldate = {2024-06-16},
  abstract = {Large language models (LLMs) are demonstrating remarkable capabilities across various tasks despite lacking a foundation in human cognition. This raises the question: can these models, beyond simply mimicking human language patterns, offer insights into the mechanisms underlying human cognition? This study explores the ability of ChatGPT to predict human performance in a language-based memory task. Building upon theories of text comprehension, we hypothesize that recognizing ambiguous sentences (e.g., "Because Bill drinks wine is never kept in the house") is facilitated by preceding them with contextually relevant information. Participants, both human and ChatGPT, were presented with pairs of sentences. The second sentence was always a garden-path sentence designed to be inherently ambiguous, while the first sentence either provided a fitting (e.g., "Bill has chronic alcoholism") or an unfitting context (e.g., "Bill likes to play golf"). We measured both human's and ChatGPT's ratings of sentence relatedness, ChatGPT's memorability ratings for the garden-path sentences, and humans' spontaneous memory for the garden-path sentences. The results revealed a striking alignment between ChatGPT's assessments and human performance. Sentences deemed more related and assessed as being more memorable by ChatGPT were indeed better remembered by humans, even though ChatGPT's internal mechanisms likely differ significantly from human cognition. This finding, which was confirmed with a robustness check employing synonyms, underscores the potential of generative AI models to predict human performance accurately. We discuss the broader implications of these findings for leveraging LLMs in the development of psychological theories and for gaining a deeper understanding of human cognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {https://zenodo.org/records/10777391},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Huff_Ulakçı_2024_Towards a Psychology of Machines.pdf;/Users/thomasgorman/Zotero/storage/3UWFE6QR/2403.html}
}

@misc{huFinegrainedComparisonPragmatic2022,
  title = {A Fine-Grained Comparison of Pragmatic Language Understanding in Humans and Language Models},
  author = {Hu, Jennifer and Floyd, Sammy and Jouravlev, Olessia and Fedorenko, Evelina and Gibson, Edward},
  year = {2022},
  month = dec,
  urldate = {2023-12-31},
  abstract = {Pragmatics and non-literal language understanding are essential to human communication, and present a long-standing challenge for artificial language models. We perform a fine-grained comparison of language models and humans on seven pragmatic phenomena, using zero-shot prompting on an expert-curated set of English materials. We ask whether models (1) select pragmatic interpretations of speaker utterances, (2) make similar error patterns as humans, and (3) use similar linguistic cues as humans to solve the tasks. We find that the largest models achieve high accuracy and match human error patterns: within incorrect responses, models favor literal interpretations over heuristic-based distractors. We also find preliminary evidence that models and humans are sensitive to similar linguistic cues. Our results suggest that pragmatic behaviors can emerge in models without explicitly constructed representations of mental states. However, models tend to struggle with phenomena relying on social expectation violations.},
  langid = {english},
  annotation = {https://github.com/HeningWang/LLM\_link},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hu et al_2022_A fine-grained comparison of pragmatic language understanding in humans and.pdf}
}

@misc{huhPlatonicRepresentationHypothesis2024,
  title = {The {{Platonic Representation Hypothesis}}},
  author = {Huh, Minyoung and Cheung, Brian and Wang, Tongzhou and Isola, Phillip},
  year = {2024},
  month = may,
  number = {arXiv:2405.07987},
  eprint = {2405.07987},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {We argue that representations in AI models, particularly deep networks, are converging. First, we survey many examples of convergence in the literature: over time and across multiple domains, the ways by which different neural networks represent data are becoming more aligned. Next, we demonstrate convergence across data modalities: as vision models and language models get larger, they measure distance between datapoints in a more and more alike way. We hypothesize that this convergence is driving toward a shared statistical model of reality, akin to Plato's concept of an ideal reality. We term such a representation the platonic representation and discuss several possible selective pressures toward it. Finally, we discuss the implications of these trends, their limitations, and counterexamples to our analysis.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  annotation = {https://github.com/minyoungg/platonic-rep\\
\\
https://phillipi.github.io/prh/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Huh et al_2024_The Platonic Representation Hypothesis.pdf}
}

@article{huoLanechangingdecisionCharacteristicsAllocation2020,
  title = {Lane-Changing-Decision Characteristics and the Allocation of Visual Attention of Drivers with an Angry Driving Style},
  author = {Huo, Dongchao and Ma, Jinfei and Chang, Ruosong},
  year = {2020},
  month = may,
  journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
  volume = {71},
  pages = {62--75},
  issn = {1369-8478},
  doi = {10.1016/j.trf.2020.03.008},
  urldate = {2024-09-16},
  abstract = {The aim of this study was to examine the interactions between the road situational risk and angry driving style on lane-changing decisions in drivers and the allocation of visual attention of angry-driving-style drivers based on video clips of driving. We employed the Tobii eye tracker to collect the eye movement markers and reaction time data of 35 drivers. First, the road type and distances of neighboring cars were used to classify situations in the video clips into high- and low-risk situations. The revised Chinese version of the Multidimensional Driving Style Inventory (MDSI-C) was used to measure the driving style of drivers. The eye movement heatmap of drivers was used to classify the areas of interest into the front view and side mirror. We found that the angry driving style and the total fixation duration on the front view of drivers in high-risk situations were predictors of the relative lane-changing decision time. Angry-driving-style drivers' relative lane-changing decision time is shorter than that of other drivers. In high-risk driving situations, the longer the total fixation duration on the front view, the shorter the relative lane-changing decision time. We also found that the angry driving style and eye movement areas of interest interacted with the mean search durations and total fixation durations of the drivers. Angry-driving-style drivers were more focused on the front view. Our study has significance for the improvement of the improvement of the allocation of visual attention of angry-driving-style drivers and the development of advanced driver assistance systems (ADAS) and connected and autonomous vehicles (CAVs).},
  keywords = {Allocation of visual attention,Driving anger,Driving situational risk,Eye-tracking,Lane-changing decision},
  file = {/Users/thomasgorman/Zotero/storage/8J8DWHCL/S1369847819304796.html}
}

@misc{huPromptingNotSubstitute2023,
  title = {Prompting Is Not a Substitute for Probability Measurements in Large Language Models},
  author = {Hu, Jennifer and Levy, Roger},
  year = {2023},
  month = oct,
  number = {arXiv:2305.13264},
  eprint = {2305.13264},
  publisher = {arXiv},
  urldate = {2024-07-03},
  abstract = {Prompting is now a dominant method for evaluating the linguistic knowledge of large language models (LLMs). While other methods directly read out models' probability distributions over strings, prompting requires models to access this internal information by processing linguistic input, thereby implicitly testing a new type of emergent ability: metalinguistic judgment. In this study, we compare metalinguistic prompting and direct probability measurements as ways of measuring models' linguistic knowledge. Broadly, we find that LLMs' metalinguistic judgments are inferior to quantities directly derived from representations. Furthermore, consistency gets worse as the prompt query diverges from direct measurements of next-word probabilities. Our findings suggest that negative results relying on metalinguistic prompts cannot be taken as conclusive evidence that an LLM lacks a particular linguistic generalization. Our results also highlight the value that is lost with the move to closed APIs where access to probability distributions is limited.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {https://github.com/jennhu/metalinguistic-prompting},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hu_Levy_2023_Prompting is not a substitute for probability measurements in large language.pdf}
}

@misc{huQuantifyingPersonaEffect2024,
  title = {Quantifying the {{Persona Effect}} in {{LLM Simulations}}},
  author = {Hu, Tiancheng and Collier, Nigel},
  year = {2024},
  month = feb,
  number = {arXiv:2402.10811},
  eprint = {2402.10811},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-25},
  abstract = {Large language models (LLMs) have shown remarkable promise in simulating human language use and behavior. In this study, we delve into the intersection of persona variables and the capability of LLMs to simulate different perspectives. We find that persona variables can explain {$<$}10\% variance in annotations in existing subjective NLP datasets. Nonetheless, incorporating them via prompting in LLMs provides modest improvement. Persona prompting is most effective on data samples where disagreements among annotators are frequent yet confined to a limited range. A linear correlation exists: the more persona variables influence human annotations, the better LLMs predictions are using persona prompting. However, when the utility of persona variables is low (i.e., explaining {$<$}10\% of human annotations), persona prompting has little effect. Most subjective NLP datasets fall into this category, casting doubt on simulating diverse perspectives in the current NLP landscape.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hu_Collier_2024_Quantifying the Persona Effect in LLM Simulations.pdf}
}

@article{hussainTutorialOpensourceLarge2024,
  title = {A Tutorial on Open-Source Large Language Models for Behavioral Science},
  author = {Hussain, Zak and Binz, Marcel and Mata, Rui and Wulff, Dirk U.},
  year = {2024},
  month = aug,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-024-02455-8},
  urldate = {2024-09-23},
  abstract = {Large language models (LLMs) have the potential to revolutionize behavioral science by accelerating and improving the research cycle, from conceptualization to data analysis. Unlike closed-source solutions, open-source frameworks for LLMs can enable transparency, reproducibility, and adherence to data protection standards, which gives them a crucial advantage for use in behavioral science. To help researchers harness the promise of LLMs, this tutorial offers a primer on the open-source Hugging Face ecosystem and demonstrates several applications that advance conceptual and empirical work in behavioral science, including feature extraction, fine-tuning of models for prediction, and generation of behavioral responses. Executable code is made available at github.com/Zak-Hussain/LLM4BeSci.git. Finally, the tutorial discusses challenges faced by research with (open-source) LLMs related to interpretability and safety and offers a perspective on future research at the intersection of language modeling and behavioral science.},
  langid = {english},
  keywords = {Artificial Intelligence,Behavioral science,Hugging face,Large language models},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Hussain et al_2024_A tutorial on open-source large language models for behavioral science.pdf}
}

@article{ibrahimPerceptionPerformanceDetectability2023,
  title = {Perception, Performance, and Detectability of Conversational Artificial Intelligence across 32 University Courses},
  author = {Ibrahim, Hazem and Liu, Fengyuan and Asim, Rohail and Battu, Balaraju and Benabderrahmane, Sidahmed and Alhafni, Bashar and Adnan, Wifag and Alhanai, Tuka and AlShebli, Bedoor and Baghdadi, Riyadh and B{\'e}langer, Jocelyn J. and Beretta, Elena and Celik, Kemal and Chaqfeh, Moumena and Daqaq, Mohammed F. and Bernoussi, Zaynab El and Fougnie, Daryl and {Garcia de Soto}, Borja and Gandolfi, Alberto and Gyorgy, Andras and Habash, Nizar and Harris, J. Andrew and Kaufman, Aaron and Kirousis, Lefteris and Kocak, Korhan and Lee, Kangsan and Lee, Seungah S. and Malik, Samreen and Maniatakos, Michail and Melcher, David and Mourad, Azzam and Park, Minsu and Rasras, Mahmoud and Reuben, Alicja and Zantout, Dania and Gleason, Nancy W. and Makovi, Kinga and Rahwan, Talal and Zaki, Yasir},
  year = {2023},
  month = aug,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {12187},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-38964-3},
  urldate = {2023-09-01},
  abstract = {The emergence of large language models has led to the development of powerful tools such as ChatGPT that can produce text indistinguishable from human-generated work. With the increasing accessibility of such technology, students across the globe may utilize it to help with their school work---a possibility that has sparked ample discussion on the integrity of student evaluation processes in the age of artificial intelligence (AI). To date, it is unclear how such tools perform compared to students on university-level courses across various disciplines. Further, students' perspectives regarding the use of such tools in school work, and educators' perspectives on treating their use as plagiarism, remain unknown. Here, we compare the performance of the state-of-the-art tool, ChatGPT, against that of students on 32 university-level courses. We also assess the degree to which its use can be detected by two classifiers designed specifically for this purpose. Additionally, we conduct a global survey across five countries, as well as a more in-depth survey at the authors' institution, to discern students' and educators' perceptions of ChatGPT's use in school work. We find that ChatGPT's performance is comparable, if not superior, to that of students in a multitude of courses. Moreover, current AI-text classifiers cannot reliably detect ChatGPT's use in school work, due to both their propensity to classify human-written answers as AI-generated, as well as the relative ease with which AI-generated text can be edited to evade detection. Finally, there seems to be an emerging consensus among students to use the tool, and among educators to treat its use as plagiarism. Our findings offer insights that could guide policy discussions addressing the integration of artificial intelligence into educational frameworks.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Information technology},
  annotation = {https://github.com/comnetsAD/ChatGPT},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ibrahim et al_2023_Perception, performance, and detectability of conversational artificial.pdf}
}

@misc{ichienLargeLanguageModel2023,
  title = {Large {{Language Model Displays Emergent Ability}} to {{Interpret Novel Literary Metaphors}}},
  author = {Ichien, Nicholas and Stamenkovi{\'c}, Du{\v s}an and Holyoak, Keith J.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.01497},
  eprint = {2308.01497},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-12},
  abstract = {Recent advances in the performance of large language models (LLMs) have sparked debate over whether, given sufficient training, high-level human abilities emerge in such generic forms of artificial intelligence (AI). Despite the exceptional performance of LLMs on a wide range of tasks involving natural language processing and reasoning, there has been sharp disagreement as to whether their abilities extend to more creative human abilities. A core example is the ability to interpret novel metaphors. Given the enormous and non-curated text corpora used to train LLMs, a serious obstacle to designing tests is the requirement of finding novel yet high-quality metaphors that are unlikely to have been included in the training data. Here we assessed the ability of GPT-4, a state-of-the-art large language model, to provide natural-language interpretations of novel literary metaphors drawn from Serbian poetry and translated into English. Despite exhibiting no signs of having been exposed to these metaphors previously, the AI system consistently produced detailed and incisive interpretations. Human judge - blind to the fact that an AI model was involved - rated metaphor interpretations generated by GPT-4 as superior to those provided by a group of college students. In interpreting reversed metaphors, GPT-4, as well as humans, exhibited signs of sensitivity to the Gricean cooperative principle. These results indicate that LLMs such as GPT-4 have acquired an emergent ability to interpret complex novel metaphors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ichien et al_2023_Large Language Model Displays Emergent Ability to Interpret Novel Literary.pdf;/Users/thomasgorman/Zotero/storage/DL6UUGTH/2308.html}
}

@article{ichienTwoComputationalApproaches2023a,
  title = {Two {{Computational Approaches}} to {{Visual Analogy}}: {{Task-Specific Models Versus Domain-General Mapping}}},
  shorttitle = {Two {{Computational Approaches}} to {{Visual Analogy}}},
  author = {Ichien, Nicholas and Liu, Qing and Fu, Shuhao and Holyoak, Keith J. and Yuille, Alan L. and Lu, Hongjing},
  year = {2023},
  journal = {Cognitive Science},
  volume = {47},
  number = {9},
  pages = {e13347},
  issn = {1551-6709},
  doi = {10.1111/cogs.13347},
  urldate = {2023-10-17},
  abstract = {Advances in artificial intelligence have raised a basic question about human intelligence: Is human reasoning best emulated by applying task-specific knowledge acquired from a wealth of prior experience, or is it based on the domain-general manipulation and comparison of mental representations? We address this question for the case of visual analogical reasoning. Using realistic images of familiar three-dimensional objects (cars and their parts), we systematically manipulated viewpoints, part relations, and entity properties in visual analogy problems. We compared human performance to that of two recent deep learning models (Siamese Network and Relation Network) that were directly trained to solve these problems and to apply their task-specific knowledge to analogical reasoning. We also developed a new model using part-based comparison (PCM) by applying a domain-general mapping procedure to learned representations of cars and their component parts. Across four-term analogies (Experiment 1) and open-ended analogies (Experiment 2), the domain-general PCM model, but not the task-specific deep learning models, generated performance similar in key aspects to that of human reasoners. These findings provide evidence that human-like analogical reasoning is unlikely to be achieved by applying deep learning with big data to a specific type of analogy problem. Rather, humans do (and machines might) achieve analogical reasoning by learning representations that encode structural information useful for multiple tasks, coupled with efficient computation of relational similarity.},
  copyright = {{\copyright} 2023 Cognitive Science Society LLC.},
  langid = {english},
  keywords = {Analogy,Computational modeling,Deep learning,Relations,Visual reasoning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ichien et al_2023_Two Computational Approaches to Visual Analogy2.pdf;/Users/thomasgorman/Zotero/storage/NRU6MQUD/cogs.html}
}

@article{ilicEvidenceInterrelatedCognitivelike2024,
  title = {Evidence of Interrelated Cognitive-like Capabilities in Large Language Models: {{Indications}} of Artificial General Intelligence or Achievement?},
  shorttitle = {Evidence of Interrelated Cognitive-like Capabilities in Large Language Models},
  author = {Ili{\'c}, David and Gignac, Gilles E.},
  year = {2024},
  month = sep,
  journal = {Intelligence},
  volume = {106},
  pages = {101858},
  issn = {0160-2896},
  doi = {10.1016/j.intell.2024.101858},
  urldate = {2024-09-23},
  abstract = {Large language models (LLMs) are advanced artificial intelligence (AI) systems that can perform a variety of tasks commonly found in human intelligence tests, such as defining words, performing calculations, and engaging in verbal reasoning. There are also substantial individual differences in LLM capacities. Given the consistent observation of a positive manifold and general intelligence factor in human samples, along with group-level factors (e.g., crystallised intelligence), we hypothesized that LLM test scores may also exhibit positive inter-correlations, which could potentially give rise to an artificial general ability (AGA) factor and one or more group-level factors. Based on a sample of 591 LLMs and scores from 12 tests aligned with fluid reasoning (Gf), domain-specific knowledge (Gkn), reading/writing (Grw), and quantitative knowledge (Gq), we found strong empirical evidence for a positive manifold and a general factor of ability. Additionally, we identified a combined Gkn/Grw group-level factor. Finally, the number of LLM parameters correlated positively with both general factor of ability and Gkn/Grw factor scores, although the effects showed diminishing returns. We interpreted our results to suggest that LLMs, like human cognitive abilities, may share a common underlying efficiency in processing information and solving problems, though whether LLMs manifest primarily achievement/expertise rather than intelligence remains to be determined. Finally, while models with greater numbers of parameters exhibit greater general cognitive-like abilities, akin to the connection between greater neuronal density and human general intelligence, other characteristics must also be involved.},
  keywords = {Artificial general intelligence,Artificial intelligence,Number of parameters},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ilić_Gignac_2024_Evidence of interrelated cognitive-like capabilities in large language models.pdf;/Users/thomasgorman/Zotero/storage/XU8LBMQV/S0160289624000527.html}
}

@misc{ippolitoPreventingVerbatimMemorization2022,
  title = {Preventing {{Verbatim Memorization}} in {{Language Models Gives}} a {{False Sense}} of {{Privacy}}},
  author = {Ippolito, Daphne and Tram{\`e}r, Florian and Nasr, Milad and Zhang, Chiyuan and Jagielski, Matthew and Lee, Katherine and {Choquette-Choo}, Christopher A. and Carlini, Nicholas},
  year = {2022},
  month = oct,
  number = {arXiv:2210.17546},
  eprint = {2210.17546},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.17546},
  urldate = {2023-02-03},
  abstract = {Studying data memorization in neural language models helps us understand the risks (e.g., to privacy or copyright) associated with models regurgitating training data, and aids in the evaluation of potential countermeasures. Many prior works -- and some recently deployed defenses -- focus on "verbatim memorization", defined as a model generation that exactly matches a substring from the training set. We argue that verbatim memorization definitions are too restrictive and fail to capture more subtle forms of memorization. Specifically, we design and implement an efficient defense based on Bloom filters that perfectly prevents all verbatim memorization. And yet, we demonstrate that this "perfect" filter does not prevent the leakage of training data. Indeed, it is easily circumvented by plausible and minimally modified "style-transfer" prompts -- and in some cases even the non-modified original prompts -- to extract memorized information. For example, instructing the model to output ALL-CAPITAL texts bypasses memorization checks based on verbatim matching. We conclude by discussing potential alternative definitions and why defining memorization is a difficult yet crucial open question for neural language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/ippolitoPreventingVerbatimMemorization2022-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Ippolito et al_2022_Preventing Verbatim Memorization in Language Models Gives a False Sense of.pdf;/Users/thomasgorman/Zotero/storage/54PGAR5R/2210.html}
}

@inproceedings{isaza-giraldoPromptGamingPilotStudy2024,
  title = {Prompt-{{Gaming}}: {{A Pilot Study}} on {{LLM-Evaluating Agent}} in a {{Meaningful Energy Game}}},
  shorttitle = {Prompt-{{Gaming}}},
  booktitle = {Extended {{Abstracts}} of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {{Isaza-Giraldo}, Andr{\'e}s and Bala, Paulo and Campos, Pedro F. and Pereira, Lucas},
  year = {2024},
  month = may,
  pages = {1--12},
  publisher = {ACM},
  address = {Honolulu HI USA},
  doi = {10.1145/3613905.3650774},
  urldate = {2024-08-11},
  isbn = {9798400703317},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Isaza-Giraldo et al_2024_Prompt-Gaming.pdf}
}

@inproceedings{islamAnalysisClimateCampaigns2023,
  title = {Analysis of {{Climate Campaigns}} on {{Social Media}} Using {{Bayesian Model Averaging}}},
  booktitle = {Proceedings of the 2023 {{AAAI}}/{{ACM Conference}} on {{AI}}, {{Ethics}}, and {{Society}}},
  author = {Islam, Tunazzina and Zhang, Ruqi and Goldwasser, Dan},
  year = {2023},
  month = aug,
  pages = {15--25},
  publisher = {ACM},
  address = {Montr{\textbackslash}'\{e\}al QC Canada},
  doi = {10.1145/3600211.3604665},
  urldate = {2024-07-03},
  isbn = {9798400702310},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Islam et al_2023_Analysis of Climate Campaigns on Social Media using Bayesian Model Averaging.pdf}
}

@misc{islamDiscoveringLatentThemes2024,
  title = {Discovering {{Latent Themes}} in {{Social Media Messaging}}: {{A Machine-in-the-Loop Approach Integrating LLMs}}},
  shorttitle = {Discovering {{Latent Themes}} in {{Social Media Messaging}}},
  author = {Islam, Tunazzina and Goldwasser, Dan},
  year = {2024},
  month = jul,
  number = {arXiv:2403.10707},
  eprint = {2403.10707},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-26},
  abstract = {Grasping the themes of social media content is key to understanding the narratives that influence public opinion and behavior. The thematic analysis goes beyond traditional topiclevel analysis, which often captures only the broadest patterns, providing deeper insights into specific and actionable themes such as ``public sentiment towards vaccination'', ``political discourse surrounding climate policies,'' etc. In this paper, we introduce a novel approach to uncovering latent themes in social media messaging. Recognizing the limitations of the traditional topic-level analysis, which tends to capture only overarching patterns, this study emphasizes the need for a finer-grained, theme-focused exploration. Traditional theme discovery methods typically involve manual processes and a human-in-the-loop approach. While valuable, these methods face challenges in scalability, consistency, and resource intensity in terms of time and cost. To address these challenges, we propose a machine-in-the-loop approach that leverages the advanced capabilities of Large Language Models (LLMs). This approach facilitates a deeper investigation into the social media discourse, revealing a variety of themes with distinct characteristics and relevance. It provides a detailed understanding of underlying nuances and efficiently maps texts to these themes, enhancing our insight into social media messaging. To demonstrate our approach, we apply our framework to contentious topics, such as climate debate and vaccine debate. We use two publicly available datasets: (1) the climate campaigns dataset of 21k Facebook ads and (2) the COVID-19 vaccine campaigns dataset of 9k Facebook ads. Our quantitative and qualitative analysis shows that our methodology yields more accurate and interpretable results compared to the baselines. Our results not only demonstrate the effectiveness of our approach in uncovering latent themes but also illuminate how these themes are tailored for demographic targeting in social media contexts. Additionally, our work sheds light on the dynamic nature of social media, revealing the shifts in the thematic focus of messaging in response to real-world events.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning,Computer Science - Social and Information Networks},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Islam_Goldwasser_2024_Discovering Latent Themes in Social Media Messaging.pdf}
}

@article{jacobsLargeLanguageModels2024,
  title = {Large Language Models Have Divergent Effects on Self-Perceptions of Mind and the Attributes Considered Uniquely Human},
  author = {Jacobs, Oliver L. and Pazhoohi, Farid and Kingstone, Alan},
  year = {2024},
  month = sep,
  journal = {Consciousness and Cognition},
  volume = {124},
  pages = {103733},
  issn = {1053-8100},
  doi = {10.1016/j.concog.2024.103733},
  urldate = {2024-08-10},
  abstract = {The rise of powerful Large Language Models (LLMs) provides a compelling opportunity to investigate the consequences of anthropomorphism, particularly regarding how their exposure may influence the way individuals view themselves (self-perception) and other people (other-perception). Using a mind perception framework, we examined attributions of agency (the ability to do) and experience (the ability to feel). Participants evaluated their agentic and experiential capabilities and the extent to which these features are uniquely human before and after exposure to LLM responses. Post-exposure, participants increased evaluations of their agentic and experiential qualities while decreasing their perception that agency and experience are considered to be uniquely human. These results indicate that anthropomorphizing LLMs impacts attributions of mind for humans in fundamentally divergent ways: enhancing the perception of one's own mind while reducing its uniqueness for others. These results open up a range of future questions regarding how anthropomorphism can affect mind perception toward humans.},
  annotation = {OSF: DOI 10.17605/OSF.IO/TYE3K},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Jacobs et al_2024_Large language models have divergent effects on self-perceptions of mind and.pdf;/Users/thomasgorman/Zotero/storage/AB9Z6US2/S1053810024001004.html}
}

@misc{jagadishEcologicallyRationalMetalearned2024,
  title = {Ecologically Rational Meta-Learned Inference Explains Human Category Learning},
  author = {Jagadish, Akshay K. and {Coda-Forno}, Julian and Thalmann, Mirko and Schulz, Eric and Binz, Marcel},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01821},
  eprint = {2402.01821},
  publisher = {arXiv},
  urldate = {2024-02-26},
  abstract = {Ecological rationality refers to the notion that humans are rational agents adapted to their environment. However, testing this theory remains challenging due to two reasons: the difficulty in defining what tasks are ecologically valid and building rational models for these tasks. In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge. We tackle the second challenge by deriving rational agents adapted to these tasks using the framework of meta-learning, leading to a class of models called ecologically rational meta-learned inference (ERMI). ERMI quantitatively explains human data better than seven other cognitive models in two different experiments. It additionally matches human behavior on a qualitative level: (1) it finds the same tasks difficult that humans find difficult, (2) it becomes more reliant on an exemplar-based strategy for assigning categories with learning, and (3) it generalizes to unseen stimuli in a human-like way. Furthermore, we show that ERMI's ecologically valid priors allow it to achieve state-of-the-art performance on the OpenML-CC18 classification benchmark.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {https://arxiv.org/html/2402.01821v1\\
\\
\\
https://akjagadish.github.io/\\
https://github.com/akjagadish/resource-rational-compositional-RL},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Jagadish et al_2024_Ecologically rational meta-learned inference explains human category learning.pdf;/Users/thomasgorman/Zotero/storage/N4GLECI8/2402.html}
}

@article{jagadishHumanlikeCategoryLearning,
  title = {Human-like {{Category Learning}} by {{Injecting Ecological Priors}} from {{Large Language Models}} into {{Neural Networks}}},
  author = {Jagadish, Akshay K and {Coda-Forno}, Julian and Thalmann, Mirko and Schulz, Eric and Binz, Marcel},
  abstract = {Ecological rationality refers to the notion that humans are rational agents adapted to their environment. However, testing this theory remains challenging due to two reasons: the difficulty in defining what tasks are ecologically valid and building rational models for these tasks. In this work, we demonstrate that large language models can generate cognitive tasks, specifically category learning tasks, that match the statistics of real-world tasks, thereby addressing the first challenge. We tackle the second challenge by deriving rational agents adapted to these tasks using the framework of meta-learning, leading to a class of models called ecologically rational meta-learned inference (ERMI). ERMI quantitatively explains human data better than seven other cognitive models in two different experiments. It additionally matches human behavior on a qualitative level: (1) it finds the same tasks difficult that humans find difficult, (2) it becomes more reliant on an exemplar-based strategy for assigning categories with learning, and (3) it generalizes to unseen stimuli in a human-like way. Furthermore, we show that ERMI's ecologically valid priors allow it to achieve state-of-the-art performance on the OpenML-CC18 classification benchmark.},
  langid = {english},
  annotation = {https://github.com/akjagadish/ermi\\
\\
https://akjagadish.github.io/ermi/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Jagadish et al_Human-like Category Learning by Injecting Ecological Priors from Large Language.pdf}
}

@inproceedings{jaimovitch-lopezThinkBigTeach2021,
  title = {Think {{Big}}, {{Teach Small}}: {{Do Language Models Distil Occam}}'s {{Razor}}?},
  shorttitle = {Think {{Big}}, {{Teach Small}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {{Jaimovitch-Lopez}, Gonzalo and Castellano Falc{\'o}n, David and Ferri, Cesar and {Hern{\'a}ndez-Orallo}, Jos{\'e}},
  year = {2021},
  volume = {34},
  pages = {1610--1623},
  publisher = {Curran Associates, Inc.},
  urldate = {2022-04-08},
  abstract = {Large language models have recently shown a remarkable ability for few-shot learning, including patterns of algorithmic nature. However, it is still an open question to determine what kind of patterns these models can capture and how many examples they need in their prompts. We frame this question as a teaching problem with strong priors, and study whether language models can identify simple algorithmic concepts from small witness sets. In particular, we explore how several GPT architectures, program induction systems and humans perform in terms of the complexity of the concept and the number of additional examples, and how much their behaviour differs. This first joint analysis of language models and machine teaching can address key questions for artificial intelligence and machine learning, such as whether some strong priors, and Occam's razor in particular, can be distilled from data, making learning from a few examples possible.},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Jaimovitch-Lopez et al_2021_Think Big, Teach Small.pdf}
}

@misc{jansenLeveragingLargeLanguage2023,
  title = {Leveraging Large Language Models for Data Analysis Automation},
  author = {Jansen, Jacqueline A and Manukyan, Art{\"u}r and Al Khoury, Nour and Akalin, Altuna},
  year = {2023},
  month = dec,
  doi = {10.1101/2023.12.11.571140},
  urldate = {2024-05-24},
  abstract = {Data analysis is constrained by a shortage of skilled experts, particularly in biology, where detailed data interpretation is vital for understanding complex biological processes and developing new treatments and diagnostics. To address this, we developed mergen, an R package that leverages Large Language Models (LLMs) for data analysis code generation and execution. Our primary goal is to enable humans to conduct data analysis by simply describing their objectives and the desired analyses for specific datasets through clear text. Our approach improves code generation via specialized prompt engineering and error feedback mechanisms. In addition, our system can execute the data analysis workflows prescribed by the LLM providing the results of the data analysis workflow for human review. We evaluated the performance of this data analysis system using various data analysis tasks. Our evaluation revealed that while LLMs effectively generate code for some data analysis tasks, challenges remain in executable code generation, especially for complex data analysis tasks. Our study contributes to a better understanding of LLM capabilities and limitations, providing software infrastructure and practical insights for their effective integration into data analysis workflows.},
  langid = {english},
  annotation = {https://github.com/BIMSBbioinfo/VoltRon\\
\\
https://bioinformatics.mdc-berlin.de/VoltRon/\\
\\
https://artur-man.github.io/VoltRon/tutorials.html},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Jansen et al_2023_Leveraging large language models for data analysis automation.pdf}
}

@article{jarvenpaaNewFrontiersInformation2024,
  title = {New {{Frontiers}} in {{Information Systems Theorizing}}: {{Human-gAI Collaboration}}},
  shorttitle = {New {{Frontiers}} in {{Information Systems Theorizing}}},
  author = {Jarvenpaa, Sirkka and Klein, Stefan},
  year = {2024},
  month = jan,
  journal = {Journal of the Association for Information Systems},
  volume = {25},
  pages = {110--121},
  doi = {10.17705/1jais.00868},
  abstract = {The Journal of the Association for Information Systems has long had a reputation for promoting theory development. Yet theory development can be experienced as risky and frustrating because of a lack of divergence and convergence---both in terms of ideas and in the social dynamics among human theorists. These dichotomies can stymie progress and lead to unfinished works. Misconceptions about theory can also hamper advances. We examine the ways in which generative artificial intelligence (gAI) tools may be useful in developing theory in information systems (IS) through human-gAI collaboration, thus forging new frontiers in IS theorizing.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Jarvenpaa_Klein_2024_New Frontiers in Information Systems Theorizing.pdf}
}

@inproceedings{jensenDesigningDesirableSmart2018,
  title = {Designing the {{Desirable Smart Home}}: {{A Study}} of {{Household Experiences}} and {{Energy Consumption Impacts}}},
  shorttitle = {Designing the {{Desirable Smart Home}}},
  booktitle = {Proceedings of the 2018 {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Jensen, Rikke Hagensby and Strengers, Yolande and Kjeldskov, Jesper and Nicholls, Larissa and Skov, Mikael B.},
  year = {2018},
  month = apr,
  pages = {1--14},
  publisher = {ACM},
  address = {Montreal QC Canada},
  doi = {10.1145/3173574.3173578},
  urldate = {2024-07-02},
  isbn = {978-1-4503-5620-6},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Jensen et al_2018_Designing the Desirable Smart Home.pdf}
}

@misc{ji-anLinkingContextLearning2024,
  title = {Linking {{In-context Learning}} in {{Transformers}} to {{Human Episodic Memory}}},
  author = {{Ji-An}, Li and Zhou, Corey Y. and Benna, Marcus K. and Mattar, Marcelo G.},
  year = {2024},
  month = may,
  number = {arXiv:2405.14992},
  eprint = {2405.14992},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {Understanding the connections between artificial and biological intelligent systems can reveal fundamental principles underlying general intelligence. While many artificial intelligence (AI) models have a neuroscience counterpart, such connections are largely missing in Transformer models and the self-attention mechanism. Here, we examine the relationship between attention heads and human episodic memory. We focus on the induction heads, which contribute to the in-context learning capabilities of Transformer-based large language models (LLMs). We demonstrate that induction heads are behaviorally, functionally, and mechanistically similar to the contextual maintenance and retrieval (CMR) model of human episodic memory. Our analyses of LLMs pre-trained on extensive text data show that CMR-like heads often emerge in the intermediate model layers and that their behavior qualitatively mirrors the memory biases seen in humans. Our findings uncover a parallel between the computational mechanisms of LLMs and human memory, offering valuable insights into both research fields.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ji-An et al_2024_Linking In-context Learning in Transformers to Human Episodic Memory.pdf}
}

@misc{jiaDecisionMakingBehaviorEvaluation2024,
  title = {Decision-{{Making Behavior Evaluation Framework}} for {{LLMs}} under {{Uncertain Context}}},
  author = {Jia, Jingru and Yuan, Zehua and Pan, Junhao and McNamara, Paul and Chen, Deming},
  year = {2024},
  month = jun,
  number = {arXiv:2406.05972},
  eprint = {2406.05972},
  publisher = {arXiv},
  urldate = {2024-08-15},
  abstract = {When making decisions under uncertainty, individuals often deviate from rational behavior, which can be evaluated across three dimensions: risk preference, probability weighting, and loss aversion. Given the widespread use of large language models (LLMs) in supporting decision-making processes, it is crucial to assess whether their behavior aligns with human norms and ethical expectations or exhibits potential biases. Although several empirical studies have investigated the rationality and social behavior performance of LLMs, their internal decisionmaking tendencies and capabilities remain inadequately understood. This paper proposes a framework, grounded in behavioral economics theories, to evaluate the decision-making behaviors of LLMs. With a multiple-choice-list experiment, we initially estimate the degree of risk preference, probability weighting, and loss aversion in a context-free setting for three commercial LLMs: ChatGPT-4.0-Turbo, Claude-3-Opus, and Gemini-1.0-pro. Our results reveal that LLMs generally exhibit patterns similar to humans, such as risk aversion and loss aversion, with a tendency to overweight small probabilities, but there are significant variations in the degree to which these behaviors are expressed across different LLMs. Further, we explore their behavior when embedded with socio-demographic features of human beings, uncovering significant disparities across various demographic characteristics. For instance, when modeled with attributes of sexual minority groups or physical disabilities, Claude-3-Opus displays increased risk aversion, leading to more conservative choices. These findings underscore the need for careful consideration of the ethical implications and potential biases in deploying LLMs in decision-making scenarios. Therefore, this study advocates for the development of standards and guidelines to ensure that LLMs operate within ethical boundaries while enhancing their utility in complex decision-making environments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Economics - Theoretical Economics},
  annotation = {https://bayesian-beagle.netlify.app/posts/decision\_making\_behavior\_evaluation\_framework\_for\_llms\_under\_uncertain\_context/2024-06-10-decision\_making\_behavior\_evaluation\_framework\_for\_llms\_under\_uncertain\_context},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Jia et al_2024_Decision-Making Behavior Evaluation Framework for LLMs under Uncertain Context.pdf}
}

@article{jiangRiskRepresentationPerception2022,
  title = {Risk {{Representation}}, {{Perception}}, and {{Propensity}} in an {{Integrated Human Lane-Change Decision Model}}},
  author = {Jiang, Longsheng and Chen, Dong and Li, Zhaojian and Wang, Yue},
  year = {2022},
  month = dec,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {23},
  number = {12},
  pages = {23474--23487},
  issn = {1558-0016},
  doi = {10.1109/TITS.2022.3207182},
  urldate = {2024-09-16},
  abstract = {Lane-change decision models with enhanced human-likeness are increasingly important as they are integral in traffic simulations for training autonomous driving algorithms. This work proposes a computational model of driver lane-change decision-making by integrating relevant human features in perception, reasoning, emotion, and decision (PRED). The PRED model describes how drivers make lane-change decisions under collision risk. Here risk is represented by probabilities and outcomes of the possible consequences. The PRED model formulates drivers' risk perception and risk propensity in its modules: the perception module is modeled with Bayesian inference; the reasoning module is modeled with Newtonian simulation; the emotion and decision module is modeled with the extended regret theory. The PRED model was fitted and tested with an empirical dataset from a naturalistic driving database. The prediction performance of the PRED model ranks higher than the selected benchmarks and is close to the state-of-the-art machine learning models. Moreover, the explicit modeling of risk propensity sheds light on an important question in transportation: what causes human drivers' risk-taking behaviors? The results support the rationale that downplaying crash consequences is the main contributor.},
  keywords = {Behavioral sciences,Cognition,Computational modeling,Decision making,decision-making,Hidden Markov models,Lane change,Liquid crystal displays,risk perception,risk propensity,traffic simulation,Vehicles},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Jiang et al_2022_Risk Representation, Perception, and Propensity in an Integrated Human.pdf;/Users/thomasgorman/Zotero/storage/WYTVGSP2/9901459.html}
}

@misc{jinWhatIfLLMs2024,
  title = {What If {{LLMs Have Different World Views}}: {{Simulating Alien Civilizations}} with {{LLM-based Agents}}},
  shorttitle = {What If {{LLMs Have Different World Views}}},
  author = {Jin, Mingyu and Wang, Beichen and Xue, Zhaoqian and Zhu, Suiyuan and Hua, Wenyue and Tang, Hua and Mei, Kai and Du, Mengnan and Zhang, Yongfeng},
  year = {2024},
  month = feb,
  number = {arXiv:2402.13184},
  eprint = {2402.13184},
  publisher = {arXiv},
  urldate = {2024-04-08},
  abstract = {In this study, we introduce "CosmoAgent," an innovative artificial intelligence framework utilizing Large Language Models (LLMs) to simulate complex interactions between human and extraterrestrial civilizations, with a special emphasis on Stephen Hawking's cautionary advice about not sending radio signals haphazardly into the universe. The goal is to assess the feasibility of peaceful coexistence while considering potential risks that could threaten well-intentioned civilizations. Employing mathematical models and state transition matrices, our approach quantitatively evaluates the development trajectories of civilizations, offering insights into future decision-making at critical points of growth and saturation. Furthermore, the paper acknowledges the vast diversity in potential living conditions across the universe, which could foster unique cosmologies, ethical codes, and worldviews among various civilizations. Recognizing the Earth-centric bias inherent in current LLM designs, we propose the novel concept of using LLMs with diverse ethical paradigms and simulating interactions between entities with distinct moral principles. This innovative research provides a new way to understand complex inter-civilizational dynamics, expanding our perspective while pioneering novel strategies for conflict resolution, crucial for preventing interstellar conflicts. We have also released the code and datasets to enable further academic investigation into this interesting area of research. The code is available at https://github.com/agiresearch/AlienAgent.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/agiresearch/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Jin et al_2024_What if LLMs Have Different World Views.pdf;/Users/thomasgorman/Zotero/storage/MTYY8V7X/2402.html}
}

@article{johnsonAdoptionUseSmart2023,
  title = {The {{Adoption}} and {{Use}} of {{Smart Assistants}} in {{Residential Homes}}: {{The Matching Hypothesis}}},
  shorttitle = {The {{Adoption}} and {{Use}} of {{Smart Assistants}} in {{Residential Homes}}},
  author = {Johnson, Nathanael and Reimer, Torsten},
  year = {2023},
  journal = {Sustainability},
  volume = {15},
  number = {12},
  pages = {1--16},
  publisher = {MDPI},
  urldate = {2024-07-02},
  abstract = {An increasing number of residential homes are equipped with smart assistants such as Cortana, Alexa, and Siri. Adoption rates and the frequency of the usage of smart assistants vary across users and residential homes. Building on the theory of uses and gratifications (UGT) and the unified theory of acceptance and use of technology 2 (UTAUT2), the objective of this paper was to examine whether the intended use of a digital assistant would moderate the effects of performance expectancy and hedonic motivation on its adoption. Two experiments ( N = 345 and N = 351) tested the hypothesis that, for utilitarian purposes, devices with high performance appraisal are preferred, whereas for entertainment purposes, devices with high hedonic appraisal are preferred. The experiments manipulated the performance expectancy and hedonic motivation towards several digital assistants by varying how the assistants were introduced. Participants were asked which assistant they would choose for a variety of utilitarian and entertainment purposes. As expected, the experiments supported the proposed matching hypothesis, revealing that the devices that were high in performance appraisal were preferred for utilitarian tasks, whereas the devices high in hedonic appraisal were preferred for entertainment needs. These results suggest that a device's introduction can change people's perceptions of the device and subsequently their decision to use it.},
  langid = {english},
  keywords = {adoption of smart home technology,communication technology affordances,smart speakers,use of digital assistants},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Johnson_Reimer_2023_The Adoption and Use of Smart Assistants in Residential Homes.pdf;/Users/thomasgorman/Zotero/storage/JZMESQQZ/v15y2023i12p9224-d1165774.html}
}

@article{johnsScalableCognitiveModelling2023,
  title = {Scalable Cognitive Modelling: {{Putting Simon}}'s (1969) Ant Back on the Beach.},
  shorttitle = {Scalable Cognitive Modelling},
  author = {Johns, Brendan T. and Jamieson, Randall K. and Jones, Michael N.},
  year = {2023},
  month = sep,
  journal = {Canadian Journal of Experimental Psychology / Revue canadienne de psychologie exp{\'e}rimentale},
  volume = {77},
  number = {3},
  pages = {185--201},
  issn = {1878-7290, 1196-1961},
  doi = {10.1037/cep0000306},
  urldate = {2024-05-25},
  abstract = {Computational modelling has played a central role in the development of theory in cognitive psychology. Recently, machine learning and big data approaches to understanding cognition have become increasingly popular. This article reviews standard approaches in computational cognitive modelling and specifies how new advanced computational approaches can be used to generate new research pathways in the cognitive sciences.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Johns et al_2023_Scalable cognitive modelling.pdf}
}

@article{jonesComparingHumansLarge2024,
  title = {Comparing {{Humans}} and {{Large Language Models}} on an {{Experimental Protocol Inventory}} for {{Theory}} of {{Mind Evaluation}} ({{EPITOME}})},
  author = {Jones, Cameron R. and Trott, Sean and Bergen, Benjamin},
  year = {2024},
  month = jun,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {12},
  pages = {803--819},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00674},
  urldate = {2024-06-30},
  abstract = {We address a growing debate about the extent to which large language models (LLMs) produce behavior consistent with Theory of Mind (ToM) in humans. We present EPITOME: a battery of six experiments that tap diverse ToM capacities, including belief attribution, emotional inference, and pragmatic reasoning. We elicit a performance baseline from human participants for each task. We use the dataset to ask whether distributional linguistic information learned by LLMs is sufficient to explain ToM in humans. We compare performance of five LLMs to a baseline of responses from human comprehenders. Results are mixed. LLMs display considerable sensitivity to mental states and match human performance in several tasks. Yet, they commit systematic errors in others, especially those requiring pragmatic reasoning on the basis of mental state information. Such uneven performance indicates that human-level ToM may require resources beyond distributional information.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Jones et al_2024_Comparing Humans and Large Language Models on an Experimental Protocol.pdf;/Users/thomasgorman/Zotero/storage/LDYGDNFX/122721.html}
}

@misc{kakarlaUsingLargeLanguage2024,
  title = {Using {{Large Language Models}} to {{Assess Tutors}}' {{Performance}} in {{Reacting}} to {{Students Making Math Errors}}},
  author = {Kakarla, Sanjit and Thomas, Danielle and Lin, Jionghao and Gupta, Shivang and Koedinger, Kenneth R.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.03238},
  eprint = {2401.03238},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-12},
  abstract = {Research suggests that tutors should adopt a strategic approach when addressing math errors made by low-efficacy students. Rather than drawing direct attention to the error, tutors should guide the students to identify and correct their mistakes on their own. While tutor lessons have introduced this pedagogical skill, human evaluation of tutors applying this strategy is arduous and time-consuming. Large language models (LLMs) show promise in providing real-time assessment to tutors during their actual tutoring sessions, yet little is known regarding their accuracy in this context. In this study, we investigate the capacity of generative AI to evaluate real-life tutors' performance in responding to students making math errors. By analyzing 50 real-life tutoring dialogues, we find both GPT-3.5-Turbo and GPT-4 demonstrate proficiency in assessing the criteria related to reacting to students making errors. However, both models exhibit limitations in recognizing instances where the student made an error. Notably, GPT-4 tends to overidentify instances of students making errors, often attributing student uncertainty or inferring potential errors where human evaluators did not. Future work will focus on enhancing generalizability by assessing a larger dataset of dialogues and evaluating learning transfer. Specifically, we will analyze the performance of tutors in real-life scenarios when responding to students' math errors before and after lesson completion on this crucial tutoring skill.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kakarla et al_2024_Using Large Language Models to Assess Tutors' Performance in Reacting to.pdf;/Users/thomasgorman/Zotero/storage/VTN68ZDG/2401.html}
}

@article{kamaruddinDriverBehaviourState2018,
  title = {Driver {{Behaviour State Recognition}} Based on {{Speech}}},
  author = {Kamaruddin, Norhaslinda and Abdul Rahman, Abdul Wahab and Mohamad Halim, Khairul Ikhwan and Mohd Noh, Muhammad Hafiq Iqmal},
  year = {2018},
  month = apr,
  journal = {TELKOMNIKA (Telecommunication Computing Electronics and Control)},
  volume = {16},
  number = {2},
  pages = {852},
  issn = {2302-9293, 1693-6930},
  doi = {10.12928/telkomnika.v16i2.8416},
  urldate = {2024-09-16},
  abstract = {Researches have linked the cause of traffic accident to driver behavior and some studies provided practical preventive measures based on different input sources. Due to its simplicity to collect, speech can be used as one of the input. The emotion information gathered from speech can be used to measure driver behavior state based on the hypothesis that emotion influences driver behavior. However, the massive amount of driving speech data may hinder optimal performance of processing and analyzing the data due to the computational complexity and time constraint. This paper presents a silence removal approach using Short Term Energy (STE) and Zero Crossing Rate (ZCR) in the pre-processing phase to reduce the unnecessary processing. Mel Frequency Cepstral Coefficient (MFCC) feature extraction method coupled with Multi-Layer Perceptron (MLP) classifier are employed to get the driver behavior state recognition performance. Experimental results demonstrated that the proposed approach can obtain comparable performance with accuracy ranging between 58.7\% and 76.6\% to differentiate four driver behavior states, namely; talking through mobile phone, laughing, sleepy and normal driving. It is envisaged that such approach can be extended for a more comprehensive driver behavior identification system that may acts as an embedded warning system for sleepy driver.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kamaruddin et al_2018_Driver Behaviour State Recognition based on Speech.pdf}
}

@article{kammerAdaptiveUseRecognition2014,
  title = {The {{Adaptive Use}} of {{Recognition}} in {{Group Decision Making}}},
  author = {K{\"a}mmer, Juliane E. and Gaissmaier, Wolfgang and Reimer, Torsten and Schermuly, Carsten C.},
  year = {2014},
  journal = {Cognitive Science},
  volume = {38},
  number = {5},
  pages = {911--942},
  issn = {1551-6709},
  doi = {10.1111/cogs.12110},
  urldate = {2024-06-22},
  abstract = {Applying the framework of ecological rationality, the authors studied the adaptivity of group decision making. In detail, they investigated whether groups apply decision strategies conditional on their composition in terms of task-relevant features. The authors focused on the recognition heuristic, so the task-relevant features were the validity of the group members' recognition and knowledge, which influenced the potential performance of group strategies. Forty-three three-member groups performed an inference task in which they had to infer which of two German companies had the higher market capitalization. Results based on the choice data support the hypothesis that groups adaptively apply the strategy that leads to the highest theoretically achievable performance. Time constraints had no effect on strategy use but did have an effect on the proportions of different types of arguments. Possible mechanisms underlying the adaptive use of recognition in group decision making are discussed.},
  langid = {english},
  keywords = {Adaptive strategy choice,Group decision making,Group discussion,Recognition heuristic,Recognition-based model},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kämmer et al_2014_The Adaptive Use of Recognition in Group Decision Making.pdf;/Users/thomasgorman/Zotero/storage/HHLBNLXY/cogs.html}
}

@article{kanayaHowCanCurrent2024,
  title = {How {{Can}} the {{Current State}} of {{AI Guide Future Conversations}} of {{General Intelligence}}?},
  author = {Kanaya, Tomoe and Magine, Ali},
  year = {2024},
  month = mar,
  journal = {Journal of Intelligence},
  volume = {12},
  number = {3},
  pages = {36},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2079-3200},
  doi = {10.3390/jintelligence12030036},
  urldate = {2024-07-10},
  abstract = {Similar to the field of human intelligence, artificial intelligence (AI) has experienced a long history of advances and controversies regarding its definition, assessment, and application. Starting over 70 years ago, AI set out to achieve a single, general-purpose technology that could overcome many tasks in a similar fashion to humans. However, until recently, implementations were based on narrowly defined tasks, making the systems inapplicable to even slight variations of the same task. With recent advances towards more generality, the contemplation of artificial general intelligence (AGI) akin to human general intelligence (HGI) can no longer be easily dismissed. We follow this line of inquiry and outline some of the key questions and conceptual challenges that must be addressed in order to integrate AGI and HGI and to enable future progress towards a unified field of general intelligence.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {artificial intelligence,general intelligence,human intelligence},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kanaya_Magine_2024_How Can the Current State of AI Guide Future Conversations of General.pdf}
}

@article{kantenbacherBetterRulesJudging2021,
  title = {Better Rules for Judging Joules: {{Exploring}} How Experts Make Decisions about Household Energy Use},
  shorttitle = {Better Rules for Judging Joules},
  author = {Kantenbacher, Joseph and Attari, Shahzeen Z.},
  year = {2021},
  month = mar,
  journal = {Energy Research \& Social Science},
  volume = {73},
  pages = {101911},
  issn = {2214-6296},
  doi = {10.1016/j.erss.2021.101911},
  urldate = {2024-07-03},
  abstract = {Public understanding of home energy use is rife with biases and misunderstandings that can stymie the adoption of efficient technologies and conservation practices. Studying how energy experts make energy-related judgments can help design decision support tools to correct misperceptions held by novices. Here we conduct interviews with electrical engineers (n~=~10), physicists (n~=~10), and energy analysts (n~=~10) to document expert judgments about energy use and to identify their cognitive shortcuts (heuristics) for household energy decision making. Performance on an energy estimation task confirmed that energy experts have more accurate estimates of home energy use than novices. We document 24 unique expert heuristics related to device functions, components, and observable cues used by experts while making energy-use judgments. A follow-up survey with the experts indicated that these expert heuristics are generally more accurate than novice heuristics. The library of heuristics created in this study can be useful additions to education programs designed to improve public energy literacy and decision making.},
  keywords = {Cognitive shortcuts,Decision support,Estimation,Expert elicitation,Interviews,Perception},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kantenbacher_Attari_2021_Better rules for judging joules.pdf;/Users/thomasgorman/Zotero/storage/UML45G5G/S2214629621000049.html}
}

@misc{kapoorLargeLanguageModels2024,
  title = {Large {{Language Models Must Be Taught}} to {{Know What They Don}}'t {{Know}}},
  author = {Kapoor, Sanyam and Gruver, Nate and Roberts, Manley and Collins, Katherine and Pal, Arka and Bhatt, Umang and Weller, Adrian and Dooley, Samuel and Goldblum, Micah and Wilson, Andrew Gordon},
  year = {2024},
  month = jun,
  number = {arXiv:2406.08391},
  eprint = {2406.08391},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-08-11},
  abstract = {When using large language models (LLMs) in high-stakes applications, we need to know when we can trust their predictions. Some works argue that prompting high-performance LLMs is sufficient to produce calibrated uncertainties, while others introduce sampling methods that can be prohibitively expensive. In this work, we first argue that prompting on its own is insufficient to achieve good calibration and then show that fine-tuning on a small dataset of correct and incorrect answers can create an uncertainty estimate with good generalization and small computational overhead. We show that a thousand graded examples are sufficient to outperform baseline methods and that training through the features of a model is necessary for good performance and tractable for large open-source models when using LoRA. We also investigate the mechanisms that enable reliable LLM uncertainty estimation, finding that many models can be used as general-purpose uncertainty estimators, applicable not just to their own uncertainties but also the uncertainty of other models. Lastly, we show that uncertainty estimates inform human use of LLMs in human-AI collaborative settings through a user study.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kapoor et al_2024_Large Language Models Must Be Taught to Know What They Don't Know.pdf}
}

@misc{karjusMachineassistedMixedMethods2023,
  title = {Machine-Assisted Mixed Methods: Augmenting Humanities and Social Sciences with Artificial Intelligence},
  shorttitle = {Machine-Assisted Mixed Methods},
  author = {Karjus, Andres},
  year = {2023},
  month = sep,
  number = {arXiv:2309.14379},
  eprint = {2309.14379},
  publisher = {arXiv},
  urldate = {2024-05-22},
  abstract = {The increasing capacities of large language models (LLMs) present an unprecedented opportunity to scale up data analytics in the humanities and social sciences, augmenting and automating qualitative analytic tasks previously typically allocated to human labor. This contribution goes beyond simply reporting on LLM task performance, proposing a systematic mixed methods framework to harness qualitative analytic expertise, machine scalability, and rigorous quantification, with attention to transparency and replicability. It builds on the mixed methods designs of quantitization or integration, and feature analysis from linguistics. 16 machine-assisted case studies are showcased as proof of concept, in 9 diverse languages and across multiple disciplines. Tasks include linguistic and discourse analysis, lexical semantic change detection, interview analysis, historical event cause inference, detection of political stance, text and idea reuse, genre composition in literature and film; social network inference from text, historical text mining, automated lexicography, missing metadata augmentation, and multimodal visual cultural analytics. They are based on novel test data as well as direct replications of past research. It is also shown how to replace opaque topic modeling, popular as a "distant reading" method, with hypothesis-driven topic classification. In contrast to the focus on English in the emerging LLM applicability literature, many examples here deal with scenarios involving smaller languages and historical texts prone to digitization distortions. In all but the most difficult tasks requiring expert knowledge, (already currently available) generative LLMs can demonstrably serve as viable research instruments and an alternative to human-only analytics. LLM (and human) annotations may contain errors and variation, but the agreement rate can and should be accounted for in subsequent statistical modeling; a bootstrapping approach is discussed. The replications among the case studies illustrate how tasks previously requiring potentially months of team effort and complex computational pipelines, can now be accomplished by an LLM-assisted scholar in a fraction of the time. Importantly, this approach is not intended to replace, but to augment researcher knowledge and skills. With these opportunities in sight, qualitative expertise and the ability to pose insightful questions have arguably never been more critical.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society},
  annotation = {https://github.com/andreskarjus/MachineAssistedMixedMethods},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Karjus_2023_Machine-assisted mixed methods.pdf}
}

@article{karlinEnergyFeedbackTechnology2014,
  title = {Energy Feedback Technology: A Review and Taxonomy of Products and Platforms},
  shorttitle = {Energy Feedback Technology},
  author = {Karlin, Beth and Ford, Rebecca and Squiers, Cassandra},
  year = {2014},
  month = jun,
  journal = {Energy Efficiency},
  volume = {7},
  number = {3},
  pages = {377--399},
  issn = {1570-6478},
  doi = {10.1007/s12053-013-9227-5},
  urldate = {2024-07-02},
  abstract = {Feedback is a promising strategy for energy conservation, and many energy feedback products (i.e. technologies with hardware) and platforms (i.e. technologies without hardware) have emerged on the market in recent years. Past research suggests that the effectiveness of feedback varies based on distinct characteristics, and proposes categories to better understand and distinguish between these characteristics. A review of existing categories, however, identified the following issues: (1) current structures group feedback technologies into four (or fewer) categories, making device distinction and selection onerous; (2) current categories often ignore technical and psychological distinctions of interest to researchers; and (3) none provide a systematic description of the specific characteristics that vary by category. This paper presents a classification structure of feedback technology, derived theoretically from a review of relevant literature and empirically via content analysis of 196 devices. A taxonomy structure of feedback technology was derived based on the characteristics of hardware, communications, control, display, and data collection. The resulting taxonomy included the following nine categories: (1) information platform, (2) management platform, (3) appliance monitor, (4) load monitor, (5) grid display, (6) sensor display, (7) networked sensor, (8) closed management network, and (9) open management network. These categories are mutual exclusive and exhaustive of the identified technologies collected and are based on characteristics, which are both stable and important to feedback provision. It is hoped that this feedback classification will be of use to both researchers and practitioners trying to leverage the potential of these technologies for energy conservation.},
  langid = {english},
  keywords = {Conservation,Electricity,Feedback,Technology},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Karlin et al_2014_Energy feedback technology.pdf}
}

@inproceedings{kaushikModellingMetricViolations2024,
  title = {Modelling Metric Violations in (Geometric) Conceptual Spaces},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Kaushik, Karthikeya and Thompson, Bill},
  year = {2024},
  volume = {46},
  abstract = {Understanding how people represent similarity relations between concepts is one of the most fundamental problems in cognitive science, with implications for many theories of learning and reasoning. Human judgments of similarity violate basic metric assumptions, leading to effects such as judgment asymmetry and the triangle inequality. These effects have been difficult to capture with modern geometric representations of conceptual structure such as vector embeddings. Here we introduce a similarity function related to a feature-based view of concepts. We show how this function can be applied to geometric representations and that the resulting algorithm can account for classic judgment effects. Using representations extracted from a Large Language Model, we computed the predictions of this approach to similarity relations among a set of everyday concepts (world countries), and evaluated these predictions against human judgments of similarity in a behavioral experiment. The model's predictions correlate with human judgments. These results offer insight into human judgments of similarity relations and the design of algorithms that align with human reasoning.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kaushik_Thompson_Modelling metric violations in (geometric) conceptual spaces.pdf}
}

@article{kawakitaGromovWassersteinUnsupervised2024,
  title = {Gromov--{{Wasserstein}} Unsupervised Alignment Reveals Structural Correspondences between the Color Similarity Structures of Humans and Large Language Models},
  author = {Kawakita, Genji and {Zeleznikow-Johnston}, Ariel and Tsuchiya, Naotsugu and Oizumi, Masafumi},
  year = {2024},
  month = jul,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {15917},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-65604-1},
  urldate = {2024-07-17},
  abstract = {Large Language Models (LLMs), such as the General Pre-trained Transformer (GPT), have shown remarkable performance in various cognitive tasks. However, it remains unclear whether these models have the ability to accurately infer human perceptual representations. Previous research has addressed this question by quantifying correlations between similarity response patterns of humans and LLMs. Correlation provides a measure of similarity, but it relies pre-defined item labels and does not distinguish category- and item- level similarity, falling short of characterizing detailed structural correspondence between humans and LLMs. To assess their structural equivalence in more detail, we propose the use of an unsupervised alignment method based on Gromov--Wasserstein optimal transport (GWOT). GWOT allows for the comparison of similarity structures without relying on pre-defined label correspondences and can reveal fine-grained structural similarities and differences that may not be detected by simple correlation analysis. Using a large dataset of similarity judgments of 93 colors, we compared the color similarity structures of humans (color-neurotypical and color-atypical participants) and two GPT models (GPT-3.5 and GPT-4). Our results show that the similarity structure of color-neurotypical participants can be remarkably well aligned with that of GPT-4 and, to a lesser extent, to that of GPT-3.5. These results contribute to the methodological advancements of comparing LLMs with human perception, and highlight the potential of unsupervised alignment methods to reveal detailed structural correspondences.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computational neuroscience,Human behaviour},
  annotation = {https://osf.io/9xwr2/\\
https://oizumi-lab.github.io/GWTune/\\
https://github.com/oizumi-lab/GWTune},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kawakita et al_2024_Gromov–Wasserstein unsupervised alignment reveals structural correspondences.pdf}
}

@article{keExploringFrontiersLLMs2024,
  title = {Exploring the {{Frontiers}} of {{LLMs}} in {{Psychological Applications}}: {{A Comprehensive Review}}},
  author = {Ke, Luoma and Tong, Song and Cheng, Peng and Peng, Kaiping},
  year = {2024},
  abstract = {This paper explores the frontiers of large language models (LLMs) in psychology applications. Psychology has undergone several theoretical changes, and the current use of Artificial Intelligence (AI) and Machine Learning, particularly LLMs, promises to open up new research directions. We provide a detailed exploration of how LLMs like ChatGPT transform psychological research. It discusses the impact of LLMs across various branches of psychology, including cognitive and behavioral, clinical and counseling, educational and developmental, and social and cultural psychology, highlighting their potential to simulate aspects of human cognition and behavior. The paper delves into the capabilities of these models to emulate human-like text generation, offering innovative tools for literature review, hypothesis generation, experimental design, experimental subjects, data analysis, academic writing, and peer review in psychology. While LLMs are essential in advancing research methodologies in psychology, the paper also cautions about their technical and ethical challenges. There are issues like data privacy, the ethical implications of using LLMs in psychological research, and the need for a deeper understanding of these models' limitations. Researchers should responsibly use LLMs in psychological studies, adhering to ethical standards and considering the potential consequences of deploying these technologies in sensitive areas. Overall, the article provides a comprehensive overview of the current state of LLMs in psychology, exploring potential benefits and challenges. It serves as a call to action for researchers to leverage LLMs' advantages responsibly while addressing associated risks.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ke et al_Exploring the Frontiers of LLMs in Psychological Applications.pdf}
}

@misc{kelloEmergentMentalLexicon2024,
  title = {Emergent {{Mental Lexicon Functions}} in {{ChatGPT}}},
  author = {Kello, Chris and Bruna, Polyphony J.},
  year = {2024},
  month = may,
  doi = {10.31234/osf.io/gka2j},
  urldate = {2024-07-09},
  abstract = {Traditional theories of the human mental lexicon posit dedicated mechanisms of processing that develop as sustained functions of brain and mind. Large Language Models (LLMs) provide a new approach in which lexical functions emerge from the learning and processing of sequences in contexts. We prompted lexical functions in ChatGPT and compared numeric responses with averaged human data for a sample of 390 words for a range of lexical variables, some derived from corpus analyses and some from Likert ratings. ChatGPT responses were moderately to highly correlated with mean values, more so for GPT-4 versus GPT-3.5, and responses were sensitive to context and human inter-rater reliability. We argue that responses were not recalled from memorized training data but were instead soft-assembled from more general-purpose representations. Emergent functions in LLMs offer a new approach to modeling language and cognitive processes.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  annotation = {https://github.com/pjbruna/chatgpt-soft-assembly},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kello_Bruna_2024_Emergent Mental Lexicon Functions in ChatGPT.pdf}
}

@misc{kimAdaptiveCollaborationStrategy2024,
  title = {Adaptive {{Collaboration Strategy}} for {{LLMs}} in {{Medical Decision Making}}},
  author = {Kim, Yubin and Park, Chanwoo and Jeong, Hyewon and Chan, Yik Siu and Xu, Xuhai and McDuff, Daniel and Breazeal, Cynthia and Park, Hae Won},
  year = {2024},
  month = apr,
  number = {arXiv:2404.15155},
  eprint = {2404.15155},
  publisher = {arXiv},
  urldate = {2024-09-23},
  abstract = {Foundation models have become invaluable in advancing the medical field. Despite their promise, the strategic deployment of LLMs for effective utility in complex medical tasks remains an open question. Our novel framework, Medical Decision-making Agents (MDAgents) aims to address this gap by automatically assigning the effective collaboration structure for LLMs. Assigned solo or group collaboration structure is tailored to the complexity of the medical task at hand, emulating real-world medical decision making processes. We evaluate our framework and baseline methods with state-of-the-art LLMs across a suite of challenging medical benchmarks: MedQA, MedMCQA, PubMedQA, DDXPlus, PMC-VQA, Path-VQA, and MedVidQA, achieving the best performance in 5 out of 7 benchmarks that require an understanding of multi-modal medical reasoning. Ablation studies reveal that MDAgents excels in adapting the number of collaborating agents to optimize efficiency and accuracy, showcasing its robustness in diverse scenarios. We also explore the dynamics of group consensus, offering insights into how collaborative agents could behave in complex clinical team dynamics. Our code can be found at https://github.com/mitmedialab/MDAgents.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/mitmedialab/MDAgents},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kim et al_2024_Adaptive Collaboration Strategy for LLMs in Medical Decision Making.pdf}
}

@misc{kimLearningBeHomo2024,
  title = {Learning to Be {{Homo Economicus}}: {{Can}} an {{LLM Learn Preferences}} from {{Choice}}},
  shorttitle = {Learning to Be {{Homo Economicus}}},
  author = {Kim, Jeongbin and Kovach, Matthew and Lee, Kyu-Min and Shin, Euncheol and Tzavellas, Hector},
  year = {2024},
  month = jan,
  number = {arXiv:2401.07345},
  eprint = {2401.07345},
  primaryclass = {econ, q-fin},
  publisher = {arXiv},
  urldate = {2024-09-26},
  abstract = {This paper explores the use of Large Language Models (LLMs) as decision aids, with a focus on their ability to learn preferences and provide personalized recommendations. To establish a baseline, we replicate standard economic experiments on choice under risk (Choi et al., 2007) with GPT, one of the most prominent LLMs, prompted to respond as (i) a human decision maker or (ii) a recommendation system for customers. With these baselines established, GPT is provided with a sample set of choices and prompted to make recommendations based on the provided data. From the data generated by GPT, we identify its (revealed) preferences and explore its ability to learn from data. Our analysis yields three results. First, GPT's choices are consistent with (expected) utility maximization theory. Second, GPT can align its recommendations with people's risk aversion, by recommending less risky portfolios to more risk-averse decision makers, highlighting GPT's potential as a personalized decision aid. Third, however, GPT demonstrates limited alignment when it comes to disappointment aversion.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Economics - General Economics},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kim et al_2024_Learning to be Homo Economicus.pdf}
}

@article{kimuraVirtualTeamsSmart2024,
  title = {Virtual {{Teams}}: {{A Smart Literature Review}} of {{Four Decades}} of {{Research}}},
  shorttitle = {Virtual {{Teams}}},
  author = {Kimura, Takuma},
  year = {2024},
  journal = {Human Behavior and Emerging Technologies},
  volume = {2024},
  number = {1},
  pages = {8373370},
  issn = {2578-1863},
  doi = {10.1155/2024/8373370},
  urldate = {2024-09-05},
  abstract = {The increasing utilization of virtual teams---driven by advancements in information and communication technology and the forces of globalization---has spurred significant growth in both theoretical and empirical research. Based on the smart literature review framework, this study harnesses artificial intelligence techniques, specifically natural language processing and topic modeling, to extensively analyze the trends in virtual team research spanning the last four decades. Analyses of a dataset comprising 2,184 articles from Scopus-indexed journals discern 16 distinct topics, encompassing critical areas such as communication, leadership, and trust. The trajectory of research topics in this field has witnessed increasing diversification over time. Key subjects such as learning, communication, trust, and leadership have consistently maintained their presence among the ten most frequently explored topics. In contrast, emerging areas such as agile development and patient care have recently become some of the most prominent themes. Employing the state-of-the-art topic modeling technique, BERTopic, this study furnishes a comprehensive and dynamic panorama of the evolving landscape within virtual team research.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kimura_2024_Virtual Teams.pdf;/Users/thomasgorman/Zotero/storage/9XRYLJNL/8373370.html}
}

@misc{kipnisSparseBenchmarkMeasure2024,
  title = {A {{Sparse Benchmark}} to {{Measure General Ability}} in {{Large Language Models}}},
  author = {Kipnis, Alex and Voudouris, Konstantinos and Buschoff, Luca M. Schulze and Schulz, Eric},
  year = {2024},
  month = jul,
  number = {arXiv:2407.12844},
  eprint = {2407.12844},
  publisher = {arXiv},
  urldate = {2024-08-11},
  abstract = {Large Language Models (LLMs) vary in their abilities on a range of tasks. Initiatives such as the Open LLM Leaderboard aim to quantify these differences with several large benchmarks (sets of test items to which an LLM can respond either correctly or incorrectly). However, high correlations within and between benchmark scores suggest that (1) there exists a small set of common underlying abilities that these benchmarks measure, and (2) items tap into redundant information and the benchmarks may thus be considerably compressed. We use data from n {$>$} 5000 LLMs to identify the most informative items of six benchmarks, ARC, GSM8K, HellaSwag, MMLU, TruthfulQA and WinoGrande (with d = 28, 632 items in total). From them we distill a sparse benchmark, metabench, that has less than 3\% of the original size of all six benchmarks combined. This new sparse benchmark goes beyond point scores by yielding estimators of the underlying benchmark-specific abilities. We show that these estimators (1) can be used to reconstruct each original individual benchmark score with, on average, 1.5\% root mean square error (RMSE), (2) reconstruct the original total score with 0.8\% RMSE, and (3) have a single underlying common factor whose Spearman correlation with the total score is r = 0.93.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  annotation = {https://github.com/adkipnis/metabench},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kipnis et al_2024_A Sparse Benchmark to Measure General Ability in Large Language Models.pdf}
}

@article{kjellRatingScalesTargeted2024,
  title = {Beyond Rating Scales: {{With}} Targeted Evaluation, Large Language Models Are Poised for Psychological Assessment},
  shorttitle = {Beyond Rating Scales},
  author = {Kjell, Oscar N. E. and Kjell, Katarina and Schwartz, H. Andrew},
  year = {2024},
  month = mar,
  journal = {Psychiatry Research},
  volume = {333},
  pages = {115667},
  issn = {0165-1781},
  doi = {10.1016/j.psychres.2023.115667},
  urldate = {2024-05-26},
  abstract = {In this narrative review, we survey recent empirical evaluations of AI-based language assessments and present a case for the technology of large language models to be poised for changing standardized psychological assessment. Artificial intelligence has been undergoing a purported ``paradigm shift'' initiated by new machine learning models, large language models (e.g., BERT, LAMMA, and that behind ChatGPT). These models have led to unprecedented accuracy over most computerized language processing tasks, from web searches to automatic machine translation and question answering, while their dialogue-based forms, like ChatGPT have captured the interest of over a million users. The success of the large language model is mostly attributed to its capability to numerically represent words in their context, long a weakness of previous attempts to automate psychological assessment from language. While potential applications for automated therapy are beginning to be studied on the heels of chatGPT's success, here we present evidence that suggests, with thorough validation of targeted deployment scenarios, that AI's newest technology can move mental health assessment away from rating scales and to instead use how people naturally communicate, in language.},
  keywords = {Artificial intelligence,Assessment,Large language models,Psychology,Transformers},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kjell et al_2024_Beyond rating scales.pdf;/Users/thomasgorman/Zotero/storage/EGXAAZ4T/S0165178123006170.html}
}

@article{kjellTextpackageRpackageAnalyzing2023,
  title = {The Text-Package: {{An R-package}} for Analyzing and Visualizing Human Language Using Natural Language Processing and Transformers.},
  shorttitle = {The Text-Package},
  author = {Kjell, Oscar and Giorgi, Salvatore and Schwartz, H. Andrew},
  year = {2023},
  month = may,
  journal = {Psychological Methods},
  issn = {1939-1463, 1082-989X},
  doi = {10.1037/met0000542},
  urldate = {2023-12-31},
  abstract = {The language that individuals use for expressing themselves contains rich psychological information. Recent significant advances in Natural Language Processing (NLP) and Deep Learning (DL), namely transformers, have resulted in large performance gains in tasks related to understanding natural language. However, these state-of-the-art methods have not yet been made easily accessible for psychology researchers, nor designed to be optimal for human-level analyses. This tutorial introduces text (https://r-text.org/), a new R-package for analyzing and visualizing human language using transformers, the latest techniques from NLP and DL. The text-package is both a modular solution for accessing state-of-the-art language models and an end-to-end solution catered for human-level analyses. Hence, text provides user-friendly functions tailored to test hypotheses in social sciences for both relatively small and large data sets. The tutorial describes methods for analyzing text, providing functions with reliable defaults that can be used off-the-shelf as well as providing a framework for the advanced users to build on for novel pipelines. The reader learns about three core methods: (1) textEmbed(): to transform text to modern transformer-based word embeddings; (2) textTrain() and textPredict(): to train predictive models with embeddings as input, and use the models to predict from; (3) textSimilarity() and textDistance(): to compute semantic similarity/distance scores between texts. The reader also learns about two extended methods: (1) textProjection()/textProjectionPlot() and (2) textCentrality()/ textCentralityPlot(): to examine and visualize text within the embedding space.},
  langid = {english},
  annotation = {https://cran.r-project.org/web/packages/text/vignettes/text.html},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kjell et al_2023_The text-package.pdf}
}

@article{koehlMeasuringLatentTrust2023a,
  title = {Measuring {{Latent Trust Patterns}} in {{Large Language Models}} in the {{Context}} of {{Human-AI Teaming}}},
  author = {Koehl, Derek and Vangsness, Lisa},
  year = {2023},
  month = oct,
  journal = {Proceedings of the Human Factors and Ergonomics Society Annual Meeting},
  volume = {67},
  doi = {10.1177/21695067231192869},
  abstract = {Qualitative self-report methods such as think-aloud procedures and open-ended response questions can provide valuable data to human factors research. These measures come with analytic weaknesses, such as researcher bias, intra- and inter-rater reliability concerns, and time-consuming coding protocols. A possible solution exists in the latent semantic patterns that exist in machine learning large language models. These semantic patterns could be used to analyze qualitative responses. This exploratory research compared the statistical quality of automated sentence coding using large language models to the benchmarks of self-report and behavioral measures within the context of trust in automation research. The results indicated that three large language models show promise as tools for analyzing qualitative responses. The study also provides insight on minimum sample sizes for model creation and offers recommendations for further validating the robustness of large language models as research tools.},
  annotation = {https://osf.io/uw5td/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Koehl_Vangsness_2023_Measuring Latent Trust Patterns in Large Language Models in the Context of2.pdf}
}

@article{kolekarHumanlikeDrivingBehaviour2020,
  title = {Human-like Driving Behaviour Emerges from a Risk-Based Driver Model},
  author = {Kolekar, Sarvesh and {de Winter}, Joost and Abbink, David},
  year = {2020},
  month = sep,
  journal = {Nature Communications},
  volume = {11},
  number = {1},
  pages = {4850},
  publisher = {Nature Publishing Group},
  issn = {2041-1723},
  doi = {10.1038/s41467-020-18353-4},
  urldate = {2022-03-08},
  abstract = {Current driving behaviour models are designed for specific scenarios, such as curve driving, obstacle avoidance, car-following, or overtaking. However, humans can drive in diverse scenarios. Can we find an underlying principle from which driving behaviour in different scenarios emerges? We propose the Driver's Risk Field (DRF), a two-dimensional field that represents the driver's belief about the probability of an event occurring. The DRF, when multiplied with the consequence of the event, provides an estimate of the driver's perceived risk. Through human-in-the-loop and computer simulations, we show that human-like driving behaviour emerges when the DRF is coupled to a controller that maintains the perceived risk below a threshold-level. The DRF model predictions concur with driving behaviour reported in literature for seven different scenarios (curve radii, lane widths, obstacle avoidance, roadside furniture, car-following, overtaking, oncoming traffic). We conclude that our generalizable DRF model is scientifically satisfying and has applications in automated vehicles.},
  copyright = {2020 The Author(s)},
  langid = {english},
  keywords = {Engineering,Psychology and behaviour,Social sciences},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Kolekar et al_2020_Human-like driving behaviour emerges from a risk-based driver model.pdf;/Users/thomasgorman/Zotero/storage/KYXMP44K/s41467-020-18353-4.html}
}

@inproceedings{kolekarHumanlikeSteeringModel2017,
  title = {A Human-like Steering Model: {{Sensitive}} to Uncertainty in the Environment},
  shorttitle = {A Human-like Steering Model},
  booktitle = {2017 {{IEEE International Conference}} on {{Systems}}, {{Man}}, and {{Cybernetics}} ({{SMC}})},
  author = {Kolekar, Sarvesh and {de Winter}, Joost and Abbink, David},
  year = {2017},
  month = oct,
  pages = {1487--1492},
  doi = {10.1109/SMC.2017.8122824},
  abstract = {The interaction between a human driver and an automated driving system may improve when the automation is designed in such a way that it behaves in a human-like manner. This paper introduces a human-like steering model, in which the driver adapts to the risk due to uncertainty in the environment. Current steering models take a risk-neutral approach, while the fields of economics and sensorimotor control suggest that humans exhibit risk-sensitive behavior. The proposed model uses a risk-sensitive optimal feedback control structure to predict steering behavior. The paper studies the effect of the risk-sensitivity parameter and compares the prediction of the risk-neutral and risk-sensitive controllers in a simulated abstraction of two scenarios: (a) driving while being subjected to lateral wind gusts and (b) overtaking an unpredictably swerving car. The simulation results show that the risk-sensitive model adapts to the uncertainty in the environment. Experimental data will be needed to validate the predictions of our model.},
  keywords = {Adaptation models,Cost function,Predictive models,Roads,Trajectory,Uncertainty,Vehicles},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Kolekar et al_2017_A human-like steering model.pdf}
}

@article{koplenigLanguagesMoreSpeakers2023,
  title = {Languages with More Speakers Tend to Be Harder to (Machine-)Learn},
  author = {Koplenig, Alexander and Wolfer, Sascha},
  year = {2023},
  month = oct,
  journal = {Scientific Reports},
  volume = {13},
  number = {1},
  pages = {18521},
  issn = {2045-2322},
  doi = {10.1038/s41598-023-45373-z},
  urldate = {2024-01-05},
  abstract = {Computational language models (LMs), most notably exemplified by the widespread success of OpenAI's ChatGPT chatbot, show impressive performance on a wide range of linguistic tasks, thus providing cognitive science and linguistics with a computational working model to empirically study different aspects of human language. Here, we use LMs to test the hypothesis that languages with more speakers tend to be easier to learn. In two experiments, we train several LMs---ranging from very simple n-gram models to state-of-the-art deep neural networks---on written cross-linguistic corpus data covering 1293 different languages and statistically estimate learning difficulty. Using a variety of quantitative methods and machine learning techniques to account for phylogenetic relatedness and geographical proximity of languages, we show that there is robust evidence for a relationship between learning difficulty and speaker population size. However, contrary to expectations derived from previous research, our results suggest that languages with more speakers tend to be harder to learn.},
  copyright = {2023 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Psychology},
  annotation = {https://osf.io/sa9x2/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Koplenig_Wolfer_2023_Languages with more speakers tend to be harder to (machine-)learn.pdf}
}

@misc{koujalgiHowMeasureHumanAI2024,
  title = {How to {{Measure Human-AI Prediction Accuracy}} in {{Explainable AI Systems}}},
  author = {Koujalgi, Sujay and Anderson, Andrew and Adenuga, Iyadunni and Soneji, Shikha and Dikkala, Rupika and Nader, Teresita Guzman and Soccio, Leo and Panda, Sourav and Das, Rupak Kumar and Burnett, Margaret and Dodge, Jonathan},
  year = {2024},
  month = aug,
  number = {arXiv:2409.00069},
  eprint = {2409.00069},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-07},
  abstract = {Assessing an AI system's behavior-particularly in Explainable AI Systems-is sometimes done empirically, by measuring people's abilities to predict the agent's next move-but how to perform such measurements? In empirical studies with humans, an obvious approach is to frame the task as binary (i.e., prediction is either right or wrong), but this does not scale. As output spaces increase, so do floor effects, because the ratio of right answers to wrong answers quickly becomes very small. The crux of the problem is that the binary framing is failing to capture the nuances of the different degrees of "wrongness." To address this, we begin by proposing three mathematical bases upon which to measure "partial wrongness." We then uses these bases to perform two analyses on sequential decision-making domains: the first is an in-lab study with 86 participants on a size-36 action space; the second is a re-analysis of a prior study on a size-4 action space. Other researchers adopting our operationalization of the prediction task and analysis methodology will improve the rigor of user studies conducted with that task, which is particularly important when the domain features a large output space.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,D.2.8},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Koujalgi et al_2024_How to Measure Human-AI Prediction Accuracy in Explainable AI Systems.pdf}
}

@misc{kujanpaaChallengesDataDrivenSimulation2024,
  title = {Challenges of {{Data-Driven Simulation}} of {{Diverse}} and {{Consistent Human Driving Behaviors}}},
  author = {Kujanp{\"a}{\"a}, Kalle and Baimukashev, Daulet and Zhu, Shibei and Azam, Shoaib and Munir, Farzeen and Alcan, Gokhan and Kyrki, Ville},
  year = {2024},
  month = jan,
  number = {arXiv:2401.03236},
  eprint = {2401.03236},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-15},
  abstract = {Building simulation environments for developing and testing autonomous vehicles necessitates that the simulators accurately model the statistical realism of the real-world environment, including the interaction with other vehicles driven by human drivers. To address this requirement, an accurate human behavior model is essential to incorporate the diversity and consistency of human driving behavior. We propose a mathematical framework for designing a data-driven simulation model that simulates human driving behavior more realistically than the currently used physics-based simulation models. Experiments conducted using the NGSIM dataset validate our hypothesis regarding the necessity of considering the complexity, diversity, and consistency of human driving behavior when aiming to develop realistic simulators.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Robotics},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kujanpää et al_2024_Challenges of Data-Driven Simulation of Diverse and Consistent Human Driving.pdf}
}

@article{kumarComparingAbstractionHumans,
  title = {Comparing {{Abstraction}} in {{Humans}} and {{Machines Using Multimodal Serial Reproduction}}},
  author = {Kumar, Sreejan and Marjieh, Raja and Zhang, Byron and Campbell, Declan and Hu, Michael Y and Bhatt, Umang and Lake, Brenden and Griffiths, Thomas L},
  abstract = {Humans extract useful abstractions of the world from noisy sensory data. Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions. Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language. To investigate the effect language on the formation of abstractions, we implement a novel multimodal serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa. We ran unimodal and multimodal chains with both humans and GPT-4 and find that adding language as a modality has a larger effect on human reproductions than GPT-4's. This suggests human visual and linguistic representations are more dissociable than those of GPT-4.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kumar et al_Comparing Abstraction in Humans and Machines Using Multimodal Serial.pdf}
}

@misc{kumarComparingAbstractionHumans2024,
  title = {Comparing {{Abstraction}} in {{Humans}} and {{Large Language Models Using Multimodal Serial Reproduction}}},
  author = {Kumar, Sreejan and Marjieh, Raja and Zhang, Byron and Campbell, Declan and Hu, Michael Y. and Bhatt, Umang and Lake, Brenden and Griffiths, Thomas L.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.03618},
  eprint = {2402.03618},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-02-10},
  abstract = {Humans extract useful abstractions of the world from noisy sensory data. Serial reproduction allows us to study how people construe the world through a paradigm similar to the game of telephone, where one person observes a stimulus and reproduces it for the next to form a chain of reproductions. Past serial reproduction experiments typically employ a single sensory modality, but humans often communicate abstractions of the world to each other through language. To investigate the effect language on the formation of abstractions, we implement a novel multimodal serial reproduction framework by asking people who receive a visual stimulus to reproduce it in a linguistic format, and vice versa. We ran unimodal and multimodal chains with both humans and GPT-4 and find that adding language as a modality has a larger effect on human reproductions than GPT-4's. This suggests human visual and linguistic representations are more dissociable than those of GPT-4.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Kumar et al_2024_Comparing Abstraction in Humans and Large Language Models Using Multimodal.pdf;/Users/thomasgorman/Zotero/storage/PD75NM7V/2402.html}
}

@misc{kumarRankingEntitiesConceptual2024,
  title = {Ranking {{Entities}} along {{Conceptual Space Dimensions}} with {{LLMs}}: {{An Analysis}} of {{Fine-Tuning Strategies}}},
  shorttitle = {Ranking {{Entities}} along {{Conceptual Space Dimensions}} with {{LLMs}}},
  author = {Kumar, Nitesh and Chatterjee, Usashi and Schockaert, Steven},
  year = {2024},
  month = feb,
  number = {arXiv:2402.15337},
  eprint = {2402.15337},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-18},
  abstract = {Conceptual spaces represent entities in terms of their primitive semantic features. Such representations are highly valuable but they are notoriously difficult to learn, especially when it comes to modelling perceptual and subjective features. Distilling conceptual spaces from Large Language Models (LLMs) has recently emerged as a promising strategy. However, existing work has been limited to probing pre-trained LLMs using relatively simple zero-shot strategies. We focus in particular on the task of ranking entities according to a given conceptual space dimension. Unfortunately, we cannot directly fine-tune LLMs on this task, because ground truth rankings for conceptual space dimensions are rare. We therefore use more readily available features as training data and analyse whether the ranking capabilities of the resulting models transfer to perceptual and subjective features. We find that this is indeed the case, to some extent, but having perceptual and subjective features in the training data seems essential for achieving the best results. We furthermore find that pointwise ranking strategies are competitive against pairwise approaches, in defiance of common wisdom.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Kumar et al_2024_Ranking Entities along Conceptual Space Dimensions with LLMs.pdf;/Users/thomasgorman/Zotero/storage/MMYCKDQZ/2402.html}
}

@article{labuzInformationApocalypseOverblown,
  title = {Information Apocalypse or Overblown Fears---What {{AI}} Mis- and Disinformation Is All about? {{Shifting}} Away from Technology toward Human Reactions},
  shorttitle = {Information Apocalypse or Overblown Fears---What {{AI}} Mis- and Disinformation Is All About?},
  author = {{\L}abuz, Mateusz and Nehring, Christopher},
  journal = {Politics \& Policy},
  volume = {n/a},
  number = {n/a},
  issn = {1747-1346},
  doi = {10.1111/polp.12617},
  urldate = {2024-07-23},
  abstract = {The rise of generative artificial intelligence (AI) has ignited a debate about its effects on the mis- and disinformation landscape. The doomsday scenarios of epistemic and information apocalypse presented for many years are recently being questioned, and the previous fears are called ``overblown.'' These phenomena are analyzed mostly through the factors of quantity and quality of AI-powered content and the potential for personalization possessed by AI. We argue that using quantitative arguments carries a high risk of underestimating the threat, especially in the context of the so-called detection challenge. We point out that this discourse is affected by the narrow conceptualization of how we understand quantity, quality, and personalization with regard to AI. In our opinion, apocalyptic visions are speculative in nature, difficult to quantify, and carry signs of a self-fulfilling prophecy, but disregarding risks hinders appropriate countermeasures against AI-powered dis- and misinformation, which adversely affects policy-making activities. We propose a paradigm shift to focus more on social reactions to technology rather than technological attributes. By expanding the understanding of the analyzed phenomena, we indicate that the potential of AI is both overestimated and underestimated and above all---still misunderstood. Related Articles Norman, Emma R., and Rafael Delfin. 2012. ``Wizards under Uncertainty: Cognitive Biases, Threat Assessment, and Misjudgments in Policy Making.'' Politics \& Policy 40(3): 369--402. https://doi.org/10.1111/j.1747-1346.2012.00356.x. Robles, Pedro, and Daniel J. Mallinson. 2023. ``Catching Up with AI: Pushing Toward a Cohesive Governance Framework.'' Politics \& Policy 51(3): 355--72. https://doi.org/10.1111/polp.12529. Veloso Meireles, Adriana. 2024. ``Digital Rights in Perspective: The Evolution of the Debate in the Internet Governance Forum.'' Politics \& Policy 52(1): 12--32. https://doi.org/10.1111/polp.12571.},
  langid = {english},
  keywords = {,AI,AI-powered mis-information,artificial intelligence,chatbots,deepfakes,deepfakes and legal evidence,desinformacion,detection software,fake news,human psychological reactions to AI,IA,informacion,informacion erronea basada en IA,information apocalypse,information manipulation,inteligencia artificial,mass-scale disinformation,personalized political campaigning,politica tecnologica,reacciones humanas,risk,social media,strategic interpretation of AI,technology policy,trust,U.S. 2024 election,uncertainty},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Łabuz_Nehring_Information apocalypse or overblown fears—what AI mis- and disinformation is.pdf;/Users/thomasgorman/Zotero/storage/J8QJURJB/polp.html}
}

@misc{lampinenCanLanguageModels2022,
  title = {Can Language Models Learn from Explanations in Context?},
  author = {Lampinen, Andrew K. and Dasgupta, Ishita and Chan, Stephanie C. Y. and Matthewson, Kory and Tessler, Michael Henry and Creswell, Antonia and McClelland, James L. and Wang, Jane X. and Hill, Felix},
  year = {2022},
  month = oct,
  number = {arXiv:2204.02329},
  eprint = {2204.02329},
  publisher = {arXiv},
  urldate = {2023-05-29},
  abstract = {Language Models (LMs) can perform new tasks by adapting to a few in-context examples. For humans, explanations that connect examples to task principles can improve learning. We therefore investigate whether explanations of few-shot examples can help LMs. We annotate questions from 40 challenging tasks with answer explanations, and various matched control explanations. We evaluate how different types of explanations, instructions, and controls affect zero- and few-shot performance. We analyze these results using statistical multilevel modeling techniques that account for the nested dependencies among conditions, tasks, prompts, and models. We find that explanations can improve performance -- even without tuning. Furthermore, explanations hand-tuned for performance on a small validation set offer substantially larger benefits, and building a prompt by selecting examples and explanations together substantially improves performance over selecting examples alone. Finally, even untuned explanations outperform carefully matched controls, suggesting that the benefits are due to the link between an example and its explanation, rather than lower-level features. However, only large models benefit. In summary, explanations can support the in-context learning of large LMs on challenging tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://console.cloud.google.com/storage/browser/dm\_some\_explanations\_for\_bigbench\_tasks;tab=objects?prefix=\&forceOnObjectsSortingFiltering=false\&pageState=(\%22StorageObjectListTable\%22:(\%22f\%22:\%22\%255B\%255D\%22))},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Lampinen et al_2022_Can language models learn from explanations in context.pdf;/Users/thomasgorman/Zotero/storage/QWETIKHG/2204.html}
}

@article{lampinenLanguageModelsHumans2024,
  title = {Language Models, like Humans, Show Content Effects on Reasoning Tasks},
  author = {Lampinen, Andrew K and Dasgupta, Ishita and Chan, Stephanie C Y and Sheahan, Hannah R and Creswell, Antonia and Kumaran, Dharshan and McClelland, James L and Hill, Felix},
  year = {2024},
  month = jul,
  journal = {PNAS Nexus},
  volume = {3},
  number = {7},
  pages = {pgae233},
  issn = {2752-6542},
  doi = {10.1093/pnasnexus/pgae233},
  urldate = {2024-07-20},
  abstract = {Abstract reasoning is a key ability for an intelligent system. Large language models (LMs) achieve above-chance performance on abstract reasoning tasks but exhibit many imperfections. However, human abstract reasoning is also imperfect. Human reasoning is affected by our real-world knowledge and beliefs, and shows notable ``content effects''; humans reason more reliably when the semantic content of a problem supports the correct logical inferences. These content-entangled reasoning patterns are central to debates about the fundamental nature of human intelligence. Here, we investigate whether language models---whose prior expectations capture some aspects of human knowledge---similarly mix content into their answers to logic problems. We explored this question across three logical reasoning tasks: natural language inference, judging the logical validity of syllogisms, and the Wason selection task. We evaluate state of the art LMs, as well as humans, and find that the LMs reflect many of the same qualitative human patterns on these tasks---like humans, models answer more accurately when the semantic content of a task supports the logical inferences. These parallels are reflected in accuracy patterns, and in some lower-level features like the relationship between LM confidence over possible answers and human response times. However, in some cases the humans and models behave differently---particularly on the Wason task, where humans perform much worse than large models, and exhibit a distinct error pattern. Our findings have implications for understanding possible contributors to these human cognitive effects, as well as the factors that influence language model performance.},
  annotation = {https://oup.silverchair-cdn.com/oup/backfile/Content\_public/Journal/pnasnexus/3/7/10.1093\_pnasnexus\_pgae233/4/pgae233\_supplementary\_data.pdf?Expires=1724432225\&Signature=JrWZ-i4I9UolQX1VYamNKhrTxksjUVCrFnxy-{\textasciitilde}IGj7qD{\textasciitilde}-YVaKFqTDIAGuG6VbZsruWoY0CaWfBKZ7VHr9d7wqoEoIXQ5OVco7A0OrgzIcN3v7ZjAPcpJf6NurOdRhbDTyTRqcCew7HNveKmQIGpktvJVpiLw9PqSZWp3NMs9L-t3iU80w5AuEaMet8f4SLDt80VLDXUAPbCkUOHs-0rdBB8OrXPV2p5WxSFT388aeAs{\textasciitilde}KRNhtoponCjdvwsO0Q6LDbgPtQrKZsLp4ebqXqGrukPCWxSsRvs{\textasciitilde}HqefodRzCRpI-NZdSCfQGRCDXS5933VYnaQvBvw5m6DY6X6OSjUGA\_\_\&Key-Pair-Id=APKAIE5G5CRDK6RD3PGA},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lampinen et al_2024_Language models, like humans, show content effects on reasoning tasks.pdf;/Users/thomasgorman/Zotero/storage/WALQY3D9/7712372.html}
}

@misc{lampinenPassiveLearningActive2023,
  title = {Passive Learning of Active Causal Strategies in Agents and Language Models},
  author = {Lampinen, Andrew Kyle and Chan, Stephanie C. Y. and Dasgupta, Ishita and Nam, Andrew J. and Wang, Jane X.},
  year = {2023},
  month = may,
  number = {arXiv:2305.16183},
  eprint = {2305.16183},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-30},
  abstract = {What can be learned about causality and experimentation from passive data? This question is salient given recent successes of passively-trained language models in interactive domains such as tool use. Passive learning is inherently limited. However, we show that purely passive learning can in fact allow an agent to learn generalizable strategies for determining and using causal structures, as long as the agent can intervene at test time. We formally illustrate that learning a strategy of first experimenting, then seeking goals, can allow generalization from passive learning in principle. We then show empirically that agents trained via imitation on expert data can indeed generalize at test time to infer and use causal links which are never present in the training data; these agents can also generalize experimentation strategies to novel variable sets never observed in training. We then show that strategies for causal intervention and exploitation can be generalized from passive data even in a more complex environment with high-dimensional observations, with the support of natural language explanations. Explanations can even allow passive learners to generalize out-of-distribution from perfectly-confounded training data. Finally, we show that language models, trained only on passive next-word prediction, can generalize causal intervention strategies from a few-shot prompt containing examples of experimentation, together with explanations and reasoning. These results highlight the surprising power of passive learning of active causal strategies, and may help to understand the behaviors and capabilities of language models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lampinen et al_2023_Passive learning of active causal strategies in agents and language models.pdf;/Users/thomasgorman/Zotero/storage/QSVP3YGC/2305.html}
}

@phdthesis{lancasterWeTrainAI2024,
  title = {We {{Train AI}}, {{Why Not Humans}}, {{Too}}? {{An Exploration}} of {{Human-AI Team Training}} for {{Future Workplace Viability}}},
  author = {Lancaster, Caitlin M},
  year = {2024},
  abstract = {The rapid integration of Artificial Intelligence (AI) in the workforce is quickly transforming the nature of workforce teams and collaborative work, leading to the emergence of Human-AI Teams (HATs). This dissertation explores the complexities of HATs, focusing on their unique dynamics and the necessity for specialized integration strategies, namely HAT team training. Unlike traditional teams, HATs combine human creativity, intuition, and decision-making with AI's computational prowess and data-processing capabilities, offering significant opportunities for innovation and efficiency in the workplace. Central to the effective functioning of HATs is the need to align human expectations with AI's capabilities and bridge knowledge gaps between human and AI teammates. This dissertation identifies key challenges in integrating AI into teams, including developing shared mental models, addressing skill-set limitations, and overcoming negative perceptions of AI. These aspects necessitate specialized considerations for their successful integration into HATs. In traditional human-human teams, training is a valuable tool for supporting team preparation and integration of any kind, pivotal to the future success of any team. Consequently, examining and designing training for HATs becomes a logical area for exploration to support effective integration. However, existing efforts in this domain have been scant and often limited in scope and under the assumption that human-human teaming principles can be directly applied to HATs. Such an approach overlooks AI's nuanced role as a teammate as opposed to a mere technological tool. This oversight has led to a substantial gap in understanding how team training strategies, successful in human-human contexts, can be effectively adapted and tailored for HATs; further, this has limited the direct design and evaluation of HAT-specific training programs. The lack of a thorough exploration of HAT-specific training approaches makes the seamless integration of HATs into the workforce more daunting and complex. Given these warrants, this dissertation explores the journey towards these aims, helping workforces to leverage the benefits of HATs better and examining how Study 3 evaluates the effectiveness of HAT team training in an applied organizational setting seeking to integrate HATs into workflows. Building on insights from the first two studies--- the need for HAT team training emphasizing teamwork and AI understanding (i.e., Study 1) and the effectiveness of tailored cross-training in enhancing knowledge and attitudes toward HATs (i.e., Study 2)---this study tested novel HAT team training strategies. The HAT Integration Training focused on AI situational awareness, effective communication, and understanding human-AI interdependencies. Using a mixed-methods embedded design, the study employed focus groups, observations, and pre-post-training comparisons to evaluate the training outcomes when working with an internal AI teammate. It demonstrated gains in objective and subjective performance and key changes in workplace trust facilitated by critical awareness and reframed expectations. Clear boundary setting, practical and actionable communication strategies, and positional modeling and examples contributed to the training's effectiveness. Additionally, the study revealed a clear need and demand for greater HAT training related to their use cases, secure and ethical AI use, and alignment with organizational resources. This was paired with an increased desire to adopt AI teammates and support measures (e.g., training) for organizational, professional, and personal purposes. This study provides valuable insights into the applicability of HAT team training methods for AI integration and bridges the gap between theoretical research and practical implementation of HATs. These three studies fill a currently understudied research gap in human-AI teaming, focusing on how HAT team training can set appropriate expectations and support an understanding of AI that can work to ease the integration of HATs in the workforce. Each study in this disseration offers recommendations for investigating and instituting HAT team training, addressing crucial gaps in the current literature and practice. This dissertation's contributions are poised to address a need for more research in the HAT field, offering approaches for future research and practical applications. As AI continues to reshape the workplace, the insights from this research will be instrumental in guiding organizations toward a future where human and AI collaboration can be effectively supported and molded for its most significant impact.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lancaster_We Train AI, Why Not Humans, Too.pdf}
}

@article{lappiGazeStrategiesDriving2022,
  title = {Gaze {{Strategies}} in {{Driving}}--{{An Ecological Approach}}},
  author = {Lappi, Otto},
  year = {2022},
  month = mar,
  journal = {Frontiers in Psychology},
  volume = {13},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2022.821440},
  urldate = {2024-09-16},
  abstract = {{$<$}p{$>$}Human performance in natural environments is deeply impressive, and still much beyond current AI. Experimental techniques, such as eye tracking, may be useful to understand the cognitive basis of this performance, and ``the human advantage.'' Driving is domain where these techniques may deployed, in tasks ranging from rigorously controlled laboratory settings through high-fidelity simulations to naturalistic experiments in the wild. This research has revealed robust patterns that can be reliably identified and replicated in the field and reproduced in the lab. The purpose of this review is to cover the basics of what is known about these gaze behaviors, and some of their implications for understanding visually guided steering. The phenomena reviewed will be of interest to those working on any domain where visual guidance and control with similar task demands is involved (e.g., many sports). The paper is intended to be accessible to the non-specialist, without oversimplifying the complexity of real-world visual behavior. The literature reviewed will provide an information base useful for researchers working on oculomotor behaviors and physiology in the lab who wish to extend their research into more naturalistic locomotor tasks, or researchers in more applied fields (sports, transportation) who wish to bring aspects of the real-world ecology under experimental scrutiny. Part of a Research Topic on Gaze Strategies in Closed Self-paced tasks, this aspect of the driving task is discussed. It is in particular emphasized why it is important to carefully separate the visual strategies driving (quite closed and self-paced) from visual behaviors relevant to other forms of driver behavior (an open-ended menagerie of behaviors). There is always a balance to strike between ecological complexity and experimental control. One way to reconcile these demands is to look for natural, real-world tasks and behavior that are rich enough to be interesting yet sufficiently constrained and well-understood to be replicated in simulators and the lab. This {$<$}italic{$>$}ecological approach{$<$}/italic{$>$} to driving as a model behavior and the way the connection between ``lab'' and ``real world'' can be spanned in this research is of interest to anyone keen to develop more ecologically representative designs for studying human gaze behavior.{$<$}/p{$>$}},
  langid = {english},
  keywords = {ecological psychology,Guiding fixations,locomotor control,look-ahead fixations,pursuit eye movements,visual guidance},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lappi_2022_Gaze Strategies in Driving–An Ecological Approach.pdf}
}

@misc{ledermanAreLanguageModels2024,
  title = {Are {{Language Models More Like Libraries}} or {{Like Librarians}}? {{Bibliotechnism}}, the {{Novel Reference Problem}}, and the {{Attitudes}} of {{LLMs}}},
  shorttitle = {Are {{Language Models More Like Libraries}} or {{Like Librarians}}?},
  author = {Lederman, Harvey and Mahowald, Kyle},
  year = {2024},
  month = feb,
  number = {arXiv:2401.04854},
  eprint = {2401.04854},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-23},
  abstract = {Are LLMs cultural technologies like photocopiers or printing presses, which transmit information but cannot create new content? A challenge for this idea, which we call bibliotechnism, is that LLMs often generate entirely novel text. We begin (Part I) with a sustained defense of bibliotechnism against this challenge showing how even entirely novel text may be meaningful only in a derivative sense, and arguing that, in particular, much novel text generated by LLMs is only derivatively meaningful. But we argue (Part II) that bibliotechnism faces a different, novel challenge, stemming from examples in which LLMs generate ``novel reference'', using novel names to refer to novel entities. Such examples could be smoothly explained if LLMs were not cultural technologies but possessed a limited form of agency (beliefs, desires, and intentions). According to interpretationism in the philosophy of mind, a system has beliefs, desires and intentions if and only if its behavior is well explained by the hypothesis that it has such states. So, according to interpretationism, cases of novel reference provide evidence that LLMs have beliefs, desires, and intentions. Given that interpretationism is a live hypothesis about the nature of these states, we suggest that cases of novel reference provide evidence that LLMs do have beliefs, desires, and intentions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lederman_Mahowald_2024_Are Language Models More Like Libraries or Like Librarians.pdf}
}

@misc{leeEffectGroupStatus2024,
  title = {The {{Effect}} of {{Group Status}} on the {{Variability}} of {{Group Representations}} in {{LLM-generated Text}}},
  author = {Lee, Messi H. J. and Montgomery, Jacob M. and Lai, Calvin K.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.08495},
  eprint = {2401.08495},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-03-03},
  abstract = {Large Language Models (LLMs) have become pervasive in everyday life, yet their inner workings remain opaque. While scholarly efforts have demonstrated LLMs' propensity to reproduce biases in their training data, they have primarily focused on the association of social groups with stereotypic attributes. In this paper, we extend this line of inquiry to investigate a bias akin to the social-psychological phenomenon where socially dominant groups are perceived to be less homogeneous than socially subordinate groups as it is reproduced by LLMs. We had ChatGPT, a state-of-the-art LLM, generate a diversity of texts about intersectional group identities and compared text homogeneity. We consistently find that LLMs portray African, Asian, and Hispanic Americans as more homogeneous than White Americans. They also portray women as more homogeneous than men, but these differences are small. Finally, we find that the effect of gender differs across racial/ethnic groups such that the effect of gender is consistent within African and Hispanic Americans but not within Asian and White Americans. We speculate possible sources of this bias in LLMs and posit that the bias has the potential to amplify biases in future LLM training and to reinforce stereotypes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lee et al_2024_The Effect of Group Status on the Variability of Group Representations in.pdf;/Users/thomasgorman/Zotero/storage/67JJ6CKM/2401.html}
}

@misc{leeInstructionTuningHuman2024,
  title = {Instruction {{Tuning}} with {{Human Curriculum}}},
  author = {Lee, Bruce W. and Cho, Hyunsoo and Yoo, Kang Min},
  year = {2024},
  month = feb,
  number = {arXiv:2310.09518},
  eprint = {2310.09518},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-14},
  abstract = {In building instruction-tuned large language models (LLMs), the importance of a deep understanding of human knowledge can be often overlooked by the importance of instruction diversification. This research proposes a novel approach to instruction tuning by integrating a structured cognitive learning methodology that takes inspiration from the systematic progression and cognitively stimulating nature of human education through two key steps. First, our synthetic instruction data generation pipeline, designed with some references to human educational frameworks, is enriched with meta-data detailing topics and cognitive rigor for each instruction. Specifically, our generation framework is infused with questions of varying levels of rigorousness, inspired by Bloom's Taxonomy, a classic educational model for structured curriculum learning. Second, during instruction tuning, we curate instructions such that questions are presented in an increasingly complex manner utilizing the information on question complexity and cognitive rigorousness produced by our data generation pipeline. Our human-inspired curriculum learning yields significant performance enhancements compared to uniform sampling or round-robin, improving MMLU by 3.06 on LLaMA 2. We conduct extensive experiments and find that the benefits of our approach are consistently observed in eight other benchmarks. We hope that our work will shed light on the post-training learning process of LLMs and its similarity with their human counterpart.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lee et al_2024_Instruction Tuning with Human Curriculum.pdf;/Users/thomasgorman/Zotero/storage/IXA484XJ/2310.html}
}

@misc{leeInteractiveExamplebasedExplanations2024,
  title = {Interactive {{Example-based Explanations}} to {{Improve Health Professionals}}' {{Onboarding}} with {{AI}} for {{Human-AI Collaborative Decision Making}}},
  author = {Lee, Min Hun and Ng, Renee Bao Xuan and Choo, Silvana Xinyi and Thilarajah, Shamala},
  year = {2024},
  month = sep,
  number = {arXiv:2409.15814},
  eprint = {2409.15814},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-29},
  abstract = {A growing research explores the usage of AI explanations on user's decision phases for human-AI collaborative decisionmaking. However, previous studies found the issues of overreliance on `wrong' AI outputs. In this paper, we propose interactive examplebased explanations to improve health professionals' onboarding with AI for their better reliance on AI during AI-assisted decision-making. We implemented an AI-based decision support system that utilizes a neural network to assess the quality of post-stroke survivors' exercises and interactive example-based explanations that systematically surface the nearest neighborhoods of a test/task sample from the training set of the AI model to assist users' onboarding with the AI model. To investigate the effect of interactive example-based explanations, we conducted a study with domain experts, health professionals to evaluate their performance and reliance on AI. Our interactive example-based explanations during onboarding assisted health professionals in having a better reliance on AI and making a higher ratio of making `right' decisions and a lower ratio of `wrong' decisions than providing only feature-based explanations during the decision-support phase. Our study discusses new challenges of assisting user's onboarding with AI for human-AI collaborative decision-making.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lee et al_2024_Interactive Example-based Explanations to Improve Health Professionals'.pdf}
}

@misc{leeLargeLanguageModels2024,
  title = {Large {{Language Models Portray Socially Subordinate Groups}} as {{More Homogeneous}}, {{Consistent}} with a {{Bias Observed}} in {{Humans}}},
  author = {Lee, Messi H. J. and Montgomery, Jacob M. and Lai, Calvin K.},
  year = {2024},
  month = jan,
  urldate = {2024-05-24},
  abstract = {Large language models (LLMs) are becoming pervasive in everyday life, yet their propensity to reproduce biases inherited from training data remains a pressing concern. Prior investigations into bias in LLMs have focused on the association of social groups with stereotypical attributes. However, this is only one form of human bias such systems may reproduce. We investigate a new form of bias in LLMs that resembles a social psychological phenomenon where socially subordinate groups are perceived as more homogeneous than socially dominant groups. We had ChatGPT, a state-of-the-art LLM, generate texts about intersectional group identities and compared those texts on measures of homogeneity. We consistently found that ChatGPT portrayed African, Asian, and Hispanic Americans as more homogeneous than White Americans, indicating that the model described racial minority groups with a narrower range of human experience. ChatGPT also portrayed women as more homogeneous than men, but these differences were small. Finally, we found that the effect of gender differed across racial/ethnic groups such that the effect of gender was consistent within African and Hispanic Americans but not within Asian and White Americans. We argue that the tendency of LLMs to describe groups as less diverse risks perpetuating stereotypes and discriminatory behavior.},
  langid = {english},
  annotation = {https://github.com/lee-messi/Homogeneity-Bias-in-LLMs\\
\\
https://osf.io/8j6xh/?view\_only=},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lee et al_2024_Large Language Models Portray Socially Subordinate Groups as More Homogeneous,.pdf}
}

@misc{leeMoreDistinctivelyBlack2024,
  title = {More {{Distinctively Black}} and {{Feminine Faces Lead}} to {{Increased Stereotyping}} in {{Vision-Language Models}}},
  author = {Lee, Messi Ho Jun and Montgomery, Jacob and Lai, Calvin K.},
  year = {2024},
  month = jun,
  doi = {10.31219/osf.io/wvxy2},
  urldate = {2024-06-23},
  abstract = {Vision Language Models (VLMs), exemplified by GPT-4V, adeptly integrate text and vision modalities. This integration enhances Large Language Models' ability to mimic human perception, allowing them to process image inputs. Despite VLMs' advanced capabilities, however, there is a concern that VLMs inherit biases of both modalities in ways that make biases more pervasive and difficult to mitigate. Our study explores how VLMs perpetuate homogeneity bias and trait associations with regards to race and gender. When prompted to write stories based on images of human faces, GPT-4V describes subordinate racial and gender groups with greater homogeneity than dominant groups and relies on distinct, yet generally positive, stereotypes. Importantly, VLM stereotyping is driven by visual cues rather than group membership alone such that faces that are rated as more prototypically Black and feminine are subject to greater stereotyping. These findings suggest that VLMs may associate subtle visual cues related to racial and gender groups with stereotypes in ways that could be challenging to mitigate. We explore the underlying reasons behind this behavior and discuss its implications and emphasize the importance of addressing these biases as VLMs come to mirror human perception.},
  copyright = {https://creativecommons.org/publicdomain/zero/1.0/legalcode},
  langid = {english},
  annotation = {https://osf.io/znumd/?view\_only=a4c48728bf3449329b83689de5df38f2\\
\\
https://github.com/lee-messi/Stereotyping-in-VLMs},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lee et al_2024_More Distinctively Black and Feminine Faces Lead to Increased Stereotyping in.pdf}
}

@inproceedings{leeOneVsMany2024,
  title = {One vs. {{Many}}: {{Comprehending Accurate Information}} from {{Multiple Erroneous}} and {{Inconsistent AI Generations}}},
  shorttitle = {One vs. {{Many}}},
  booktitle = {The 2024 {{ACM Conference}} on {{Fairness}}, {{Accountability}}, and {{Transparency}}},
  author = {Lee, Yoonjoo and Son, Kihoon and Kim, Tae Soo and Kim, Jisu and Chung, John Joon Young and Adar, Eytan and Kim, Juho},
  year = {2024},
  month = jun,
  eprint = {2405.05581},
  primaryclass = {cs},
  pages = {2518--2531},
  doi = {10.1145/3630106.3662681},
  urldate = {2024-06-30},
  abstract = {As Large Language Models (LLMs) are nondeterministic, the same input can generate different outputs, some of which may be incorrect or hallucinated. If run again, the LLM may correct itself and produce the correct answer. Unfortunately, most LLM-powered systems resort to single results which, correct or not, users accept. Having the LLM produce multiple outputs may help identify disagreements or alternatives. However, it is not obvious how the user will interpret conflicts or inconsistencies. To this end, we investigate how users perceive the AI model and comprehend the generated information when they receive multiple, potentially inconsistent, outputs. Through a preliminary study, we identified five types of output inconsistencies. Based on these categories, we conducted a study ({$N$} = 252) in which participants were given one or more LLMgenerated passages to an information-seeking question. We found that inconsistency within multiple LLM-generated outputs lowered the participants' perceived AI capacity, while also increasing their comprehension of the given information. Specifically, we observed that this positive effect of inconsistencies was most significant for participants who read two passages, compared to those who read three. Based on these findings, we present design implications that, instead of regarding LLM output inconsistencies as a drawback, we can reveal the potential inconsistencies to transparently indicate the limitations of these models and promote critical LLM usage.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lee et al_2024_One vs.pdf}
}

@misc{leeScaleDiversityCoefficient2023,
  title = {Beyond {{Scale}}: The {{Diversity Coefficient}} as a {{Data Quality Metric Demonstrates LLMs}} Are {{Pre-trained}} on {{Formally Diverse Data}}},
  shorttitle = {Beyond {{Scale}}},
  author = {Lee, Alycia and Miranda, Brando and Sundar, Sudharsan and Koyejo, Sanmi},
  year = {2023},
  month = sep,
  number = {arXiv:2306.13840},
  eprint = {2306.13840},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2306.13840},
  urldate = {2024-06-30},
  abstract = {Current trends to pre-train capable Large Language Models (LLMs) mostly focus on scaling of model and dataset size. However, the quality of pre-training data is an important factor for training powerful LLMs, yet it is a nebulous concept that has not been fully characterized. Therefore, we use the recently proposed Task2Vec diversity coefficient to ground and understand formal aspects of data quality, to go beyond scale alone. Specifically, we measure the diversity coefficient of publicly available pre-training datasets to demonstrate that their formal diversity is high when compared to theoretical lower and upper bounds. In addition, to build confidence in the diversity coefficient, we conduct interpretability experiments and find that the coefficient aligns with intuitive properties of diversity, e.g., it increases as the number of latent concepts increases. We conclude the diversity coefficient is reliable, show it's high for publicly available LLM datasets, and conjecture it can be used to build useful diverse datasets for LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lee et al_2023_Beyond Scale.pdf;/Users/thomasgorman/Zotero/storage/6MTXAUHS/2306.html}
}

@article{lehrChatGPTResearchScientist2024,
  title = {{{ChatGPT}} as {{Research Scientist}}: {{Probing GPT}}'s Capabilities as a {{Research Librarian}}, {{Research Ethicist}}, {{Data Generator}}, and {{Data Predictor}}},
  shorttitle = {{{ChatGPT}} as {{Research Scientist}}},
  author = {Lehr, Steven A. and Caliskan, Aylin and Liyanage, Suneragiri and Banaji, Mahzarin R.},
  year = {2024},
  month = aug,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {121},
  number = {35},
  pages = {e2404328121},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2404328121},
  urldate = {2024-09-23},
  abstract = {How good a research scientist is ChatGPT? We systematically probed the capabilities of GPT-3.5 and GPT-4 across four central components of the scientific process: as a Research Librarian, Research Ethicist, Data Generator, and Novel Data Predictor, using psychological science as a testing field. In Study 1 (Research Librarian), unlike human researchers, GPT-3.5 and GPT-4 hallucinated, authoritatively generating fictional references 36.0\% and 5.4\% of the time, respectively, although GPT-4 exhibited an evolving capacity to acknowledge its fictions. In Study 2 (Research Ethicist), GPT-4 (though not GPT-3.5) proved capable of detecting violations like p-hacking in fictional research protocols, correcting 88.6\% of blatantly presented issues, and 72.6\% of subtly presented issues. In Study 3 (Data Generator), both models consistently replicated patterns of cultural bias previously discovered in large language corpora, indicating that ChatGPT can simulate known results, an antecedent to usefulness for both data generation and skills like hypothesis generation. Contrastingly, in Study 4 (Novel Data Predictor), neither model was successful at predicting new results absent in their training data, and neither appeared to leverage substantially new information when predicting more vs. less novel outcomes. Together, these results suggest that GPT is a flawed but rapidly improving librarian, a decent research ethicist already, capable of data generation in simple domains with known characteristics but poor at predicting novel patterns of empirical data to aid future experimentation.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lehr et al_2024_ChatGPT as Research Scientist.pdf}
}

@misc{lengLLMAgentsExhibit2024,
  title = {Do {{LLM Agents Exhibit Social Behavior}}?},
  author = {Leng, Yan and Yuan, Yuan},
  year = {2024},
  month = feb,
  number = {arXiv:2312.15198},
  eprint = {2312.15198},
  primaryclass = {cs, econ, q-fin},
  publisher = {arXiv},
  urldate = {2024-09-23},
  abstract = {The advances of Large Language Models (LLMs) are expanding their utility in both academic research and practical applications. Recent social science research has explored the use of these ``black-box'' LLM agents for simulating complex social systems and potentially substituting human subjects in experiments. Our study delves into this emerging domain, investigating the extent to which LLMs exhibit key social interaction principles, such as social learning, social preference, and cooperative behavior (indirect reciprocity), in their interactions with humans and other agents. We develop a framework for our study, wherein classical laboratory experiments involving human subjects are adapted to use LLM agents. This approach involves step-by-step reasoning that mirrors human cognitive processes and zero-shot learning to assess the innate preferences of LLMs. Our analysis of LLM agents' behavior includes both the primary effects and an in-depth examination of the underlying mechanisms. Focusing on GPT-4, our analyses suggest that LLM agents appear to exhibit a range of human-like social behaviors such as distributional and reciprocity preferences, responsiveness to group identity cues, engagement in indirect reciprocity, and social learning capabilities. However, our analysis also reveals notable differences: LLMs demonstrate a pronounced fairness preference, weaker positive reciprocity, and a more calculating approach in social learning compared to humans. These insights indicate that while LLMs hold great promise for applications in social science research, such as in laboratory experiments and agent-based modeling, the subtle behavioral differences between LLM agents and humans warrant further investigation. Careful examination and development of protocols in evaluating the social behaviors of LLMs are necessary before directly applying these models to emulate human behavior.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Social and Information Networks,Economics - General Economics},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Leng_Yuan_2024_Do LLM Agents Exhibit Social Behavior.pdf}
}

@article{leon-dominguezPotentialCognitiveRisks2024,
  title = {Potential Cognitive Risks of Generative Transformer-Based {{AI}} Chatbots on Higher Order Executive Functions},
  author = {{Le{\'o}n-Dom{\'i}nguez}, Umberto},
  year = {2024},
  month = may,
  journal = {Neuropsychology},
  volume = {38},
  number = {4},
  pages = {293--308},
  publisher = {American Psychological Association},
  issn = {0894-4105},
  doi = {10.1037/neu0000948},
  urldate = {2024-09-05},
  abstract = {Background: Chat generative retrained transformer (ChatGPT) represents a groundbreaking advancement in Artificial Intelligence (AI-chatbot) technology, utilizing transformer algorithms to enhance natural language processing and facilitating their use for addressing specific tasks. These AI chatbots can respond to questions by generating verbal instructions similar to those a person would provide during the problem-solving process. Aim: ChatGPT has become the fastest growing software in terms of user adoption in history, leading to an anticipated widespread use of this technology in the general population. Current literature is predominantly focused on the functional aspects of these technologies, but the field has not yet explored hypotheses on how these AI chatbots could impact the evolutionary aspects of human cognitive development. Thesis: The 'neuronal recycling hypothesis' posits that the brain undergoes structural transformation by incorporating new cultural tools into 'neural niches,' consequently altering individual cognition. In the case of technological tools, it has been established that they reduce the cognitive demand needed to solve tasks through a process called 'cognitive offloading.' In this theoretical article, three hypotheses were proposed via forward inference about how algorithms such as ChatGPT and similar models may influence the cognitive processes and structures of upcoming generations. Conclusions: By forecasting the neurocognitive effects of these technologies, educational and political communities can anticipate future scenarios and formulate strategic plans to either mitigate or enhance the cognitive influence that these factors may have on the general population. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
  keywords = {Algorithms,Artificial Intelligence,chat generative pretrained transformer,Chatbots,cognition,Cognitive Hypothesis Testing,cognitive offloading,Cognitive Processes,cultural tools,Neurocognition,neuronal recycling hypothesis,Risk Assessment},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/León-Domínguez_2024_Potential cognitive risks of generative transformer-based AI chatbots on higher.pdf}
}

@misc{leporiDoorsPerceptionVision2024,
  title = {Beyond the {{Doors}} of {{Perception}}: {{Vision Transformers Represent Relations Between Objects}}},
  shorttitle = {Beyond the {{Doors}} of {{Perception}}},
  author = {Lepori, Michael A. and Tartaglini, Alexa R. and Vong, Wai Keen and Serre, Thomas and Lake, Brenden M. and Pavlick, Ellie},
  year = {2024},
  month = jun,
  number = {arXiv:2406.15955},
  eprint = {2406.15955},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-29},
  abstract = {Though vision transformers (ViTs) have achieved state-of-the-art performance in a variety of settings, they exhibit surprising failures when performing tasks involving visual relations. This begs the question: how do ViTs attempt to perform tasks that require computing visual relations between objects? Prior efforts to interpret ViTs tend to focus on characterizing relevant low-level visual features. In contrast, we adopt methods from mechanistic interpretability to study the higher-level visual algorithms that ViTs use to perform abstract visual reasoning. We present a case study of a fundamental, yet surprisingly difficult, relational reasoning task: judging whether two visual entities are the same or different. We find that pretrained ViTs fine-tuned on this task often exhibit two qualitatively different stages of processing despite having no obvious inductive biases to do so: 1) a perceptual stage wherein local object features are extracted and stored in a disentangled representation, and 2) a relational stage wherein object representations are compared. In the second stage, we find evidence that ViTs can learn to represent somewhat abstract visual relations, a capability that has long been considered out of reach for artificial neural networks. Finally, we demonstrate that failure points at either stage can prevent a model from learning a generalizable solution to our fairly simple tasks. By understanding ViTs in terms of discrete processing stages, one can more precisely diagnose and rectify shortcomings of existing and future models.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lepori et al_2024_Beyond the Doors of Perception.pdf}
}

@misc{leungQuantifyingConceptDefinition2024,
  title = {Quantifying {{Concept Definition Heterogeneity}} in {{Academic Texts}}: {{Insights}} into {{Variability}} in {{Conceptualization}}},
  shorttitle = {Quantifying {{Concept Definition Heterogeneity}} in {{Academic Texts}}},
  author = {Leung, Anna Yi and Melev, Ivan and Schmalz, Xenia},
  year = {2024},
  month = may,
  publisher = {OSF},
  doi = {10.31219/osf.io/gu7b5},
  urldate = {2024-07-03},
  abstract = {Academic texts often define a given concept using words. Across texts, definitions may represent the same or different meanings of the given concept, especially for concepts in interdisciplinary fields, such as cognitive science. Understanding the variability of definitions and the reasons for this variability may facilitate the replicability of science: variations in concept definitions may indicate or provoke a heterogeneous understanding of the concept, leading to differences between studies in the methodology and, subsequently, the results. We developed proxy measures of the variability in definitions of each concept using information theory and natural language processing (i.e., entropy, KL divergence, and lexical diversity). We collected and analyzed 2212 definitions of 216 concepts associated with "open scholarship" as a use case study. We found strong correlations between the entropy-based proxy measures and the number of definitions and unique words yielded by concepts. However, we found no correlation between lexical diversity and the entropy-based proxies. This indicates that concepts that yield definitions with different wordings in the literature (high on lexical diversity) do not necessarily reflect an inconsistent understanding of concepts' meaning across contexts (high on the entropy-based measures). The proposed methodology can help identify concepts that yield heterogeneous definitions in academic texts and concepts that are consistently defined in a semi-automated manner.},
  archiveprefix = {OSF},
  langid = {american},
  keywords = {conceptualization,FORRT glossary,open science,replicability,term specificity},
  annotation = {https://osf.io/qy38x/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Leung et al_2024_Quantifying Concept Definition Heterogeneity in Academic Texts.pdf}
}

@misc{liangCanGPT4Replicate2023,
  title = {Can {{GPT-4 Replicate Empirical Software Engineering Research}}?},
  author = {Liang, Jenny T. and Badea, Carmen and Bird, Christian and DeLine, Robert and Ford, Denae and Forsgren, Nicole and Zimmermann, Thomas},
  year = {2023},
  month = oct,
  number = {arXiv:2310.01727},
  eprint = {2310.01727},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {Empirical software engineering research on production systems has brought forth a better understanding of the software engineering process for practitioners and researchers alike. However, only a small subset of production systems is studied, limiting the impact of this research. While software engineering practitioners benefit from replicating research on their own data, this poses its own set of challenges, since performing replications requires a deep understanding of research methodologies and subtle nuances in software engineering data. Given that large language models (LLMs), such as GPT-4, show promise in tackling both software engineering- and science-related tasks, these models could help democratize empirical software engineering research. In this paper, we examine LLMs' abilities to perform replications of empirical software engineering research on new data. We specifically study their ability to surface assumptions made in empirical software engineering research methodologies, as well as their ability to plan and generate code for analysis pipelines on seven empirical software engineering papers. We perform a user study with 14 participants with software engineering research expertise, who evaluate GPT-4-generated assumptions and analysis plans (i.e., a list of module specifications) from the papers. We find that GPT-4 is able to surface correct assumptions, but struggle to generate ones that reflect common knowledge about software engineering data. In a manual analysis of the generated code, we find that the GPT-4-generated code contains the correct high-level logic, given a subset of the methodology. However, the code contains many small implementation-level errors, reflecting a lack of software engineering knowledge. Our findings have implications for leveraging LLMs for software engineering research as well as practitioner data scientists in software teams.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  annotation = {https://figshare.com/s/603602d213e22cdcc491},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Liang et al_2023_Can GPT-4 Replicate Empirical Software Engineering Research.pdf;/Users/thomasgorman/Zotero/storage/IC9LKPSV/2310.html}
}

@article{liangRealTimeDetectionDriver2007,
  title = {Real-{{Time Detection}} of {{Driver Cognitive Distraction Using Support Vector Machines}}},
  author = {Liang, Yulan and Reyes, Michelle L. and Lee, John D.},
  year = {2007},
  month = jun,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {8},
  number = {2},
  pages = {340--350},
  issn = {1558-0016},
  doi = {10.1109/TITS.2007.895298},
  urldate = {2024-09-16},
  abstract = {As use of in-vehicle information systems (IVISs) such as cell phones, navigation systems, and satellite radios has increased, driver distraction has become an important and growing safety concern. A promising way to overcome this problem is to detect driver distraction and adapt in-vehicle systems accordingly to mitigate such distractions. To realize this strategy, this paper applied support vector machines (SVMs), which is a data mining method, to develop a real-time approach for detecting cognitive distraction using drivers' eye movements and driving performance data. Data were collected in a simulator experiment in which ten participants interacted with an IVIS while driving. The data were used to train and test both SVM and logistic regression models, and three different model characteristics were investigated: how distraction was defined, which data were input to the model, and how the input data were summarized. The results show that the SVM models were able to detect driver distraction with an average accuracy of 81.1\%, outperforming more traditional logistic regression models. The best performing model (96.1\% accuracy) resulted when distraction was defined using experimental conditions (i.e., IVIS drive or baseline drive), the input data were comprised of eye movement and driving measures, and these data were summarized over a 40-s window with 95\% overlap of windows. These results demonstrate that eye movements and simple measures of driving performance can be used to detect driver distraction in real time. Potential applications of this paper include the design of adaptive in-vehicle systems and the evaluation of driver distraction},
  keywords = {Cellular phones,Classification,Drives,driving performance,eye movement,Information systems,logistic regression,Logistics,Motion measurement,Radio navigation,Safety,Satellite broadcasting,Satellite navigation systems,secondary task,support vector machine (SVM),Support vector machines},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Liang et al_2007_Real-Time Detection of Driver Cognitive Distraction Using Support Vector.pdf;/Users/thomasgorman/Zotero/storage/YLWZU4EK/4220657.html}
}

@misc{liaoAITransparencyAge2023,
  title = {{{AI Transparency}} in the {{Age}} of {{LLMs}}: {{A Human-Centered Research Roadmap}}},
  shorttitle = {{{AI Transparency}} in the {{Age}} of {{LLMs}}},
  author = {Liao, Q. Vera and Vaughan, Jennifer Wortman},
  year = {2023},
  month = jun,
  number = {arXiv:2306.01941},
  eprint = {2306.01941},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-08},
  abstract = {The rise of powerful large language models (LLMs) brings about tremendous opportunities for innovation but also looming risks for individuals and society at large. We have reached a pivotal moment for ensuring that LLMs and LLM-infused applications are developed and deployed responsibly. However, a central pillar of responsible AI -- transparency -- is largely missing from the current discourse around LLMs. It is paramount to pursue new approaches to provide transparency for LLMs, and years of research at the intersection of AI and human-computer interaction (HCI) highlight that we must do so with a human-centered perspective: Transparency is fundamentally about supporting appropriate human understanding, and this understanding is sought by different stakeholders with different goals in different contexts. In this new era of LLMs, we must develop and design approaches to transparency by considering the needs of stakeholders in the emerging LLM ecosystem, the novel types of LLM-infused applications being built, and the new usage patterns and challenges around LLMs, all while building on lessons learned about how people process, interact with, and make use of information. We reflect on the unique challenges that arise in providing transparency for LLMs, along with lessons learned from HCI and responsible AI research that has taken a human-centered perspective on AI transparency. We then lay out four common approaches that the community has taken to achieve transparency -- model reporting, publishing evaluation results, providing explanations, and communicating uncertainty -- and call out open questions around how these approaches may or may not be applied to LLMs. We hope this provides a starting point for discussion and a useful roadmap for future research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Liao_Vaughan_2023_AI Transparency in the Age of LLMs.pdf;/Users/thomasgorman/Zotero/storage/XRADVM2F/2306.html}
}

@misc{liItHotDay2024,
  title = {Save {{It}} for the "{{Hot}}" {{Day}}: {{An LLM-Empowered Visual Analytics System}} for {{Heat Risk Management}}},
  shorttitle = {Save {{It}} for the "{{Hot}}" {{Day}}},
  author = {Li, Haobo and {Kam-Kwai}, Wong and Luo, Yan and Chen, Juntong and Liu, Chengzhong and Zhang, Yaxuan and Lau, Alexis Kai Hon and Qu, Huamin and Liu, Dongyu},
  year = {2024},
  month = jun,
  number = {arXiv:2406.03317},
  eprint = {2406.03317},
  publisher = {arXiv},
  urldate = {2024-07-03},
  abstract = {The escalating frequency and intensity of heat-related climate events, particularly heatwaves, emphasize the pressing need for advanced heat risk management strategies. Current approaches, primarily relying on numerical models, face challenges in spatial-temporal resolution and in capturing the dynamic interplay of environmental, social, and behavioral factors affecting heat risks. This has led to difficulties in translating risk assessments into effective mitigation actions. Recognizing these problems, we introduce a novel approach leveraging the burgeoning capabilities of Large Language Models (LLMs) to extract rich and contextual insights from news reports. We hence propose an LLM-empowered visual analytics system, Havior, that integrates the precise, data-driven insights of numerical models with nuanced news report information. This hybrid approach enables a more comprehensive assessment of heat risks and better identification, assessment, and mitigation of heat-related threats. The system incorporates novel visualization designs, such as ``thermoglyph'' and news glyph, enhancing intuitive understanding and analysis of heat risks. The integration of LLM-based techniques also enables advanced information retrieval and semantic knowledge extraction that can be guided by experts' analytics needs. Our case studies on two cities that faced significant heatwave events and interviews with five experts have demonstrated the usefulness of our system in providing in-depth and actionable insights for heat risk management.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction,Computer Science - Multimedia},
  annotation = {https://osf.io/dx5wt/?view\_only=d44d6a39856d4c26b9452fc3f9be64b6},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Li et al_2024_Save It for the Hot Day.pdf}
}

@misc{liMemoryConsciousnessLarge2024,
  title = {Memory, {{Consciousness}} and {{Large Language Model}}},
  author = {Li, Jitang and Li, Jinzheng},
  year = {2024},
  month = jan,
  number = {arXiv:2401.02509},
  eprint = {2401.02509},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-02-27},
  abstract = {With the development in cognitive science and Large Language Models (LLMs), increasing connections have come to light between these two distinct fields. Building upon these connections, we propose a conjecture suggesting the existence of a duality between LLMs and Tulving's theory of memory. We identify a potential correspondence between Tulving's synergistic ecphory model (SEM) of retrieval and the emergent abilities observed in LLMs, serving as supporting evidence for our conjecture. Furthermore, we speculate that consciousness may be considered a form of emergent ability based on this duality. We also discuss how other theories of consciousness intersect with our research.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Li_Li_2024_Memory, Consciousness and Large Language Model.pdf;/Users/thomasgorman/Zotero/storage/FGS5CPGR/2401.html}
}

@misc{liMoreAgentsAll2024,
  title = {More {{Agents Is All You Need}}},
  author = {Li, Junyou and Zhang, Qin and Yu, Yangbin and Fu, Qiang and Ye, Deheng},
  year = {2024},
  month = feb,
  number = {arXiv:2402.05120},
  eprint = {2402.05120},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-07},
  abstract = {We find that, simply via a sampling-and-voting method, the performance of large language models (LLMs) scales with the number of agents instantiated. Also, this method is orthogonal to existing complicated methods to further enhance LLMs, while the degree of enhancement is correlated to the task difficulty. We conduct comprehensive experiments on a wide range of LLM benchmarks to verify the presence of our finding, and to study the properties that can facilitate its occurrence. Our code is publicly available at: {\textbackslash}url\{https://anonymous.4open.science/r/more\_agent\_is\_all\_you\_need\}.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Li et al_2024_More Agents Is All You Need.pdf;/Users/thomasgorman/Zotero/storage/SWPDWX4W/2402.html}
}

@misc{linAILanguageModels2024,
  title = {{{AI}} Language Models as Role-Playing Tools, Not Human Participants},
  author = {Lin, Zhicheng},
  year = {2024},
  month = feb,
  number = {arXiv:2402.04470},
  eprint = {2402.04470},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-10},
  abstract = {Advances in AI invite misuse of language models as replacements for human participants. We argue that treating their responses as glimpses into an average human mind fundamentally mischaracterizes these statistical algorithms and that language models should be embraced as flexible simulation tools, able to mimic diverse behaviors without possessing human traits themselves.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lin_2024_AI language models as role-playing tools, not human participants.pdf;/Users/thomasgorman/Zotero/storage/FB6AX5YB/2402.html}
}

@misc{linContrastiveCorpusAttribution2023,
  title = {Contrastive {{Corpus Attribution}} for {{Explaining Representations}}},
  author = {Lin, Chris and Chen, Hugh and Kim, Chanwoo and Lee, Su-In},
  year = {2023},
  month = jun,
  number = {arXiv:2210.00107},
  eprint = {2210.00107},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2210.00107},
  urldate = {2023-09-12},
  abstract = {Despite the widespread use of unsupervised models, very few methods are designed to explain them. Most explanation methods explain a scalar model output. However, unsupervised models output representation vectors, the elements of which are not good candidates to explain because they lack semantic meaning. To bridge this gap, recent works defined a scalar explanation output: a dot product-based similarity in the representation space to the sample being explained (i.e., an explicand). Although this enabled explanations of unsupervised models, the interpretation of this approach can still be opaque because similarity to the explicand's representation may not be meaningful to humans. To address this, we propose contrastive corpus similarity, a novel and semantically meaningful scalar explanation output based on a reference corpus and a contrasting foil set of samples. We demonstrate that contrastive corpus similarity is compatible with many post-hoc feature attribution methods to generate COntrastive COrpus Attributions (COCOA) and quantitatively verify that features important to the corpus are identified. We showcase the utility of COCOA in two ways: (i) we draw insights by explaining augmentations of the same image in a contrastive learning setting (SimCLR); and (ii) we perform zero-shot object localization by explaining the similarity of image representations to jointly learned text representations (CLIP).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computer Vision and Pattern Recognition,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lin et al_2023_Contrastive Corpus Attribution for Explaining Representations.pdf;/Users/thomasgorman/Zotero/storage/V9EXKBIE/2210.html}
}

@article{linzenSyntacticStructureDeep2021,
  title = {Syntactic {{Structure}} from {{Deep Learning}}},
  author = {Linzen, Tal and Baroni, Marco},
  year = {2021},
  month = jan,
  journal = {Annual Review of Linguistics},
  volume = {7},
  number = {Volume 7, 2021},
  pages = {195--212},
  publisher = {Annual Reviews},
  issn = {2333-9683, 2333-9691},
  doi = {10.1146/annurev-linguistics-032020-051035},
  urldate = {2024-05-22},
  abstract = {Modern deep neural networks achieve impressive performance in engineering applications that require extensive linguistic skills, such as machine translation. This success has sparked interest in probing whether these models are inducing human-like grammatical knowledge from the raw data they are exposed to and, consequently, whether they can shed new light on long-standing debates concerning the innate structure necessary for language acquisition. In this article, we survey representative studies of the syntactic abilities of deep networks and discuss the broader implications that this work has for theoretical linguistics.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Linzen_Baroni_2021_Syntactic Structure from Deep Learning.pdf;/Users/thomasgorman/Zotero/storage/XX2VBZFY/annurev-linguistics-032020-051035.html}
}

@misc{liSystematicGeneralizationEmergent2022,
  title = {Systematic {{Generalization}} and {{Emergent Structures}} in {{Transformers Trained}} on {{Structured Tasks}}},
  author = {Li, Yuxuan and McClelland, James L.},
  year = {2022},
  month = dec,
  number = {arXiv:2210.00400},
  eprint = {2210.00400},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-01-12},
  abstract = {Transformer networks have seen great success in natural language processing and machine vision, where task objectives such as next word prediction and image classification benefit from nuanced context sensitivity across high-dimensional inputs. However, there is an ongoing debate about how and when transformers can acquire highly structured behavior and achieve systematic generalization. Here, we explore how well a causal transformer can perform a set of algorithmic tasks, including copying, sorting, and hierarchical compositions of these operations. We demonstrate strong generalization to sequences longer than those used in training by replacing the standard positional encoding typically used in transformers with labels arbitrarily paired with items in the sequence. We search for the layer and head configuration sufficient to solve these tasks, then probe for signs of systematic processing in latent representations and attention patterns. We show that two-layer transformers learn reliable solutions to multi-level problems, develop signs of task decomposition, and encode input items in a way that encourages the exploitation of shared computation across related tasks. These results provide key insights into how attention layers support structured computation both within a task and across multiple tasks.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,I.2.6},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/liSystematicGeneralizationEmergent2022-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Li_McClelland_2022_Systematic Generalization and Emergent Structures in Transformers Trained on.pdf;/Users/thomasgorman/Zotero/storage/MSNBF66U/2210.html}
}

@misc{liuAligningLargeLanguage2023,
  title = {Aligning {{Large Language Models}} with {{Human Preferences}} through {{Representation Engineering}}},
  author = {Liu, Wenhao and Wang, Xiaohua and Wu, Muling and Li, Tianlong and Lv, Changze and Ling, Zixuan and Zhu, Jianhao and Zhang, Cenyuan and Zheng, Xiaoqing and Huang, Xuanjing},
  year = {2023},
  month = dec,
  number = {arXiv:2312.15997},
  eprint = {2312.15997},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-23},
  abstract = {Aligning large language models (LLMs) with human preferences is crucial for enhancing their utility in terms of helpfulness, truthfulness, safety, harmlessness, and interestingness. Existing methods for achieving this alignment often involves employing reinforcement learning from human feedback (RLHF) to fine-tune LLMs based on human labels assessing the relative quality of model responses. Nevertheless, RLHF is susceptible to instability during fine-tuning and presents challenges in implementation.Drawing inspiration from the emerging field of representation engineering (RepE), this study aims to identify relevant representations for high-level human preferences embedded in patterns of activity within an LLM, and achieve precise control of model behavior by transforming its representations. This novel approach, denoted as Representation Alignment from Human Feedback (RAHF), proves to be effective, computationally efficient, and easy to implement.Extensive experiments demonstrate the efficacy of RAHF in not only capturing but also manipulating representations to align with a broad spectrum of human preferences or values, rather than being confined to a singular concept or function (e.g. honesty or bias). RAHF's versatility in accommodating diverse human preferences shows its potential for advancing LLM performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Liu et al_2023_Aligning Large Language Models with Human Preferences through Representation.pdf;/Users/thomasgorman/Zotero/storage/KBW6R3XQ/2312.html}
}

@article{liuHumanlikeResponseModel2024,
  title = {A Human-like Response Model for Following Vehicles in Lane-Changing Scenario},
  author = {Liu, Rui and Zhao, Xuan and Yuan, Tian and Li, Haipeng and Bu, Tengchen and Zhu, Xichan and Ma, Jian},
  year = {2024},
  month = mar,
  journal = {Proceedings of the Institution of Mechanical Engineers, Part D: Journal of Automobile Engineering},
  volume = {238},
  number = {4},
  pages = {760--773},
  publisher = {IMECHE},
  issn = {0954-4070},
  doi = {10.1177/09544070221135384},
  urldate = {2024-09-16},
  abstract = {A human-like following vehicle (FV) model for lane-changing scenario was developed. First, the non-linear relationship between FV responses and relative speed was integrated. Normalized transformation was performed to better investigate the correlation characteristics between variables. Second, environmental factors and kinematic parameters were characterized using a hierarchical linear model. Next, vehicle control modes were incorporated. Finally, a human-like response model (HRM) for the FV was proposed and tested. It is found that the brake intensity of the FV is affected by lane-changing direction, but it is not affected by turn-signal usage or traffic density. Turn-signal usage and traffic density are related to kinematic parameters. Lane-changers use their turn signal more when relative velocity is negative or when traffic density is high. Therefore, turn-signal usage and traffic density will become redundant variables when kinematic parameters are considered. Driving data verification shows that the HRM can match both non-critical and critical lane-changing cases and can simulate the no reaction behavior of the FV in lane-changing, which can help to improve the human-like driving ability of intelligent vehicles.},
  langid = {english}
}

@misc{liuLargeLanguageModels2024,
  title = {Large {{Language Models Assume People}} Are {{More Rational}} than {{We Really}} Are},
  author = {Liu, Ryan and Geng, Jiayi and Peterson, Joshua C. and Sucholutsky, Ilia and Griffiths, Thomas L.},
  year = {2024},
  month = jul,
  number = {arXiv:2406.17055},
  eprint = {2406.17055},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-16},
  abstract = {In order for AI systems to communicate effectively with people, they must understand how we make decisions. However, people's decisions are not always rational, so the implicit internal models of human decision-making in Large Language Models (LLMs) must account for this. Previous empirical evidence seems to suggest that these implicit models are accurate --- LLMs offer believable proxies of human behavior, acting how we expect humans would in everyday interactions. However, by comparing LLM behavior and predictions to a large dataset of human decisions, we find that this is actually not the case: when both simulating and predicting people's choices, a suite of cutting-edge LLMs (GPT-4o \& 4-Turbo, Llama-3-8B \& 70B, Claude 3 Opus) assume that people are more rational than we really are. Specifically, these models deviate from human behavior and align more closely with a classic model of rational choice --- expected value theory. Interestingly, people also tend to assume that other people are rational when interpreting their behavior. As a consequence, when we compare the inferences that LLMs and people draw from the decisions of others using another psychological dataset, we find that these inferences are highly correlated. Thus, the implicit decision-making models of LLMs appear to be aligned with the human expectation that other people will act rationally, rather than with how people actually act.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Liu et al_2024_Large Language Models Assume People are More Rational than We Really are.pdf}
}

@misc{liuUnderstandingLLMsComprehensive2024,
  title = {Understanding {{LLMs}}: {{A Comprehensive Overview}} from {{Training}} to {{Inference}}},
  shorttitle = {Understanding {{LLMs}}},
  author = {Liu, Yiheng and He, Hao and Han, Tianle and Zhang, Xu and Liu, Mengyuan and Tian, Jiaming and Zhang, Yutong and Wang, Jiaqi and Gao, Xiaohui and Zhong, Tianyang and Pan, Yi and Xu, Shaochen and Wu, Zihao and Liu, Zhengliang and Zhang, Xin and Zhang, Shu and Hu, Xintao and Zhang, Tuo and Qiang, Ning and Liu, Tianming and Ge, Bao},
  year = {2024},
  month = jan,
  number = {arXiv:2401.02038},
  eprint = {2401.02038},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-07},
  abstract = {The introduction of ChatGPT has led to a significant increase in the utilization of Large Language Models (LLMs) for addressing downstream tasks. There's an increasing focus on cost-efficient training and deployment within this context. Low-cost training and deployment of LLMs represent the future development trend. This paper reviews the evolution of large language model training techniques and inference deployment technologies aligned with this emerging trend. The discussion on training includes various aspects, including data preprocessing, training architecture, pre-training tasks, parallel training, and relevant content related to model fine-tuning. On the inference side, the paper covers topics such as model compression, parallel computation, memory scheduling, and structural optimization. It also explores LLMs' utilization and provides insights into their future development.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Liu et al_2024_Understanding LLMs.pdf;/Users/thomasgorman/Zotero/storage/KFHB7Z83/2401.html}
}

@misc{liuUnravelingMechanicsLearningBased2024,
  title = {Unraveling the {{Mechanics}} of {{Learning-Based Demonstration Selection}} for {{In-Context Learning}}},
  author = {Liu, Hui and Wang, Wenya and Sun, Hao and Tian, Chris Xing and Kong, Chenqi and Dong, Xin and Li, Haoliang},
  year = {2024},
  month = jun,
  number = {arXiv:2406.11890},
  eprint = {2406.11890},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-31},
  abstract = {Large Language Models (LLMs) have demonstrated impressive in-context learning (ICL) capabilities from few-shot demonstration exemplars. While recent learningbased demonstration selection methods have proven beneficial to ICL by choosing more useful exemplars, their underlying mechanisms are opaque, hindering efforts to address limitations such as high training costs and poor generalization across tasks. These methods generally assume the selection process captures similarities between the exemplar and the target instance, however, it remains unknown what kinds of similarities are captured and vital to performing ICL. To dive into this question, we analyze the working mechanisms of the learning-based demonstration selection methods and empirically identify two important factors related to similarity measurement: 1) The ability to integrate different levels of task-agnostic text similarities between the input of exemplars and test cases enhances generalization power across different tasks. 2) Incorporating task-specific labels when measuring the similarities significantly improves the performance on each specific task. We validate these two findings through extensive quantitative and qualitative analyses across ten datasets and various LLMs. Based on our findings, we introduce two effective yet simplified exemplar selection methods catering to task-agnostic and task-specific demands, eliminating the costly LLM inference overhead.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Liu et al_2024_Unraveling the Mechanics of Learning-Based Demonstration Selection for.pdf}
}

@misc{liVariableSequenceLength2023,
  title = {Variable {{Sequence Length Training}} for {{Long-Context Large Language Models}}},
  author = {Li, Siyun},
  year = {2023},
  month = jul,
  journal = {Cerebras},
  urldate = {2023-08-04},
  abstract = {Training large language models with long sequence lengths is prohibitive in practice and expensive due to long training times. In this article, we introduce you to a simple-to-use training method called Variable Sequence Length (VSL), which can reduce wall-clock times for training large language models with long sequence lengths capabilities without any changes in model architecture and training hyperparameters. Training a GPT model using VSL (2k sequence length followed by 8k sequence length) uses 29\% fewer FLOPs over training with 8k sequence length all the way through while achieving the same model performance. Variable Sequence Length Training The pre-training of most LLMs is done with a fixed sequence length, for example, 2k tokens per sequence, throughout the entire training duration. One can follow this standard training method for models with long sequences, such as 8k and above. However, this will increase training times due to the compute overhead introduced by longer sequences. Figure 2 depicts the Variable Sequence Length (VSL) method, which breaks standard LLM training into two stages. In the first stage, Stage-1, the model is trained with sequence lengths much shorter than the desired long sequence length, for example, 2k tokens per sequence. This is followed by an adaptation or fine-tuning phase, called Stage-2, where the model is trained for the desired long sequence length, for example, 8k tokens per sequence. The choice of the short sequence length and the fraction of pre-training steps for Stage-1 depends on the desired reduction in pre-training FLOPs and available compute resources. A model trained with a smaller sequence length and a significant fraction of steps in Stage-1 will result in a larger reduction in pre-training FLOPs. We do not modify other hyperparameters, such as optimizer states or learning rate schedules between the two stages.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/VSZN3CMT/variable-sequence-length-training-for-long-context-large-language-models.html}
}

@article{loaiza-ramirezConsumersCareCompanies2022,
  title = {Do Consumers Care about Companies' Efforts in Greening Supply Chains? {{Analyzing}} the Role of Protected Values and the Halo Effect in Product Evaluation},
  shorttitle = {Do Consumers Care about Companies' Efforts in Greening Supply Chains?},
  author = {{Loaiza-Ram{\'i}rez}, Juan Pablo and {Moreno-Mantilla}, Carlos Eduardo and Reimer, Torsten},
  year = {2022},
  month = mar,
  journal = {Cleaner Logistics and Supply Chain},
  volume = {3},
  pages = {100027},
  issn = {2772-3909},
  doi = {10.1016/j.clscn.2021.100027},
  urldate = {2024-07-11},
  abstract = {Widespread efforts are being made to mitigate environmental degradation driven by human activities. From a supply chain management perspective, companies aim at improving their environmental and organizational performance along their supply chain simultaneously. Since consumers are the sources of manufacturing companies' profitability, companies are interested in understanding the extent to which consumers care about their green practices. However, while some consumers would have a higher willingness to pay a premium (WPP) or purchase intention (PI) for environmentally differentiated products, others would not. Moreover, there is scant evidence regarding the integrated effects of intra- and inter-organizational green supply chain practices on green consumerism. Therefore, this study adopts two psychological approaches (i.e., protected values and halo effect) to describe this relationship based on two models that encompass mediation and moderation effects. Data were collected from 351 Colombian university students through a behavioral experiment with three product-based conditions, and the hypotheses were tested using two-instance repeated-measures linear regressions and non-parametric tests. The results indicate that perceived product performance mediates the effect of green supply chain practices on consumers' WPP and PI (halo effect). Additionally, consumers' moral orientation toward the environment (protected values) moderates the effects of green supply chain practices on consumers' WPP, PI and perceived product performance. The study found that people who hold protected values evaluate products better not just for its green attributes, but because of their increased perception of the products' performance. The contributions are centered on the role of psychological approaches in green supply chain studies to understand consumers' preferences.},
  keywords = {Consumer behavior,Environmental sustainability,Green supply chain,Halo effect,Protected values,Willingness to pay},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Loaiza-Ramírez et al_2022_Do consumers care about companies’ efforts in greening supply chains.pdf}
}

@article{loaiza-ramirezWhoPrefersRenewable2022,
  title = {Who Prefers Renewable Energy? {{A}} Moderated Mediation Model Including Perceived Comfort and Consumers' Protected Values in Green Energy Adoption and Willingness to Pay a Premium},
  shorttitle = {Who Prefers Renewable Energy?},
  author = {{Loaiza-Ram{\'i}rez}, Juan Pablo and Reimer, Torsten and {Moreno-Mantilla}, Carlos Eduardo},
  year = {2022},
  month = sep,
  journal = {Energy Research \& Social Science},
  volume = {91},
  pages = {102753},
  issn = {2214-6296},
  doi = {10.1016/j.erss.2022.102753},
  urldate = {2024-07-11},
  abstract = {Drawing from research on the halo effect and protected values, consumers' adoption intentions and willingness to pay a premium for renewable energy were explored. Two theoretical models that involve moderated mediation were tested through two-instance repeated-measures linear regressions and non-parametric tests in a behavioral experiment with an Amazon MTurk sample. In line with the expected halo effect, the effects of the renewability of the energy sources on consumers' adoption intentions and willingness to pay a premium were mediated through consumers' perceived comfort. These mediation effects were stronger among consumers with high protected values compared to those with low protected values. The results suggest that the positive evaluations of renewable energies by consumers with high protected values are mainly driven by those values. Conversely, consumers with low protected values would have lower adoption intentions, would be less willing to pay more, and they would not feel comfort at home when using renewable energy compared to consumers with high protected values.},
  keywords = {Adoption intention,Consumer behavior,Halo effect,Protected values,Renewable energies,Willingness to pay a premium},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Loaiza-Ramírez et al_2022_Who prefers renewable energy.pdf;/Users/thomasgorman/Zotero/storage/RFJYCAMU/S2214629622002572.html}
}

@misc{loreStrategicBehaviorLarge2023,
  title = {Strategic {{Behavior}} of {{Large Language Models}}: {{Game Structure}} vs. {{Contextual Framing}}},
  shorttitle = {Strategic {{Behavior}} of {{Large Language Models}}},
  author = {Lor{\`e}, Nunzio and Heydari, Babak},
  year = {2023},
  month = sep,
  number = {arXiv:2309.05898},
  eprint = {2309.05898},
  primaryclass = {cs, econ},
  publisher = {arXiv},
  urldate = {2023-09-17},
  abstract = {This paper investigates the strategic decision-making capabilities of three Large Language Models (LLMs): GPT-3.5, GPT-4, and LLaMa-2, within the framework of game theory. Utilizing four canonical two-player games -- Prisoner's Dilemma, Stag Hunt, Snowdrift, and Prisoner's Delight -- we explore how these models navigate social dilemmas, situations where players can either cooperate for a collective benefit or defect for individual gain. Crucially, we extend our analysis to examine the role of contextual framing, such as diplomatic relations or casual friendships, in shaping the models' decisions. Our findings reveal a complex landscape: while GPT-3.5 is highly sensitive to contextual framing, it shows limited ability to engage in abstract strategic reasoning. Both GPT-4 and LLaMa-2 adjust their strategies based on game structure and context, but LLaMa-2 exhibits a more nuanced understanding of the games' underlying mechanics. These results highlight the current limitations and varied proficiencies of LLMs in strategic decision-making, cautioning against their unqualified use in tasks requiring complex strategic reasoning.},
  archiveprefix = {arXiv},
  keywords = {91C99 (Primary) 91A05 91A10 91F99 (Secondary),Computer Science - Artificial Intelligence,Computer Science - Computer Science and Game Theory,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Economics - Theoretical Economics,I.2.8,J.4,K.4.m},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lorè_Heydari_2023_Strategic Behavior of Large Language Models.pdf;/Users/thomasgorman/Zotero/storage/NBGLB48Y/2309.html}
}

@inproceedings{loyaExploringSensitivityLLMs2023,
  title = {Exploring the {{Sensitivity}} of {{LLMs}}' {{Decision-Making Capabilities}}: {{Insights}} from {{Prompt Variation}} and {{Hyperparameters}}},
  shorttitle = {Exploring the {{Sensitivity}} of {{LLMs}}' {{Decision-Making Capabilities}}},
  booktitle = {Findings of the {{Association}} for {{Computational Linguistics}}: {{EMNLP}} 2023},
  author = {Loya, Manikanta and Sinha, Divya Anand and Futrell, Richard},
  year = {2023},
  pages = {3711--3716},
  doi = {10.18653/v1/2023.findings-emnlp.241},
  urldate = {2024-01-04},
  abstract = {The advancement of Large Language Models (LLMs) has led to their widespread use across a broad spectrum of tasks including decision making. Prior studies have compared the decision making abilities of LLMs with those of humans from a psychological perspective. However, these studies have not always properly accounted for the sensitivity of LLMs' behavior to hyperparameters and variations in the prompt. In this study, we examine LLMs' performance on the Horizon decision making task studied by Binz and Schulz (2023) analyzing how LLMs respond to variations in prompts and hyperparameters. By experimenting on three OpenAI language models possessing different capabilities, we observe that the decision making abilities fluctuate based on the input prompts and temperature settings. Contrary to previous findings language models display a human-like exploration exploitation tradeoff after simple adjustments to the prompt.},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/manikanta-72/Sensitivity-of-LLM-s-Decision-Making-Capabilities},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Loya et al_2023_Exploring the Sensitivity of LLMs' Decision-Making Capabilities.pdf}
}

@misc{luAIScientistFully2024,
  title = {The {{AI Scientist}}: {{Towards Fully Automated Open-Ended Scientific Discovery}}},
  shorttitle = {The {{AI Scientist}}},
  author = {Lu, Chris and Lu, Cong and Lange, Robert Tjarko and Foerster, Jakob and Clune, Jeff and Ha, David},
  year = {2024},
  month = aug,
  number = {arXiv:2408.06292},
  eprint = {2408.06292},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.06292},
  urldate = {2024-08-13},
  abstract = {One of the grand challenges of artificial general intelligence is developing agents capable of conducting scientific research and discovering new knowledge. While frontier models have already been used as aids to human scientists, e.g. for brainstorming ideas, writing code, or prediction tasks, they still conduct only a small part of the scientific process. This paper presents the first comprehensive framework for fully automatic scientific discovery, enabling frontier large language models to perform research independently and communicate their findings. We introduce The AI Scientist, which generates novel research ideas, writes code, executes experiments, visualizes results, describes its findings by writing a full scientific paper, and then runs a simulated review process for evaluation. In principle, this process can be repeated to iteratively develop ideas in an open-ended fashion, acting like the human scientific community. We demonstrate its versatility by applying it to three distinct subfields of machine learning: diffusion modeling, transformer-based language modeling, and learning dynamics. Each idea is implemented and developed into a full paper at a cost of less than \$15 per paper. To evaluate the generated papers, we design and validate an automated reviewer, which we show achieves near-human performance in evaluating paper scores. The AI Scientist can produce papers that exceed the acceptance threshold at a top machine learning conference as judged by our automated reviewer. This approach signifies the beginning of a new era in scientific discovery in machine learning: bringing the transformative benefits of AI agents to the entire research process of AI itself, and taking us closer to a world where endless affordable creativity and innovation can be unleashed on the world's most challenging problems. Our code is open-sourced at https://github.com/SakanaAI/AI-Scientist},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lu et al_2024_The AI Scientist.pdf;/Users/thomasgorman/Zotero/storage/8Y4C8PPY/2408.html}
}

@article{luanWhenDoesDiversity2012,
  title = {When {{Does Diversity Trump Ability}} (and {{Vice Versa}}) in {{Group Decision Making}}? {{A Simulation Study}}},
  shorttitle = {When {{Does Diversity Trump Ability}} (and {{Vice Versa}}) in {{Group Decision Making}}?},
  author = {Luan, Shenghua and Katsikopoulos, Konstantinos V. and Reimer, Torsten},
  year = {2012},
  month = feb,
  journal = {PLOS ONE},
  volume = {7},
  number = {2},
  pages = {e31043},
  publisher = {Public Library of Science},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0031043},
  urldate = {2024-06-22},
  abstract = {It is often unclear which factor plays a more critical role in determining a group's performance: the diversity among members of the group or their individual abilities. In this study, we addressed this ``diversity vs. ability'' issue in a decision-making task. We conducted three simulation studies in which we manipulated agents' individual ability (or accuracy, in the context of our investigation) and group diversity by varying (1) the heuristics agents used to search task-relevant information (i.e., cues); (2) the size of their groups; (3) how much they had learned about a good cue search order; and (4) the magnitude of errors in the information they searched. In each study, we found that a manipulation reducing agents' individual accuracy simultaneously increased their group's diversity, leading to a conflict between the two. These conflicts enabled us to identify certain conditions under which diversity trumps individual accuracy, and vice versa. Specifically, we found that individual accuracy is more important in task environments in which cues differ greatly in the quality of their information, and diversity matters more when such differences are relatively small. Changing the size of a group and the amount of learning by an agent had a limited impact on this general effect of task environment. Furthermore, we found that a group achieves its highest accuracy when there is an intermediate amount of errors in the cue information, regardless of the environment and the heuristic used, an effect that we believe has not been previously reported and warrants further investigation.},
  langid = {english},
  keywords = {Agent-based modeling,Decision making,Flowers,Foraging,Honey bees,Learning,Learning curves,Reducing agents},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Luan et al_2012_When Does Diversity Trump Ability (and Vice Versa) in Group Decision Making.pdf}
}

@inproceedings{luAreEmergentAbilities2024,
  title = {Are {{Emergent Abilities}} in {{Large Language Models}} Just {{In-Context Learning}}?},
  booktitle = {Proceedings of the 62nd {{Annual Meeting}} of the {{Association}} for {{Computational Linguistics}} ({{Volume}} 1: {{Long Papers}})},
  author = {Lu, Sheng and Bigoulaeva, Irina and Sachdeva, Rachneet and Tayyar Madabushi, Harish and Gurevych, Iryna},
  editor = {Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek},
  year = {2024},
  month = aug,
  pages = {5098--5139},
  publisher = {Association for Computational Linguistics},
  address = {Bangkok, Thailand},
  urldate = {2024-08-19},
  abstract = {Large language models, comprising billions of parameters and pre-trained on extensive web-scale corpora, have been claimed to acquire certain capabilities without having been specifically trained on them. These capabilities, referred to as ``emergent abilities,'' have been a driving force in discussions regarding the potentials and risks of language models. A key challenge in evaluating emergent abilities is that they are confounded by model competencies that arise through alternative prompting techniques, including in-context learning, which is the ability of models to complete a task based on a few examples. We present a novel theory that explains emergent abilities, taking into account their potential confounding factors, and rigorously substantiate this theory through over 1000 experiments. Our findings suggest that purported emergent abilities are not truly emergent, but result from a combination of in-context learning, model memory, and linguistic knowledge. Our work is a foundational step in explaining language model performance, providing a template for their efficient use and clarifying the paradox of their ability to excel in some instances while faltering in others. Thus, we demonstrate that their capabilities should not be overestimated.},
  annotation = {https://github.com/UKPLab/on-emergence\\
\\
https://h-tayyarmadabushi.github.io/Emergent\_Abilities\_and\_in-Context\_Learning/\\
\\
https://www.reddit.com/r/science/comments/1ev4f04/chatgpt\_and\_other\_large\_language\_models\_llms/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lu et al_2024_Are Emergent Abilities in Large Language Models just In-Context Learning.pdf}
}

@article{luDoesMoreAdvice2024,
  title = {Does {{More Advice Help}}? {{The Effects}} of {{Second Opinions}} in {{AI-Assisted Decision Making}}},
  shorttitle = {Does {{More Advice Help}}?},
  author = {Lu, Zhuoran and Wang, Dakuo and Yin, Ming},
  year = {2024},
  month = apr,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {8},
  number = {CSCW1},
  pages = {1--31},
  issn = {2573-0142},
  doi = {10.1145/3653708},
  urldate = {2024-09-26},
  abstract = {AI assistance in decision-making has become popular, yet people's inappropriate reliance on AI often leads to unsatisfactory human-AI collaboration performance. In this paper, through three pre-registered, randomized human subject experiments, we explore whether and how the provision of second opinions may affect decision-makers' behavior and performance in AI-assisted decision-making. We find that if both the AI model's decision recommendation and a second opinion are always presented together, decision-makers reduce their over-reliance on AI while increase their under-reliance on AI, regardless whether the second opinion is generated by a peer or another AI model. However, if decision-makers have the control to decide when to solicit a peer's second opinion, we find that their active solicitations of second opinions have the potential to mitigate over-reliance on AI without inducing increased under-reliance in some cases. We conclude by discussing the implications of our findings for promoting effective human-AI collaborations in decision-making.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lu et al_2024_Does More Advice Help.pdf}
}

@article{luHumanlikeDecisionMaking2023,
  title = {Human-like Decision Making for Lane Change Based on the Cognitive Map and Hierarchical Reinforcement Learning},
  author = {Lu, Chao and Lu, Hongliang and Chen, Danni and Wang, Haoyang and Li, Penghui and Gong, Jianwei},
  year = {2023},
  month = nov,
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {156},
  pages = {104328},
  issn = {0968-090X},
  doi = {10.1016/j.trc.2023.104328},
  urldate = {2024-09-16},
  abstract = {Human-like decision making is crucial to developing an autonomous driving system (ADS) with high acceptance. Inspired by the cognitive map, this paper proposes a hierarchical reinforcement learning (HRL)-based framework with sound biological plausibility named Cog-MP, which combines the cognitive map and motion primitive (MP) in human-like decision making. In the proposed Cog-MP, three general levels involved in ADS are integrated in a top--bottom way, including operational, decision-making, and cognitive levels. The proposed Cog-MP is used to make human-like decisions in lane-changing scenarios, focusing on three aspects: human-like lane decision, human-like path decision, and decision optimization. The proposed framework is validated on two groups of realistic lane-change data, of which one group is used to train cognitions towards different styles of driving behaviors, and the other group is to provide validation scenarios. Experimental results show that the proposed framework can generate human-like decisions and perform soundly regarding the three considered aspects, demonstrating a promising prospect in developing a brain-inspired human-like ADS.},
  keywords = {Cognitive map,Hierarchical reinforcement learning,Human-like autonomous driving system,Human-like decision making,Lane change},
  file = {/Users/thomasgorman/Zotero/storage/ZM3YCLLN/S0968090X23003170.html}
}

@misc{luoLargeLanguageModels2024,
  title = {Large Language Models Surpass Human Experts in Predicting Neuroscience Results},
  author = {Luo, Xiaoliang and Rechardt, Akilles and Sun, Guangzhi and Nejad, Kevin K. and Y{\'a}{\~n}ez, Felipe and Yilmaz, Bati and Lee, Kangjoo and Cohen, Alexandra O. and Borghesani, Valentina and Pashkov, Anton and Marinazzo, Daniele and Nicholas, Jonathan and Salatiello, Alessandro and Sucholutsky, Ilia and Minervini, Pasquale and Razavi, Sepehr and Rocca, Roberta and Yusifov, Elkhan and Okalova, Tereza and Gu, Nianlong and Ferianc, Martin and Khona, Mikail and Patil, Kaustubh R. and Lee, Pui-Shee and Mata, Rui and Myers, Nicholas E. and Bizley, Jennifer K. and Musslick, Sebastian and Bilgin, Isil Poyraz and Niso, Guiomar and Ales, Justin M. and Gaebler, Michael and Murty, N. Apurva Ratan and {Loued-Khenissi}, Leyla and Behler, Anna and Hall, Chloe M. and Dafflon, Jessica and Bao, Sherry Dongqi and Love, Bradley C.},
  year = {2024},
  month = mar,
  number = {arXiv:2403.03230},
  eprint = {2403.03230},
  publisher = {arXiv},
  urldate = {2024-05-19},
  abstract = {Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition},
  annotation = {https://www.youtube.com/watch?v=sDt4-Q\_jz7g\&ab\_channel=BCL},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Luo et al_2024_Large language models surpass human experts in predicting neuroscience results2.pdf}
}

@misc{luoLargeLanguageModels2024a,
  title = {Large Language Models Surpass Human Experts in Predicting Neuroscience Results},
  author = {Luo, Xiaoliang and Rechardt, Akilles and Sun, Guangzhi and Nejad, Kevin K. and Y{\'a}{\~n}ez, Felipe and Yilmaz, Bati and Lee, Kangjoo and Cohen, Alexandra O. and Borghesani, Valentina and Pashkov, Anton and Marinazzo, Daniele and Nicholas, Jonathan and Salatiello, Alessandro and Sucholutsky, Ilia and Minervini, Pasquale and Razavi, Sepehr and Rocca, Roberta and Yusifov, Elkhan and Okalova, Tereza and Gu, Nianlong and Ferianc, Martin and Khona, Mikail and Patil, Kaustubh R. and Lee, Pui-Shee and Mata, Rui and Myers, Nicholas E. and Bizley, Jennifer K. and Musslick, Sebastian and Bilgin, Isil Poyraz and Niso, Guiomar and Ales, Justin M. and Gaebler, Michael and Murty, N. Apurva Ratan and Hall, Chloe M. and Dafflon, Jessica and Bao, Sherry Dongqi and Love, Bradley C.},
  year = {2024},
  month = mar,
  number = {arXiv:2403.03230},
  eprint = {2403.03230},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-03-09},
  abstract = {Scientific discoveries often hinge on synthesizing decades of research, a task that potentially outstrips human information processing capacities. Large language models (LLMs) offer a solution. LLMs trained on the vast scientific literature could potentially integrate noisy yet interrelated findings to forecast novel results better than human experts. To evaluate this possibility, we created BrainBench, a forward-looking benchmark for predicting neuroscience results. We find that LLMs surpass experts in predicting experimental outcomes. BrainGPT, an LLM we tuned on the neuroscience literature, performed better yet. Like human experts, when LLMs were confident in their predictions, they were more likely to be correct, which presages a future where humans and LLMs team together to make discoveries. Our approach is not neuroscience-specific and is transferable to other knowledge-intensive endeavors.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Luo et al_2024_Large language models surpass human experts in predicting neuroscience results.pdf;/Users/thomasgorman/Zotero/storage/3K2VIAYK/2403.html}
}

@misc{luoMatchingDomainExperts2024,
  title = {Matching Domain Experts by Training from Scratch on Domain Knowledge},
  author = {Luo, Xiaoliang and Sun, Guangzhi and Love, Bradley C.},
  year = {2024},
  month = may,
  number = {arXiv:2405.09395},
  eprint = {2405.09395},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-05-18},
  abstract = {Recently, large language models (LLMs) have outperformed human experts in predicting the results of neuroscience experiments (Luo et al., 2024). What is the basis for this performance? One possibility is that statistical patterns in that specific scientific literature, as opposed to emergent reasoning abilities arising from broader training, underlie LLMs' performance. To evaluate this possibility, we trained (next word prediction) a relatively small 124M-parameter GPT-2 model on 1.3 billion tokens of domain-specific knowledge. Despite being orders of magnitude smaller than larger LLMs trained on trillions of tokens, small models achieved expert-level performance in predicting neuroscience results. Small models trained on the neuroscience literature succeeded when they were trained from scratch using a tokenizer specifically trained on neuroscience text or when the neuroscience literature was used to finetune a pretrained GPT-2. Our results indicate that expert-level performance may be attained by even small LLMs through domain-specific, autoregressive training approaches.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Luo et al_2024_Matching domain experts by training from scratch on domain knowledge.pdf}
}

@article{luoPotentialRolesLarge2024,
  title = {Potential {{Roles}} of {{Large Language Models}} in the {{Production}} of {{Systematic Reviews}} and {{Meta-Analyses}}},
  author = {Luo, Xufei and Chen, Fengxian and Zhu, Di and Wang, Ling and Wang, Zijun and Liu, Hui and Lyu, Meng and Wang, Ye and Wang, Qi and Chen, Yaolong},
  year = {2024},
  month = jun,
  journal = {Journal of Medical Internet Research},
  volume = {26},
  number = {1},
  pages = {e56780},
  publisher = {JMIR Publications Inc., Toronto, Canada},
  doi = {10.2196/56780},
  urldate = {2024-08-14},
  abstract = {Large language models (LLMs) such as ChatGPT have become widely applied in the field of medical research. In the process of conducting systematic reviews, similar tools can be used to expedite various steps, including defining clinical questions, performing the literature search, document screening, information extraction, and language refinement, thereby conserving resources and enhancing efficiency. However, when using LLMs, attention should be paid to transparent reporting, distinguishing between genuine and false content, and avoiding academic misconduct. In this viewpoint, we highlight the potential roles of LLMs in the creation of systematic reviews and meta-analyses, elucidating their advantages, limitations, and future research directions, aiming to provide insights and guidance for authors planning systematic reviews and meta-analyses.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Luo et al_2024_Potential Roles of Large Language Models in the Production of Systematic.pdf;/Users/thomasgorman/Zotero/storage/D7BZK23Y/e56780.html}
}

@article{luProbabilisticAnalogicalMapping2022,
  title = {Probabilistic Analogical Mapping with Semantic Relation Networks},
  author = {Lu, Hongjing and Ichien, Nicholas and Holyoak, Keith J.},
  year = {2022},
  month = oct,
  journal = {Psychological Review},
  volume = {129},
  number = {5},
  pages = {1078--1103},
  publisher = {American Psychological Association},
  issn = {0033-295X},
  doi = {10.1037/rev0000358},
  urldate = {2024-05-26},
  abstract = {The human ability to flexibly reason using analogies with domain-general content depends on mechanisms for identifying relations between concepts, and for mapping concepts and their relations across analogs. Building on a recent model of how semantic relations can be learned from nonrelational word embeddings, we present a new computational model of mapping between two analogs. The model adopts a Bayesian framework for probabilistic graph matching, operating on semantic relation networks constructed from distributed representations of individual concepts and of relations between concepts. Through comparisons of model predictions with human performance in a novel mapping task requiring integration of multiple relations, as well as in several classic studies, we demonstrate that the model accounts for a broad range of phenomena involving analogical mapping by both adults and children. We also show the potential for extending the model to deal with analog retrieval. Our approach demonstrates that human-like analogical mapping can emerge from comparison mechanisms applied to rich semantic representations of individual concepts and relations. (PsycInfo Database Record (c) 2022 APA, all rights reserved)},
  keywords = {analogy,Analogy,Computational Modeling,Concepts,distributed representations,Human Information Storage,machine learning,Machine Learning,mapping,Prediction,Probability,relations,Semantic Networks},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lu et al_2022_Probabilistic analogical mapping with semantic relation networks.pdf}
}

@misc{luSmallLanguageModels2024,
  title = {Small {{Language Models}}: {{Survey}}, {{Measurements}}, and {{Insights}}},
  shorttitle = {Small {{Language Models}}},
  author = {Lu, Zhenyan and Li, Xiang and Cai, Dongqi and Yi, Rongjie and Liu, Fangming and Zhang, Xiwen and Lane, Nicholas D. and Xu, Mengwei},
  year = {2024},
  month = sep,
  number = {arXiv:2409.15790},
  eprint = {2409.15790},
  publisher = {arXiv},
  urldate = {2024-09-28},
  abstract = {Small language models (SLMs), despite their widespread adoption in modern smart devices, have received significantly less academic attention compared to their large language model (LLM) counterparts, which are predominantly deployed in data centers and cloud environments. While researchers continue to improve the capabilities of LLMs in the pursuit of artificial general intelligence, SLM research aims to make machine intelligence more accessible, affordable, and efficient for everyday tasks. Focusing on transformer-based, decoder-only language models with 100M--5B parameters, we survey 59 state-of-the-art open-source SLMs, analyzing their technical innovations across three axes: architectures, training datasets, and training algorithms. In addition, we evaluate their capabilities in various domains, including commonsense reasoning, in-context learning, mathematics, and coding. To gain further insight into their on-device runtime costs, we benchmark their inference latency and memory footprints. Through in-depth analysis of our benchmarking data, we offer valuable insights to advance research in this field.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/UbiquitousLearning/SLM\_Survey},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Lu et al_2024_Small Language Models.pdf}
}

@misc{maAIBetterProgramming2023,
  title = {Is {{AI}} the Better Programming Partner? {{Human-Human Pair Programming}} vs. {{Human-AI pAIr Programming}}},
  shorttitle = {Is {{AI}} the Better Programming Partner?},
  author = {Ma, Qianou and Wu, Tongshuang and Koedinger, Kenneth},
  year = {2023},
  month = jun,
  number = {arXiv:2306.05153},
  eprint = {2306.05153},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-06-14},
  abstract = {The emergence of large-language models (LLMs) that excel at code generation and commercial products such as GitHub's Copilot has sparked interest in human-AI pair programming (referred to as "pAIr programming") where an AI system collaborates with a human programmer. While traditional pair programming between humans has been extensively studied, it remains uncertain whether its findings can be applied to human-AI pair programming. We compare human-human and human-AI pair programming, exploring their similarities and differences in interaction, measures, benefits, and challenges. We find that the effectiveness of both approaches is mixed in the literature (though the measures used for pAIr programming are not as comprehensive). We summarize moderating factors on the success of human-human pair programming, which provides opportunities for pAIr programming research. For example, mismatched expertise makes pair programming less productive, therefore well-designed AI programming assistants may adapt to differences in expertise levels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ma et al_2023_Is AI the better programming partner.pdf;/Users/thomasgorman/Zotero/storage/IQPI97DR/2306.html}
}

@article{macmillan-scottIrrationalityCognitiveBiases2024,
  title = {({{Ir}})Rationality and Cognitive Biases in Large Language Models},
  author = {{Macmillan-Scott}, Olivia and Musolesi, Mirco},
  year = {2024},
  month = jun,
  journal = {Royal Society Open Science},
  volume = {11},
  number = {6},
  pages = {240255},
  issn = {2054-5703},
  doi = {10.1098/rsos.240255},
  urldate = {2024-07-01},
  abstract = {Do large language models (LLMs) display rational reasoning? LLMs have been shown to contain human biases due to the data they have been trained on; whether this is reflected in rational reasoning remains less clear. In this paper, we answer this question by evaluating seven language models using tasks from the cognitive psychology literature. We find that, like humans, LLMs display irrationality in these tasks. However, the way this irrationality is displayed does not reflect that shown by humans. When incorrect answers are given by LLMs to these tasks, they are often incorrect in ways that differ from human-like biases. On top of this, the LLMs reveal an additional layer of irrationality in the significant inconsistency of the responses. Aside from the experimental results, this paper seeks to make a methodological contribution by showing how we can assess and compare different capabilities of these types of models, in this case with respect to rational reasoning.},
  langid = {english},
  annotation = {https://github.com/oliviams/LLM\_Rationality\\
\\
https://zenodo.org/records/10966401},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Macmillan-Scott_Musolesi_2024_(Ir)rationality and cognitive biases in large language models.pdf}
}

@article{maddiganChat2VISGeneratingData2023,
  title = {{{Chat2VIS}}: {{Generating Data Visualizations}} via {{Natural Language Using ChatGPT}}, {{Codex}} and {{GPT-3 Large Language Models}}},
  shorttitle = {{{Chat2VIS}}},
  author = {Maddigan, Paula and Susnjak, Teo},
  year = {2023},
  journal = {IEEE Access},
  volume = {11},
  pages = {45181--45193},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2023.3274199},
  urldate = {2023-11-30},
  abstract = {The field of data visualisation has long aimed to devise solutions for generating visualisations directly from natural language text. Research in Natural Language Interfaces (NLIs) has contributed towards the development of such techniques. However, the implementation of workable NLIs has always been challenging due to the inherent ambiguity of natural language, as well as in consequence of unclear and poorly written user queries which pose problems for existing language models in discerning user intent. Instead of pursuing the usual path of developing new iterations of language models, this study uniquely proposes leveraging the advancements in pre-trained large language models (LLMs) such as ChatGPT and GPT-3 to convert free-form natural language directly into code for appropriate visualisations. This paper presents a novel system, Chat2VIS, which takes advantage of the capabilities of LLMs and demonstrates how, with effective prompt engineering, the complex problem of language understanding can be solved more efficiently, resulting in simpler and more accurate end-to-end solutions than prior approaches. Chat2VIS shows that LLMs together with the proposed prompts offer a reliable approach to rendering visualisations from natural language queries, even when queries are highly misspecified and underspecified. This solution also presents a significant reduction in costs for the development of NLI systems, while attaining greater visualisation inference abilities compared to traditional NLP approaches that use hand-crafted grammar rules and tailored models. This study also presents how LLM prompts can be constructed in a way that preserves data security and privacy while being generalisable to different datasets. This work compares the performance of GPT-3, Codex and ChatGPT across several case studies and contrasts the performances with prior studies.},
  langid = {english},
  annotation = {https://github.com/nl4dv/nl4dv/tree/master},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Maddigan_Susnjak_2023_Chat2VIS.pdf}
}

@article{mahowaldDissociatingLanguageThought2024,
  title = {Dissociating Language and Thought in Large Language Models},
  author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
  year = {2024},
  month = mar,
  journal = {Trends in Cognitive Sciences},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2024.01.011},
  urldate = {2024-05-23},
  abstract = {Large language models (LLMs) have come closest among all models to date to mastering human language, yet opinions about their linguistic and cognitive capabilities remain split. Here, we evaluate LLMs using a distinction between formal linguistic competence (knowledge of linguistic rules and patterns) and functional linguistic competence (understanding and using language in the world). We ground this distinction in human neuroscience, which has shown that formal and functional competence rely on different neural mechanisms. Although LLMs are surprisingly good at formal competence, their performance on functional competence tasks remains spotty and often requires specialized fine-tuning and/or coupling with external modules. We posit that models that use language in human-like ways would need to master both of these competence types, which, in turn, could require the emergence of separate mechanisms specialized for formal versus functional linguistic competence.},
  keywords = {cognitive neuroscience,computational modeling,language and thought,large language models,linguistic competence},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Mahowald et al_2024_Dissociating language and thought in large language models.pdf;/Users/thomasgorman/Zotero/storage/EWUS7BTG/S1364661324000275.html}
}

@misc{maHumanAIDeliberationDesign2024,
  title = {Towards {{Human-AI Deliberation}}: {{Design}} and {{Evaluation}} of {{LLM-Empowered Deliberative AI}} for {{AI-Assisted Decision-Making}}},
  shorttitle = {Towards {{Human-AI Deliberation}}},
  author = {Ma, Shuai and Chen, Qiaoyi and Wang, Xinru and Zheng, Chengbo and Peng, Zhenhui and Yin, Ming and Ma, Xiaojuan},
  year = {2024},
  month = mar,
  number = {arXiv:2403.16812},
  eprint = {2403.16812},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-26},
  abstract = {In AI-assisted decision-making, humans often passively review AI's suggestion and decide whether to accept or reject it as a whole. In such a paradigm, humans are found to rarely trigger analytical thinking and face difficulties in communicating the nuances of conflicting opinions to the AI when disagreements occur. To tackle this challenge, we propose Human-AI Deliberation, a novel framework to promote human reflection and discussion on conflicting human-AI opinions in decision-making. Based on theories in human deliberation, this framework engages humans and AI in dimension-level opinion elicitation, deliberative discussion, and decision updates. To empower AI with deliberative capabilities, we designed Deliberative AI, which leverages large language models (LLMs) as a bridge between humans and domain-specific models to enable flexible conversational interactions and faithful information provision. An exploratory evaluation on a graduate admissions task shows that Deliberative AI outperforms conventional explainable AI (XAI) assistants in improving humans' appropriate reliance and task performance. Based on a mixed-methods analysis of participant behavior, perception, user experience, and open-ended feedback, we draw implications for future AI-assisted decision tool design.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ma et al_2024_Towards Human-AI Deliberation.pdf}
}

@article{malloyApplyingGenerativeArtificial2024,
  title = {Applying {{Generative Artificial Intelligence}} to Cognitive Models of Decision Making},
  author = {Malloy, Tyler and Gonzalez, Cleotilde},
  year = {2024},
  month = may,
  journal = {Frontiers in Psychology},
  volume = {15},
  pages = {1387948},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2024.1387948},
  urldate = {2024-08-16},
  abstract = {Introduction               Generative Artificial Intelligence has made significant impacts in many fields, including computational cognitive modeling of decision making, although these applications have not yet been theoretically related to each other. This work introduces a categorization of applications of Generative Artificial Intelligence to cognitive models of decision making.                                         Methods               This categorization is used to compare the existing literature and to provide insight into the design of an ablation study to evaluate our proposed model in three experimental paradigms. These experiments used for model comparison involve modeling human learning and decision making based on both visual information and natural language, in tasks that vary in realism and complexity. This comparison of applications takes as its basis Instance-Based Learning Theory, a theory of experiential decision making from which many models have emerged and been applied to a variety of domains and applications.                                         Results               The best performing model from the ablation we performed used a generative model to both create memory representations as well as predict participant actions. The results of this comparison demonstrates the importance of generative models in both forming memories and predicting actions in decision-modeling research.                                         Discussion               In this work, we present a model that integrates generative and cognitive models, using a variety of stimuli, applications, and training methods. These results can provide guidelines for cognitive modelers and decision making researchers interested in integrating Generative AI into their methods.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Malloy_Gonzalez_2024_Applying Generative Artificial Intelligence to cognitive models of decision.pdf}
}

@article{malloyEfficientVisualRepresentations2024,
  title = {Efficient {{Visual Representations}} for {{Learning}} and {{Decision Making}}},
  author = {Malloy, Tyler and Sims, Chris},
  year = {2024},
  month = jun,
  journal = {Psychological Review},
  volume = {in press},
  abstract = {The efficient representation of visual information is essential for learning and decision making due to the complexity and uncertainty of the world, as well as inherent constraints on the capacity of cognitive systems. We hypothesize that biological agents learn to efficiently represent visual information in a manner that balances performance across multiple potentially competing objectives. In this paper we examine two such objectives: storing information in a manner that supports accurate recollection (maximizing veridicality) and storing information in a manner that facilitates utility-based decision making (maximizing behavioral utility). That these two objectives may be in conflict is not immediately obvious. Our hypothesis suggests that neither behavior nor representation formation can be fully understood by studying either in isolation, with information processing constraints exerting an over-arching influence. Alongside this hypothesis we develop a computational model of representation formation and behavior motivated by recent methods in machine learning and neuroscience. The resulting model explains both the beneficial aspects of human visual learning, such as fast acquisition and high generalization, as well as the biases that result from information constraints. To test this model, we developed two experimental paradigms, in decision making and learning, to evaluate how well the model's predictions match human behavior. A key feature of the proposed model is that it predicts the occurrence of commonly found biases in human decision making, resulting from the desire to form efficient representations of visual information that are useful for behavioral goals in learning and decision making and optimized under an information processing constraint}
}

@misc{malloyLeveragingCognitiveModel2024,
  title = {Leveraging a {{Cognitive Model}} to {{Measure Subjective Similarity}} of {{Human}} and {{GPT-4 Written Content}}},
  author = {Malloy, Tyler and Ferreira, Maria Jos{\'e} and Fang, Fei and Gonzalez, Cleotilde},
  year = {2024},
  month = aug,
  number = {arXiv:2409.00269},
  eprint = {2409.00269},
  publisher = {arXiv},
  urldate = {2024-09-23},
  abstract = {Cosine similarity between two documents can be computed using token embeddings formed by Large Language Models (LLMs) such as GPT-4, and used to categorize those documents across a range of uses. However, these similarities are ultimately dependent on the corpora used to train these LLMs, and may not reflect subjective similarity of individuals or how their biases and constraints impact similarity metrics. This lack of cognitively-aware personalization of similarity metrics can be particularly problematic in educational and recommendation settings where there is a limited number of individual judgements of category or preference, and biases can be particularly relevant. To address this, we rely on an integration of an Instance-Based Learning (IBL) cognitive model with LLM embeddings to develop the Instance-Based Individualized Similarity (IBIS) metric. This similarity metric is beneficial in that it takes into account individual biases and constraints in a manner that is grounded in the cognitive mechanisms of decision making. To evaluate the IBIS metric, we also introduce a dataset of human categorizations of emails as being either dangerous (phishing) or safe (ham). This dataset is used to demonstrate the benefits of leveraging a cognitive model to measure the subjective similarity of human participants in an educational setting.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/TylerJamesMalloy/cognitive-similarity},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Malloy et al_2024_Leveraging a Cognitive Model to Measure Subjective Similarity of Human and.pdf}
}

@article{mamoImpactCognitiveLoad2024,
  title = {The Impact of Cognitive Load on a Lane Change Task ({{LCT}}) among Male Autistic Individuals: {{A}} Driving Simulator Study},
  shorttitle = {The Impact of Cognitive Load on a Lane Change Task ({{LCT}}) among Male Autistic Individuals},
  author = {Mamo, Wondwesen Girma and Alhajyaseen, Wael K. M. and Brijs, Kris and Dirix, H{\'e}l{\`e}ne and Vanroelen, Giovanni and Hussain, Qinaat and Brijs, Tom and Ross, Veerle},
  year = {2024},
  month = oct,
  journal = {Transportation Research Part F: Traffic Psychology and Behaviour},
  volume = {106},
  pages = {27--43},
  issn = {1369-8478},
  doi = {10.1016/j.trf.2024.07.030},
  urldate = {2024-09-16},
  abstract = {This study investigated the impact of cognitive load on driving among autistic individuals with the use of an adapted, driving simulator-based, Lane Change Task (LCT). A secondary task was used to induce increasing verbal WM load. A total of 51 male participants, 17 autistic and 34 non-autistic individuals participated in the study. Participants drove the simulator-based LCT without (baseline) and with a three-level auditory-verbal response N-back task (i.e., 0-back,1-back, and 2-back) developed to tax working memory capacity. The included driving parameters were: mean deviation in the lane change path (MDEV), percentage of correct lane changes (PCL) in response to a lane change sign, and lane change initiation (LCI). The percentage of error rate (PER) was included to measure participants' performance on the secondary task. Dual-task performance of both groups deteriorated with increasing cognitive load, but this effect was more pronounced in the autistic group. Specifically, the performance of both group on MDEV, PCL, and PER suffered from the increasing cognitive load. Nevertheless, neither PCL nor LCI differ between autistic and non-autistic participants. Notably, LCI also deteriorated with increasing cognitive load for non-autistic participants, but not for autistic participants. Similar to previous research, it is suggested that distracted driving should be eliminated as much as possible before occurring in the first place. Specific suggestions for eliminating distraction in autistic drivers are provided.},
  keywords = {Autism,Cognitive load,Driving simulator,Lane Change Task,N-back task},
  file = {/Users/thomasgorman/Zotero/storage/3JRBY3J5/S1369847824001992.html}
}

@misc{marjiehPredictingHumanSimilarity2022,
  title = {Predicting {{Human Similarity Judgments Using Large Language Models}}},
  author = {Marjieh, Raja and Sucholutsky, Ilia and Sumers, Theodore R. and Jacoby, Nori and Griffiths, Thomas L.},
  year = {2022},
  month = feb,
  number = {arXiv:2202.04728},
  eprint = {2202.04728},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-25},
  abstract = {Similarity judgments provide a well-established method for accessing mental representations, with applications in psychology, neuroscience and machine learning. However, collecting similarity judgments can be prohibitively expensive for naturalistic datasets as the number of comparisons grows quadratically in the number of stimuli. One way to tackle this problem is to construct approximation procedures that rely on more accessible proxies for predicting similarity. Here we leverage recent advances in language models and online recruitment, proposing an efficient domain-general procedure for predicting human similarity judgments based on text descriptions. Intuitively, similar stimuli are likely to evoke similar descriptions, allowing us to use description similarity to predict pairwise similarity judgments. Crucially, the number of descriptions required grows only linearly with the number of stimuli, drastically reducing the amount of data required. We test this procedure on six datasets of naturalistic images and show that our models outperform previous approaches based on visual information.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Marjieh et al_2022_Predicting Human Similarity Judgments Using Large Language Models.pdf}
}

@article{marjiehTaskAllocationTeams2024,
  title = {Task {{Allocation}} in {{Teams}} as a {{Multi-Armed Bandit}}},
  author = {Marjieh, Raja and Gokhale, Anand and Bullo, Francesco and Griffiths, Thomas L},
  year = {2024},
  abstract = {Humans rely on efficient distribution of resources to transcend the abilities of individuals. Successful task allocation, whether in small teams or across large institutions, depends on individuals' ability to discern their own and others' strengths and weaknesses, and to optimally act on them. This dependence creates a tension between exploring the capabilities of others and exploiting the knowledge acquired so far, which can be challenging. How do people navigate this tension? To address this question, we propose a novel task allocation paradigm in which a human agent is asked to repeatedly allocate tasks in three distinct classes (categorizing a blurry image, detecting a noisy voice command, and solving an anagram) between themselves and two other (bot) team members to maximize team performance. We show that this problem can be recast as a combinatorial multi-armed bandit which allows us to compare people's performance against two well-known strategies, Thompson Sampling and Upper Confidence Bound (UCB). We find that humans are able to successfully integrate information about the capabilities of different team members to infer optimal allocations, and in some cases perform on par with these optimal strategies. Our approach opens up new avenues for studying the mechanisms underlying collective cooperation in teams.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Marjieh et al_Task Allocation in Teams as a Multi-Armed Bandit.pdf}
}

@misc{marjiehWordsAreAll2023,
  title = {Words Are All You Need? {{Language}} as an Approximation for Human Similarity Judgments},
  shorttitle = {Words Are All You Need?},
  author = {Marjieh, Raja and {van Rijn}, Pol and Sucholutsky, Ilia and Sumers, Theodore R. and Lee, Harin and Griffiths, Thomas L. and Jacoby, Nori},
  year = {2023},
  month = feb,
  number = {arXiv:2206.04105},
  eprint = {2206.04105},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-06-06},
  abstract = {Human similarity judgments are a powerful supervision signal for machine learning applications based on techniques such as contrastive learning, information retrieval, and model alignment, but classical methods for collecting human similarity judgments are too expensive to be used at scale. Recent methods propose using pre-trained deep neural networks (DNNs) to approximate human similarity, but pre-trained DNNs may not be available for certain domains (e.g., medical images, low-resource languages) and their performance in approximating human similarity has not been extensively tested. We conducted an evaluation of 611 pre-trained models across three domains -- images, audio, video -- and found that there is a large gap in performance between human similarity judgments and pre-trained DNNs. To address this gap, we propose a new class of similarity approximation methods based on language. To collect the language data required by these new methods, we also developed and validated a novel adaptive tag collection pipeline. We find that our proposed language-based methods are significantly cheaper, in the number of human judgments, than classical methods, but still improve performance over the DNN-based methods. Finally, we also develop `stacked' methods that combine language embeddings with DNN embeddings, and find that these consistently provide the best approximations for human similarity across all three of our modalities. Based on the results of this comprehensive study, we provide a concise guide for researchers interested in collecting or approximating human similarity data. To accompany this guide, we also release all of the similarity and language data, a total of 206,339 human judgments, that we collected in our experiments, along with a detailed breakdown of all modeling results.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Marjieh et al_2023_Words are all you need.pdf}
}

@article{markkulaSustainedSensorimotorControl2018,
  title = {Sustained Sensorimotor Control as Intermittent Decisions about Prediction Errors: Computational Framework and Application to Ground Vehicle Steering},
  shorttitle = {Sustained Sensorimotor Control as Intermittent Decisions about Prediction Errors},
  author = {Markkula, Gustav and Boer, Erwin and Romano, Richard and Merat, Natasha},
  year = {2018},
  month = jun,
  journal = {Biological Cybernetics},
  volume = {112},
  number = {3},
  pages = {181--207},
  issn = {1432-0770},
  doi = {10.1007/s00422-017-0743-9},
  urldate = {2024-09-16},
  abstract = {A conceptual and computational framework is proposed for modelling of human sensorimotor control and is exemplified for the sensorimotor task of steering a car. The framework emphasises control intermittency and extends on existing models by suggesting that the nervous system implements intermittent control using a combination of (1) motor primitives, (2) prediction of sensory outcomes of motor actions, and (3) evidence accumulation of prediction errors. It is shown that approximate but useful sensory predictions in the intermittent control context can be constructed without detailed forward models, as a superposition of simple prediction primitives, resembling neurobiologically observed corollary discharges. The proposed mathematical framework allows straightforward extension to intermittent behaviour from existing one-dimensional continuous models in the linear control and ecological psychology traditions. Empirical data from a driving simulator are used in model-fitting analyses to test some of the framework's main theoretical predictions: it is shown that human steering control, in routine lane-keeping and in a demanding near-limit task, is better described as a sequence of discrete stepwise control adjustments, than as continuous control. Results on the possible roles of sensory prediction in control adjustment amplitudes, and of evidence accumulation mechanisms in control onset timing, show trends that match the theoretical predictions; these warrant further investigation. The results for the accumulation-based model align with other recent literature, in a possibly converging case against the type of threshold mechanisms that are often assumed in existing models of intermittent control.},
  langid = {english},
  keywords = {Corollary discharge,Evidence accumulation,Motor primitive,Sensorimotor control,Sensory prediction,Steering},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Markkula et al_2018_Sustained sensorimotor control as intermittent decisions about prediction errors.pdf}
}

@article{markkulaSustainedSensorimotorControl2018a,
  title = {Sustained Sensorimotor Control as Intermittent Decisions about Prediction Errors: Computational Framework and Application to Ground Vehicle Steering},
  shorttitle = {Sustained Sensorimotor Control as Intermittent Decisions about Prediction Errors},
  author = {Markkula, Gustav and Boer, Erwin and Romano, Richard and Merat, Natasha},
  year = {2018},
  month = jun,
  journal = {Biological Cybernetics},
  volume = {112},
  number = {3},
  pages = {181--207},
  issn = {1432-0770},
  doi = {10.1007/s00422-017-0743-9},
  urldate = {2021-11-10},
  abstract = {A conceptual and computational framework is proposed for modelling of human sensorimotor control and is exemplified for the sensorimotor task of steering a car. The framework emphasises control intermittency and extends on existing models by suggesting that the nervous system implements intermittent control using a combination of (1) motor primitives, (2) prediction of sensory outcomes of motor actions, and (3) evidence accumulation of prediction errors. It is shown that approximate but useful sensory predictions in the intermittent control context can be constructed without detailed forward models, as a superposition of simple prediction primitives, resembling neurobiologically observed corollary discharges. The proposed mathematical framework allows straightforward extension to intermittent behaviour from existing one-dimensional continuous models in the linear control and ecological psychology traditions. Empirical data from a driving simulator are used in model-fitting analyses to test some of the framework's main theoretical predictions: it is shown that human steering control, in routine lane-keeping and in a demanding near-limit task, is better described as a sequence of discrete stepwise control adjustments, than as continuous control. Results on the possible roles of sensory prediction in control adjustment amplitudes, and of evidence accumulation mechanisms in control onset timing, show trends that match the theoretical predictions; these warrant further investigation. The results for the accumulation-based model align with other recent literature, in a possibly converging case against the type of threshold mechanisms that are often assumed in existing models of intermittent control.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/8IX9CTAB/Markkula et al. - 2018 - Sustained sensorimotor control as intermittent dec.pdf}
}

@article{markowitzCanGenerativeAI2024,
  title = {Can Generative {{AI}} Infer Thinking Style from Language? {{Evaluating}} the Utility of {{AI}} as a Psychological Text Analysis Tool},
  shorttitle = {Can Generative {{AI}} Infer Thinking Style from Language?},
  author = {Markowitz, David M.},
  year = {2024},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {56},
  number = {4},
  pages = {3548--3559},
  issn = {1554-3528},
  doi = {10.3758/s13428-024-02344-0},
  urldate = {2024-07-28},
  abstract = {Generative AI, short for Generative Artificial Intelligence, a class of artificial intelligence systems, is not currently the choice technology for text analysis, but prior work suggests it may have some utility to assess dynamics like emotion. The current work builds upon this empirical foundation to consider how analytic thinking scores from a large language model chatbot, ChatGPT, were linked to analytic thinking scores from dictionary-based tools like Linguistic Inquiry and Word Count (LIWC). Using over 16,000 texts from four samples and tested against three prompts and two large language models (GPT-3.5, GPT-4), the evidence suggests there were small associations between ChatGPT and LIWC analytic thinking scores (meta-analytic effect sizes: .058 {$<$} rs {$<$} .304; ps {$<$} .001). When given the formula to calculate the LIWC analytic thinking index, ChatGPT performed incorrect mathematical operations in 22\% of the cases, suggesting basic word and number processing may be unreliable with large language models. Researchers should be cautious when using AI for text analysis.},
  langid = {english},
  keywords = {Analytic thinking,Generative AI,Large language models,LIWC,Text analysis},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Markowitz_2024_Can generative AI infer thinking style from language.pdf}
}

@misc{marksGeometryTruthEmergent2023,
  title = {The {{Geometry}} of {{Truth}}: {{Emergent Linear Structure}} in {{Large Language Model Representations}} of {{True}}/{{False Datasets}}},
  shorttitle = {The {{Geometry}} of {{Truth}}},
  author = {Marks, Samuel and Tegmark, Max},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06824},
  eprint = {2310.06824},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-13},
  abstract = {Large Language Models (LLMs) have impressive capabilities, but are also prone to outputting falsehoods. Recent work has developed techniques for inferring whether a LLM is telling the truth by training probes on the LLM's internal activations. However, this line of work is controversial, with some authors pointing out failures of these probes to generalize in basic ways, among other conceptual issues. In this work, we curate high-quality datasets of true/false statements and use them to study in detail the structure of LLM representations of truth, drawing on three lines of evidence: 1. Visualizations of LLM true/false statement representations, which reveal clear linear structure. 2. Transfer experiments in which probes trained on one dataset generalize to different datasets. 3. Causal evidence obtained by surgically intervening in a LLM's forward pass, causing it to treat false statements as true and vice versa. Overall, we present evidence that language models linearly represent the truth or falsehood of factual statements. We also introduce a novel technique, mass-mean probing, which generalizes better and is more causally implicated in model outputs than other probing techniques.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Marks_Tegmark_2023_The Geometry of Truth.pdf;/Users/thomasgorman/Zotero/storage/6SVUGF87/2310.html}
}

@article{martinezUsingLargeLanguage,
  title = {Using Large Language Models to Estimate Features of Multi-Word Expressions: {{Concreteness}}, Valence, Arousal},
  author = {Mart{\'i}nez, Gonzalo and Molero, Juan Diego and Gonz{\'a}lez, Sandra and Conde, Javier and Brysbaert, Marc and Reviriego, Pedro},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Martínez et al_Using large language models to estimate features of multi-word expressions.pdf}
}

@article{matzPotentialGenerativeAI2024,
  title = {The Potential of Generative {{AI}} for Personalized Persuasion at Scale},
  author = {Matz, S. C. and Teeny, J. D. and Vaid, S. S. and Peters, H. and Harari, G. M. and Cerf, M.},
  year = {2024},
  month = feb,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {4692},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-53755-0},
  urldate = {2024-06-29},
  abstract = {Matching the language or content of a message to the psychological profile of its recipient (known as ``personalized persuasion'') is widely considered to be one of the most effective messaging strategies. We demonstrate that the rapid advances in large language models (LLMs), like ChatGPT, could accelerate this influence by making personalized persuasion scalable. Across four studies (consisting of seven sub-studies; total N\,=\,1788), we show that personalized messages crafted by ChatGPT exhibit significantly more influence than non-personalized messages. This was true across different domains of persuasion (e.g., marketing of consumer products, political appeals for climate action), psychological profiles (e.g., personality traits, political ideology, moral foundations), and when only providing the LLM with a single, short prompt naming or describing the targeted psychological dimension. Thus, our findings are among the first to demonstrate the potential for LLMs to automate, and thereby scale, the use of personalized persuasion in ways that enhance its effectiveness and efficiency. We discuss the implications for researchers, practitioners, and the general public.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Psychology},
  annotation = {https://osf.io/79wcm/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Matz et al_2024_The potential of generative AI for personalized persuasion at scale.pdf}
}

@article{mccoyHowMuchLanguage2021,
  title = {How Much Do Language Models Copy from Their Training Data? {{Evaluating}} Linguistic Novelty in Text Generation Using {{RAVEN}}},
  shorttitle = {How Much Do Language Models Copy from Their Training Data?},
  author = {McCoy, R. Thomas and Smolensky, Paul and Linzen, Tal and Gao, Jianfeng and Celikyilmaz, Asli},
  year = {2021},
  month = nov,
  journal = {arXiv:2111.09509 [cs]},
  eprint = {2111.09509},
  primaryclass = {cs},
  urldate = {2022-05-10},
  abstract = {Current language models can generate high-quality text. Are they simply copying text they have seen before, or have they learned generalizable linguistic abstractions? To tease apart these possibilities, we introduce RAVEN, a suite of analyses for assessing the novelty of generated text, focusing on sequential structure (n-grams) and syntactic structure. We apply these analyses to four neural language models (an LSTM, a Transformer, Transformer-XL, and GPT-2). For local structure - e.g., individual dependencies - model-generated text is substantially less novel than our baseline of human-generated text from each model's test set. For larger-scale structure - e.g., overall sentence structure - model-generated text is as novel or even more novel than the human-generated baseline, but models still sometimes copy substantially, in some cases duplicating passages over 1,000 words long from the training set. We also perform extensive manual analysis showing that GPT-2's novel text is usually well-formed morphologically and syntactically but has reasonably frequent semantic issues (e.g., being self-contradictory).},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/McCoy et al_2021_How much do language models copy from their training data.pdf;/Users/thomasgorman/Zotero/storage/7RIEXHK4/2111.html}
}

@misc{mcgrathHowCanDeep2023,
  title = {How {{Can Deep Neural Networks Inform Theory}} in {{Psychological Science}}?},
  author = {McGrath, Sam and Russin, Jacob and Pavlick, Ellie and Feiman, Roman},
  year = {2023},
  month = nov,
  doi = {10.31234/osf.io/j5ckf},
  urldate = {2024-08-11},
  abstract = {Over the last decade, deep neural networks (DNNs) have transformed the state of the art in artificial intelligence. In domains like language production and reasoning, long considered uniquely human abilities, models like GPT-4 have proven capable of strikingly human-like performance. However, in contrast to classical symbolic models, neural networks can be inscrutable even to their designers, making it unclear what significance, if any, they have for theories of human cognition. Two extreme reactions are common. Neural network enthusiasts argue that, because the inner workings of DNNs do not seem to resemble any of the traditional constructs of psychological or linguistic theory, their success renders these theories obsolete and motivates a radical paradigm shift. Neural network skeptics instead take this inability to interpret DNNs in psychological terms to mean that their success is irrelevant to psychological science. In this paper, we review recent work that suggests that the internal mechanisms of DNNs can, in fact, be interpreted in the functional terms characteristic of psychological explanations. We argue that this undermines the shared assumption of both extremes and opens the door for DNNs to inform theories of cognition and its development.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/McGrath et al_2023_How Can Deep Neural Networks Inform Theory in Psychological Science.pdf}
}

@article{mcwilliamsTransactiveMemorySystems2024,
  title = {Transactive Memory Systems in Superteams: The Effect of an Intelligent Assistant in Virtual Teams},
  shorttitle = {Transactive Memory Systems in Superteams},
  author = {McWilliams, Denise J. and Randolph, Adriane B.},
  year = {2024},
  month = jan,
  journal = {Information Technology \& People},
  volume = {ahead-of-print},
  number = {ahead-of-print},
  publisher = {Emerald Publishing Limited},
  issn = {0959-3845},
  doi = {10.1108/ITP-12-2022-0918},
  urldate = {2024-09-18},
  abstract = {Purpose Researchers explore the impact of an intelligent assistant in virtual teams by applying the theoretical lens of a transactive memory system (TMS) to understand the relationships between trust in a specific technology, knowledge sharing and knowledge application. Design/methodology/approach An online survey was administered to a Qualtrics-curated panel of individual, US-based virtual team members utilizing an intelligent assistant with team collaboration software. Partial least squares structural equation modeling (PLS-SEM) was utilized to examine the hypothesized relationships of interest. Findings Results suggest that knowledge application is strongly influenced by trust in a specific technology and knowledge sharing. Additionally, a transactive memory system positively increases trust in the intelligent assistant, and similarly, trust in the intelligent assistant has a significant positive relationship with knowledge sharing. Originality/value The research model contributes to our understanding of the impact of an intelligent assistant in virtual teams. Although the transactive memory system construct has been explored in various contexts and models, few have explored the impact of an intelligent assistant and trust in a specific technology.},
  keywords = {Artificial intelligence,Intelligent assistant,Knowledge application,Knowledge sharing,Transactive memory system,Trust,Virtual teams},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/McWilliams_Randolph_2024_Transactive memory systems in superteams.pdf}
}

@techreport{mechera-ostrovskyNaturalLanguageModels2023,
  type = {Preprint},
  title = {From {{Natural Language Models}} to {{Cognitive Model Validation}}: {{A Theoretical Framework}}},
  shorttitle = {From {{Natural Language Models}} to {{Cognitive Model Validation}}},
  author = {{Mechera-Ostrovsky}, Tehilla and Newell, Ben R},
  year = {2023},
  month = sep,
  institution = {PsyArXiv},
  doi = {10.31234/osf.io/r6det},
  urldate = {2023-09-17},
  abstract = {We propose a novel technique that uses individuals' verbal descriptions of their problem-solving strategies to validate psychological processes assumed by computational cognitive models. We capitalize on recent advances in Natural Language Processing models (NLP), in particular their context-sensitivity, to classify participants' unstructured verbal descriptions. We illustrate our approach with an experiment examining how people integrate social and private information when making risky decisions. We contrast NLP model outputs derived from participants' verbal descriptions with the assumptions underlying a computational model fit to the behavioural data. We discuss ways to refine and improve this technique, and argue that verbal descriptions are a valuable and under-utilized source of data for holding cognitive models accountable to their psychological assumptions.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Mechera-Ostrovsky_Newell_2023_From Natural Language Models to Cognitive Model Validation.pdf}
}

@article{megahedHowGenerativeAI2023,
  title = {How {{Generative AI}} Models Such as {{ChatGPT}} Can Be ({{Mis}}){{Used}} in {{SPC Practice}}, {{Education}}, and {{Research}}? {{An Exploratory Study}}},
  shorttitle = {How {{Generative AI}} Models Such as {{ChatGPT}} Can Be ({{Mis}}){{Used}} in {{SPC Practice}}, {{Education}}, and {{Research}}?},
  author = {Megahed, Fadel M. and Chen, Ying-Ju and Ferris, Joshua A. and Knoth, Sven and {Jones-Farmer}, L. Allison},
  year = {2023},
  month = jun,
  journal = {Quality Engineering},
  pages = {1--29},
  issn = {0898-2112, 1532-4222},
  doi = {10.1080/08982112.2023.2206479},
  urldate = {2023-11-30},
  abstract = {Generative Artificial Intelligence (AI) models such as OpenAI's ChatGPT have the potential to revolutionize Statistical Process Control (SPC) practice, learning, and research. However, these tools are in the early stages of development and can be easily misused or misunderstood. In this paper, we give an overview of the development of Generative AI. Specifically, we explore ChatGPT's ability to provide code, explain basic concepts, and create knowledge related to SPC practice, learning, and research. By investigating responses to structured prompts, we highlight the benefits and limitations of the results. Our study indicates that the current version of ChatGPT performs well for structured tasks, such as translating code from one language to another and explaining well-known concepts but struggles with more nuanced tasks, such as explaining less widely known terms and creating code from scratch. We find that using new AI tools may help practitioners, educators, and researchers to be more efficient and productive. However, in their current stages of development, some results are misleading and wrong. Overall, the use of generative AI models in SPC must be properly validated and used in conjunction with other methods to ensure accurate results.},
  keywords = {62P30,Computer Science - Machine Learning,G.3,G.4,J.2,J.6},
  annotation = {https://github.com/fmegahed/llm\_expository},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Megahed et al_2023_How Generative AI models such as ChatGPT can be (Mis)Used in SPC Practice,.pdf;/Users/thomasgorman/Zotero/storage/3X2AXMG6/2302.html}
}

@article{meirUnderstandingComplexTraffic2020,
  title = {Understanding Complex Traffic Road Scenes: {{The}} Case of Child-Pedestrians' Hazard Perception},
  shorttitle = {Understanding Complex Traffic Road Scenes},
  author = {Meir, Anat and {Oron-Gilad}, Tal},
  year = {2020},
  month = feb,
  journal = {Journal of Safety Research},
  volume = {72},
  pages = {111--126},
  issn = {0022-4375},
  doi = {10.1016/j.jsr.2019.12.014},
  urldate = {2024-09-16},
  abstract = {Introduction Understanding the shortcomings of child-pedestrians in evaluating traffic situations may contribute to producing intervention techniques that may increase their awareness to potential hazards as well as inform and inspire designers of autonomous vehicle and infrastructure systems to deal with the complications of crossing pedestrians. Method: The present work examined pedestrians' hazard-perception (HP) skills in complex traffic scenes. Two experiments explored how pedestrians' HP abilities vary with age and experience. In the first, adults and youngsters (7--13-year-olds) were presented with pairs of photographs displaying traffic situations and instructed to compare between the hazard levels of the two. Findings revealed a marked trend where experienced-adults tended to rate photographs depicting field of view partially obscured by parked vehicles as more hazardous. Moreover, adults tended to rate photographs depicting vehicles closer to the crossing site as more hazardous. Lastly, adults tended to rate photographs depicting complex configurations like traffic circles, as more hazardous than T-junctions. Results: Findings suggested that youngsters may be highly influenced by cueing. Next, pedestrians' HP was tested using a crossing decision task. Participants observed traffic scenes presented in a dynamic simulated environment of an urban road from a pedestrian's perspective and pressed a response button whenever they assumed it was safe to cross. Compared to experienced-adults and 7--8-year-olds, 9--13-year-olds presented a less decisive performance. Compared to previous findings regarding simpler road crossing configurations, most participants, regardless of age, related more to the approaching vehicles and presence of a pedestrian crossing while refraining from addressing the road configuration. Implications for road-safety are discussed.},
  keywords = {Child-pedestrians,Hazard Perception,Paired comparison,Road crossing,Virtual reality},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Meir_Oron-Gilad_2020_Understanding complex traffic road scenes.pdf;/Users/thomasgorman/Zotero/storage/NIFSPAWP/S0022437519306723.html}
}

@article{meiTuringTestWhether2024,
  title = {A {{Turing}} Test of Whether {{AI}} Chatbots Are Behaviorally Similar to Humans},
  author = {Mei, Qiaozhu and Xie, Yutong and Yuan, Walter and Jackson, Matthew O.},
  year = {2024},
  month = feb,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {121},
  number = {9},
  pages = {e2313925121},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2313925121},
  urldate = {2024-07-10},
  abstract = {We administer a Turing test to AI chatbots. We examine how chatbots behave in a suite of classic behavioral games that are designed to elicit characteristics such as trust, fairness, risk-aversion, cooperation, etc., as well as how they respond to a traditional Big-5 psychological survey that measures personality traits. ChatGPT-4 exhibits behavioral and personality traits that are statistically indistinguishable from a random human from tens of thousands of human subjects from more than 50 countries. Chatbots also modify their behavior based on previous experience and contexts ``as if'' they were learning from the interactions and change their behavior in response to different framings of the same strategic situation. Their behaviors are often distinct from average and modal human behaviors, in which case they tend to behave on the more altruistic and cooperative end of the distribution. We estimate that they act as if they are maximizing an average of their own and partner's payoffs.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Mei et al_2024_A Turing test of whether AI chatbots are behaviorally similar to humans.pdf}
}

@article{mengAIEmergesFrontier2024,
  title = {{{AI}} Emerges as the Frontier in Behavioral Science},
  author = {Meng, Juanjuan},
  year = {2024},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {121},
  number = {10},
  pages = {e2401336121},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2401336121},
  urldate = {2024-07-10}
}

@misc{mengChartAssisstantUniversalChart2024,
  title = {{{ChartAssisstant}}: {{A Universal Chart Multimodal Language Model}} via {{Chart-to-Table Pre-training}} and {{Multitask Instruction Tuning}}},
  shorttitle = {{{ChartAssisstant}}},
  author = {Meng, Fanqing and Shao, Wenqi and Lu, Quanfeng and Gao, Peng and Zhang, Kaipeng and Qiao, Yu and Luo, Ping},
  year = {2024},
  month = feb,
  number = {arXiv:2401.02384},
  eprint = {2401.02384},
  publisher = {arXiv},
  urldate = {2024-03-03},
  abstract = {Charts play a vital role in data visualization, understanding data patterns, and informed decision-making. However, their unique combination of graphical elements (e.g., bars, lines) and textual components (e.g., labels, legends) poses challenges for general-purpose multimodal models. While vision-language models trained on chart data excel in comprehension, they struggle with generalization. To address these challenges, we propose ChartAssistant, a chart-based vision-language model for universal chart comprehension and reasoning. ChartAssistant leverages ChartSFT, a comprehensive dataset covering diverse chart-related tasks with basic (e.g. bars and pies) and specialized (e.g. radars, and bubbles) chart types. It undergoes a two-stage training process, starting with pre-training on chart-to-table parsing to align chart and text, followed by multitask instruction-following fine-tuning. This approach enables ChartAssistant to achieve competitive performance across various chart tasks. Experimental results demonstrate significant performance gains over the state-of-the-art UniChart and Chartllama method, especially outperforming them on real-world chart data with zero-shot setting. The code and data are available at https://github.com/OpenGVLab/ChartAst.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition},
  annotation = {https://github.com/OpenGVLab/ChartAst},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Meng et al_2024_ChartAssisstant.pdf;/Users/thomasgorman/Zotero/storage/7W5ZFD8P/2401.html}
}

@article{meratHighlyAutomatedDriving2012,
  title = {Highly {{Automated Driving}}, {{Secondary Task Performance}}, and {{Driver State}}},
  author = {Merat, Natasha and Jamson, A. Hamish and Lai, Frank C. H. and Carsten, Oliver},
  year = {2012},
  month = oct,
  journal = {Human Factors},
  volume = {54},
  number = {5},
  pages = {762--771},
  publisher = {SAGE Publications Inc},
  issn = {0018-7208},
  doi = {10.1177/0018720812442087},
  urldate = {2024-09-16},
  abstract = {Objective:A driving simulator study compared the effect of changes in workload on performance in manual and highly automated driving. Changes in driver state were also observed by examining variations in blink patterns.Background:With the addition of a greater number of advanced driver assistance systems in vehicles, the driver's role is likely to alter in the future from an operator in manual driving to a supervisor of highly automated cars. Understanding the implications of such advancements on drivers and road safety is important.Method:A total of 50 participants were recruited for this study and drove the simulator in both manual and highly automated mode. As well as comparing the effect of adjustments in driving-related workload on performance, the effect of a secondary Twenty Questions Task was also investigated.Results:In the absence of the secondary task, drivers' response to critical incidents was similar in manual and highly automated driving conditions. The worst performance was observed when drivers were required to regain control of driving in the automated mode while distracted by the secondary task. Blink frequency patterns were more consistent for manual than automated driving but were generally suppressed during conditions of high workload.Conclusion:Highly automated driving did not have a deleterious effect on driver performance, when attention was not diverted to the distracting secondary task.Application:As the number of systems implemented in cars increases, an understanding of the implications of such automation on drivers' situation awareness, workload, and ability to remain engaged with the driving task is important.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Merat et al_2012_Highly Automated Driving, Secondary Task Performance, and Driver State.pdf}
}

@article{messeriArtificialIntelligenceIllusions2024,
  title = {Artificial Intelligence and Illusions of Understanding in Scientific Research},
  author = {Messeri, Lisa and Crockett, M. J.},
  year = {2024},
  month = mar,
  journal = {Nature},
  volume = {627},
  number = {8002},
  pages = {49--58},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07146-0},
  urldate = {2024-07-28},
  abstract = {Scientists are enthusiastically imagining ways in which artificial intelligence (AI) tools might improve research. Why are AI tools so attractive and what are the risks of implementing them across the research pipeline? Here we develop a taxonomy of scientists' visions for AI, observing that their appeal comes from promises to improve productivity and objectivity by overcoming human shortcomings. But proposed AI solutions can also exploit our cognitive limitations, making us vulnerable to illusions of understanding in which we believe we understand more about the world than we actually do. Such illusions obscure the scientific community's ability to see the formation of scientific monocultures, in which some types of methods, questions and viewpoints come to dominate alternative approaches, making science less innovative and more vulnerable to errors. The proliferation of AI tools in science risks introducing a phase of scientific enquiry in which we produce more but understand less. By analysing the appeal of these tools, we provide a framework for advancing discussions of responsible knowledge production in the age of AI.},
  copyright = {2024 Springer Nature Limited},
  langid = {english},
  keywords = {Human behaviour,Interdisciplinary studies,Lab life,Research management,Social anthropology},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Messeri_Crockett_2024_Artificial intelligence and illusions of understanding in scientific research.pdf}
}

@misc{michelmannLargeLanguageModels2023,
  title = {Large Language Models Can Segment Narrative Events Similarly to Humans},
  author = {Michelmann, Sebastian and Kumar, Manoj and Norman, Kenneth A. and Toneva, Mariya},
  year = {2023},
  month = jan,
  number = {arXiv:2301.10297},
  eprint = {2301.10297},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-05-22},
  abstract = {Humans perceive discrete events such as "restaurant visits" and "train rides" in their continuous experience. One important prerequisite for studying human event perception is the ability of researchers to quantify when one event ends and another begins. Typically, this information is derived by aggregating behavioral annotations from several observers. Here we present an alternative computational approach where event boundaries are derived using a large language model, GPT-3, instead of using human annotations. We demonstrate that GPT-3 can segment continuous narrative text into events. GPT-3-annotated events are significantly correlated with human event annotations. Furthermore, these GPT-derived annotations achieve a good approximation of the ``consensus'' solution (obtained by averaging across human annotations); the boundaries identified by GPT-3 are closer to the consensus, on average, than boundaries identified by individual human annotators. This finding suggests that GPT-3 provides a feasible solution for automated event annotations, and it demonstrates a further parallel between human cognition and prediction in large language models. In the future, GPT-3 may thereby help to elucidate the principles underlying human event perception.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Michelmann et al_2023_Large language models can segment narrative events similarly to humans.pdf}
}

@article{milickaLargeLanguageModels2024,
  title = {Large Language Models Are Able to Downplay Their Cognitive Abilities to Fit the Persona They Simulate},
  author = {Mili{\v c}ka, Ji{\v r}{\'i} and Marklov{\'a}, Anna and VanSlambrouck, Kl{\'a}ra and Posp{\'i}{\v s}ilov{\'a}, Eva and {\v S}imsov{\'a}, Jana and Harvan, Samuel and Drobil, Ond{\v r}ej},
  year = {2024},
  month = mar,
  journal = {PLOS ONE},
  volume = {19},
  number = {3},
  pages = {e0298522},
  issn = {1932-6203},
  doi = {10.1371/journal.pone.0298522},
  urldate = {2024-04-06},
  abstract = {This study explores the capabilities of large language models to replicate the behavior of individuals with underdeveloped cognitive and language skills. Specifically, we investigate whether these models can simulate child-like language and cognitive development while solving false-belief tasks, namely, change-of-location and unexpected-content tasks. GPT-3.5-turbo and GPT-4 models by OpenAI were prompted to simulate children (N = 1296) aged one to six years. This simulation was instantiated through three types of prompts: plain zero-shot, chain-of-thoughts, and primed-by-corpus. We evaluated the correctness of responses to assess the models' capacity to mimic the cognitive skills of the simulated children. Both models displayed a pattern of increasing correctness in their responses and rising language complexity. That is in correspondence with a gradual enhancement in linguistic and cognitive abilities during child development, which is described in the vast body of research literature on child development. GPT-4 generally exhibited a closer alignment with the developmental curve observed in `real' children. However, it displayed hyper-accuracy under certain conditions, notably in the primed-by-corpus prompt type. Task type, prompt type, and the choice of language model influenced developmental patterns, while temperature and the gender of the simulated parent and child did not consistently impact results. We conducted analyses of linguistic complexity, examining utterance length and Kolmogorov complexity. These analyses revealed a gradual increase in linguistic complexity corresponding to the age of the simulated children, regardless of other variables. These findings show that the language models are capable of downplaying their abilities to achieve a faithful simulation of prompted personas.},
  langid = {english},
  keywords = {Charts,Children,Chocolate,Cognitive linguistics,Kolmogorov complexity,Language,Psycholinguistics,Theory of mind},
  annotation = {https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0298522},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Milička et al_2024_Large language models are able to downplay their cognitive abilities to fit the.pdf}
}

@misc{milliereAnthropocentricBiasPossibility2024,
  title = {Anthropocentric Bias and the Possibility of Artificial Cognition},
  author = {Milli{\`e}re, Rapha{\"e}l and Rathkopf, Charles},
  year = {2024},
  month = jul,
  number = {arXiv:2407.03859},
  eprint = {2407.03859},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-16},
  abstract = {Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence (Type-I), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent (Type-II). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Millière_Rathkopf_2024_Anthropocentric bias and the possibility of artificial cognition.pdf}
}

@misc{millierePhilosophicalIntroductionLanguage2024,
  title = {A {{Philosophical Introduction}} to {{Language Models}} -- {{Part I}}: {{Continuity With Classic Debates}}},
  shorttitle = {A {{Philosophical Introduction}} to {{Language Models}} -- {{Part I}}},
  author = {Milli{\`e}re, Rapha{\"e}l and Buckner, Cameron},
  year = {2024},
  month = jan,
  number = {arXiv:2401.03910},
  eprint = {2401.03910},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-14},
  abstract = {Large language models like GPT-4 have achieved remarkable proficiency in a broad spectrum of language-based tasks, some of which are traditionally associated with hallmarks of human intelligence. This has prompted ongoing disagreements about the extent to which we can meaningfully ascribe any kind of linguistic or cognitive competence to language models. Such questions have deep philosophical roots, echoing longstanding debates about the status of artificial neural networks as cognitive models. This article -- the first part of two companion papers -- serves both as a primer on language models for philosophers, and as an opinionated survey of their significance in relation to classic debates in the philosophy cognitive science, artificial intelligence, and linguistics. We cover topics such as compositionality, language acquisition, semantic competence, grounding, world models, and the transmission of cultural knowledge. We argue that the success of language models challenges several long-held assumptions about artificial neural networks. However, we also highlight the need for further empirical investigation to better understand their internal mechanisms. This sets the stage for the companion paper (Part II), which turns to novel empirical methods for probing the inner workings of language models, and new philosophical questions prompted by their latest developments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Millière_Buckner_2024_A Philosophical Introduction to Language Models -- Part I.pdf;/Users/thomasgorman/Zotero/storage/ISIUU72W/2401.html}
}

@misc{millierePhilosophyCognitiveScience2024,
  title = {Philosophy of {{Cognitive Science}} in the {{Age}} of {{Deep Learning}}},
  author = {Milli{\`e}re, Rapha{\"e}l},
  year = {2024},
  month = may,
  number = {arXiv:2405.04048},
  eprint = {2405.04048},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-21},
  abstract = {Deep learning has enabled major advances across most areas of artificial intelligence research. This remarkable progress extends beyond mere engineering achievements and holds significant relevance for the philosophy of cognitive science. Deep neural networks have made significant strides in overcoming the limitations of older connectionist models that once occupied the centre stage of philosophical debates about cognition. This development is directly relevant to long-standing theoretical debates in the philosophy of cognitive science. Furthermore, ongoing methodological challenges related to the comparative evaluation of deep neural networks stand to benefit greatly from interdisciplinary collaboration with philosophy and cognitive science. The time is ripe for philosophers to explore foundational issues related to deep learning and cognition; this perspective paper surveys key areas where their contributions can be especially fruitful.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Millière_2024_Philosophy of Cognitive Science in the Age of Deep Learning.pdf}
}

@misc{mitchellComparingHumansGPT42023,
  title = {Comparing {{Humans}}, {{GPT-4}}, and {{GPT-4V On Abstraction}} and {{Reasoning Tasks}}},
  author = {Mitchell, Melanie and Palmarini, Alessandro B. and Moskvichev, Arseny},
  year = {2023},
  month = nov,
  number = {arXiv:2311.09247},
  eprint = {2311.09247},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {We explore the abstract reasoning abilities of text-only and multimodal versions of GPT-4, using the ConceptARC benchmark [10], which is designed to evaluate robust understanding and reasoning with core-knowledge concepts. We extend the work of Moskvichev et al. [10] by evaluating GPT-4 on more detailed, one-shot prompting (rather than simple, zero-shot prompts) with text versions of ConceptARC tasks, and by evaluating GPT-4V, the multimodal version of GPT-4, on zero- and one-shot prompts using image versions of the simplest tasks. Our experimental results support the conclusion that neither version of GPT-4 has developed robust abstraction abilities at humanlike levels.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Mitchell et al_2023_Comparing Humans, GPT-4, and GPT-4V On Abstraction and Reasoning Tasks.pdf;/Users/thomasgorman/Zotero/storage/UK5K9FZG/2311.html}
}

@article{mitchellDebateUnderstandingAIs2023,
  title = {The Debate over Understanding in {{AI}}'s Large Language Models},
  author = {Mitchell, Melanie and Krakauer, David C.},
  year = {2023},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {13},
  pages = {e2215907120},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2215907120},
  urldate = {2024-07-10},
  abstract = {We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language---and the physical and social situations language encodes---in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Mitchell_Krakauer_2023_The debate over understanding in AI’s large language models.pdf}
}

@article{mizumotoExploringPotentialUsing2023,
  title = {Exploring the Potential of Using an {{AI}} Language Model for Automated Essay Scoring},
  author = {Mizumoto, Atsushi and Eguchi, Masaki},
  year = {2023},
  month = aug,
  journal = {Research Methods in Applied Linguistics},
  volume = {2},
  number = {2},
  pages = {100050},
  issn = {2772-7661},
  doi = {10.1016/j.rmal.2023.100050},
  urldate = {2023-09-01},
  abstract = {The widespread adoption of ChatGPT, an AI language model, has the potential to bring about significant changes to the research, teaching, and learning of foreign languages. The present study aims to leverage this technology to perform automated essay scoring (AES) and evaluate its reliability and accuracy. Specifically, we utilized the GPT-3 text-davinci-003 model to automatically score all 12,100 essays contained in the ETS Corpus of Non-Native Written English (TOEFL11) and compared these scores to benchmark levels. The study also explored the extent to which linguistic features influence AES with GPT. The results showed that AES using GPT has a certain level of accuracy and reliability and could provide valuable support for human evaluations. Furthermore, the analysis revealed that utilizing linguistic features could enhance the accuracy of the scoring. These findings suggest that AI language models, such as ChatGPT, can be effectively utilized as AES tools, potentially revolutionizing methods of writing evaluation and feedback in both research and practice. The paper concludes by discussing the practical implications of using GPT for AES and exploring prospective future considerations.},
  keywords = {Automated essay scoring (AES),GPT (Generative Pre-trained Transformer),Linguistic features,Natural language processing (NLP),Transformer-based large language models},
  annotation = {https://osf.io/pf564/\\
\\
https://osf.io/89mhx\\
\\
https://www.iris-database.org/details/5FEK8-eJ6Wq},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Mizumoto_Eguchi_2023_Exploring the potential of using an AI language model for automated essay.pdf;/Users/thomasgorman/Zotero/storage/MIR6ETA8/S2772766123000101.html}
}

@article{mohamedProposedModelDistinguishing2024,
  title = {A Proposed Model for Distinguishing between Human-Based and {{ChatGPT}} Content in Scientific Articles.},
  author = {Mohamed, Toka A. and Khafgy, Mohamed H. and ElSedawy, Ahmed B. and Ismail, Ahmed S.},
  year = {2024},
  journal = {IEEE Access},
  pages = {1--1},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3448315},
  urldate = {2024-08-31},
  abstract = {This study introduces an innovative approach to address the growing challenge of detecting and distinguishing ChatGPT-generated content within scientific articles, particularly in the context of Learning Management Systems (LMS). Leveraging state-of-the-art large language models, including Robustly Optimized BERT Pretraining (RoBERTa), Text-to-Text Transfer Transformer (T5), and Generative Pre-trained Transformers (EleutherAI GPT-Neo-125M), our methodology focuses on the incorporation of the LMS concept into the research framework. To construct a comprehensive dataset representative of the diverse landscape of scientific abstracts, samples of the dataset are gathered from articles produced by human authors and those generated by ChatGPT within the LMS framework. The models (RoBERTa, T5, and EleutherAI GPT-Neo-125M) were subsequently trained on this unique dataset, showcasing their adaptability to the distinct characteristics of both human-generated and AI-generated content within the LMS context. The efficacy of our approach was rigorously evaluated using a range of metrics, resulting in an outstanding accuracy exceeding 99\%. This achievement underscores the robustness of our methodology in successfully discerning content generated by ChatGPT within the LMS and that authored by human contributors, thereby advancing the field of content differentiation in scientific discourse.},
  keywords = {Accuracy,AI Content,Artificial intelligence,Chatbots,ChatGPT,Content management,Data models,EleutherAI GPT-Neo-125M,GPT-3.5,Large language models,LLM,LMS,RoBERTa,T5,Training,Transformers},
  file = {/Users/thomasgorman/Zotero/storage/T7UKJNAP/Mohamed et al. - 2024 - A proposed model for distinguishing between human-.pdf;/Users/thomasgorman/Zotero/storage/NIHIJEQ4/10643547.html}
}

@article{mohammadiWaitItsAll2024,
  title = {Wait, {{It}}'s {{All Token Noise}}? {{Always Has Been}}: {{Interpreting LLM Behavior Using Shapley Value}}},
  shorttitle = {Wait, {{It}}'s {{All Token Noise}}?},
  author = {Mohammadi, Behnam},
  year = {2024},
  journal = {SSRN Electronic Journal},
  issn = {1556-5068},
  doi = {10.2139/ssrn.4759713},
  urldate = {2024-08-16},
  abstract = {The emergence of large language models (LLMs) has opened up exciting possibilities for simulating human behavior and cognitive processes, with potential applications in various domains, including marketing research and consumer behavior analysis. However, the validity of utilizing LLMs as stand-ins for human subjects remains uncertain due to glaring divergences that suggest fundamentally different underlying processes at play and the sensitivity of LLM responses to prompt variations. This paper presents a novel approach based on Shapley values from cooperative game theory to interpret LLM behavior and quantify the relative contribution of each prompt component to the model's output. Through two applications---a discrete choice experiment and an investigation of cognitive biases---we demonstrate how the Shapley value method can uncover what we term ``token noise'' effects, a phenomenon where LLM decisions are disproportionately influenced by tokens providing minimal informative content. This phenomenon raises concerns about the robustness and generalizability of insights obtained from LLMs in the context of human behavior simulation. Our model-agnostic approach extends its utility to proprietary LLMs, providing a valuable tool for marketers and researchers to strategically optimize prompts and mitigate apparent cognitive biases. Our findings underscore the need for a more nuanced understanding of the factors driving LLM responses before relying on them as substitutes for human subjects in research settings. We emphasize the importance of researchers reporting results conditioned on specific prompt templates and exercising caution when drawing parallels between human behavior and LLMs.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Mohammadi_2024_Wait, It’s All Token Noise.pdf}
}

@misc{mondorfAccuracyEvaluatingReasoning2024,
  title = {Beyond {{Accuracy}}: {{Evaluating}} the {{Reasoning Behavior}} of {{Large Language Models}} -- {{A Survey}}},
  shorttitle = {Beyond {{Accuracy}}},
  author = {Mondorf, Philipp and Plank, Barbara},
  year = {2024},
  month = apr,
  number = {arXiv:2404.01869},
  eprint = {2404.01869},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-10},
  abstract = {Large language models (LLMs) have recently shown impressive performance on tasks involving reasoning, leading to a lively debate on whether these models possess reasoning capabilities similar to humans. However, despite these successes, the depth of LLMs' reasoning abilities remains uncertain. This uncertainty partly stems from the predominant focus on task performance, measured through shallow accuracy metrics, rather than a thorough investigation of the models' reasoning behavior. This paper seeks to address this gap by providing a comprehensive review of studies that go beyond task accuracy, offering deeper insights into the models' reasoning processes. Furthermore, we survey prevalent methodologies to evaluate the reasoning behavior of LLMs, emphasizing current trends and efforts towards more nuanced reasoning analyses. Our review suggests that LLMs tend to rely on surface-level patterns and correlations in their training data, rather than on genuine reasoning abilities. Additionally, we identify the need for further research that delineates the key differences between human and LLM-based reasoning. Through this survey, we aim to shed light on the complex reasoning processes within LLMs.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Mondorf_Plank_2024_Beyond Accuracy.pdf}
}

@misc{moskvichevConceptARCBenchmarkEvaluating2023,
  title = {The {{ConceptARC Benchmark}}: {{Evaluating Understanding}} and {{Generalization}} in the {{ARC Domain}}},
  shorttitle = {The {{ConceptARC Benchmark}}},
  author = {Moskvichev, Arseny and Odouard, Victor Vikram and Mitchell, Melanie},
  year = {2023},
  month = may,
  number = {arXiv:2305.07141},
  eprint = {2305.07141},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.07141},
  urldate = {2023-10-17},
  abstract = {The abilities to form and abstract concepts is key to human intelligence, but such abilities remain lacking in state-of-the-art AI systems. There has been substantial research on conceptual abstraction in AI, particularly using idealized domains such as Raven's Progressive Matrices and Bongard problems, but even when AI systems succeed on such problems, the systems are rarely evaluated in depth to see if they have actually grasped the concepts they are meant to capture. In this paper we describe an in-depth evaluation benchmark for the Abstraction and Reasoning Corpus (ARC), a collection of few-shot abstraction and analogy problems developed by Chollet [2019]. In particular, we describe ConceptARC, a new, publicly available benchmark in the ARC domain that systematically assesses abstraction and generalization abilities on a number of basic spatial and semantic concepts. ConceptARC differs from the original ARC dataset in that it is specifically organized around "concept groups" -- sets of problems that focus on specific concepts and that are vary in complexity and level of abstraction. We report results on testing humans on this benchmark as well as three machine solvers: the top two programs from a 2021 ARC competition and OpenAI's GPT-4. Our results show that humans substantially outperform the machine solvers on this benchmark, showing abilities to abstract and generalize concepts that are not yet captured by AI systems. We believe that this benchmark will spur improvements in the development of AI systems for conceptual abstraction and in the effective evaluation of such systems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Moskvichev et al_2023_The ConceptARC Benchmark.pdf;/Users/thomasgorman/Zotero/storage/Z2EL495K/2305.html}
}

@article{motokiMoreHumanHuman2024,
  title = {More Human than Human: Measuring {{ChatGPT}} Political Bias},
  shorttitle = {More Human than Human},
  author = {Motoki, Fabio and Pinho Neto, Valdemar and Rodrigues, Victor},
  year = {2024},
  month = jan,
  journal = {Public Choice},
  volume = {198},
  number = {1},
  pages = {3--23},
  issn = {1573-7101},
  doi = {10.1007/s11127-023-01097-2},
  urldate = {2024-04-07},
  abstract = {We investigate the political bias of a large language model (LLM), ChatGPT, which has become popular for retrieving factual information and generating content. Although ChatGPT assures that it is impartial, the literature suggests that LLMs exhibit bias involving race, gender, religion, and political orientation. Political bias in LLMs can have adverse political and electoral consequences similar to bias from traditional and social media. Moreover, political bias can be harder to detect and eradicate than gender or racial bias. We propose a novel empirical design to infer whether ChatGPT has political biases by requesting it to impersonate someone from a given side of the political spectrum and comparing these answers with its default. We also propose dose-response, placebo, and profession-politics alignment robustness tests. To reduce concerns about the randomness of the generated text, we collect answers to the same questions 100 times, with question order randomized on each round. We find robust evidence that ChatGPT presents a significant and systematic political bias toward the Democrats in the US, Lula in Brazil, and the Labour Party in the UK. These results translate into real concerns that ChatGPT, and LLMs in general, can extend or even amplify the existing challenges involving political processes posed by the Internet and social media. Our findings have important implications for policymakers, media, politics, and academia stakeholders.},
  langid = {english},
  keywords = {Bias,C10,C89,ChatGPT,D83,L86,Large language models,Political bias,Z00},
  annotation = {https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/KGMEYI},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Motoki et al_2024_More human than human.pdf}
}

@article{mullerNostalgiaEuropeanParty2023,
  title = {Nostalgia in {{European Party Politics}}: {{A Text-Based Measurement Approach}}},
  shorttitle = {Nostalgia in {{European Party Politics}}},
  author = {M{\"u}ller, Stefan and Proksch, Sven-Oliver},
  year = {2023},
  month = nov,
  journal = {British Journal of Political Science},
  pages = {1--13},
  issn = {0007-1234, 1469-2112},
  doi = {10.1017/S0007123423000571},
  urldate = {2024-05-24},
  abstract = {Traditional research on political parties pays little attention to the temporal focus of communication. It usually concentrates on promises, issue attention, and policy positions. This lack of scholarly attention is surprising, given that voters respond to nostalgic rhetoric and may even adjust issue positions when policy is framed in nostalgic terms. This article presents a novel dataset, PolNos, which contains six text-based measures of nostalgic rhetoric in 1,648 party manifestos across 24 European democracies from 1946 to 2018. The measures combine dictionaries, word embeddings, sentiment approaches, and supervised machine learning. Our analysis yields a consistent result: nostalgia is most prevalent in manifestos of culturally conservative parties, notably Christian democratic, nationalist, and radical right parties. However, substantial variation remains regarding regional differences and whether nostalgia concerns the economy or culture. We discuss the implications and use of our dataset for studying political parties, party competition, and elections.},
  langid = {english},
  keywords = {elections,nostalgia,party competition,quantitative text analysis},
  annotation = {https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/W8VGJF},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Müller_Proksch_2023_Nostalgia in European Party Politics.pdf}
}

@misc{muskerSemanticStructureMappingLLM2024,
  title = {Semantic {{Structure-Mapping}} in {{LLM}} and {{Human Analogical Reasoning}}},
  author = {Musker, Sam and Duchnowski, Alex and Milli{\`e}re, Rapha{\"e}l and Pavlick, Ellie},
  year = {2024},
  month = jun,
  number = {arXiv:2406.13803},
  eprint = {2406.13803},
  publisher = {arXiv},
  urldate = {2024-06-30},
  abstract = {Analogical reasoning is considered core to human learning and cognition. Recent studies have compared the analogical reasoning abilities of human subjects and Large Language Models (LLMs) on abstract symbol manipulation tasks, such as letter string analogies. However, these studies largely neglect analogical reasoning over semantically meaningful symbols, such as natural language words. This ability to draw analogies that link language to non-linguistic domains, which we term semantic structure-mapping, is thought to play a crucial role in language acquisition and broader cognitive development. We test human subjects and LLMs on analogical reasoning tasks that require the transfer of semantic structure and content from one domain to another. Advanced LLMs match human performance across many task variations. However, humans and LLMs respond differently to certain task variations and semantic distractors. Overall, our data suggest that LLMs are approaching human-level performance on these important cognitive tasks, but are not yet entirely human like.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/AnonymousReview123/Semantic\_Structure\_Mapping\_Anon},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Musker et al_2024_Semantic Structure-Mapping in LLM and Human Analogical Reasoning.pdf}
}

@article{muslimEffectsGenderAge2021,
  title = {Effects of Gender, Age, Experience, and Practice on Driver Reaction and Acceptance of Traffic Jam Chauffeur Systems},
  author = {Muslim, Husam and Itoh, Makoto and Liang, Cho Kiu and {Antona-Makoshi}, Jacobo and Uchida, Nobuyuki},
  year = {2021},
  month = sep,
  journal = {Scientific Reports},
  volume = {11},
  number = {1},
  pages = {17874},
  publisher = {Nature Publishing Group},
  issn = {2045-2322},
  doi = {10.1038/s41598-021-97374-5},
  urldate = {2024-09-16},
  abstract = {This study conducted a driving simulation experiment to compare four automated driving systems (ADS) designs during lane change demanding traffic situations on highways while accounting for the drivers' gender, age, experience, and practice. A lane-change maneuver was required when the automated vehicle approaches traffic congestion on the left-hand lane. ADS-1 can only reduce the speed to synchronize with the congestion. ADS-2 reduces the speed and issues an optional request to intervene, advising the driver to change lanes manually. ADS-3 offers to overtake the congestion autonomously if the driver approves it. ADS-4 overtakes the congestion autonomously without the driver's approval. Results of drivers' reaction, acceptance, and trust indicated that differences between ADS designs increase when considering the combined effect of drivers' demographic factors more than the individual effect of each factor. However, the more ADS seems to have driver-like capacities, the more impact of demographic factors is expected. While preliminary, these findings may help us understand how ADS users' behavior can differ based on the interaction between human demographic factors and system design.},
  copyright = {2021 The Author(s)},
  langid = {english},
  keywords = {Electrical and electronic engineering,Risk factors},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Muslim et al_2021_Effects of gender, age, experience, and practice on driver reaction and.pdf}
}

@misc{musslickAutomatingPracticeScience2024,
  title = {Automating the {{Practice}} of {{Science}} -- {{Opportunities}}, {{Challenges}}, and {{Implications}}},
  author = {Musslick, Sebastian and Bartlett, Laura K. and Chandramouli, Suyog H. and Dubova, Marina and Gobet, Fernand and Griffiths, Thomas L. and Hullman, Jessica and King, Ross D. and Kutz, J. Nathan and Lucas, Christopher G. and Mahesh, Suhas and Pestilli, Franco and Sloman, Sabina J. and Holmes, William R.},
  year = {2024},
  month = aug,
  number = {arXiv:2409.05890},
  eprint = {2409.05890},
  primaryclass = {physics},
  publisher = {arXiv},
  urldate = {2024-09-14},
  abstract = {Automation transformed various aspects of our human civilization, revolutionizing industries and streamlining processes. In the domain of scientific inquiry, automated approaches emerged as powerful tools, holding promise for accelerating discovery, enhancing reproducibility, and overcoming the traditional impediments to scientific progress. This article evaluates the scope of automation within scientific practice and assesses recent approaches. Furthermore, it discusses different perspectives to the following questions: Where do the greatest opportunities lie for automation in scientific practice?; What are the current bottlenecks of automating scientific practice?; and What are significant ethical and practical consequences of automating scientific practice? By discussing the motivations behind automated science, analyzing the hurdles encountered, and examining its implications, this article invites researchers, policymakers, and stakeholders to navigate the rapidly evolving frontier of automated scientific practice.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society,Physics - Physics and Society},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Musslick et al_2024_Automating the Practice of Science -- Opportunities, Challenges, and.pdf}
}

@misc{naharFakesVaryingShades2024,
  title = {Fakes of {{Varying Shades}}: {{How Warning Affects Human Perception}} and {{Engagement Regarding LLM Hallucinations}}},
  shorttitle = {Fakes of {{Varying Shades}}},
  author = {Nahar, Mahjabin and Seo, Haeseung and Lee, Eun-Ju and Xiong, Aiping and Lee, Dongwon},
  year = {2024},
  month = apr,
  number = {arXiv:2404.03745},
  eprint = {2404.03745},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-23},
  abstract = {The widespread adoption and transformative effects of large language models (LLMs) have sparked concerns regarding their capacity to produce inaccurate and fictitious content, referred to as `hallucinations'. Given the potential risks associated with hallucinations, humans should be able to identify them. This research aims to understand the human perception of LLM hallucinations by systematically varying the degree of hallucination (genuine, minor hallucination, major hallucination) and examining its interaction with warning (i.e., a warning of potential inaccuracies: absent vs. present). Participants (N = 419) from Prolific rated the perceived accuracy and engaged with content (e.g., like, dislike, share) in a Q/A format. Results indicate that humans rank content as truthful in the order genuine {$>$}minor hallucination {$>$}major hallucination and user engagement behaviors mirror this pattern. More importantly, we observed that warning improves hallucination detection without significantly affecting the perceived truthfulness of genuine content. We conclude by offering insights for future tools to aid human detection of hallucinations.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Nahar et al_2024_Fakes of Varying Shades.pdf}
}

@misc{namikoshiUsingLLMsModel2024,
  title = {Using {{LLMs}} to {{Model}} the {{Beliefs}} and {{Preferences}} of {{Targeted Populations}}},
  author = {Namikoshi, Keiichi and Filipowicz, Alex and Shamma, David A. and Iliev, Rumen and Hogan, Candice L. and Arechiga, Nikos},
  year = {2024},
  month = mar,
  number = {arXiv:2403.20252},
  eprint = {2403.20252},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-07},
  abstract = {We consider the problem of aligning a large language model (LLM) to model the preferences of a human population. Modeling the beliefs, preferences, and behaviors of a specific population can be useful for a variety of different applications, such as conducting simulated focus groups for new products, conducting virtual surveys, and testing behavioral interventions, especially for interventions that are expensive, impractical, or unethical. Existing work has had mixed success using LLMs to accurately model human behavior in different contexts. We benchmark and evaluate two well-known fine-tuning approaches and evaluate the resulting populations on their ability to match the preferences of real human respondents on a survey of preferences for battery electric vehicles (BEVs). We evaluate our models against their ability to match population-wide statistics as well as their ability to match individual responses, and we investigate the role of temperature in controlling the trade-offs between these two. Additionally, we propose and evaluate a novel loss term to improve model performance on responses that require a numeric response.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Namikoshi et al_2024_Using LLMs to Model the Beliefs and Preferences of Targeted Populations.pdf}
}

@misc{namSystematicHumanLearning2023,
  title = {Systematic Human Learning and Generalization from a Brief Tutorial with Explanatory Feedback},
  author = {Nam, Andrew J. and McClelland, James L.},
  year = {2023},
  month = mar,
  number = {arXiv:2107.06994},
  eprint = {2107.06994},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2107.06994},
  urldate = {2023-08-11},
  abstract = {Neural networks have long been used to model human intelligence, capturing elements of behavior and cognition, and their neural basis. Recent advancements in deep learning have enabled neural network models to reach and even surpass human levels of intelligence in many respects, yet unlike humans, their ability to learn new tasks quickly remains a challenge. People can reason not only in familiar domains, but can also rapidly learn to reason through novel problems and situations, raising the question of how well modern neural network models capture human intelligence and in which ways they diverge. In this work, we explore this gap by investigating human adults' ability to learn an abstract reasoning task based on Sudoku from a brief instructional tutorial with explanatory feedback for incorrect responses using a narrow range of training examples. We find that participants who master the task do so within a small number of trials and generalize well to puzzles outside of the training range. We also find that most of those who master the task can describe a valid solution strategy, and such participants perform better on transfer puzzles than those whose strategy descriptions are vague or incomplete. Interestingly, fewer than half of our human participants were successful in acquiring a valid solution strategy, and this ability is associated with high school mathematics education. We consider the challenges these findings pose for building computational models that capture all aspects of our findings and point toward a possible role for learning to engage in explanation-based reasoning to support rapid learning and generalization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Symbolic Computation},
  annotation = {https://osf.io/7rehu},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Nam_McClelland_2023_Systematic human learning and generalization from a brief tutorial with.pdf;/Users/thomasgorman/Zotero/storage/6B8GYTFV/2107.html}
}

@article{narechaniaVITALITYPromotingSerendipitous2022,
  title = {{{VITALITY}}: {{Promoting Serendipitous Discovery}} of {{Academic Literature}} with {{Transformers}} \& {{Visual Analytics}}},
  shorttitle = {{{VITALITY}}},
  author = {Narechania, Arpit and Karduni, Alireza and Wesslen, Ryan and Wall, Emily},
  year = {2022},
  month = jan,
  journal = {IEEE Transactions on Visualization and Computer Graphics},
  volume = {28},
  number = {1},
  pages = {486--496},
  issn = {1077-2626, 1941-0506, 2160-9306},
  doi = {10.1109/TVCG.2021.3114820},
  urldate = {2024-05-21},
  abstract = {There are a few prominent practices for conducting reviews of academic literature, including searching for specific keywords on Google Scholar or checking citations from some initial seed paper(s). These approaches serve a critical purpose for academic literature reviews, yet there remain challenges in identifying relevant literature when similar work may utilize different terminology (e.g., mixed-initiative visual analytics papers may not use the same terminology as papers on model-steering, yet the two topics are relevant to one another). In this paper, we introduce a system, VITALITY, intended to complement existing practices. In particular, VITALITY promotes serendipitous discovery of relevant literature using transformer language models, allowing users to find semantically similar papers in a word embedding space given (1) a list of input paper(s) or (2) a working abstract. VITALITY visualizes this document-level embedding space in an interactive 2-D scatterplot using dimension reduction. VITALITY also summarizes meta information about the document corpus or search query, including keywords and co-authors, and allows users to save and export papers for use in a literature review. We present qualitative findings from an evaluation of VITALITY, suggesting it can be a promising complementary technique for conducting academic literature reviews. Furthermore, we contribute data from 38 popular data visualization publication venues in VITALITY, and we provide scrapers for the open-source community to continue to grow the list of supported venues.},
  copyright = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Narechania et al_2022_VITALITY.pdf}
}

@misc{nasrScalableExtractionTraining2023,
  title = {Scalable {{Extraction}} of {{Training Data}} from ({{Production}}) {{Language Models}}},
  author = {Nasr, Milad and Carlini, Nicholas and Hayase, Jonathan and Jagielski, Matthew and Cooper, A. Feder and Ippolito, Daphne and {Choquette-Choo}, Christopher A. and Wallace, Eric and Tram{\`e}r, Florian and Lee, Katherine},
  year = {2023},
  month = nov,
  number = {arXiv:2311.17035},
  eprint = {2311.17035},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.17035},
  urldate = {2023-11-30},
  abstract = {This paper studies extractable memorization: training data that an adversary can efficiently extract by querying a machine learning model without prior knowledge of the training dataset. We show an adversary can extract gigabytes of training data from open-source language models like Pythia or GPT-Neo, semi-open models like LLaMA or Falcon, and closed models like ChatGPT. Existing techniques from the literature suffice to attack unaligned models; in order to attack the aligned ChatGPT, we develop a new divergence attack that causes the model to diverge from its chatbot-style generations and emit training data at a rate 150x higher than when behaving properly. Our methods show practical attacks can recover far more data than previously thought, and reveal that current alignment techniques do not eliminate memorization.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Cryptography and Security,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Nasr et al_2023_Scalable Extraction of Training Data from (Production) Language Models.pdf;/Users/thomasgorman/Zotero/storage/DVUAVNF4/2311.html}
}

@article{nematiImpactInformationbasedInterventions2020,
  title = {The Impact of Information-Based Interventions on Conservation Behavior: {{A}} Meta-Analysis},
  shorttitle = {The Impact of Information-Based Interventions on Conservation Behavior},
  author = {Nemati, Mehdi and Penn, Jerrod},
  year = {2020},
  month = nov,
  journal = {Resource and Energy Economics},
  volume = {62},
  pages = {101201},
  issn = {0928-7655},
  doi = {10.1016/j.reseneeco.2020.101201},
  urldate = {2024-07-02},
  abstract = {Interest in using information-based interventions to induce energy and water conservation has increased in recent years but have shown mixed evidence of their effectiveness. This paper seeks to answer two main questions - whether these programs are broadly effective in inducing conservation, and what are the most effective versions of these programs. Using a meta-analysis of 116 studies, we examine the effects of information-based interventions on residential customers' consumption of electricity, gas, and water. We find evidence of publication bias in this literature. After correcting for publication bias, meta-analysis results indicate that information-based interventions reduce consumption by an average of 6.24\%, 95\% CI [-10.72, -1.76]. In addition, we find that studies employing RCTs find smaller conservation effects, (-5.2\%, 95\% CI [-9.53, -0.51]). Our results show that the effectiveness of information-based interventions at the household level are significantly larger than those at the aggregate level (such as dorms and buildings). Finally, interventions with a shorter duration or with more frequent reporting show larger estimated effect sizes.},
  keywords = {Conservation,Information,Meta-analysis,Nudges,Publication bias,Random forest,Social norms},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Nemati_Penn_2020_The impact of information-based interventions on conservation behavior.pdf;/Users/thomasgorman/Zotero/storage/5SMTJT4P/S0928765518303828.html}
}

@misc{neumannDiverseDivisiveLLMs2024,
  title = {Diverse, but {{Divisive}}: {{LLMs Can Exaggerate Gender Differences}} in {{Opinion Related}} to {{Harms}} of {{Misinformation}}},
  shorttitle = {Diverse, but {{Divisive}}},
  author = {Neumann, Terrence and Lee, Sooyong and {De-Arteaga}, Maria and Fazelpour, Sina and Lease, Matthew},
  year = {2024},
  month = jan,
  number = {arXiv:2401.16558},
  eprint = {2401.16558},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-23},
  abstract = {The pervasive spread of misinformation and disinformation poses a significant threat to society. Professional fact-checkers play a key role in addressing this threat, but the vast scale of the problem forces them to prioritize their limited resources. This prioritization may consider a range of factors, such as varying risks of harm posed to specific groups of people. In this work, we investigate potential implications of using a large language model (LLM) to facilitate such prioritization. Because fact-checking impacts a wide range of diverse segments of society, it is important that diverse views are represented in the claim prioritization process. This paper examines whether a LLM can reflect the views of various groups when assessing the harms of misinformation, focusing on gender as a primary variable. We pose two central questions: (1) To what extent do prompts with explicit gender references reflect gender differences in opinion in the United States on topics of social relevance? and (2) To what extent do gender-neutral prompts align with gendered viewpoints on those topics? To analyze these questions, we present the TopicMisinfo dataset, containing 160 fact-checked claims from diverse topics, supplemented by nearly 1600 human annotations with subjective perceptions and annotator demographics. Analyzing responses to gender-specific and neutral prompts, we find that GPT 3.5-Turbo reflects empirically observed gender differences in opinion but amplifies the extent of these differences. These findings illuminate AI's complex role in moderating online communication, with implications for fact-checkers, algorithm designers, and the use of crowd-workers as annotators. We also release the TopicMisinfo dataset to support continuing research in the community.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Neumann et al_2024_Diverse, but Divisive.pdf}
}

@article{newellNudgingEnergyEfficiency2014,
  title = {Nudging {{Energy Efficiency Behavior}}: {{The Role}} of {{Information Labels}}},
  shorttitle = {Nudging {{Energy Efficiency Behavior}}},
  author = {Newell, Richard G. and Siikam{\"a}ki, Juha},
  year = {2014},
  month = dec,
  journal = {Journal of the Association of Environmental and Resource Economists},
  volume = {1},
  number = {4},
  pages = {555--598},
  publisher = {The University of Chicago Press},
  issn = {2333-5955},
  doi = {10.1086/679281},
  urldate = {2024-07-02},
  abstract = {We use choice experiments and randomized information treatments to study the effectiveness of alternative energy efficiency labels in guiding households' energy efficiency decisions. We disentangle the relative importance of different types of information and distinguish it from intertemporal behavior. We find that insufficient information can lead to considerable undervaluation of energy efficiency. Simple information on the monetary value of energy savings was the most important element guiding cost-efficient energy efficiency investments, with information on physical energy use and carbon dioxide emissions having additional but lesser importance. The degree to which the current US EnergyGuide label guided cost-efficient decisions depends on the discount rate. Using elicited individual discount rates, the current EnergyGuide label came very close to guiding cost-efficient decisions. Using a uniform 5\% discount rate, the current label led to one-third undervaluation of energy efficiency. Our results reinforce the centrality of discounting in understanding individual behavior and guiding policy.},
  keywords = {C91,Choice experiment,D12,D83,D91,Discounting,Energy efficiency behavior,Gap,H43,Information label,Q41,Q48,Time preference gap},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Newell_Siikamäki_2014_Nudging Energy Efficiency Behavior.pdf}
}

@misc{nguDiversityMeasuresDomainIndependent2023,
  title = {Diversity {{Measures}}: {{Domain-Independent Proxies}} for {{Failure}} in {{Language Model Queries}}},
  shorttitle = {Diversity {{Measures}}},
  author = {Ngu, Noel and Lee, Nathaniel and Shakarian, Paulo},
  year = {2023},
  month = aug,
  number = {arXiv:2308.11189},
  eprint = {2308.11189},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-27},
  abstract = {Error prediction in large language models often relies on domain-specific information. In this paper, we present measures for quantification of error in the response of a large language model based on the diversity of responses to a given prompt - hence independent of the underlying application. We describe how three such measures - based on entropy, Gini impurity, and centroid distance - can be employed. We perform a suite of experiments on multiple datasets and temperature settings to demonstrate that these measures strongly correlate with the probability of failure. Additionally, we present empirical results demonstrating how these measures can be applied to few-shot prompting, chain-of-thought reasoning, and error detection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ngu et al_2023_Diversity Measures.pdf;/Users/thomasgorman/Zotero/storage/URUK9KN5/2308.html}
}

@article{nguyenHumanBiasAI2024,
  title = {Human {{Bias}} in {{AI Models}}? {{Anchoring Effects}} and {{Mitigation Strategies}} in {{Large Language Models}}},
  shorttitle = {Human {{Bias}} in {{AI Models}}?},
  author = {Nguyen, Jeremy},
  year = {2024},
  month = aug,
  journal = {Journal of Behavioral and Experimental Finance},
  pages = {100971},
  issn = {2214-6350},
  doi = {10.1016/j.jbef.2024.100971},
  urldate = {2024-08-31},
  abstract = {This study builds on the seminal work of Tversky and Kahneman (1974), exploring the presence and extent of anchoring bias in forecasts generated by four Large Language Models (LLMs): GPT-4, Claude 2, Gemini Pro and GPT-3.5. In contrast to recent findings of advanced reasoning capabilities in LLMs, our randomised controlled trials reveal the presence of anchoring bias across all models: forecasts are significantly influenced by prior mention of high or low values. We examine two mitigation prompting strategies, `Chain of Thought' and `ignore previous', finding limited and varying degrees of effectiveness. Our results extend the anchoring bias research in finance beyond human decision-making to encompass LLMs, highlighting the importance of deliberate and informed prompting in AI forecasting in both ad hoc LLM use and in crafting few-shot examples.},
  keywords = {anchoring bias,artificial intelligence},
  annotation = {https://openrouter.ai/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Nguyen_2024_Human Bias in AI Models.pdf;/Users/thomasgorman/Zotero/storage/RAYMSJ7W/S2214635024000868.html}
}

@misc{nguyenPredictingUnderstandingHuman2024,
  title = {Predicting and {{Understanding Human Action Decisions}}: {{Insights}} from {{Large Language Models}} and {{Cognitive Instance-Based Learning}}},
  shorttitle = {Predicting and {{Understanding Human Action Decisions}}},
  author = {Nguyen, Thuy Ngoc and Jamale, Kasturi and Gonzalez, Cleotilde},
  year = {2024},
  month = aug,
  number = {arXiv:2407.09281},
  eprint = {2407.09281},
  publisher = {arXiv},
  urldate = {2024-08-10},
  abstract = {Large Language Models (LLMs) have demonstrated their capabilities across various tasks, from language translation to complex reasoning. Understanding and predicting human behavior and biases are crucial for artificial intelligence (AI)assisted systems to provide useful assistance, yet it remains an open question whether these models can achieve this. This paper addresses this gap by leveraging the reasoning and generative capabilities of the LLMs to predict human behavior in two sequential decision-making tasks. These tasks involve balancing between exploitative and exploratory actions and handling delayed feedback---both essential for simulating real-life decision processes. We compare the performance of LLMs with a cognitive instance-based learning (IBL) model, which imitates human experiential decisionmaking. Our findings indicate that LLMs excel at rapidly incorporating feedback to enhance prediction accuracy. In contrast, the cognitive IBL model better accounts for human exploratory behaviors and effectively captures loss aversion bias --- the tendency to choose a sub-optimal goal with fewer stepcost penalties rather than exploring to find the optimal choice, even with limited experience. The results highlight the benefits of integrating LLMs with cognitive architectures, suggesting that this synergy could enhance the modeling and understanding of complex human decision-making patterns.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {Experiment 1: https://osf.io/2ycm6 2Experiment 2: https://osf.io/hxfyq},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Nguyen et al_2024_Predicting and Understanding Human Action Decisions.pdf}
}

@misc{niCHATREPORTDemocratizingSustainability2023,
  title = {{{CHATREPORT}}: {{Democratizing Sustainability Disclosure Analysis}} through {{LLM-based Tools}}},
  shorttitle = {{{CHATREPORT}}},
  author = {Ni, Jingwei and Bingler, Julia and {Colesanti-Senni}, Chiara and Kraus, Mathias and Gostlow, Glen and Schimanski, Tobias and Stammbach, Dominik and Vaghefi, Saeid Ashraf and Wang, Qian and Webersinke, Nicolas and Wekhof, Tobias and Yu, Tingyu and Leippold, Markus},
  year = {2023},
  month = oct,
  number = {arXiv:2307.15770},
  eprint = {2307.15770},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-05},
  abstract = {In the face of climate change, are companies really taking substantial steps toward more sustainable operations? A comprehensive answer lies in the dense, information-rich landscape of corporate sustainability reports. However, the sheer volume and complexity of these reports make human analysis very costly. Therefore, only a few entities worldwide have the resources to analyze these reports at scale, which leads to a lack of transparency in sustainability reporting. Empowering stakeholders with LLM-based automatic analysis tools can be a promising way to democratize sustainability report analysis. However, developing such tools is challenging due to (1) the hallucination of LLMs and (2) the inefficiency of bringing domain experts into the AI development loop. In this paper, we ChatReport, a novel LLM-based system to automate the analysis of corporate sustainability reports, addressing existing challenges by (1) making the answers traceable to reduce the harm of hallucination and (2) actively involving domain experts in the development loop. We make our methodology, annotated datasets, and generated analyses of 1015 reports publicly available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ni et al_2023_CHATREPORT.pdf}
}

@inproceedings{nisiotiCollectiveInnovationGroups2024,
  title = {Collective {{Innovation}} in {{Groups}} of {{Large Language Models}}},
  booktitle = {{{ALIFE}} 2024: {{Proceedings}} of the 2024 {{Artificial Life Conference}}},
  author = {Nisioti, Eleni and Risi, Sebastian and Momennejad, Ida and Oudeyer, Pierre-Yves and {Moulin-Frier}, Cl{\'e}ment},
  year = {2024},
  month = jul,
  eprint = {2407.05377},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.1162/isal_a_00730},
  urldate = {2024-07-12},
  abstract = {Human culture relies on collective innovation: our ability to continuously explore how existing elements in our environment can be combined to create new ones. Language is hypothesized to play a key role in human culture, driving individual cognitive capacities and shaping communication. Yet the majority of models of collective innovation assign no cognitive capacities or language abilities to agents. Here, we contribute a computational study of collective innovation where agents are Large Language Models (LLMs) that play Little Alchemy 2, a creative video game originally developed for humans that, as we argue, captures useful aspects of innovation landscapes not present in previous test-beds. We, first, study an LLM in isolation and discover that it exhibits both useful skills and crucial limitations. We, then, study groups of LLMs that share information related to their behaviour and focus on the effect of social connectivity on collective performance. In agreement with previous human and computational studies, we observe that groups with dynamic connectivity out-compete fully-connected groups. Our work reveals opportunities and challenges for future studies of collective innovation that are becoming increasingly relevant as Generative Artificial Intelligence algorithms and humans innovate alongside each other.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Nisioti et al_2024_Collective Innovation in Groups of Large Language Models.pdf}
}

@article{nisiotiTextLifeReciprocal,
  title = {From {{Text}} to {{Life}}: {{On}} the {{Reciprocal Relationship}} between {{Artificial Life}} and {{Large Language Models}}},
  author = {Nisioti, Eleni and Glanois, Claire and Najarro, Elias and Dai, Andrew and Meyerson, Elliot and Pedersen, Joachim Winther and Teodorescu, Laetitia and Hayes, Conor F and Sudhakaran, Shyam and Risi, Sebastian},
  abstract = {Large Language Models (LLMs) have taken the field of AI by storm, but their adoption in the field of Artificial Life (ALife) has been, so far, relatively reserved. In this work we investigate the potential synergies betweens LLMs and ALife, drawing on a large body of research in the two fields. We explore the potential of LLMs as tools for ALife research, for example, as operators for evolutionary computation or the generation of open-ended environments. Reciprocally, principles of ALife, such as self-organization, collective intelligence and evolvability can provide an opportunity for shaping the development and functionalities of LLMs, leading to more adaptive and responsive models. By investigating this dynamic interplay, the paper aims to inspire innovative crossover approaches for both ALife and LLM research. Along the way, we examine the extent to which LLMs appear to increasingly exhibit properties such as emergence or collective intelligence, expanding beyond their original goal of generating text, and potentially redefining our perception of lifelike intelligence in artificial systems.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Nisioti et al_From Text to Life.pdf}
}

@misc{ohFrequencyExplainsInverse2024,
  title = {Frequency {{Explains}} the {{Inverse Correlation}} of {{Large Language Models}}' {{Size}}, {{Training Data Amount}}, and {{Surprisal}}'s {{Fit}} to {{Reading Times}}},
  author = {Oh, Byung-Doh and Yue, Shisen and Schuler, William},
  year = {2024},
  month = feb,
  number = {arXiv:2402.02255},
  eprint = {2402.02255},
  publisher = {arXiv},
  urldate = {2024-03-02},
  abstract = {Recent studies have shown that as Transformer-based language models become larger and are trained on very large amounts of data, the fit of their surprisal estimates to naturalistic human reading times degrades. The current work presents a series of analyses showing that word frequency is a key explanatory factor underlying these two trends. First, residual errors from four language model families on four corpora show that the inverse correlation between model size and fit to reading times is the strongest on the subset of least frequent words, which is driven by excessively accurate predictions of larger model variants. Additionally, training dynamics reveal that during later training steps, all model variants learn to predict rare words and that larger model variants do so more accurately, which explains the detrimental effect of both training data amount and model size on fit to reading times. Finally, a feature attribution analysis demonstrates that larger model variants are able to accurately predict rare words based on both an effectively longer context window size as well as stronger local associations compared to smaller model variants. Taken together, these results indicate that Transformer-based language models' surprisal estimates diverge from human-like expectations due to the superhumanly complex associations they learn for predicting rare words.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/byungdoh/llm\_surprisal},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Oh et al_2024_Frequency Explains the Inverse Correlation of Large Language Models' Size,.pdf;/Users/thomasgorman/Zotero/storage/GA89XYRL/2402.html}
}

@article{ondulaSentimentalAgentsExploring,
  title = {Sentimental {{Agents}}: {{Exploring Deliberation}}, {{Cognitive Biases}}, and {{Decision-making}} in {{LLM-based Multiagent Systems}}},
  author = {Ondula, Elizabeth A and Orner, Daniele and Mwangi, Nick Mumero and Rusti, Casandra},
  abstract = {How does sentiment affect deliberative opinion dynamics in multi-agent systems using Large Language Models (LLMs)? In this paper, we introduce Sentimental Agents, a framework designed to study collaborative decision-making in a society of agents, each equipped with a distinct Mental Model of Self. We propose a method to integrate sentiment analysis and a non-Bayesian update mechanism, to analyze and interpret agents' beliefs and interactions systematically. This method allows us to observe the volatility of the sentiment associated with different agent statements, as well as the change in opinion throughout the agents' conversation. We further use it to model and compare collaborative decision-making approaches. We situate these agents in a simulated Human Resource recruiting environment as a case study to evaluate a candidate's fit for a role. We present a set of metrics to assess the quality of the agents' output. Finally, we explore cognitive biases in the agents' individual and collective opinion formation, a fundamental step to enhance decision-making capabilities and mitigate distortions in the system and the agents' collective reasoning.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ondula et al_Sentimental Agents.pdf}
}

@misc{ongGPTologyComputationalModels2024,
  title = {{{GPT-ology}}, {{Computational Models}}, {{Silicon Sampling}}: {{How}} Should We Think about {{LLMs}} in {{Cognitive Science}}?},
  shorttitle = {{{GPT-ology}}, {{Computational Models}}, {{Silicon Sampling}}},
  author = {Ong, Desmond C.},
  year = {2024},
  month = jun,
  number = {arXiv:2406.09464},
  eprint = {2406.09464},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-04},
  abstract = {Large Language Models have taken the cognitive science world by storm. It is perhaps timely now to take stock of the various research paradigms that have been used to make scientific inferences about ``cognition'' in these models or about human cognition. We review several emerging research paradigms---GPT-ology, LLMsas-computational-models, and ``silicon sampling''--- and review recent papers that have used LLMs under these paradigms. In doing so, we discuss their claims as well as challenges to scientific inference under these various paradigms. We highlight several outstanding issues about LLMs that have to be addressed to push our science forward: closed-source vs open-sourced models; (the lack of visibility of) training data; and reproducibility in LLM research, including forming conventions on new task ``hyperparameters'' like instructions and prompts.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Ong_2024_GPT-ology, Computational Models, Silicon Sampling.pdf}
}

@article{organisciakSemanticDistanceAutomated2023,
  title = {Beyond Semantic Distance: {{Automated}} Scoring of Divergent Thinking Greatly Improves with Large Language Models},
  shorttitle = {Beyond Semantic Distance},
  author = {Organisciak, Peter and Acar, Selcuk and Dumas, Denis and Berthiaume, Kelly},
  year = {2023},
  month = sep,
  journal = {Thinking Skills and Creativity},
  volume = {49},
  pages = {101356},
  issn = {1871-1871},
  doi = {10.1016/j.tsc.2023.101356},
  urldate = {2023-09-26},
  abstract = {Automated scoring for divergent thinking (DT) seeks to overcome a key obstacle to creativity measurement: the effort, cost, and reliability of scoring open-ended tests. For a common test of DT, the Alternate Uses Task (AUT), the primary automated approach casts the problem as a semantic distance between a prompt and the resulting idea in a text model. This work presents an alternative approach that greatly surpasses the performance of the best existing semantic distance approaches. Our system, Ocsai, fine-tunes deep neural network-based large-language models (LLMs) on human-judged responses. Trained and evaluated against one of the largest collections of human-judged AUT responses, with 27 thousand responses collected from nine past studies, our fine-tuned large-language-models achieved up to r~=~0.81 correlation with human raters, greatly surpassing current systems (r~=~0.12--0.26). Further, learning transfers well to new test items and the approach is still robust with small numbers of training labels. We also compare prompt-based zero-shot and few-shot approaches, using GPT-3, ChatGPT, and GPT-4. This work also suggests a limit to the underlying assumptions of the semantic distance model, showing that a purely semantic approach that uses the stronger language representation of LLMs, while still improving on existing systems, does not achieve comparable improvements to our fine-tuned system. The increase in performance can support stronger applications and interventions in DT and opens the space of automated DT scoring to new areas for improving and understanding this branch of methods.},
  keywords = {Alternate uses test,Automated scoring,Divergent thinking,Large-language models},
  annotation = {https://openscoring.du.edu/},
  file = {/Users/thomasgorman/Zotero/storage/W2HKE7KP/S1871187123001256.html}
}

@article{orhanLearningHighlevelVisual2024,
  title = {Learning High-Level Visual Representations from a Child's Perspective without Strong Inductive Biases},
  author = {Orhan, A. Emin and Lake, Brenden M.},
  year = {2024},
  month = mar,
  journal = {Nature Machine Intelligence},
  volume = {6},
  number = {3},
  pages = {271--283},
  issn = {2522-5839},
  doi = {10.1038/s42256-024-00802-0},
  urldate = {2024-05-26},
  abstract = {Young children develop sophisticated internal models of the world based on their visual experience. Can such models be learned from a child's visual experience without strong inductive biases? To investigate this, we train state-of-the-art neural networks on a realistic proxy of a child's visual experience without any explicit supervision or domain-specific inductive biases. Specifically, we train both embedding models and generative models on 200 hours of headcam video from a single child collected over two years and comprehensively evaluate their performance in downstream tasks using various reference models as yardsticks. On average, the best embedding models perform at a respectable 70\% of a high-performance ImageNet-trained model, despite substantial differences in training data. They also learn broad semantic categories and object localization capabilities without explicit supervision, but they are less object-centric than models trained on all of ImageNet. Generative models trained with the same data successfully extrapolate simple properties of partially masked objects, like their rough outline, texture, colour or orientation, but struggle with finer object details. We replicate our experiments with two other children and find remarkably consistent results. Broadly useful high-level visual representations are thus robustly learnable from a sample of a child's visual experience without strong inductive biases.},
  copyright = {2024 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Human behaviour},
  annotation = {https://github.com/eminorhan/silicon-menagerie},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Orhan_Lake_2024_Learning high-level visual representations from a child’s perspective without.pdf}
}

@article{orwigLanguageCreativityEvidence2024,
  title = {The {{Language}} of {{Creativity}}: {{Evidence}} from {{Humans}} and {{Large Language Models}}},
  shorttitle = {The {{Language}} of {{Creativity}}},
  author = {Orwig, William and Edenbaum, Emma R. and Greene, Joshua D. and Schacter, Daniel L.},
  year = {2024},
  journal = {The Journal of Creative Behavior},
  volume = {58},
  number = {1},
  pages = {128--136},
  issn = {2162-6057},
  doi = {10.1002/jocb.636},
  urldate = {2024-05-25},
  abstract = {Recent developments in computerized scoring via semantic distance have provided automated assessments of verbal creativity. Here, we extend past work, applying computational linguistic approaches to characterize salient features of creative text. We hypothesize that, in addition to semantic diversity, the degree to which a story includes perceptual details, thus transporting the reader to another time and place, would be predictive of creativity. Additionally, we explore the use of generative language models to supplement human data collection and examine the extent to which machine-generated stories can mimic human creativity. We collect 600 short stories from human participants and GPT-3, subsequently randomized and assessed on their creative quality. Results indicate that the presence of perceptual details, in conjunction with semantic diversity, is highly predictive of creativity. These results were replicated in an independent sample of stories (n = 120) generated by GPT-4. We do not observe a significant difference between human and AI-generated stories in terms of creativity ratings, and we also observe positive correlations between human and AI assessments of creativity. Implications and future directions are discussed.},
  langid = {english},
  keywords = {artificial intelligence,creativity,large language models,semantic distance},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Orwig et al_2024_The Language of Creativity.pdf;/Users/thomasgorman/Zotero/storage/4FIYICQZ/jocb.html}
}

@article{palenzuelaModelingSecondLanguage2022,
  title = {Modeling {{Second Language Acquisition}} with Pre-Trained Neural Language Models},
  author = {Palenzuela, {\'A}lvaro J. Jim{\'e}nez and Frasincar, Flavius and Tru{\c s}c{\v a}, Maria Mihaela},
  year = {2022},
  month = nov,
  journal = {Expert Systems with Applications},
  volume = {207},
  pages = {117871},
  issn = {0957-4174},
  doi = {10.1016/j.eswa.2022.117871},
  urldate = {2022-08-17},
  abstract = {Prediction of language mistakes is a task introduced by Duolingo as part of the Second Language Acquisition Modeling topic that aims to learn from the history of mistakes to improve the experience of language learners. Using transfer learning by means of pre-trained language models, we propose a framework that can learn the actual mistakes distribution according to which faraway words of a sentence have a higher chance to produce errors. To adapt the information provided by the pre-trained language models, more approaches based on feature extraction or fine-tuning were tried. However, according to our experiments, integrating these two options in a stack-and-finetune approach seems to be more appropriate for our task. Regarding the comparison of language models in terms of model distillation, we notice that distillation does not affect the effectiveness while significantly reducing the training time. We conclude that the model complexity should be adjusted to the specifics of the analyzed problem and the distillation is an efficient option for low complexity corpora without considerably affecting the overall performance.},
  langid = {english},
  keywords = {Feature extraction,Fine-tuning,jupyter_notebook,Model Distillation,Pre-trained Language model,Python_Code,Second Language Acquisition},
  annotation = {https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/8SWHNO},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Palenzuela et al_2022_Modeling Second Language Acquisition with pre-trained neural language models.pdf;/Users/thomasgorman/Zotero/storage/DRXW7ATK/S0957417422011241.html}
}

@misc{papachristouNetworkFormationDynamics2024,
  title = {Network {{Formation}} and {{Dynamics Among Multi-LLMs}}},
  author = {Papachristou, Marios and Yuan, Yuan},
  year = {2024},
  month = jun,
  number = {arXiv:2402.10659},
  eprint = {2402.10659},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-23},
  abstract = {Social networks shape opinions, behaviors, and information dissemination in human societies. As large language models (LLMs) increasingly integrate into social and professional environments, understanding their behavior within the context of social interactions and networks becomes essential. Our study analyzes LLMs' network formation behavior to examine whether the dynamics of multiple LLMs are similar to or different from human social dynamics. We observe that LLMs exhibit key social network principles, including preferential attachment, triadic closure, homophily, community structure, and the small-world phenomenon, when asked about their preferences in network formation. We also investigate LLMs' decision-making based on real-world networks, revealing that triadic closure and homophily have a stronger influence than preferential attachment and that LLMs perform well in network formation predictions. Overall, our study opens up new possibilities for using LLMs in network science research and helps develop socially aware LLMs by shedding light on their social interaction behaviors and exploring their impacts on social dynamics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Multiagent Systems,Computer Science - Social and Information Networks},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Papachristou_Yuan_2024_Network Formation and Dynamics Among Multi-LLMs.pdf}
}

@misc{parkAIDeceptionSurvey2023,
  title = {{{AI Deception}}: {{A Survey}} of {{Examples}}, {{Risks}}, and {{Potential Solutions}}},
  shorttitle = {{{AI Deception}}},
  author = {Park, Peter S. and Goldstein, Simon and O'Gara, Aidan and Chen, Michael and Hendrycks, Dan},
  year = {2023},
  month = aug,
  number = {arXiv:2308.14752},
  eprint = {2308.14752},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.14752},
  urldate = {2023-09-04},
  abstract = {This paper argues that a range of current AI systems have learned how to deceive humans. We define deception as the systematic inducement of false beliefs in the pursuit of some outcome other than the truth. We first survey empirical examples of AI deception, discussing both special-use AI systems (including Meta's CICERO) built for specific competitive situations, and general-purpose AI systems (such as large language models). Next, we detail several risks from AI deception, such as fraud, election tampering, and losing control of AI systems. Finally, we outline several potential solutions to the problems posed by AI deception: first, regulatory frameworks should subject AI systems that are capable of deception to robust risk-assessment requirements; second, policymakers should implement bot-or-not laws; and finally, policymakers should prioritize the funding of relevant research, including tools to detect AI deception and to make AI systems less deceptive. Policymakers, researchers, and the broader public should work proactively to prevent AI deception from destabilizing the shared foundations of our society.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Park et al_2023_AI Deception.pdf;/Users/thomasgorman/Zotero/storage/U2KSMFSM/2308.html}
}

@article{parkDiminishedDiversitythoughtStandard2024,
  title = {Diminished Diversity-of-Thought in a Standard Large Language Model},
  author = {Park, Peter S. and Schoenegger, Philipp and Zhu, Chongyang},
  year = {2024},
  month = jan,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02307-x},
  urldate = {2024-01-14},
  abstract = {We test whether large language models (LLMs) can be used to simulate human participants in social-science studies. To do this, we ran replications of 14 studies from the Many Labs 2 replication project with OpenAI's text-davinci-003 model, colloquially known as GPT-3.5. Based on our pre-registered analyses, we find that among the eight studies we could analyse, our GPT sample replicated 37.5\% of the original results and 37.5\% of the Many Labs 2 results. However, we were unable to analyse the remaining six studies due to an unexpected phenomenon we call the ``correct answer'' effect. Different runs of GPT-3.5 answered nuanced questions probing political orientation, economic preference, judgement, and moral philosophy with zero or near-zero variation in responses: with the supposedly ``correct answer.'' In one exploratory follow-up study, we found that a ``correct answer'' was robust to changing the demographic details that precede the prompt. In another, we found that most but not all ``correct answers'' were robust to changing the order of answer choices. One of our most striking findings occurred in our replication of the Moral Foundations Theory survey results, where we found GPT-3.5 identifying as a political conservative in 99.6\% of the cases, and as a liberal in 99.3\% of the cases in the reverse-order condition. However, both self-reported `GPT conservatives' and `GPT liberals' showed right-leaning moral foundations. Our results cast doubts on the validity of using LLMs as a general replacement for human participants in the social sciences. Our results also raise concerns that a hypothetical AI-led future may be subject to a diminished diversity of thought.},
  langid = {english},
  keywords = {Artificial intelligence,Demographic effects,Diversity of thought,GPT-3.5,Large language models,Many Labs 2,Order effects,Psychology,Replication,Social science},
  annotation = {https://osf.io/dzp8t/?view\_only=45fff3953884443d81b628cdd5d50f7a},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Park et al_2024_Diminished diversity-of-thought in a standard large language model.pdf}
}

@misc{parkGenerativeAgentsInteractive2023,
  title = {Generative {{Agents}}: {{Interactive Simulacra}} of {{Human Behavior}}},
  shorttitle = {Generative {{Agents}}},
  author = {Park, Joon Sung and O'Brien, Joseph C. and Cai, Carrie J. and Morris, Meredith Ringel and Liang, Percy and Bernstein, Michael S.},
  year = {2023},
  month = aug,
  number = {arXiv:2304.03442},
  eprint = {2304.03442},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-31},
  abstract = {Believable proxies of human behavior can empower interactive applications ranging from immersive environments to rehearsal spaces for interpersonal communication to prototyping tools. In this paper, we introduce generative agents--computational software agents that simulate believable human behavior. Generative agents wake up, cook breakfast, and head to work; artists paint, while authors write; they form opinions, notice each other, and initiate conversations; they remember and reflect on days past as they plan the next day. To enable generative agents, we describe an architecture that extends a large language model to store a complete record of the agent's experiences using natural language, synthesize those memories over time into higher-level reflections, and retrieve them dynamically to plan behavior. We instantiate generative agents to populate an interactive sandbox environment inspired by The Sims, where end users can interact with a small town of twenty five agents using natural language. In an evaluation, these generative agents produce believable individual and emergent social behaviors: for example, starting with only a single user-specified notion that one agent wants to throw a Valentine's Day party, the agents autonomously spread invitations to the party over the next two days, make new acquaintances, ask each other out on dates to the party, and coordinate to show up for the party together at the right time. We demonstrate through ablation that the components of our agent architecture--observation, planning, and reflection--each contribute critically to the believability of agent behavior. By fusing large language models with computational, interactive agents, this work introduces architectural and interaction patterns for enabling believable simulations of human behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Park et al_2023_Generative Agents.pdf;/Users/thomasgorman/Zotero/storage/SLDIHU6U/2304.html}
}

@misc{payandehHowSusceptibleAre2023,
  title = {How Susceptible Are {{LLMs}} to {{Logical Fallacies}}?},
  author = {Payandeh, Amirreza and Pluth, Dan and Hosier, Jordan and Xiao, Xuesu and Gurbani, Vijay K.},
  year = {2023},
  month = aug,
  number = {arXiv:2308.09853},
  eprint = {2308.09853},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2308.09853},
  urldate = {2023-09-04},
  abstract = {This paper investigates the rational thinking capability of Large Language Models (LLMs) in multi-round argumentative debates by exploring the impact of fallacious arguments on their logical reasoning performance. More specifically, we present Logic Competence Measurement Benchmark (LOGICOM), a diagnostic benchmark to assess the robustness of LLMs against logical fallacies. LOGICOM involves two agents: a persuader and a debater engaging in a multi-round debate on a controversial topic, where the persuader tries to convince the debater of the correctness of its claim. First, LOGICOM assesses the potential of LLMs to change their opinions through reasoning. Then, it evaluates the debater's performance in logical reasoning by contrasting the scenario where the persuader employs logical fallacies against one where logical reasoning is used. We use this benchmark to evaluate the performance of GPT-3.5 and GPT-4 using a dataset containing controversial topics, claims, and reasons supporting them. Our findings indicate that both GPT-3.5 and GPT-4 can adjust their opinion through reasoning. However, when presented with logical fallacies, GPT-3.5 and GPT-4 are erroneously convinced 41\% and 69\% more often, respectively, compared to when logical reasoning is used. Finally, we introduce a new dataset containing over 5k pairs of logical vs. fallacious arguments. The source code and dataset of this work are made publicly available.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Payandeh et al_2023_How susceptible are LLMs to Logical Fallacies.pdf;/Users/thomasgorman/Zotero/storage/4ASWTKCL/2308.html}
}

@article{pekkanenComputationalModelDrivers2018,
  title = {A Computational Model for Driver's Cognitive State, Visual Perception and Intermittent Attention in a Distracted Car Following Task},
  author = {Pekkanen, Jami and Lappi, Otto and Rinkkala, Paavo and Tuhkanen, Samuel and Frantsi, Roosa and Summala, Heikki},
  year = {2018},
  month = sep,
  journal = {Royal Society Open Science},
  volume = {5},
  number = {9},
  pages = {180194},
  issn = {2054-5703},
  doi = {10.1098/rsos.180194},
  urldate = {2024-09-16},
  abstract = {We present a computational model of intermittent visual sampling and locomotor control in a simple yet representative task of a car driver following another vehicle. The model has a number of features that take it beyond the current state of the art in modelling natural tasks, and driving in particular. First, unlike most control theoretical models in vision science and engineering---where control is directly based on observable (optical) variables---actions are based on a temporally enduring internal representation. Second, unlike the more sophisticated engineering driver models based on internal representations, our model explicitly aims to be psychologically plausible, in particular in modelling perceptual processes and their limitations. Third, unlike most psychological models, it is implemented as an actual simulation model capable of full task performance (visual sampling and longitudinal control). The model is developed and validated using a dataset from a simplified car-following experiment (               N               = 40, in both three-dimensional virtual reality and a real instrumented vehicle). The results replicate our previously reported connection between time headway and visual attention. The model reproduces this connection and predicts that it emerges from control of action uncertainty. Implications for traffic psychological models and future developments for psychologically plausible yet computationally rigorous models of full natural task performance are discussed.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Pekkanen et al_2018_A computational model for driver's cognitive state, visual perception and.pdf}
}

@article{pellertAIPsychometricsAssessing2023,
  title = {{{AI Psychometrics}}: {{Assessing}} the {{Psychological Profiles}} of {{Large Language Models Through Psychometric Inventories}}},
  author = {Pellert, Max and Lechner, Clemens M and Wagner, Claudia and Rammstedt, Beatrice and Strohmaier, Markus},
  year = {2023},
  journal = {Perspectives on Psychological Science},
  abstract = {We illustrate how standard psychometric inventories originally designed for assessing noncognitive human traits can be repurposed as diagnostic tools to evaluate analogous traits in large language models (LLMs). We start from the assumption that LLMs, inadvertently yet inevitably, acquire psychological traits (metaphorically speaking) from the vast text corpora on which they are trained. Such corpora contain sediments of the personalities, values, beliefs, and biases of the countless human authors of these texts, which LLMs learn through a complex training process. The traits that LLMs acquire in such a way can potentially influence their behavior, that is, their outputs in downstream tasks and applications in which they are employed, which in turn may have real-world consequences for individuals and social groups. By eliciting LLMs' responses to language-based psychometric inventories, we can bring their traits to light. Psychometric profiling enables researchers to study and compare LLMs in terms of noncognitive characteristics, thereby providing a window into the personalities, values, beliefs, and biases these models exhibit (or mimic). We discuss the history of similar ideas and outline possible psychometric approaches for LLMs. We demonstrate one promising approach, zero-shot classification, for several LLMs and psychometric inventories. We conclude by highlighting open challenges and future avenues of research for AI Psychometrics.},
  langid = {english},
  annotation = {https://github.com/maxpel/psyai\_materials},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Pellert et al_AI Psychometrics.pdf}
}

@article{pendyalaEnhancingCognitionEfficacy2022,
  title = {Enhancing the {{Cognition}} and {{Efficacy}} of {{Machine Learning Through Similarity}}},
  author = {Pendyala, Vishnu and Amireddy, Rakesh},
  year = {2022},
  month = aug,
  journal = {SN Computer Science},
  volume = {3},
  number = {6},
  pages = {442},
  issn = {2661-8907},
  doi = {10.1007/s42979-022-01339-y},
  urldate = {2024-03-03},
  abstract = {Purpose: Similarity is a key element of machine learning and can make human learning much more effective as well. One of the goals of this paper is to expound on this aspect. We identify real-world concepts similar to hard-to-understand theories to enhance the learning experience and comprehension of a machine learning student. The second goal is to enhance the work in the current literature that uses similarity for transcoding. We uniquely try transcoding from Python to R and vice versa, something that was not attempted before, by identifying similarities in a latent embedding space. Methods: We list several real-world analogies to show similarities with and simplify the machine learning narrative. Next, we use Cross-Lingual Model Pretraining, Denoising Auto-encoding, and Back-translation to automatically identify similarities between the programming languages, Python and R and convert code in one to another. Results: In the course of teaching machine learning to undergraduate, graduate, and general pool of students, the first author found that relating the concepts to real-world examples listed in this paper greatly enhanced student comprehension and made the topics much more approachable despite the math and the methods involved. When it comes to transcoding, in spite of the fact that Python and R are substantially different, we obtained reasonable success measured using various evaluation metrics and methods as described in the paper. Conclusion: Machines and human beings predominantly learn by way of similarity, a finding that can be explored further in both the machine and human learning domains.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Pendyala_Amireddy_2022_Enhancing the Cognition and Efficacy of Machine Learning Through Similarity.pdf}
}

@misc{perezCulturalEvolutionPopulations2024,
  title = {Cultural Evolution in Populations of {{Large Language Models}}},
  author = {Perez, J{\'e}r{\'e}my and L{\'e}ger, Corentin and {Ovando-Tellez}, Marcela and Foulon, Chris and Dussauld, Joan and Oudeyer, Pierre-Yves and {Moulin-Frier}, Cl{\'e}ment},
  year = {2024},
  month = mar,
  number = {arXiv:2403.08882},
  eprint = {2403.08882},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2403.08882},
  urldate = {2024-05-11},
  abstract = {Research in cultural evolution aims at providing causal explanations for the change of culture over time. Over the past decades, this field has generated an important body of knowledge, using experimental, historical, and computational methods. While computational models have been very successful at generating testable hypotheses about the effects of several factors, such as population structure or transmission biases, some phenomena have so far been more complex to capture using agent-based and formal models. This is in particular the case for the effect of the transformations of social information induced by evolved cognitive mechanisms. We here propose that leveraging the capacity of Large Language Models (LLMs) to mimic human behavior may be fruitful to address this gap. On top of being an useful approximation of human cultural dynamics, multi-agents models featuring generative agents are also important to study for their own sake. Indeed, as artificial agents are bound to participate more and more to the evolution of culture, it is crucial to better understand the dynamics of machine-generated cultural evolution. We here present a framework for simulating cultural evolution in populations of LLMs, allowing the manipulation of variables known to be important in cultural evolution, such as network structure, personality, and the way social information is aggregated and transformed. The software we developed for conducting these simulations is open-source and features an intuitive user-interface, which we hope will help to build bridges between the fields of cultural evolution and generative artificial intelligence.},
  archiveprefix = {arXiv},
  keywords = {68T50,Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems,I.2.7,Quantitative Biology - Populations and Evolution},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Perez et al_2024_Cultural evolution in populations of Large Language Models.pdf;/Users/thomasgorman/Zotero/storage/7NZJRML5/2403.html}
}

@misc{perezWhenLLMsPlay2024,
  title = {When {{LLMs Play}} the {{Telephone Game}}: {{Cumulative Changes}} and {{Attractors}} in {{Iterated Cultural Transmissions}}},
  shorttitle = {When {{LLMs Play}} the {{Telephone Game}}},
  author = {Perez, J{\'e}r{\'e}my and L{\'e}ger, Corentin and Kova{\v c}, Grgur and Colas, C{\'e}dric and Molinaro, Gaia and Derex, Maxime and Oudeyer, Pierre-Yves and {Moulin-Frier}, Cl{\'e}ment},
  year = {2024},
  month = jul,
  number = {arXiv:2407.04503},
  eprint = {2407.04503},
  primaryclass = {physics},
  publisher = {arXiv},
  urldate = {2024-07-23},
  abstract = {As large language models (LLMs) start interacting with each other and generating an increasing amount of text online, it becomes crucial to better understand how information is transformed as it passes from one LLM to the next. While significant research has examined individual LLM behaviors, existing studies have largely overlooked the collective behaviors and information distortions arising from iterated LLM interactions. Small biases, negligible at the single output level, risk being amplified in iterated interactions, potentially leading the content to evolve towards attractor states. In a series of telephone game experiments, we apply a transmission chain design borrowed from the human cultural evolution literature: LLM agents iteratively receive, produce, and transmit texts from the previous to the next agent in the chain. By tracking the evolution of text toxicity, positivity, difficulty, and length across transmission chains, we uncover the existence of biases and attractors, and study their dependence on the initial text, the instructions, language model, and model size. For instance, we find that more open-ended instructions lead to stronger attraction effects compared to more constrained tasks. We also find that different text properties display different sensitivity to attraction effects, with toxicity leading to stronger attractors than length. These findings highlight the importance of accounting for multi-step transmission dynamics and represent a first step towards a more comprehensive understanding of LLM cultural dynamics.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T50,Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems,I.2.7,Physics - Physics and Society},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Perez et al_2024_When LLMs Play the Telephone Game.pdf}
}

@misc{petersenCanLanguageModels2023,
  title = {Can Language Models Learn Analogical Reasoning? {{Investigating}} Training Objectives and Comparisons to Human Performance},
  shorttitle = {Can Language Models Learn Analogical Reasoning?},
  author = {Petersen, Molly R. and {van der Plas}, Lonneke},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05597},
  eprint = {2310.05597},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.05597},
  urldate = {2023-10-17},
  abstract = {While analogies are a common way to evaluate word embeddings in NLP, it is also of interest to investigate whether or not analogical reasoning is a task in itself that can be learned. In this paper, we test several ways to learn basic analogical reasoning, specifically focusing on analogies that are more typical of what is used to evaluate analogical reasoning in humans than those in commonly used NLP benchmarks. Our experiments find that models are able to learn analogical reasoning, even with a small amount of data. We additionally compare our models to a dataset with a human baseline, and find that after training, models approach human performance.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Petersen_van der Plas_2023_Can language models learn analogical reasoning.pdf;/Users/thomasgorman/Zotero/storage/RLGERNQU/2310.html}
}

@article{petersonParallelogramsRevisitedExploring2020,
  title = {Parallelograms Revisited: {{Exploring}} the Limitations of Vector Space Models for Simple Analogies},
  shorttitle = {Parallelograms Revisited},
  author = {Peterson, Joshua C. and Chen, Dawn and Griffiths, Thomas L.},
  year = {2020},
  month = dec,
  journal = {Cognition},
  volume = {205},
  pages = {104440},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2020.104440},
  urldate = {2024-05-26},
  abstract = {Classic psychological theories have demonstrated the power and limitations of spatial representations, providing geometric tools for reasoning about the similarity of objects and showing that human intuitions sometimes violate the constraints of geometric spaces. Recent machine learning methods for deriving vector-space embeddings of words have begun to garner attention for their surprising capacity to capture simple analogies consistently across large corpora, giving new life to a classic model of analogies as parallelograms that was first proposed and briefly explored by psychologists. We evaluate the parallelogram model of analogy as applied to modern data-driven word embeddings, providing a detailed analysis of the extent to which this approach captures human behavior in the domain of word pairs. Using a large novel benchmark dataset of human analogy completions, we show that word similarity alone surprisingly captures some aspects of human responses better than the parallelogram model. To gain a fine-grained picture of how well these models predict relational similarity, we also collect a large dataset of human relational similarity judgments and find that the parallelogram model captures some semantic relationships better than others. Finally, we provide evidence for deeper limitations of the parallelogram model of analogy based on the intrinsic geometric constraints of vector spaces, paralleling classic results for item similarity. Taken together, these results show that while modern word embeddings do an impressive job of capturing semantic similarity at scale, the parallelogram model alone is insufficient to account for how people form even the simplest analogies.},
  keywords = {Analogy,Relational similarity,Vector space models},
  annotation = {https://data.mendeley.com/datasets/nzzmhwfgcf/1},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Peterson et al_2020_Parallelograms revisited.pdf;/Users/thomasgorman/Zotero/storage/E6M64RZR/S0010027720302596.html}
}

@article{petilliVectorSpacesDRM2024,
  title = {From Vector Spaces to {{DRM}} Lists: {{False Memory Generator}}, a Software for Automated Generation of Lists of Stimuli Inducing False Memories},
  shorttitle = {From Vector Spaces to {{DRM}} Lists},
  author = {Petilli, Marco A. and Marelli, Marco and Mazzoni, Giuliana and Marchetti, Michela and Rinaldi, Luca and Gatti, Daniele},
  year = {2024},
  month = jun,
  journal = {Behavior Research Methods},
  volume = {56},
  number = {4},
  pages = {3779--3793},
  issn = {1554-3528},
  doi = {10.3758/s13428-024-02425-0},
  urldate = {2024-07-12},
  abstract = {The formation of false memories is one of the most widely studied topics in cognitive psychology. The Deese--Roediger--McDermott (DRM) paradigm is a powerful tool for investigating false memories and revealing the cognitive mechanisms subserving their formation. In this task, participants first memorize a list of words (encoding phase) and next have to indicate whether words presented in a new list were part of the initially memorized one (recognition phase). By employing DRM lists optimized to investigate semantic effects, previous studies highlighted a crucial role of semantic processes in false memory generation, showing that new words semantically related to the studied ones tend to be more erroneously recognized (compared to new words less semantically related). Despite the strengths of the DRM task, this paradigm faces a major limitation in list construction due to its reliance on human-based association norms, posing both practical and theoretical concerns. To address these issues, we developed the False Memory Generator (FMG), an automated and data-driven tool for generating DRM lists, which exploits similarity relationships between items populating a vector space. Here, we present FMG and demonstrate the validity of the lists generated in successfully replicating well-known semantic effects on false memory production. FMG potentially has broad applications by allowing for testing false memory production in domains that go well beyond the current possibilities, as it can be in principle applied to any vector space encoding properties related to word referents (e.g., lexical, orthographic, phonological, sensory, affective, etc.) or other type of stimuli (e.g., images, sounds, etc.).},
  langid = {english},
  keywords = {Distributional semantics,DRM,False memory,Vector space},
  annotation = {https://osf.io/gsrfu/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Petilli et al_2024_From vector spaces to DRM lists.pdf}
}

@article{pezzuloGeneratingMeaningActive2024,
  title = {Generating Meaning: Active Inference and the Scope and Limits of Passive {{AI}}},
  shorttitle = {Generating Meaning},
  author = {Pezzulo, Giovanni and Parr, Thomas and Cisek, Paul and Clark, Andy and Friston, Karl},
  year = {2024},
  month = feb,
  journal = {Trends in Cognitive Sciences},
  volume = {28},
  number = {2},
  pages = {97--112},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2023.10.002},
  urldate = {2024-06-04},
  abstract = {Prominent accounts of sentient behavior depict brains as generative models of organismic interaction with the world, evincing intriguing similarities with current advances in generative artificial intelligence (AI). However, because they con- tend with the control of purposive, life-sustaining sensorimotor interactions, the generative models of living organisms are inextricably anchored to the body and world. Unlike the passive models learned by generative AI systems, they must capture and control the sensory consequences of action. This allows embodied agents to intervene upon their worlds in ways that constantly put their best models to the test, thus providing a solid bedrock that is -- we argue -- essential to the development of genuine understanding. We review the resulting implications and consider future directions for generative AI.},
  langid = {english},
  pmid = {37973519},
  keywords = {active inference,embodied cognition,foundation models,generative AI,large language models,predictive processing},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Pezzulo et al_2024_Generating meaning.pdf}
}

@article{piantadosiModernLanguageModels2023,
  title = {Modern Language Models Refute {{Chomsky}}'s Approach to Language},
  author = {Piantadosi, Steven T},
  year = {2023},
  doi = {https://lingbuzz.net/lingbuzz/007180},
  abstract = {The rise and success of large language models undermines virtually every strong claim for the innateness of language that has been proposed by generative linguistics. Modern machine learning has subverted and bypassed the entire theoretical framework of Chomsky's approach, including its core claims to particular insights, principles, structures, and processes. I describe the sense in which modern language models implement genuine theories of language, including representations of syntactic and semantic structure. I highlight the relationship between contemporary models and prior approaches in linguistics, namely those based on gradient computations and memorized constructions. I also respond to several critiques of large language models, including claims that they can't answer ``why'' questions, and skepticism that they are informative about real life acquisition. Most notably, large language models have attained remarkable success at discovering grammar without using any of the methods that some in linguistics insisted were necessary for a science of language to progress.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Piantadosi_Modern language models refute Chomsky’s approach to language.pdf}
}

@article{piantadosiWhyConceptsAre2024,
  title = {Why Concepts Are (Probably) Vectors},
  author = {Piantadosi, Steven T. and Muller, Dyana C. Y. and Rule, Joshua S. and Kaushik, Karthikeya and Gorenstein, Mark and Leib, Elena R. and Sanford, Emily},
  year = {2024},
  month = aug,
  journal = {Trends in Cognitive Sciences},
  issn = {1364-6613},
  doi = {10.1016/j.tics.2024.06.011},
  urldate = {2024-08-10},
  abstract = {For decades, cognitive scientists have debated what kind of representation might characterize human concepts. Whatever the format of the representation, it must allow for the computation of varied properties, including similarities, features, categories, definitions, and relations. It must also support the development of theories, ad hoc categories, and knowledge of procedures. Here, we discuss why vector-based representations provide a compelling account that can meet all these needs while being plausibly encoded into neural architectures. This view has become especially promising with recent advances in both large language models and vector symbolic architectures. These innovations show how vectors can handle many properties traditionally thought to be out of reach for neural models, including compositionality, definitions, structures, and symbolic computational processes.},
  keywords = {church encoding,concepts,conceptual role,vector,vector symbolic architecture},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Piantadosi et al_2024_Why concepts are (probably) vectors.pdf;/Users/thomasgorman/Zotero/storage/D5E9DQLV/S1364661324001712.html}
}

@misc{piattiCooperateCollapseEmergence2024,
  title = {Cooperate or {{Collapse}}: {{Emergence}} of {{Sustainable Cooperation}} in a {{Society}} of {{LLM Agents}}},
  shorttitle = {Cooperate or {{Collapse}}},
  author = {Piatti, Giorgio and Jin, Zhijing and {Kleiman-Weiner}, Max and Sch{\"o}lkopf, Bernhard and Sachan, Mrinmaya and Mihalcea, Rada},
  year = {2024},
  month = jul,
  number = {arXiv:2404.16698},
  eprint = {2404.16698},
  publisher = {arXiv},
  urldate = {2024-09-23},
  abstract = {As AI systems pervade human life, ensuring that large language models (LLMs) make safe decisions remains a significant challenge. We introduce the Governance of the Commons Simulation (GovSim), a generative simulation platform designed to study strategic interactions and cooperative decision-making in LLMs. In GovSim, a society of AI agents must collectively balance exploiting a common resource with sustaining it for future use. This environment enables the study of how ethical considerations, strategic planning, and negotiation skills impact cooperative outcomes. We develop an LLM-based agent architecture and test it with the leading open and closed LLMs. We find that all but the most powerful LLM agents fail to achieve a sustainable equilibrium in GovSim, with the highest survival rate below 54\%. Ablations reveal that successful multi-agent communication between agents is critical for achieving cooperation in these cases. Furthermore, our analyses show that the failure to achieve sustainable cooperation in most LLMs stems from their inability to formulate and analyze hypotheses about the long-term effects of their actions on the equilibrium of the group. Finally, we show that agents that leverage "Universalization"-based reasoning, a theory of moral thinking, are able to achieve significantly better sustainability. Taken together, GovSim enables us to study the mechanisms that underlie sustainable self-government with specificity and scale. We open source the full suite of our research results, including the simulation environment, agent prompts, and a comprehensive web interface.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/giorgiopiatti/GovSim},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Piatti et al_2024_Cooperate or Collapse.pdf}
}

@article{pingModelingDriverRisk2018,
  title = {Modeling {{Driver Risk Perception}} on {{City Roads Using Deep Learning}}},
  author = {Ping, Peng and Sheng, Yuan and Qin, Wenhu and Miyajima, Chiyomi and Takeda, Kazuya},
  year = {2018},
  journal = {IEEE Access},
  volume = {6},
  pages = {68850--68866},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2018.2879887},
  urldate = {2024-09-16},
  abstract = {Research on how risk is perceived by drivers is vital to driving behavior research and driving safety. As risk can be divided into subjective and objective risk, in this paper, we focus on modeling subjective risk perception by drivers using a deep learning method. Different drivers often perceive different levels of subjective risk under the same driving conditions. In addition, different driving conditions or driving events will have different effects on drivers. Based on these two risk perception features, in this paper, we first design an experiment on a city road with two lanes to assess the level of subjective risk perceived by drivers belonging to different groups. We then use a deep learning network-based method to abstract features of the driving environment. These environmental features are integrated with driver risk perception data and this information is used as training and testing data for the learning network. Finally, a long-short-term memory-based method is adopted to model the subjective risk perception of individual drivers based on traffic conditions and vehicle operation data from the driver's vehicle. Our results show that the proposed method can effectively model the subjective risk perception behavior of drivers, allowing for end-to-end risk perception prediction in future driving assistance systems.},
  keywords = {Accidents,deep learning,Driving behavior modeling,Hazards,risk perception,Roads,traffic safety,Urban areas,Vehicles},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ping et al_2018_Modeling Driver Risk Perception on City Roads Using Deep Learning.pdf;/Users/thomasgorman/Zotero/storage/934D6BYF/8525270.html}
}

@misc{poldrackAIassistedCodingExperiments2023,
  title = {{{AI-assisted}} Coding: {{Experiments}} with {{GPT-4}}},
  shorttitle = {{{AI-assisted}} Coding},
  author = {Poldrack, Russell A. and Lu, Thomas and Begu{\v s}, Ga{\v s}per},
  year = {2023},
  month = apr,
  number = {arXiv:2304.13187},
  eprint = {2304.13187},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-11},
  abstract = {Artificial intelligence (AI) tools based on large language models have acheived human-level performance on some computer programming tasks. We report several experiments using GPT-4 to generate computer code. These experiments demonstrate that AI code generation using the current generation of tools, while powerful, requires substantial human validation to ensure accurate performance. We also demonstrate that GPT-4 refactoring of existing code can significantly improve that code along several established metrics for code quality, and we show that GPT-4 can generate tests with substantial coverage, but that many of the tests fail when applied to the associated code. These findings suggest that while AI coding tools are very powerful, they still require humans in the loop to ensure validity and accuracy of the results.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Software Engineering},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Poldrack et al_2023_AI-assisted coding.pdf;/Users/thomasgorman/Zotero/storage/C6THFDC7/2304.html}
}

@misc{prystawskiPsychologicallyinformedChainthoughtPrompts2022,
  title = {Psychologically-Informed Chain-of-Thought Prompts for Metaphor Understanding in Large Language Models},
  author = {Prystawski, Ben and Thibodeau, Paul and Goodman, Noah},
  year = {2022},
  month = sep,
  number = {arXiv:2209.08141},
  eprint = {2209.08141},
  publisher = {arXiv},
  urldate = {2023-01-12},
  abstract = {Probabilistic models of language understanding are interpretable and structured, for instance models of metaphor understanding describe inference about latent topics and features. However, these models are manually designed for a specific task. Large language models (LLMs) can perform many tasks through in-context learning, but they lack the clear structure of probabilistic models. In this paper, we use chain-of-thought prompts to introduce structures from probabilistic models into LLMs. These prompts lead the model to infer latent variables and reason about their relationships to choose appropriate paraphrases for metaphors. The latent variables and relationships chosen are informed by theories of metaphor understanding from cognitive psychology. We apply these prompts to the two largest versions of GPT-3 and show that they can improve paraphrase selection.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/benpry/chain-of-thought-metaphor},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/prystawskiPsychologicallyinformedChainofthoughtPrompts2022-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Prystawski et al_2022_Psychologically-informed chain-of-thought prompts for metaphor understanding in.pdf;/Users/thomasgorman/Zotero/storage/79TATQFY/2209.html}
}

@inproceedings{qianTakeItLeave2024,
  title = {Take {{It}}, {{Leave It}}, or {{Fix It}}: {{Measuring Productivity}} and {{Trust}} in {{Human-AI Collaboration}}},
  shorttitle = {Take {{It}}, {{Leave It}}, or {{Fix It}}},
  booktitle = {Proceedings of the 29th {{International Conference}} on {{Intelligent User Interfaces}}},
  author = {Qian, Crystal and Wexler, James},
  year = {2024},
  month = mar,
  pages = {370--384},
  publisher = {ACM},
  address = {Greenville SC USA},
  doi = {10.1145/3640543.3645198},
  urldate = {2024-07-23},
  isbn = {9798400705083},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Qian_Wexler_2024_Take It, Leave It, or Fix It.pdf}
}

@misc{qiMutualReasoningMakes2024,
  title = {Mutual {{Reasoning Makes Smaller LLMs Stronger Problem-Solvers}}},
  author = {Qi, Zhenting and Ma, Mingyuan and Xu, Jiahang and Zhang, Li Lyna and Yang, Fan and Yang, Mao},
  year = {2024},
  month = aug,
  number = {arXiv:2408.06195},
  eprint = {2408.06195},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2408.06195},
  urldate = {2024-08-13},
  abstract = {This paper introduces rStar, a self-play mutual reasoning approach that significantly improves reasoning capabilities of small language models (SLMs) without fine-tuning or superior models. rStar decouples reasoning into a self-play mutual generation-discrimination process. First, a target SLM augments the Monte Carlo Tree Search (MCTS) with a rich set of human-like reasoning actions to construct higher quality reasoning trajectories. Next, another SLM, with capabilities similar to the target SLM, acts as a discriminator to verify each trajectory generated by the target SLM. The mutually agreed reasoning trajectories are considered mutual consistent, thus are more likely to be correct. Extensive experiments across five SLMs demonstrate rStar can effectively solve diverse reasoning problems, including GSM8K, GSM-Hard, MATH, SVAMP, and StrategyQA. Remarkably, rStar boosts GSM8K accuracy from 12.51\% to 63.91\% for LLaMA2-7B, from 36.46\% to 81.88\% for Mistral-7B, from 74.53\% to 91.13\% for LLaMA3-8B-Instruct. Code will be available at https://github.com/zhentingqi/rStar.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Qi et al_2024_Mutual Reasoning Makes Smaller LLMs Stronger Problem-Solvers.pdf;/Users/thomasgorman/Zotero/storage/QPTSDK7L/2408.html}
}

@misc{qiuPhenomenalPuzzlingTesting2024,
  title = {Phenomenal {{Yet Puzzling}}: {{Testing Inductive Reasoning Capabilities}} of {{Language Models}} with {{Hypothesis Refinement}}},
  shorttitle = {Phenomenal {{Yet Puzzling}}},
  author = {Qiu, Linlu and Jiang, Liwei and Lu, Ximing and Sclar, Melanie and Pyatkin, Valentina and Bhagavatula, Chandra and Wang, Bailin and Kim, Yoon and Choi, Yejin and Dziri, Nouha and Ren, Xiang},
  year = {2024},
  month = may,
  number = {arXiv:2310.08559},
  eprint = {2310.08559},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-11},
  abstract = {The ability to derive underlying principles from a handful of observations and then generalize to novel situations -- known as inductive reasoning -- is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through iterative hypothesis refinement, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal hypothesis proposers (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling inductive reasoners, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Qiu et al_2024_Phenomenal Yet Puzzling.pdf}
}

@article{raikovAcceleratingHumanComputer2024,
  title = {Accelerating Human--Computer Interaction through Convergent Conditions for {{LLM}} Explanation},
  author = {Raikov, Aleksandr and Giretti, Alberto and Pirani, Massimiliano and Spalazzi, Luca and Guo, Meng},
  year = {2024},
  month = may,
  journal = {Frontiers in Artificial Intelligence},
  volume = {7},
  pages = {1406773},
  issn = {2624-8212},
  doi = {10.3389/frai.2024.1406773},
  urldate = {2024-09-23},
  abstract = {The article addresses the accelerating human--machine interaction using the large language model (LLM). It goes beyond the traditional logical paradigms of explainable artificial intelligence (XAI) by considering poor-formalizable cognitive semantical interpretations of LLM. XAI is immersed in a hybrid space, where humans and machines have crucial distinctions during the digitisation of the interaction process. The author's convergent methodology ensures the conditions for making XAI purposeful and sustainable. This methodology is based on the inverse problem-solving method, cognitive modeling, genetic algorithm, neural network, causal loop dynamics, and eigenform realization. It has been shown that decision-makers need to create unique structural conditions for information processes, using LLM to accelerate the convergence of collective problem solving. The implementations have been carried out during the collective strategic planning in situational centers. The study is helpful for the advancement of explainable LLM in many branches of economy, science and technology.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Raikov et al_2024_Accelerating human–computer interaction through convergent conditions for LLM.pdf}
}

@misc{rajArtificialIntelligenceEffect2023,
  title = {Art-Ificial {{Intelligence}}: {{The Effect}} of {{AI Disclosure}} on {{Evaluations}} of {{Creative Content}}},
  shorttitle = {Art-Ificial {{Intelligence}}},
  author = {Raj, Manav and Berg, Justin and Seamans, Rob},
  year = {2023},
  month = mar,
  number = {arXiv:2303.06217},
  eprint = {2303.06217},
  primaryclass = {cs, econ, q-fin},
  publisher = {arXiv},
  urldate = {2023-03-16},
  abstract = {The emergence of generative AI technologies, such as OpenAI's ChatGPT chatbot, has expanded the scope of tasks that AI tools can accomplish and enabled AI-generated creative content. In this study, we explore how disclosure regarding the use of AI in the creation of creative content affects human evaluation of such content. In a series of pre-registered experimental studies, we show that AI disclosure has no meaningful effect on evaluation either for creative or descriptive short stories, but that AI disclosure has a negative effect on evaluations for emotionally evocative poems written in the first person. We interpret this result to suggest that reactions to AI-generated content may be negative when the content is viewed as distinctly "human." We discuss the implications of this work and outline planned pathways of research to better understand whether and when AI disclosure may affect the evaluation of creative content.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computers and Society,Economics - General Economics},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/rajArtificialIntelligenceEffect2023-zotero.md;/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Raj et al_2023_Art-ificial Intelligence.pdf;/Users/thomasgorman/Zotero/storage/GBQ7UQM9/2303.html}
}

@misc{rajendranLearningInterpretableConcepts2024,
  title = {Learning {{Interpretable Concepts}}: {{Unifying Causal Representation Learning}} and {{Foundation Models}}},
  shorttitle = {Learning {{Interpretable Concepts}}},
  author = {Rajendran, Goutham and Buchholz, Simon and Aragam, Bryon and Sch{\"o}lkopf, Bernhard and Ravikumar, Pradeep},
  year = {2024},
  month = feb,
  number = {arXiv:2402.09236},
  eprint = {2402.09236},
  primaryclass = {cs, math, stat},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {To build intelligent machine learning systems, there are two broad approaches. One approach is to build inherently interpretable models, as endeavored by the growing field of causal representation learning. The other approach is to build highly-performant foundation models and then invest efforts into understanding how they work. In this work, we relate these two approaches and study how to learn human-interpretable concepts from data. Weaving together ideas from both fields, we formally define a notion of concepts and show that they can be provably recovered from diverse data. Experiments on synthetic data and large language models show the utility of our unified approach.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Mathematics - Statistics Theory,Statistics - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Rajendran et al_2024_Learning Interpretable Concepts.pdf}
}

@article{rakittaCognitiveBiasesBuilding2021,
  title = {Cognitive {{Biases}} in {{Building Energy Decisions}}},
  author = {Rakitta, Maic and Wernery, Jannis},
  year = {2021},
  month = jan,
  journal = {Sustainability},
  volume = {13},
  number = {17},
  pages = {9960},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2071-1050},
  doi = {10.3390/su13179960},
  urldate = {2024-07-02},
  abstract = {Research on sustainability in the building sector currently focuses mainly on technical solutions while little attention is given to how behaviour influences the uptake of these solutions. Bounded rationality may have a significant impact on the effective implementation of more sustainable technologies that are already available. However, empirical evidence on the effects of bounded rationality in the building sector, such as cognitive biases, is still lacking. Here, we present an empirical investigation of four cognitive biases in the building environment, namely the framing, anchor, default, and decoy effect. For that, energy-related decisions situations were presented to approximately 270 participants in an online survey. Our results show that awareness of greenhouse gas emissions from buildings can be raised through framing that the willingness to pay more for an energy-efficient home can be increased by presenting it as default, and that the choices can be shifted towards more energy-efficient appliances by using a decoy. The hypothesis that anchoring increases the willingness to pay more for the installation of a solar system could not be supported. These findings decrease the lack of empirical data on cognitive biases in the context of buildings and further indicate the potential of choice architecture in the building environment. The influence of cognitive biases in energy-related decisions should be used to increase the adaptation of sustainable technologies.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {anchor effect,bounded rationality,building energy,cognitive biases,decision behaviour,decoy effect,default effect,energy-efficiency,framing effect,sustainability},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Rakitta_Wernery_2021_Cognitive Biases in Building Energy Decisions.pdf}
}

@misc{raneConceptAlignment2024,
  title = {Concept {{Alignment}}},
  author = {Rane, Sunayana and Bruna, Polyphony J. and Sucholutsky, Ilia and Kello, Christopher and Griffiths, Thomas L.},
  year = {2024},
  month = jan,
  number = {arXiv:2401.08672},
  eprint = {2401.08672},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-07-12},
  abstract = {Discussion of AI alignment (alignment between humans and AI systems) has focused on value alignment, broadly referring to creating AI systems that share human values. We argue that before we can even attempt to align values, it is imperative that AI systems and humans align the concepts they use to understand the world. We integrate ideas from philosophy, cognitive science, and deep learning to explain the need for concept alignment, not just value alignment, between humans and machines. We summarize existing accounts of how humans and machines currently learn concepts, and we outline opportunities and challenges in the path towards shared concepts. Finally, we explain how we can leverage the tools already being developed in cognitive science and AI research to accelerate progress towards concept alignment.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Rane et al_2024_Concept Alignment.pdf}
}

@misc{raneConceptAlignmentPrerequisite2023,
  title = {Concept {{Alignment}} as a {{Prerequisite}} for {{Value Alignment}}},
  author = {Rane, Sunayana and Ho, Mark and Sucholutsky, Ilia and Griffiths, Thomas L.},
  year = {2023},
  month = oct,
  number = {arXiv:2310.20059},
  eprint = {2310.20059},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {Value alignment is essential for building AI systems that can safely and reliably interact with people. However, what a person values -- and is even capable of valuing -- depends on the concepts that they are currently using to understand and evaluate what happens in the world. The dependence of values on concepts means that concept alignment is a prerequisite for value alignment -- agents need to align their representation of a situation with that of humans in order to successfully align their values. Here, we formally analyze the concept alignment problem in the inverse reinforcement learning setting, show how neglecting concept alignment can lead to systematic value mis-alignment, and describe an approach that helps minimize such failure modes by jointly reasoning about a person's concepts and values. Additionally, we report experimental results with human participants showing that humans reason about the concepts used by an agent when acting intentionally, in line with our joint reasoning model.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Rane et al_2023_Concept Alignment as a Prerequisite for Value Alignment.pdf;/Users/thomasgorman/Zotero/storage/IYG4G9GC/2310.html}
}

@misc{rasalOptimalDecisionMaking2024,
  title = {Optimal {{Decision Making Through Scenario Simulations Using Large Language Models}}},
  author = {Rasal, Sumedh and Hauer, E. J.},
  year = {2024},
  month = jul,
  number = {arXiv:2407.06486},
  eprint = {2407.06486},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-15},
  abstract = {The rapid evolution of Large Language Models (LLMs) has markedly expanded their application across diverse domains, transforming how complex problems are approached and solved. Initially conceived to predict subsequent words in texts, these models have transcended their original design to comprehend and respond to the underlying contexts of queries. Today, LLMs routinely perform tasks that once seemed formidable, such as writing essays, poems, stories, and even developing software code. As their capabilities continue to grow, so too do the expectations of their performance in even more sophisticated domains.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Rasal_Hauer_2024_Optimal Decision Making Through Scenario Simulations Using Large Language Models.pdf}
}

@article{rashidiChatGPTConundrumHumangenerated2023,
  title = {The {{ChatGPT}} Conundrum: {{Human-generated}} Scientific Manuscripts Misidentified as {{AI}} Creations by {{AI}} Text Detection Tool},
  shorttitle = {The {{ChatGPT}} Conundrum},
  author = {Rashidi, Hooman H. and Fennell, Brandon D. and Albahra, Samer and Hu, Bo and Gorbett, Tom},
  year = {2023},
  month = jan,
  journal = {Journal of Pathology Informatics},
  volume = {14},
  pages = {100342},
  issn = {2153-3539},
  doi = {10.1016/j.jpi.2023.100342},
  urldate = {2024-01-29},
  abstract = {AI Chat Bots such as ChatGPT are revolutionizing our AI capabilities, especially in text generation, to help expedite many tasks, but they introduce new dilemmas. The detection of AI-generated text has become a subject of great debate considering the AI text detector's known and unexpected limitations. Thus far, much research in this area has focused on the detection of AI-generated text; however, the goal of this study was to evaluate the opposite scenario, an AI-text detection tool's ability to discriminate human-generated text. Thousands of abstracts from several of the most well-known scientific journals were used to test the predictive capabilities of these detection tools, assessing abstracts from 1980 to 2023. We found that the AI text detector erroneously identified up to 8\% of the known real abstracts as AI-generated text. This further highlights the current limitations of such detection tools and argues for novel detectors or combined approaches that can address this shortcoming and minimize its unanticipated consequences as we navigate this new AI landscape.},
  keywords = {Chat-GPT,Generative AI,GPT,Human-generated,Large Language Model,LLM,Machine learning,Text detection},
  file = {/Users/thomasgorman/Zotero/storage/28A7955K/S2153353923001566.html}
}

@article{rastogiDecidingFastSlow2022,
  title = {Deciding {{Fast}} and {{Slow}}: {{The Role}} of {{Cognitive Biases}} in {{AI-assisted Decision-making}}},
  shorttitle = {Deciding {{Fast}} and {{Slow}}},
  author = {Rastogi, Charvi and Zhang, Yunfeng and Wei, Dennis and Varshney, Kush R. and Dhurandhar, Amit and Tomsett, Richard},
  year = {2022},
  month = mar,
  journal = {Proceedings of the ACM on Human-Computer Interaction},
  volume = {6},
  number = {CSCW1},
  pages = {1--22},
  issn = {2573-0142},
  doi = {10.1145/3512930},
  urldate = {2024-07-02},
  abstract = {Several strands of research have aimed to bridge the gap between artificial intelligence (AI) and human decision-makers in AI-assisted decision-making, where humans are the consumers of AI model predictions and the ultimate decision-makers in high-stakes applications. However, people's perception and understanding are often distorted by their cognitive biases, such as confirmation bias, anchoring bias, availability bias, to name a few. In this work, we use knowledge from the field of cognitive science to account for cognitive biases in the human-AI collaborative decision-making setting, and mitigate their negative effects on collaborative performance. To this end, we mathematically model cognitive biases and provide a general framework through which researchers and practitioners can understand the interplay between cognitive biases and human-AI accuracy. We then focus specifically on anchoring bias, a bias commonly encountered in human-AI collaboration. We implement a time-based de-anchoring strategy and conduct our first user experiment that validates its effectiveness in human-AI collaborative decision-making. With this result, we design a time allocation strategy for a resource-constrained setting that achieves optimal human-AI collaboration under some assumptions. We, then, conduct a second user experiment which shows that our time allocation strategy with explanation can effectively de-anchor the human and improve collaborative performance when the AI model has low confidence and is incorrect.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Rastogi et al_2022_Deciding Fast and Slow.pdf}
}

@misc{rathjeGPTEffectiveTool2023,
  title = {{{GPT}} Is an Effective Tool for Multilingual Psychological Text Analysis},
  author = {Rathje, Steve and Mirea, Dan-Mircea and Sucholutsky, Ilia and Marjieh, Raja and Robertson, Claire and Bavel, Jay J. Van},
  year = {2023},
  month = may,
  publisher = {PsyArXiv},
  doi = {10.31234/osf.io/sekf5},
  urldate = {2023-07-24},
  abstract = {The social and behavioral sciences have been increasingly using automated text analysis to measure psychological constructs in text. We explore whether GPT, the large-language model underlying the artificial intelligence chatbot ChatGPT, can be used as a tool for automated psychological text analysis in various languages. Across 15 datasets (n = 31,789 manually annotated tweets and news headlines), we tested whether GPT-3.5 and GPT-4 can accurately detect psychological constructs (sentiment, discrete emotions, and offensiveness) across 12 languages (English, Arabic, Indonesian, and Turkish, as well as eight African languages including Swahili, Amharic, Yoruba and Kinyarwanda). We found that GPT performs much better than English-language dictionary-based text analysis (r = 0.66-0.75 for correlations between manual annotations and GPT-4, as opposed to r = 0.20-0.30 for correlations between manual annotations and dictionary methods). Further, GPT performs nearly as well as or better than several fine-tuned machine learning models, though GPT had poorer performance in African languages and in comparison to more recent fine-tuned models. Overall, GPT may be superior to many existing methods of automated text analysis, since it achieves relatively high accuracy across many languages, requires no training data, and is easy to use with simple prompts (e.g., ``is this text negative?'') and little coding experience. We provide sample code for analyzing text with the GPT application programming interface. GPT and other large-language models may be the future of psychological text analysis, and may help facilitate more cross-linguistic research with understudied languages.},
  langid = {american},
  keywords = {Artificial Intelligence,Emotion,GPT,Large Language Models,Machine Learning,Social and Behavioral Sciences,Social and Personality Psychology,Text Analysis},
  annotation = {https://osf.io/6pnb2/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Rathje et al_2023_GPT is an effective tool for multilingual psychological text analysis.pdf}
}

@misc{raventosPretrainingTaskDiversity2023,
  title = {Pretraining Task Diversity and the Emergence of Non-{{Bayesian}} in-Context Learning for Regression},
  author = {Ravent{\'o}s, Allan and Paul, Mansheej and Chen, Feng and Ganguli, Surya},
  year = {2023},
  month = jun,
  number = {arXiv:2306.15063},
  eprint = {2306.15063},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-08-19},
  abstract = {Pretrained transformers exhibit the remarkable ability of in-context learning (ICL): they can learn tasks from just a few examples provided in the prompt without updating any weights. This raises a foundational question: can ICL solve fundamentally \${\textbackslash}textit\{new\}\$ tasks that are very different from those seen during pretraining? To probe this question, we examine ICL's performance on linear regression while varying the diversity of tasks in the pretraining dataset. We empirically demonstrate a \${\textbackslash}textit\{task diversity threshold\}\$ for the emergence of ICL. Below this threshold, the pretrained transformer cannot solve unseen regression tasks as it behaves like a Bayesian estimator with the \${\textbackslash}textit\{non-diverse pretraining task distribution\}\$ as the prior. Beyond this threshold, the transformer significantly outperforms this estimator; its behavior aligns with that of ridge regression, corresponding to a Gaussian prior over \${\textbackslash}textit\{all tasks\}\$, including those not seen during pretraining. These results highlight that, when pretrained on data with task diversity greater than the threshold, transformers \${\textbackslash}textit\{can\}\$ solve fundamentally new tasks in-context. Importantly, this capability hinges on it deviating from the Bayes optimal estimator with the pretraining distribution as the prior. This study underscores, in a concrete example, the critical role of task diversity, alongside data and model scale, in the emergence of ICL. Code is available at https://github.com/mansheej/icl-task-diversity.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Raventós et al_2023_Pretraining task diversity and the emergence of non-Bayesian in-context.pdf;/Users/thomasgorman/Zotero/storage/JW95DUFS/2306.html}
}

@article{recarteMentalWorkloadDriving2003,
  title = {Mental Workload While Driving: {{Effects}} on Visual Search, Discrimination, and Decision Making.},
  shorttitle = {Mental Workload While Driving},
  author = {Recarte, Miguel A. and Nunes, Luis M.},
  year = {2003},
  journal = {Journal of Experimental Psychology: Applied},
  volume = {9},
  number = {2},
  pages = {119--137},
  issn = {1939-2192, 1076-898X},
  doi = {10.1037/1076-898X.9.2.119},
  urldate = {2024-09-16},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Recarte_Nunes_2003_Mental workload while driving.pdf}
}

@article{reimerAttributionsPoorGroup2001,
  title = {Attributions for {{Poor Group Performance}} as a {{Predictor}} of {{Perspective-Taking}} and {{Subsequent Group Achievement}}: {{A Process Model}}},
  shorttitle = {Attributions for {{Poor Group Performance}} as a {{Predictor}} of {{Perspective-Taking}} and {{Subsequent Group Achievement}}},
  author = {Reimer, Torsten},
  year = {2001},
  month = jan,
  journal = {Group Processes \& Intergroup Relations},
  volume = {4},
  number = {1},
  pages = {31--47},
  publisher = {SAGE Publications Ltd},
  issn = {1368-4302},
  doi = {10.1177/1368430201041003},
  urldate = {2024-07-03},
  abstract = {Several studies on group problem solving have shown that perspective-taking may affect group performance. In this paper, a model is outlined in which the effect of performance attributions on group achievement is assumed to be mediated by the quality of group members' partner spaces (i.e. by their ability to recognize their partners' perspective on a problem). According to the model, the extent to which individuals try to take their partner's perspective is a function of their attributions for poor performance: the more group members attribute negative feedback on performance or disagreement about the correct problem solution to themselves relative to their partners, the more they will try to generate an adequate partner space. Additionally, it is assumed that the quality of the partner spaces has a direct impact on group achievement. These basic assumptions were confirmed by a study in which participants were asked to solve Tower of Hanoi problems in dyads, although attributions themselves did not directly predict achievement.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer_2001_Attributions for Poor Group Performance as a Predictor of Perspective-Taking.pdf}
}

@article{reimerDecisionMakingGroupsAttenuate2010,
  title = {Decision-{{Making Groups Attenuate}} the {{Discussion Bias}} in {{Favor}} of {{Shared Information}}: {{A Meta-Analysis}}.},
  shorttitle = {Decision-{{Making Groups Attenuate}} the {{Discussion Bias}} in {{Favor}} of {{Shared Information}}},
  author = {Reimer, Torsten and Reimer, Andrea and Czienskowski, Uwe},
  year = {2010},
  month = mar,
  journal = {Communication Monographs},
  volume = {77},
  number = {1},
  pages = {121--142},
  publisher = {Taylor \& Francis Ltd},
  issn = {0363-7751},
  doi = {10.1080/03637750903514318},
  urldate = {2024-06-22},
  abstract = {Groups often focus their discussions on information that all members know at the outset. To test how robust the sampling advantage for shared information is, a meta-analysis was conducted. The analysis integrated findings from 20 publications (45 independent effects), in which information sharedness was manipulated. Groups discussed more shared than unshared information overall. However, the observed sampling advantage was smaller than expected. Groups attenuated the discussion bias in particular when they had to choose among a small number of decision alternatives and when they had less than 30 minutes discussion time. Moreover, groups performing a hidden-profile task tended to display a smaller discussion bias than groups performing tasks with equally attractive alternatives.},
  langid = {english},
  keywords = {Communications research,Discussion,Group decision making,Information sharing,Interpersonal communication,Meta-analysis},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_2010_Decision-Making Groups Attenuate the Discussion Bias in Favor of Shared.pdf}
}

@article{reimerEffectsInformationEnvironment2007,
  title = {Effects of the {{Information Environment}} on {{Group Discussions}} and {{Decisions}} in the {{Hidden-Profile Paradigm}}},
  author = {Reimer, Torsten and Kuendig, Sascha and Hoffrage, Ulrich and Park, Ernest and Hinsz, Verlin},
  year = {2007},
  month = mar,
  journal = {Communication Monographs},
  volume = {74},
  number = {1},
  pages = {1--28},
  issn = {0363-7751, 1479-5787},
  doi = {10.1080/03637750701209947},
  urldate = {2024-07-03},
  abstract = {Research on the Information Sampling Model (ISM) revealed that information items that are known to all group members at the outset (shared information) are more likely to be mentioned during discussion than information items that are only known to individual members (unshared information) (Stasser \& Titus, 1985; Wittenbaum, Hollingshead, \& Botero, 2004). In prior studies involving the ISM, groups typically functioned in a very specific information environment: All information items were provided in form of unique cues, which described only one of the choice alternatives among which the groups had to choose. Because this specific information environment may impact group discussions and decisions, we included an experimental condition incorporating common cues. In contrast to unique cues, common cues provide information on each and every choice option. As expected, groups in the commoncue condition showed a weaker sampling advantage for shared information, and chose the hidden-profile alternative more often than groups in the classic unique-cue condition.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_2007_Effects of the Information Environment on Group Discussions and Decisions in.pdf}
}

@article{reimerImpactAgeCognitive2013,
  title = {Impact of Age and Cognitive Demand on Lane Choice and Changing under Actual Highway Conditions},
  author = {Reimer, Bryan and Donmez, Birsen and Lavalli{\`e}re, Martin and Mehler, Bruce and Coughlin, Joseph F. and Teasdale, Normand},
  year = {2013},
  month = mar,
  journal = {Accident Analysis \& Prevention},
  volume = {52},
  pages = {125--132},
  issn = {0001-4575},
  doi = {10.1016/j.aap.2012.12.008},
  urldate = {2024-09-16},
  abstract = {Previous research suggests that drivers change lanes less frequently during periods of heightened cognitive load. However, lane changing behavior of different age groups under varying levels of cognitive demand is not well understood. The majority of studies which have evaluated lane changing behavior under cognitive workload have been conducted in driving simulators. Consequently, it is unclear if the patterns observed in these simulation studies carry over to actual driving. This paper evaluates data from an on-road study to determine the effects of age and cognitive demand on lane choice and lane changing behavior. Three age groups (20--29, 40--49, and 60--69) were monitored in an instrumented vehicle. The 40's age group had 147\% higher odds of exhibiting a lane change than the 60's group. In addition, drivers in their 60's were less likely to drive on the leftmost lane compared to drivers in their 20's and 40's. These results could be interpreted as evidence that older adults adopt a more conservative driving style as reflected in being less likely to choose the leftmost lane than the younger groups and less likely to change lanes than drivers in their 40's. Regardless of demand level, cognitive workload reduced the frequency of lane changes for all age groups. This suggests that in general drivers of all ages attempt to regulate their behavior in a risk reducing direction when under added cognitive demand. The extent to which such self-regulation fully compensates for the impact of added cognitive demand remains an open question.},
  keywords = {Aging,Cognitive workload,Distraction,Lane change behavior,Lane choice,Passing},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_2013_Impact of age and cognitive demand on lane choice and changing under actual.pdf;/Users/thomasgorman/Zotero/storage/LQ4G6M6G/S0001457512004307.html}
}

@article{reimerInformationAggregationGroups,
  title = {Information {{Aggregation}} in {{Groups}}: {{The Approach}} of {{Simple Group Heuristics}} ({{SIGH}})},
  author = {Reimer, Torsten and Hoffrage, Ulrich},
  abstract = {A new framework is introduced that models group decision making by using simple group heuristics (SIGH). We report results of a set of simulations that systematically varied (a) the group members' strategies (compensatory unit weight model, UWM, and a noncompensatory lexicographic heuristic, LEX), (b) the distribution of cue validities (J-shaped vs. linear), and (c) the quantity and quality of shared information. Individual decisions were aggregated by using a majority decision rule (proportionality in case of ties). (1) The simulations revealed strong effects of the distribution of cue validities on group performance. When validities were linearly distributed, UWM gained an 8\% better accuracy than LEX by considering all cues. Yet, if cue validities followed a J-shaped distribution, the much more frugal LEX surpassed the UWM by achieving a 16\% higher accuracy. (2) This effect was robust across different quantities of shared information. (3) Systematic allocation of information in favour of valid or invalid cues showed that the performance of UWM mainly depended on mean validity, whereas the performance of LEX was more strongly affected by the degree to which the most valid cues were shared.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer_Hoffrage_Information Aggregation in Groups.pdf}
}

@article{reimerInterplayHeuristicSystematic,
  title = {On the {{Interplay}} between {{Heuristic}} and {{Systematic Processes}} in {{Persuasion}}},
  author = {Reimer, Torsten and Mata, Rui and Katsikopoulos, Konstantinos and Opwis, Klaus},
  abstract = {Dual-process models of persuasion (e.g., Heuristic Systematic Model) contrast the use of heuristics with systematic information processing. However, a great deal of attention is increasingly being devoted to the interplay between the two types of processing. We propose a multistage view that builds on dual-process models of persuasion but emphasizes the interplay between processing modes. According to this multistage view, there are contexts in which receivers first use systematic processes to derive information about expertise from argument quality and, subsequently, make use of the expertise heuristic to arrive at an attitude. We show that results of a classic study (Petty, Cacioppo, \& Goldman, 1981) are compatible with this view. Additionally, we report results of a study, in which the effect of argument quality on receivers' attitudes was partially mediated by perceived source expertise (Reimer, 2003). Two follow-up studies revealed that this mediation tended to be stronger among receivers reporting low self-expertise than among receivers reporting high selfexpertise.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_On the Interplay between Heuristic and Systematic Processes in Persuasion.pdf}
}

@article{reimerNaiveGroupsCan2010,
  title = {Na{\"i}ve {{Groups Can Solve}} the {{Hidden-Profile Problem}}},
  author = {Reimer, Torsten and Reimer, Andrea and Hinsz, Verlin B.},
  year = {2010},
  month = jun,
  journal = {Human Communication Research},
  volume = {36},
  number = {3},
  pages = {443--467},
  issn = {03603989, 14682958},
  doi = {10.1111/j.1468-2958.2010.01383.x},
  urldate = {2024-06-22},
  copyright = {http://doi.wiley.com/10.1002/tdm\_license\_1.1},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_2010_Naïve Groups Can Solve the Hidden-Profile Problem.pdf}
}

@incollection{reimerNumericCommunicationRisk2014,
  title = {Numeric {{Communication}} of {{Risk}}},
  author = {Reimer, Torsten and Jones, Christina and Skubisz, Christine},
  year = {2014},
  month = jan,
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_2014_Numeric Communication of Risk.pdf}
}

@incollection{reimerPositiveNegativeTransfer2004,
  title = {Positive and {{Negative Transfer Effects}} in {{Groups}}},
  booktitle = {The {{Routines}} of {{Decision Making}}},
  author = {Reimer, Torsten and Bornstein, Anne-Louise and Opwis, Klaus},
  year = {2004},
  publisher = {Psychology Press},
  abstract = {In the literature on group decision making and problem solving, it is often mentioned but rarely tested that groups may benefit from developing routines (Gersick \& Hackman, 1990). In particular, routines may help groups effectively reduce coordination requirements when solving interdependent tasks. A group that has developed a routine for solving a problem does not have to negotiate about each new task and about which group member will carry out which single step of the task (Malone \& Crowstone, 1994). However, as we know from the literature on learning transfer (VanLehn, 1996) and routine decision making (Betsch, Haberstroh, \& Hoehle, 2002), acquiring a routine sometimes impairs rather than improves performance. Such a negative learning transfer is most likely if problem solvers are provided with a novel task that shares surface features with the learning task but requires a different problem solution (VanLehn, 1989). If the acquisition of a routine leads to improved group performance, crucial to continued improvement is whether groups are able to detect changes in the task environment and adequately respond to novel task demands. Are groups more likely than individuals to detect such changes and adapt their routine to novel tasks?},
  isbn = {978-1-4106-1182-6}
}

@incollection{reimerProbabilisticPersuasionBrunswikian2012,
  title = {Probabilistic {{Persuasion}}: {{A Brunswikian Theory}} of {{Argumentation}}},
  shorttitle = {Probabilistic {{Persuasion}}},
  booktitle = {Simple {{Heuristics}} in a {{Social World}}},
  author = {Reimer, Torsten and Hertwig, Ralph and Sipek, Sanja},
  editor = {Hertwig, Ralph and Hoffrage, Ulrich and Research Group, Abc},
  year = {2012},
  month = nov,
  edition = {1},
  pages = {103--134},
  publisher = {Oxford University Press},
  doi = {10.1093/acprof:oso/9780195388435.003.0004},
  urldate = {2024-06-22},
  abstract = {Abstract             The Brunswikian lens model has been widely used to describe how individuals integrate information when making a decision (Brunswik, 1943; Dhami, Hertwig, \& Hoffrage, 2004). The chapter applies and extends the lens model to a persuasion context. Specifically, the chapter introduces the probabilistic persuasion theory (PPT) as a framework within which the quality of arguments can be defined and measured, and the cognitive processes involved in the selection and in the reception of arguments can be modeled. Construing persuasion within the framework of PPT has the surplus value of opening the door to a rich literature on information processing models in judgment and decision making. The chapter outlines basic assumptions of the new theory, exemplify its application, and discuss its heuristic value. The chapter begins by briefly reviewing dual-process models of persuasion and how they account for the impact of arguments on attitudes. Second, the chapter critically discusses the theories' implications for human rationality, particularly their equation of heuristic processing with irrationality. Third, the chapter describes basic tenets of PPT as an alternative account of persuasion that is based on a Brunswikian framework (Hammond \& Stewart, 2001). PPT asserts that persuasion can be construed as a decision-making process, in which a communicator provides information with the goal to influence a receiver's judgments and decisions. The chapter demonstrates how PPT can be used to specify these influence processes and to study the cognitive processes involved in the selection and reception of arguments. Forth, the chapter derives five testable predictions of the new theory and describe preliminary experimental evidence in support of this account.},
  isbn = {978-0-19-538843-5 978-0-19-995008-9},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_2012_Probabilistic Persuasion.pdf}
}

@article{reimerPublicSupportCounterterrorism2023,
  title = {Public Support for Counterterrorism Efforts Using Probabilistic Computing Technologies to Decipher Terrorist Communication on the Internet},
  author = {Reimer, Torsten and Johnson, Nathanael},
  year = {2023},
  month = jul,
  journal = {Current Psychology},
  volume = {42},
  number = {20},
  pages = {16908--16922},
  issn = {1936-4733},
  doi = {10.1007/s12144-022-02753-4},
  urldate = {2024-07-02},
  abstract = {Advancements in big data analytics offer new avenues for the analysis and deciphering of suspicious activities on the internet. One promising new technology to increase the identification of terrorism threats is based on probabilistic computing. The technology promises to provide more efficient problem solutions in encryption and cybersecurity. Probabilistic computing technologies use large amounts of data, though,~which raises potential privacy concerns. A study (N\,=\,1,023) was conducted to survey public support for using probabilistic computing technologies to increase counterterrorism efforts. Overall, strong support was found for the use of publicly available personal information (e.g., personal websites). Regarding private personal information (e.g., online conversations), respondents perceived it to be more appropriate to use information from out-group members (non-American citizens) than from in-group members (American citizens). In line with a social-identity account, this form of in-group favoritism was strongest among respondents displaying a combination of strong national identities and strong privacy concerns.},
  langid = {english},
  keywords = {Counterterrorism,Ingroup favoritism,Privacy concerns,Social identity theory},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer_Johnson_2023_Public support for counterterrorism efforts using probabilistic computing.pdf}
}

@inproceedings{reimerRoutineProblemSolving2002,
  title = {Routine {{Problem Solving}} in {{Groups}}},
  booktitle = {Proceedings of the {{Annual Meeting}} of the {{Cognitive Science Society}}},
  author = {Reimer, Torsten and Opwis, Klaus and Bornstein, Anne-Louise},
  year = {2002},
  volume = {24},
  pages = {780--785},
  doi = {10.4324/9781315782379-169},
  urldate = {2024-06-22},
  abstract = {Routines may help groups to effectively reduce coordination requirements when solving interdependent tasks. However, routine problem solving always involves the risk of a negative transfer, which appears if a routine is applied to novel problems even though it is inappropriate. In this experiment, negative transfer was produced by first teaching individuals a procedure for solving the Tower of Hanoi problem. Next, participants were asked to solve several transfer tasks either individually or in pairs. However, the routine could not be applied directly to the transfer tasks but led to a long detour. As expected, the individuals surpassed the dyads, who insisted more strongly on their routine. This result fits with studies that corroborate the claim that groups are prone to a ``principle of inertia'' when solving problems or making decisions.},
  isbn = {978-1-315-78237-9},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_2019_Routine Problem Solving in Groups.pdf}
}

@article{reimerSharedCoordinatedCognition2006,
  title = {Shared and Coordinated Cognition in Competitive and Dynamic Task Environments: {{An}} Information-processing Perspective for Team Sports},
  shorttitle = {Shared and Coordinated Cognition in Competitive and Dynamic Task Environments},
  author = {Reimer, Torsten and Park, Ernest and Hinsz, Verlin},
  year = {2006},
  month = feb,
  journal = {International Journal of Sport and Exercise Psychology},
  volume = {4},
  doi = {10.1080/1612197X.2006.9671804},
  abstract = {From a groups-as-information-processors perspective, the notion of shared cognition is crucial to the understanding of team performance. This approach is used to comprehend the effectiveness of sports teams. Typically, sports teams are placed in a dynamic environment in which tasks are highly interdependent. Individual actions have to be coordinated with regard to the team objectives and with regard to the opponent team's actions. Although sports are considered behavioral tasks by their nature, performance may be strongly affected by cognitive processes. We review studies and give examples that demonstrate that the degree to which cognitions are shared and coordinated among the members of such teams influences the extent to which the individual actions are coordinated. The sharing and coordinating of cognitions pertain to the attention, information-processing, and action phases of the decision making and behavior process. We also discuss how feedback and coaching can affect information processing in teams such that coordination of actions is more likely to arise},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_2006_Shared and coordinated cognition in competitive and dynamic task environments.pdf}
}

@article{reimerThermostatAnchorsTemperature2024,
  title = {Thermostat {{Anchors}}: {{Do Temperature Scale Characteristics Affect}} the {{Selection}} of {{Temperature Setpoints}} for {{Residential Homes}}?},
  shorttitle = {Thermostat {{Anchors}}},
  author = {Reimer, Torsten and Oh, Jeonghyun and {Loaiza-Ram{\'i}rez}, Juan Pablo and Barber, Hayden},
  year = {2024},
  month = jan,
  journal = {Sustainability},
  volume = {16},
  number = {6},
  pages = {2540},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2071-1050},
  doi = {10.3390/su16062540},
  urldate = {2024-07-02},
  abstract = {Characteristics of scales, such as the labels that are used on scales, have been shown to affect judgments. The scale-dependency hypothesis predicts specific effects of the properties of a temperature scale on residents' choices of temperature setpoints. Based on the literature on anchoring in judgment and decision making, we assessed the effects of the displayed current temperature, midpoint, range, and increment of temperature scales on the selection of setpoint temperatures for residential homes. Participants (N = 384) were asked to imagine that they work as a manager of a residential apartment complex and to select, in this function, setpoint temperatures for incoming residents. The experiment revealed independent effects of the current temperature as well as the midpoint and range of the used scale on the selected setpoints. The scale increment did not systematically affect the chosen temperatures.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {anchoring and adjustment,information representation,sustainable behavior,temperature scales,temperature setpoints},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_2024_Thermostat Anchors.pdf}
}

@article{reimerUseHeuristicsPersuasion2004,
  title = {The Use of Heuristics in Persuasion: {{Deriving}} Cues on Source Expertise from Argument {{Quality}}},
  author = {Reimer, Torsten and Mata, Rui and Stoecklin, Markus},
  year = {2004},
  journal = {Current Research in Social Psychology,},
  volume = {10},
  number = {6},
  abstract = {Dual-process models of persuasion contrast the expertise heuristic "experts' statements can be trusted" with systematic processing of message content. Studies in which source expertise and argument quality were simultaneously manipulated revealed that the expertise manipulation affects attitudes when receivers are not highly motivated to scrutinize the provided message. In contrast, when receivers are highly motivated and are able to scrutinize a message their attitude is usually affected by argument quality but is independent of the expertise cue. We argue that this does not rule out that receivers still make use of the expertise heuristic. Rather, they may consider argument quality to infer the expertise of the source. We show that a classic study (Petty, Cacioppo, \& Goldman, 1981) may be interpreted by this alternative explanation and present a study, in which the effect of argument quality on receivers' attitudes was partially mediated by perceived source expertise. This mediation tended to be stronger among receivers reporting low self-expertise than among receivers reporting high self-expertise.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_The use of heuristics in persuasion.pdf}
}

@article{reimerUseRecognitionGroup2004,
  title = {The Use of Recognition in Group Decision-Making},
  author = {Reimer, Torsten and Katsikopoulos, Konstantinos V.},
  year = {2004},
  journal = {Cognitive Science},
  volume = {28},
  number = {6},
  pages = {1009--1029},
  issn = {1551-6709},
  doi = {10.1207/s15516709cog2806_6},
  urldate = {2024-06-22},
  abstract = {Goldstein and Gigerenzer (2002) [Models of ecological rationality: The recognition heuristic. Psychological Review, 109 (1), 75--90] found evidence for the use of the recognition heuristic. For example, if an individual recognizes only one of two cities, they tend to infer that the recognized city has a larger population. A prediction that follows is that of the less-is-more effect: Recognizing fewer cities leads, under certain conditions, to more accurate inferences than recognizing more cities. We extend the recognition heuristic to group decision-making by developing majority and lexicographic models of how recognition information is used by groups. We formally show when the less-is-more effect is predicted in groups and we present a study where three-member groups performed the population comparison task. Several aspects of our data indicate that members who can use the recognition heuristic are, not in all but in most cases, more influential in the group decision process than members who cannot use the heuristic. We also observed the less-is-more effect and found that models assuming that members who can use the recognition heuristic are more influential better predict when the effect occurs.},
  langid = {english},
  keywords = {Less-is-more effect,Lexicographic model,Majority rule,Recognition heuristic},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer_Katsikopoulos_2004_The use of recognition in group decision-making.pdf;/Users/thomasgorman/Zotero/storage/27JY8TWD/s15516709cog2806_6.html}
}

@book{reimerWhatMethodologiesAre2020,
  title = {What {{Methodologies}} Are {{Needed}} to {{Study Group Communication}}? {{A Bounded-Rationality Perspective}}},
  shorttitle = {What {{Methodologies}} Are {{Needed}} to {{Study Group Communication}}?},
  author = {Reimer, Torsten and Dolick, Kirstin and Barber, Hayden and Oh, Jeonghyun},
  year = {2020},
  month = dec,
  abstract = {A proposal in favor of a meta-theoretical approach to the study of group communication is advanced, that has not received much attention in group communication scholarship: The study of the bounded rationality of groups and teams. The notion of bounded rationality comes with an invitation to analyze group communication from the vantage point of an adaptation process that involves the communication processes that are employed by groups along with characteristics of the environments in which groups are situated. The general concept of bounded rationality is introduced and some promises that this meta-theoretical lens offers to group communication scholarship are described. Three methodological signature characteristics are highlighted: The development and test of process models, the analysis and description of the ecological and social environments of groups, and the development of representative designs in the study of groups.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Reimer et al_2020_What Methodologies are Needed to Study Group Communication.pdf}
}

@misc{requeimaLLMProcessesNumerical2024,
  title = {{{LLM Processes}}: {{Numerical Predictive Distributions Conditioned}} on {{Natural Language}}},
  shorttitle = {{{LLM Processes}}},
  author = {Requeima, James and Bronskill, John and Choi, Dami and Turner, Richard E. and Duvenaud, David},
  year = {2024},
  month = may,
  number = {arXiv:2405.12856},
  eprint = {2405.12856},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  urldate = {2024-05-25},
  abstract = {Machine learning practitioners often face significant challenges in formally integrating their prior knowledge and beliefs into predictive models, limiting the potential for nuanced and context-aware analyses. Moreover, the expertise needed to integrate this prior knowledge into probabilistic modeling typically limits the application of these models to specialists. Our goal is to build a regression model that can process numerical data and make probabilistic predictions at arbitrary locations, guided by natural language text which describes a user's prior knowledge. Large Language Models (LLMs) provide a useful starting point for designing such a tool since they 1) provide an interface where users can incorporate expert insights in natural language and 2) provide an opportunity for leveraging latent problem-relevant knowledge encoded in LLMs that users may not have themselves. We start by exploring strategies for eliciting explicit, coherent numerical predictive distributions from LLMs. We examine these joint predictive distributions, which we call LLM Processes, over arbitrarily-many quantities in settings such as forecasting, multi-dimensional regression, black-box optimization, and image modeling. We investigate the practical details of prompting to elicit coherent predictive distributions, and demonstrate their effectiveness at regression. Finally, we demonstrate the ability to usefully incorporate text into numerical predictions, improving predictive performance and giving quantitative structure that reflects qualitative descriptions. This lets us begin to explore the rich, grounded hypothesis space that LLMs implicitly encode.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Requeima et al_2024_LLM Processes.pdf}
}

@techreport{richieSemanticRepresentationsExtracted2018,
  title = {Semantic Representations Extracted from Large Language Corpora Predict High-Level Human Judgment in Seven Diverse Behavioral Domains},
  author = {Richie, Russell and Zou, Wanling and Bhatia, Sudeep},
  year = {2018},
  month = dec,
  institution = {PsyArXiv},
  urldate = {2022-04-20},
  abstract = {Recent advances in machine learning, combined with the increased availability of large natural language datasets, have made it possible to uncover semantic representations that characterize what people know about and associate with a wide range of objects and concepts. In this paper, we examine the power of word embeddings, a popular approach for uncovering semantic representations, for studying high-level human judgment. Word embeddings are typically applied to linguistic and semantic tasks, however we show that word embeddings can be used to predict complex theoretically- and practically-relevant human perceptions and evaluations in domains as diverse as social cognition, health behavior, risk perception, organizational behavior, and marketing. By learning mappings from word embeddings directly onto judgment ratings, we outperform a similarity-based baseline as well as common metrics of human inter-rater reliability. Word embeddings are also able to identify the concepts that are most associated with observed perceptions and evaluations, and can thus shed light on the psychological substrates of judgment. Overall, we provide new methods and insights for predicting and understanding high-level human judgment, with important applications across the social and behavioral sciences.},
  langid = {american},
  keywords = {Cognitive Psychology,Computational Linguistics,Concepts and Categories,judgment,Judgment and Decision Making,Linguistics,machine learning,Psycholinguistics and Neurolinguistics,semantic memory,Social and Behavioral Sciences,word embeddings},
  annotation = {https://github.com/drussellmrichie/embeddings\_to\_judgments},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Richie et al_2018_Semantic representations extracted from large language corpora predict.pdf}
}

@misc{roadsDimensionsDimensionality2024,
  title = {The {{Dimensions}} of {{Dimensionality}}},
  author = {Roads, Brett D. and Love, Bradley C.},
  year = {2024},
  urldate = {2024-07-26},
  abstract = {Cognitive scientists often infer multi-dimensional representations from data. Whether the data involve text, neuroimaging, neural networks, or human judgments, researchers frequently infer and analyze latent representational spaces (i.e., embeddings). However, the properties of a latent representation (e.g., prediction performance, interpretability, compactness) depend on the inference procedure, which can vary widely across endeavors. For example, dimensions are not always globally interpretable and the dimensionality of different embeddings may not be readily comparable. Moreover, the dichotomy between multidimensional spaces and purportedly richer representational formats, such as graph representations, is misleading. We review what the different notions of dimension in cognitive science imply for how these latent representations should be used and interpreted.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Roads_Love_2024_The Dimensions of Dimensionality.pdf}
}

@article{rondoraDriverBehavioralClassification2022,
  title = {Driver {{Behavioral Classification}} on {{Curves Based}} on the {{Relationship}} between {{Speed}}, {{Trajectories}}, and {{Eye Movements}}: {{A Driving Simulator Study}}},
  shorttitle = {Driver {{Behavioral Classification}} on {{Curves Based}} on the {{Relationship}} between {{Speed}}, {{Trajectories}}, and {{Eye Movements}}},
  author = {Rondora, Maria Emilia Schio and Pirdavani, Ali and Larocca, Ana Paula C.},
  year = {2022},
  month = jan,
  journal = {Sustainability},
  volume = {14},
  number = {10},
  pages = {6241},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2071-1050},
  doi = {10.3390/su14106241},
  urldate = {2024-09-16},
  abstract = {Horizontal curves of rural highways are prone to a considerably high number of fatalities because an erroneous perception can lead to unsafe driving. This generally occurs when a driver fails to notice the highway geometry or changes in the driving environment, particularly curved segments. This study aimed to understand the geometric characteristics of curved segments, such as radius and approach tangents, on the driving performance towards minimizing vehicle crashes. Speed profiles and lateral position, the most common indicators of successful negotiation in curves, and eye movements were recorded during an experiment conducted in a fixed-base driving simulator equipped with an eye-tracking system with a road infrastructure (a three-lane highway) and its surroundings. A driving simulator can faithfully reproduce any situation and enable sustainable research because it is a high-tech and cost-effective tool allowing repeatability in a laboratory. The experiment was conducted with 28 drivers who covered approximately 500 test kilometers with 90 horizontal curves comprising nine different combinations of radii and approach tangent lengths. The drivers' behavior on each curve was classified as ideal, normal, intermediate, cutting, or correcting according to their trajectories and speed changes for analyses of the performance parameters and their correlation conducted by factorial ANOVA and Pearson chi-square tests. The cross-tabulation results indicated that the safest behavior significantly increased when the curve radius increased, and the performance measures of curve radii were greatly affected. However, the driving behavior was not affected by the approach tangent length. The results revealed segments of the road that require a driver's closer attention for essential vehicle control, critical information, and vehicle control in different parts of the task.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {curve negotiation,driving simulator,eye movements,speed,trajectory classification},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Rondora et al_2022_Driver Behavioral Classification on Curves Based on the Relationship between.pdf}
}

@misc{ronyExploringPotentialLarge2024,
  title = {Exploring the {{Potential}} of the {{Large Language Models}} ({{LLMs}}) in {{Identifying Misleading News Headlines}}},
  author = {Rony, Md Main Uddin and Haque, Md Mahfuzul and Ali, Mohammad and Alam, Ahmed Shatil and Hassan, Naeemul},
  year = {2024},
  month = may,
  number = {arXiv:2405.03153},
  eprint = {2405.03153},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-23},
  abstract = {In the digital age, the prevalence of misleading news headlines poses a significant challenge to information integrity, necessitating robust detection mechanisms. This study explores the efficacy of Large Language Models (LLMs) in identifying misleading versus non-misleading news headlines. Utilizing a dataset of 60 articles, sourced from both reputable and questionable outlets across health, science \& tech, and business domains, we employ three LLMs---ChatGPT-3.5, ChatGPT-4, and Gemini---for classification. Our analysis reveals significant variance in model performance, with ChatGPT-4 demonstrating superior accuracy, especially in cases with unanimous annotator agreement on misleading headlines. The study emphasizes the importance of human-centered evaluation in developing LLMs that can navigate the complexities of misinformation detection, aligning technical proficiency with nuanced human judgment. Our findings contribute to the discourse on AI ethics, emphasizing the need for models that are not only technically advanced but also ethically aligned and sensitive to the subtleties of human interpretation.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Rony et al_2024_Exploring the Potential of the Large Language Models (LLMs) in Identifying.pdf}
}

@misc{rossLLMEconomicusMapping2024,
  title = {{{LLM}} Economicus? {{Mapping}} the {{Behavioral Biases}} of {{LLMs}} via {{Utility Theory}}},
  shorttitle = {{{LLM}} Economicus?},
  author = {Ross, Jillian and Kim, Yoon and Lo, Andrew W.},
  year = {2024},
  month = aug,
  number = {arXiv:2408.02784},
  eprint = {2408.02784},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-16},
  abstract = {Humans are not homo economicus (i.e., rational economic beings). As humans, we exhibit systematic behavioral biases such as loss aversion, anchoring, framing, etc., which lead us to make suboptimal economic decisions. Insofar as such biases may be embedded in text data on which large language models (LLMs) are trained, to what extent are LLMs prone to the same behavioral biases? Understanding these biases in LLMs is crucial for deploying LLMs to support human decision-making. We propose utility theory---a paradigm at the core of modern economic theory---as an approach to evaluate the economic biases of LLMs. Utility theory enables the quantification and comparison of economic behavior against benchmarks such as perfect rationality or human behavior. To demonstrate our approach, we quantify and compare the economic behavior of a variety of open- and closed-source LLMs. We find that the economic behavior of current LLMs is neither entirely human-like nor entirely economicus-like. We also find that most current LLMs struggle to maintain consistent economic behavior across settings. Finally, we illustrate how our approach can measure the effect of interventions such as prompting on economic biases.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ross et al_2024_LLM economicus.pdf}
}

@article{ruleSymbolicMetaprogramSearch2024,
  title = {Symbolic Metaprogram Search Improves Learning Efficiency and Explains Rule Learning in Humans},
  author = {Rule, Joshua S. and Piantadosi, Steven T. and Cropper, Andrew and Ellis, Kevin and Nye, Maxwell and Tenenbaum, Joshua B.},
  year = {2024},
  month = aug,
  journal = {Nature Communications},
  volume = {15},
  number = {1},
  pages = {6847},
  issn = {2041-1723},
  doi = {10.1038/s41467-024-50966-x},
  urldate = {2024-08-13},
  abstract = {Throughout their lives, humans seem to learn a variety of rules for things like applying category labels, following procedures, and explaining causal relationships. These rules are often algorithmically rich but are nonetheless acquired with minimal data and computation. Symbolic models based on program learning successfully explain rule-learning in many domains, but performance degrades quickly as program complexity increases. It remains unclear how to scale symbolic rule-learning methods to model human performance in challenging domains. Here we show that symbolic search over the space of metaprograms---programs that revise programs---dramatically improves learning efficiency. On a behavioral benchmark of 100 algorithmically rich rules, this approach fits human learning more accurately than alternative models while also using orders of magnitude less search. The computation required to match median human performance is consistent with conservative estimates of human thinking time. Our results suggest that metaprogram-like representations may help human learners to efficiently acquire rules.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computer science,Human behaviour},
  annotation = {https://github.com/joshrule/program-induction\\
\\
https://osf.io/gq2hj/\\
\\
https://github.com/joshrule/term-rewriting-rs},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Rule et al_2024_Symbolic metaprogram search improves learning efficiency and explains rule.pdf}
}

@misc{russinFregeChatGPTCompositionality2024,
  title = {From {{Frege}} to {{chatGPT}}: {{Compositionality}} in Language, Cognition, and Deep Neural Networks},
  shorttitle = {From {{Frege}} to {{chatGPT}}},
  author = {Russin, Jacob and McGrath, Sam Whitman and Williams, Danielle J. and {Elber-Dorozko}, Lotem},
  year = {2024},
  month = may,
  number = {arXiv:2405.15164},
  eprint = {2405.15164},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-03},
  abstract = {Compositionality has long been considered a key explanatory property underlying human intelligence: arbitrary concepts can be composed into novel complex combinations, permitting the acquisition of an open ended, potentially infinite expressive capacity from finite learning experiences. Influential arguments have held that neural networks fail to explain this aspect of behavior, leading many to dismiss them as viable models of human cognition. Over the last decade, however, modern deep neural networks (DNNs)---which share the same fundamental design principles as their predecessors---have come to dominate artificial intelligence, exhibiting the most advanced cognitive behaviors ever demonstrated in machines. In particular, large language models (LLMs), DNNs trained to predict the next word on a large corpus of text, have proven capable of sophisticated behaviors such as writing syntactically complex sentences without grammatical errors, producing cogent chains of reasoning, and even writing original computer programs---all behaviors thought to require compositional processing. In this chapter, we survey recent empirical work from machine learning for a broad audience in philosophy, cognitive science, and neuroscience, situating recent breakthroughs in deep learning and LLMs within the broader context of philosophical arguments about compositionality. In particular, our review emphasizes two approaches to endowing neural networks with compositional generalization capabilities: (1) architectural inductive biases, and (2) metalearning, or learning to learn. We also present findings suggesting that LLM pretraining can be understood as a kind of metalearning, and can thereby equip DNNs with compositional generalization abilities in a similar way. We conclude by discussing the implications that these findings may have for the study of compositionality in human cognition and by suggesting avenues for future research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Russin et al_2024_From Frege to chatGPT.pdf}
}

@misc{russinHumanCurriculumEffects2024,
  title = {Human {{Curriculum Effects Emerge}} with {{In-Context Learning}} in {{Neural Networks}}},
  author = {Russin, Jacob and Pavlick, Ellie and Frank, Michael J.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.08674},
  eprint = {2402.08674},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-03-07},
  abstract = {Human learning is sensitive to rule-like structure and the curriculum of examples used for training. In tasks governed by succinct rules, learning is more robust when related examples are blocked across trials, but in the absence of such rules, interleaving is more effective. To date, no neural model has simultaneously captured these seemingly contradictory effects. Here we show that this same tradeoff spontaneously emerges with "in-context learning" (ICL) both in neural networks trained with metalearning and in large language models (LLMs). ICL is the ability to learn new tasks "in context" - without weight changes - via an inner-loop algorithm implemented in activation dynamics. Experiments with pretrained LLMs and metalearning transformers show that ICL exhibits the blocking advantage demonstrated in humans on a task involving rule-like structure, and conversely, that concurrent in-weight learning reproduces the interleaving advantage observed in humans on tasks lacking such structure.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Russin et al_2024_Human Curriculum Effects Emerge with In-Context Learning in Neural Networks.pdf;/Users/thomasgorman/Zotero/storage/XCXEKP2S/2402.html}
}

@misc{sahinucSystematicTaskExploration2024,
  title = {Systematic {{Task Exploration}} with {{LLMs}}: {{A Study}} in {{Citation Text Generation}}},
  shorttitle = {Systematic {{Task Exploration}} with {{LLMs}}},
  author = {{\c S}ahinu{\c c}, Furkan and Kuznetsov, Ilia and Hou, Yufang and Gurevych, Iryna},
  year = {2024},
  month = jul,
  number = {arXiv:2407.04046},
  eprint = {2407.04046},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-14},
  abstract = {Large language models (LLMs) bring unprecedented flexibility in defining and executing complex, creative natural language generation (NLG) tasks. Yet, this flexibility brings new challenges, as it introduces new degrees of freedom in formulating the task inputs and instructions and in evaluating model performance. To facilitate the exploration of creative NLG tasks, we propose a three-component research framework that consists of systematic input manipulation, reference data, and output measurement. We use this framework to explore citation text generation -- a popular scholarly NLP task that lacks consensus on the task definition and evaluation metric and has not yet been tackled within the LLM paradigm. Our results highlight the importance of systematically investigating both task instruction and input configuration when prompting LLMs, and reveal non-trivial relationships between different evaluation metrics used for citation text generation. Additional human generation and human evaluation experiments provide new qualitative insights into the task to guide future research in citation text generation. We make our code1 and data2 publicly available.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Şahinuç et al_2024_Systematic Task Exploration with LLMs.pdf}
}

@misc{salehFollowMeAIEnergyEfficient2024,
  title = {Follow-{{Me AI}}: {{Energy-Efficient User Interaction}} with {{Smart Environments}}},
  shorttitle = {Follow-{{Me AI}}},
  author = {Saleh, Alaa and Donta, Praveen Kumar and Morabito, Roberto and Motlagh, Naser Hossein and Lov{\'e}n, Lauri},
  year = {2024},
  month = apr,
  number = {arXiv:2404.12486},
  eprint = {2404.12486},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-11},
  abstract = {This article introduces Follow-Me AI, a concept designed to enhance user interactions with smart environments, optimize energy use, and provide better control over data captured by these environments. Through AI agents that accompany users, Follow-Me AI negotiates data management based on user consent, aligns environmental controls as well as user communication and computes resources available in the environment with user preferences, and predicts user behavior to proactively adjust the smart environment. The manuscript illustrates this concept with a detailed example of Follow-Me AI in a smart campus setting, detailing the interactions with the building's management system for optimal comfort and efficiency. Finally, this article looks into the challenges and opportunities related to Follow-Me AI.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Distributed Parallel and Cluster Computing,Computer Science - Emerging Technologies,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Saleh et al_2024_Follow-Me AI.pdf}
}

@misc{salewskiContextImpersonationReveals2023,
  title = {In-{{Context Impersonation Reveals Large Language Models}}' {{Strengths}} and {{Biases}}},
  author = {Salewski, Leonard and Alaniz, Stephan and {Rio-Torto}, Isabel and Schulz, Eric and Akata, Zeynep},
  year = {2023},
  month = may,
  number = {arXiv:2305.14930},
  eprint = {2305.14930},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-05-28},
  abstract = {In everyday conversations, humans can take on different roles and adapt their vocabulary to their chosen roles. We explore whether LLMs can take on, that is impersonate, different roles when they generate text in-context. We ask LLMs to assume different personas before solving vision and language tasks. We do this by prefixing the prompt with a persona that is associated either with a social identity or domain expertise. In a multi-armed bandit task, we find that LLMs pretending to be children of different ages recover human-like developmental stages of exploration. In a language-based reasoning task, we find that LLMs impersonating domain experts perform better than LLMs impersonating non-domain experts. Finally, we test whether LLMs' impersonations are complementary to visual information when describing different categories. We find that impersonation can improve performance: an LLM prompted to be a bird expert describes birds better than one prompted to be a car expert. However, impersonation can also uncover LLMs' biases: an LLM prompted to be a man describes cars better than one prompted to be a woman. These findings demonstrate that LLMs are capable of taking on diverse roles and that this in-context impersonation can be used to uncover their hidden strengths and biases.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Salewski et al_2023_In-Context Impersonation Reveals Large Language Models' Strengths and Biases.pdf;/Users/thomasgorman/Zotero/storage/KF62XJL5/2305.html}
}

@incollection{salikutlukInteractingLargeLanguage2023,
  title = {Interacting with {{Large Language Models}}: {{A Case Study}} on {{AI-Aided Brainstorming}} for {{Guesstimation Problems}}},
  shorttitle = {Interacting with {{Large Language Models}}},
  booktitle = {Frontiers in {{Artificial Intelligence}} and {{Applications}}},
  author = {Salikutluk, Vildan and Koert, Dorothea and J{\"a}kel, Frank},
  editor = {Lukowicz, Paul and Mayer, Sven and Koch, Janin and {Shawe-Taylor}, John and Tiddi, Ilaria},
  year = {2023},
  month = jun,
  publisher = {IOS Press},
  doi = {10.3233/FAIA230081},
  urldate = {2023-11-25},
  abstract = {Designing cooperative AI-systems that do not automate tasks but rather aid human cognition is challenging and requires human-centered design approaches. Here, we introduce AI-aided brainstorming for solving guesstimation problems, i.e. estimating quantities from incomplete information, as a testbed for human-AI interaction with large language models (LLMs). In a think-aloud study, we found that humans decompose guesstimation questions into sub-questions and often replace them with semantically related ones. If they fail to brainstorm related questions, they often get stuck and do not find a solution. Therefore, to support this brainstorming process, we prompted a large language model (GPT-3) with successful replacements from our think-aloud data. In follow-up studies, we tested whether the availability of this tool improves participants' answers. While the tool successfully produced human-like suggestions, participants were reluctant to use it. From our findings, we conclude that for human-AI interaction with LLMs to be successful AI-systems must complement rather than mimic a user's associations.},
  isbn = {978-1-64368-394-2 978-1-64368-395-9},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Salikutluk et al_2023_Interacting with Large Language Models.pdf}
}

@article{salvucciLaneChangeDetectionUsing2007,
  title = {Lane-{{Change Detection Using}} a {{Computational Driver Model}}},
  author = {Salvucci, Dario D. and Mandalia, Hiren M. and Kuge, Nobuyuki and Yamamura, Tomohiro},
  year = {2007},
  month = jun,
  journal = {Human Factors},
  volume = {49},
  number = {3},
  pages = {532--542},
  publisher = {SAGE Publications Inc},
  issn = {0018-7208},
  doi = {10.1518/001872007X200157},
  urldate = {2024-09-16},
  abstract = {Objective: This paper introduces a robust, real-time system for detecting driver lane changes. Background: As intelligent transportation systems evolve to assist drivers in their intended behaviors, the systems have demonstrated a need for methods of inferring driver intentions and detecting intended maneuvers. Method: Using a ?model tracing? methodology, our system simulates a set of possible driver intentions and their resulting behaviors using a simplification of a previously validated computational model of driver behavior. The system compares the model's simulated behavior with a driver's actual observed behavior and thus continually infers the driver's unobservable intentions from her or his observable actions. Results: For data collected in a driving simulator, the system detects 82\% of lane changes within 0.5 s of maneuver onset (assuming a 5\% false alarm rate), 93\% within 1 s, and 95\% before the vehicle moves one fourth of the lane width laterally. For data collected from an instrumented vehicle, the system detects 61\% within 0.5 s, 77\% within 1 s, and 84\% before the vehicle moves one-fourth of the lane width laterally. Conclusion: The model-tracing system is the first system to demonstrate high sample-by-sample accuracy at low false alarm rates as well as high accuracy over the course of a lane change with respect to time and lateral movement. Application: By providing robust real-time detection of driver lane changes, the system shows good promise for incorporation into the next generation of intelligent transportation systems.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Salvucci et al_2007_Lane-Change Detection Using a Computational Driver Model.pdf}
}

@article{salvucciModelingDriverBehavior2006,
  title = {Modeling {{Driver Behavior}} in a {{Cognitive Architecture}}},
  author = {Salvucci, Dario D.},
  year = {2006},
  month = jun,
  journal = {Human Factors},
  volume = {48},
  number = {2},
  pages = {362--380},
  publisher = {SAGE Publications Inc},
  issn = {0018-7208},
  doi = {10.1518/001872006777724417},
  urldate = {2024-09-16},
  abstract = {Objective: This paper explores the development of a rigorous computational model of driver behavior in a cognitive architecture--a computational framework with underlying psychological theories that incorporate basic properties and limitations of the human system. Background: Computational modeling has emerged as a powerful tool for studying the complex task of driving, allowing researchers to simulate driver behavior and explore the parameters and constraints of this behavior. Method: An integrated driver model developed in the ACT-R (Adaptive Control of Thought-Rational) cognitive architecture is described that focuses on the component processes of control, monitoring, and decision making in a multilane highway environment. Results: This model accounts for the steering profiles, lateral position profiles, and gaze distributions of human drivers during lane keeping, curve negotiation, and lane changing. Conclusion: The model demonstrates how cognitive architectures facilitate understanding of driver behavior in the context of general human abilities and constraints and how the driving domain benefits cognitive architectures by pushing model development toward more complex, realistic tasks. Application: The model can also serve as a core computational engine for practical applications that predict and recognize driver behavior and distraction.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Salvucci_2006_Modeling Driver Behavior in a Cognitive Architecture.pdf}
}

@article{santosoHumanErrorsTraffic2019,
  title = {Human Errors in Traffic Accidents: Differences between Car Drivers and Motorcyclists' Experience},
  shorttitle = {Human Errors in Traffic Accidents},
  author = {Santoso, Guritnaningsih P and Maulina, Dewi},
  year = {2019},
  month = dec,
  journal = {Psychological Research on Urban Society},
  volume = {2},
  number = {2},
  pages = {118},
  issn = {2615-8582, 2620-3960},
  doi = {10.7454/proust.v2i2.69},
  urldate = {2024-09-16},
  abstract = {Traffic accidents have become one of the main causes of death in Indonesia. The biggest contributor to traffic accidents are motorcyclists. According to police records, human error plays a major role in the occurrence of accidents. The aim of this study is to analyze the potential types of human error that contribute to traffic accidents, as well as the psychological factors that underlie traffic accidents experienced by car drivers and motorcyclists. Data was collected by interviewing five car drivers and five motorcyclists. Results show that the car drivers tend to perform a type of human error which is classified as lapses, while the motorcyclists tend to do an error of slips. For psychological factors that underlie traffic accident, results show that both car drivers and motorcyclists made recognition errors, i.e. did not estimate distance, time, and speed. They also made decision errors, i.e. did not avoid the situation immediately, and performance errors, i.e. a motorcyclist stepped on the gas pedal by mistake. Other errors done by the car drivers were being sleepy and drunk, whereas other errors done by motorcyclists were not having a riding license and feeling tired. The implication of this study is to make the drivers/riders aware of the importance of cognitive aspects in driving.},
  copyright = {https://creativecommons.org/licenses/by/4.0},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Santoso_Maulina_2019_Human errors in traffic accidents.pdf}
}

@article{sartoriLanguageModelsPsychological2023,
  title = {Language Models and Psychological Sciences},
  author = {Sartori, Giuseppe and Orr{\`u}, Graziella},
  year = {2023},
  month = oct,
  journal = {Frontiers in Psychology},
  volume = {14},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2023.1279317},
  urldate = {2024-05-25},
  abstract = {{$<$}p{$>$}Large language models (LLMs) are demonstrating impressive performance on many reasoning and problem-solving tasks from cognitive psychology. When tested, their accuracy is often on par with average neurotypical adults, challenging long-standing critiques of associative models. Here we analyse recent findings at the intersection of LLMs and cognitive science. Here we discuss how modern LLMs resurrect associationist principles, with abilities like long-distance associations enabling complex reasoning. While limitations remain in areas like causal cognition and planning, phenomena like emergence suggest room for growth. Providing examples and increasing the dimensions of the network are methods that further improve LLM abilities, mirroring facilitation effects in human cognition. Analysis of LLMs errors provides insight into human cognitive biases. Overall, we argue LLMs represent a promising development for cognitive modelling, enabling new explorations of the mechanisms underlying intelligence and reasoning from an associationist point of view. Carefully evaluating LLMs with the tools of cognitive psychology will further understand the building blocks of the human mind.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Associationism,ChatGPT,Cognitive Psychology,Large language models,LLMS},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Sartori_Orrù_2023_Language models and psychological sciences.pdf}
}

@article{schille-hudsonBigHotBright2019,
  title = {Big, Hot, or Bright? {{Integrating}} Cues to Perceive Home Energy Use},
  author = {{Schille-Hudson}, Eleanor B and Margehtis, Tyler and Miniard, Deidra and Landy, David},
  year = {2019},
  journal = {Proceedings of the 41st Cognitive Science Society},
  abstract = {Despite constantly using energy and having extensive interactions with household appliances, people consistently mis-estimate the amount of energy that is used by home appliances. This poses major problems for conservation efforts, while also presenting an interesting case study in human perception. Since many forms of energy used are not directly perceptible, and since the amount of energy that is being used by an appliance is often difficult to infer from appearances alone, people often rely on cues. Some of these cues are more reliable than others and previous literature has investigated which of these cues people rely on. However, past literature has always studied these proximal cues in isolation---despite the fact that, during real-world perception, people are always integrating a variety of cues. Here, we investigate how people rely on a variety of cues, and how individual differences in the reliance on those cues predicts the ability to estimate home energy use.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Schille-Hudson et al_Big, hot, or bright.pdf}
}

@article{schlundAlgorithmicHumanSurveillance2024,
  title = {Algorithmic versus Human Surveillance Leads to Lower Perceptions of Autonomy and Increased Resistance},
  author = {Schlund, Rachel and Zitek, Emily M.},
  year = {2024},
  month = jun,
  journal = {Communications Psychology},
  volume = {2},
  number = {1},
  pages = {1--9},
  issn = {2731-9121},
  doi = {10.1038/s44271-024-00102-8},
  urldate = {2024-07-09},
  abstract = {Past research indicates that people tend to react adversely to surveillance, but does it matter if advanced technologies such as artificial intelligence~conduct surveillance rather than humans? Across four experiments (Study 1, N\,=\,107; Study 2, N\,=\,157; Study 3, N\,=\,117; Study 4, N\,=\,814), we examined how participants reacted to monitoring and evaluation by human or algorithmic surveillance when recalling instances of surveillance from their lives (Study 1), generating ideas (Studies 2 and 3), or imagining working in a call center (Study 4). Our results revealed that participants subjected to algorithmic (v. human) surveillance perceived they had less autonomy (Studies 1, 3, and 4), criticized the surveillance more (Studies 1-3), performed worse (Studies 2 and 3), and reported greater intentions to resist (Studies 1 and 4). Framing the purpose of the algorithmic surveillance as developmental, and thus informational, as opposed to evaluative, mitigated the perception of decreased autonomy and level of resistance (Study 4).},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Psychology,Society},
  annotation = {https://osf.io/3ztpm/?view\_only=91c58da2216f4633b98d5fa1cb849808},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Schlund_Zitek_2024_Algorithmic versus human surveillance leads to lower perceptions of autonomy.pdf}
}

@article{schneiderEmergenceEnhancedIntelligence2024,
  title = {The Emergence of Enhanced Intelligence in a Brain-Inspired Cognitive Architecture},
  author = {Schneider, Howard},
  year = {2024},
  month = may,
  journal = {Frontiers in Computational Neuroscience},
  volume = {18},
  pages = {1367712},
  issn = {1662-5188},
  doi = {10.3389/fncom.2024.1367712},
  urldate = {2024-07-23},
  abstract = {The Causal Cognitive Architecture is a brain-inspired cognitive architecture developed from the hypothesis that the navigation circuits in the ancestors of mammals duplicated to eventually form the neocortex. Thus, millions of neocortical minicolumns are functionally modeled in the architecture as millions of ``navigation maps.'' An investigation of a cognitive architecture based on these navigation maps has previously shown that modest changes in the architecture allow the ready emergence of human cognitive abilities such as grounded, full causal decision-making, full analogical reasoning, and near-full compositional language abilities. In this study, additional biologically plausible modest changes to the architecture are considered and show the emergence of super-human planning abilities. The architecture should be considered as a viable alternative pathway toward the development of more advanced artificial intelligence, as well as to give insight into the emergence of natural human intelligence.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Schneider_2024_The emergence of enhanced intelligence in a brain-inspired cognitive.pdf}
}

@misc{schoeneggerAIAugmentedPredictionsLLM2024,
  title = {{{AI-Augmented Predictions}}: {{LLM Assistants Improve Human Forecasting Accuracy}}},
  shorttitle = {{{AI-Augmented Predictions}}},
  author = {Schoenegger, Philipp and Park, Peter S. and Karger, Ezra and Tetlock, Philip E.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.07862},
  eprint = {2402.07862},
  publisher = {arXiv},
  urldate = {2024-05-25},
  abstract = {Large language models (LLMs) show impressive capabilities, matching and sometimes exceeding human performance in many domains. This study explores the potential of LLMs to augment judgement in forecasting tasks. We evaluated the impact on forecasting accuracy of two GPT-4-Turbo assistants: one designed to provide high-quality advice (`superforecasting'), and the other designed to be overconfident and base-rate-neglecting. Participants (N = 991) had the option to consult their assigned LLM assistant throughout the study, in contrast to a control group that used a less advanced model (DaVinci-003) without direct forecasting support. Our preregistered analyses reveal that LLM augmentation significantly enhances forecasting accuracy by 23\% across both types of assistants, compared to the control group. This improvement occurs despite the superforecasting assistant's higher accuracy in predictions, indicating the augmentation's benefit is not solely due to model prediction accuracy. Exploratory analyses showed a pronounced effect in one forecasting item, without which we find that the superforecasting assistant increased accuracy by 43\%, compared with 28\% for the biased assistant. We further examine whether LLM augmentation disproportionately benefits less skilled forecasters, degrades the wisdom-of-the-crowd by reducing prediction diversity, or varies in effectiveness with question difficulty. Our findings do not consistently support these hypotheses. Our results suggest that access to an LLM assistant, even a biased one, can be a helpful decision aid in cognitively demanding tasks where the answer is not known at the time of interaction.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  annotation = {https://osf.io/d9rhx?view\_only=c631c477026a41f3bd4e6b7a4e546157},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Schoenegger et al_2024_AI-Augmented Predictions.pdf}
}

@misc{schoeneggerWisdomSiliconCrowd2024,
  title = {Wisdom of the {{Silicon Crowd}}: {{LLM Ensemble Prediction Capabilities Rival Human Crowd Accuracy}}},
  shorttitle = {Wisdom of the {{Silicon Crowd}}},
  author = {Schoenegger, Philipp and Tuminauskaite, Indre and Park, Peter S. and Tetlock, Philip E.},
  year = {2024},
  month = jul,
  number = {arXiv:2402.19379},
  eprint = {2402.19379},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-16},
  abstract = {Human forecasting accuracy in practice relies on the `wisdom of the crowd' effect, in which predictions about future events are significantly improved by aggregating across a crowd of individual forecasters. Past work on the forecasting ability of large language models (LLMs) suggests that frontier LLMs, as individual forecasters, underperform compared to the gold standard of a human crowd forecasting tournament aggregate. In Study 1, we expand this research by using an LLM ensemble approach consisting of a crowd of twelve LLMs. We compare the aggregated LLM predictions on 31 binary questions to that of a crowd of 925 human forecasters from a three-month forecasting tournament. Our preregistered main analysis shows that the LLM crowd outperforms a simple no-information benchmark and is not statistically different from the human crowd. In exploratory analyses, we find that these two approaches are equivalent with respect to medium-effect-size equivalence bounds. We also observe an acquiescence effect, with mean model predictions being significantly above 50\%, despite an almost even split of positive and negative resolutions. Moreover, in Study 2, we test whether LLM predictions (of GPT-4 and Claude 2) can be improved by drawing on human cognitive output. We find that both models' forecasting accuracy benefits from exposure to the median human prediction as information, improving accuracy by between 17\% and 28\%: though this leads to less accurate predictions than simply averaging human and machine forecasts. Our results suggest that LLMs can achieve forecasting accuracy rivaling that of human crowd forecasting tournaments: via the simple, practically applicable method of forecast aggregation. This replicates the `wisdom of the crowd' effect for LLMs, and opens up their use for a variety of applications throughout society.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Schoenegger et al_2024_Wisdom of the Silicon Crowd.pdf}
}

@misc{schubertContextLearningAgents2024,
  title = {In-Context Learning Agents Are Asymmetric Belief Updaters},
  author = {Schubert, Johannes A. and Jagadish, Akshay K. and Binz, Marcel and Schulz, Eric},
  year = {2024},
  month = feb,
  number = {arXiv:2402.03969},
  eprint = {2402.03969},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-10},
  abstract = {We study the in-context learning dynamics of large language models (LLMs) using three instrumental learning tasks adapted from cognitive psychology. We find that LLMs update their beliefs in an asymmetric manner and learn more from better-than-expected outcomes than from worse-than-expected ones. Furthermore, we show that this effect reverses when learning about counterfactual feedback and disappears when no agency is implied. We corroborate these findings by investigating idealized in-context learning agents derived through meta-reinforcement learning, where we observe similar patterns. Taken together, our results contribute to our understanding of how in-context learning works by highlighting that the framing of a problem significantly influences how learning occurs, a phenomenon also observed in human cognition.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Schubert et al_2024_In-context learning agents are asymmetric belief updaters.pdf;/Users/thomasgorman/Zotero/storage/MR7GLCLM/2402.html}
}

@article{schwartzWhatPeopleConsumption2015,
  title = {What {{People Do}} with {{Consumption Feedback}}: {{A Long-Term Living Lab Study}} of a {{Home Energy Management System}}},
  shorttitle = {What {{People Do}} with {{Consumption Feedback}}},
  author = {Schwartz, Tobias and Stevens, Gunnar and Jakobi, Timo and Denef, Sebastian and Ramirez, Leonardo and Wulf, Volker and Randall, Dave},
  year = {2015},
  month = nov,
  journal = {Interacting with Computers},
  volume = {27},
  number = {6},
  pages = {551--576},
  issn = {0953-5438, 1873-7951},
  doi = {10.1093/iwc/iwu009},
  urldate = {2024-07-02},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Schwartz et al_2015_What People Do with Consumption Feedback.pdf}
}

@article{schwenkSimpleHeuristicsComplex2008,
  title = {Simple {{Heuristics}} in {{Complex Networks}}: {{Models}} of {{Social Influence}}},
  author = {Schwenk, Gero and Reimer, Torsten},
  year = {2008},
  journal = {Journal of Artificial Societies and Social Simulation},
  volume = {11},
  number = {34},
  abstract = {The concept of heuristic decision making is adapted to dynamic influence processes in social networks. We report results of a set of simulations, in which we systematically varied: a) the agents' strategies for contacting fellow group members and integrating collected information, and (b) features of their social environment --- the distribution of members' status, and the degree of clustering in their network. As major outcome variables, we measured the speed with which the process settled, the distributions of agents' final preferences, and the rate with which high-status members changed their initial preferences. The impact of the agents' decision strategies on the dynamics and outcomes of the influence process depended on features of their social environment. This held in particular true when agents contacted all of the neighbors with whom they were connected. When agents focused on high-status members and did not contact low-status neighbors, the process typically settled more quickly, yielded larger majority factions and fewer preference changes. A case study exemplifies the empirical application of the model.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Gero Schwenk and Torsten Reimer_ Simple Heuristics in Complex Networks.pdf}
}

@article{schwitzgebelCreatingLargeLanguage2024,
  title = {Creating a Large Language Model of a Philosopher},
  author = {Schwitzgebel, Eric and Schwitzgebel, David and Strasser, Anna},
  year = {2024},
  journal = {Mind \& Language},
  volume = {39},
  number = {2},
  pages = {237--259},
  issn = {1468-0017},
  doi = {10.1111/mila.12466},
  urldate = {2024-07-10},
  abstract = {Can large language models produce expert-quality philosophical texts? To investigate this, we fine-tuned GPT-3 with the works of philosopher Daniel Dennett. To evaluate the model, we asked the real Dennett 10 philosophical questions and then posed the same questions to the language model, collecting four responses for each question without cherry-picking. Experts on Dennett's work succeeded at distinguishing the Dennett-generated and machine-generated answers above chance but substantially short of our expectations. Philosophy blog readers performed similarly to the experts, while ordinary research participants were near chance distinguishing GPT-3's responses from those of an ``actual human philosopher''.},
  langid = {english},
  keywords = {artificial intelligence,Daniel C. Dennett,human-machine discrimination,language models,philosophical expertise},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Schwitzgebel et al_2024_Creating a large language model of a philosopher.pdf;/Users/thomasgorman/Zotero/storage/2FVHNVLG/mila.html}
}

@article{sciannameoInformationExtractionMedical2024,
  title = {Information Extraction from Medical Case Reports Using {{OpenAI InstructGPT}}},
  author = {Sciannameo, Veronica and Pagliari, Daniele Jahier and Urru, Sara and Grimaldi, Piercesare and Ocagli, Honoria and {Ahsani-Nasab}, Sara and Comoretto, Rosanna Irene and Gregori, Dario and Berchialla, Paola},
  year = {2024},
  month = oct,
  journal = {Computer Methods and Programs in Biomedicine},
  volume = {255},
  pages = {108326},
  issn = {01692607},
  doi = {10.1016/j.cmpb.2024.108326},
  urldate = {2024-08-11},
  abstract = {Background and objective: Researchers commonly use automated solutions such as Natural Language Processing (NLP) systems to extract clinical information from large volumes of unstructured data. However, clinical text's poor semantic structure and domain-specific vocabulary can make it challenging to develop a one-size-fits-all solution. Large Language Models (LLMs), such as OpenAI's Generative Pre-Trained Transformer 3 (GPT-3), offer a promising solution for capturing and standardizing unstructured clinical information. This study evaluated the performance of InstructGPT, a family of models derived from LLM GPT-3, to extract relevant patient information from medical case reports and discussed the advantages and disadvantages of LLMs versus dedicated NLP methods. Methods: In this paper, 208 articles related to case reports of foreign body injuries in children were identified by searching PubMed, Scopus, and Web of Science. A reviewer manually extracted information on sex, age, the object that caused the injury, and the injured body part for each patient to build a gold standard to compare the performance of InstructGPT. Results: InstructGPT achieved high accuracy in classifying the sex, age, object and body part involved in the injury, with 94\%, 82\%, 94\% and 89\%, respectively. When excluding articles for which InstructGPT could not retrieve any information, the accuracy for determining the child's sex and age improved to 97\%, and the accuracy for identifying the injured body part improved to 93\%. InstructGPT was also able to extract information from non-English language articles. Conclusions: The study highlights that LLMs have the potential to eliminate the necessity for task-specific training (zero-shot extraction), allowing the retrieval of clinical information from unstructured natural language text, particularly from published scientific literature like case reports, by directly utilizing the PDF file of the article without any pre-processing and without requiring any technical expertise in NLP or Machine Learning. The diverse nature of the corpus, which includes articles written in languages other than English, some of which contain a wide range of clinical details while others lack information, adds to the strength of the study.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Sciannameo et al_2024_Information extraction from medical case reports using OpenAI InstructGPT.pdf}
}

@article{shabahangGeneralizationRetrievalUsing2022,
  title = {Generalization at {{Retrieval Using Associative Networks}} with {{Transient Weight Changes}}},
  author = {Shabahang, Kevin D. and Yim, Hyungwook and Dennis, Simon J.},
  year = {2022},
  month = mar,
  journal = {Computational Brain \& Behavior},
  volume = {5},
  number = {1},
  pages = {124--155},
  issn = {2522-087X},
  doi = {10.1007/s42113-022-00127-4},
  urldate = {2024-03-18},
  abstract = {Without having seen a bigram like ``her buffalo'', you can easily tell that it is congruent because ``buffalo'' can be aligned with more common nouns like ``cat'' or ``dog'' that have been seen in contexts like ``her cat'' or ``her dog''---the novel bigram structurally aligns with representations in memory. We present a new class of associative nets we call Dynamic-Eigen-Nets, and provide simulations that show how they generalize to patterns that are structurally aligned with the training domain. Linear-Associative-Nets respond with the same pattern regardless of input, motivating the introduction of saturation to facilitate other response states. However, models using saturation cannot readily generalize to novel, but structurally aligned patterns. Dynamic-Eigen-Nets address this problem by dynamically biasing the eigenspectrum towards external input using temporary weight changes. We demonstrate how a two-slot Dynamic-Eigen-Net trained on a text corpus provides an account of bigram judgment-of-grammaticality and lexical decision tasks, showing it can better capture syntactic regularities from the corpus compared to the Brain-State-in-a-Box and the Linear-Associative-Net. We end with a simulation showing how a Dynamic-Eigen-Net is sensitive to syntactic violations introduced in bigrams, even after the associations that encode those bigrams are deleted from memory. Over all simulations, the Dynamic-Eigen-Net reliably outperforms the Brain-State-in-a-Box and the Linear-Associative-Net. We propose Dynamic-Eigen-Nets as associative nets that generalize at retrieval, instead of encoding, through recurrent feedback.},
  langid = {english},
  keywords = {Auto-associative,Content addressable memory,Generalization,Pattern-completion,Recurrent neural network,Short-term-plasticity},
  annotation = {https://osf.io/g4axy/\\
\\
https://osf.io/ngc9a/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Shabahang et al_2022_Generalization at Retrieval Using Associative Networks with Transient Weight.pdf}
}

@article{shahmohammadiLanguageVisionStudy2023,
  title = {Language with Vision: {{A}} Study on Grounded Word and Sentence Embeddings},
  shorttitle = {Language with Vision},
  author = {Shahmohammadi, Hassan and Heitmeier, Maria and {Shafaei-Bajestan}, Elnaz and Lensch, Hendrik P. A. and Baayen, R. Harald},
  year = {2023},
  month = dec,
  journal = {Behavior Research Methods},
  issn = {1554-3528},
  doi = {10.3758/s13428-023-02294-z},
  urldate = {2024-05-26},
  abstract = {Grounding language in vision is an active field of research seeking to construct cognitively plausible word and sentence representations by incorporating perceptual knowledge from vision into text-based representations. Despite many attempts at language grounding, achieving an optimal equilibrium between textual representations of the language and our embodied experiences remains an open field. Some common concerns are the following. Is visual grounding advantageous for abstract words, or is its effectiveness restricted to concrete words? What is the optimal way of bridging the gap between text and vision? To what extent is perceptual knowledge from images advantageous for acquiring high-quality embeddings? Leveraging the current advances in machine learning and natural language processing, the present study addresses these questions by proposing a simple yet very effective computational grounding model for pre-trained word embeddings. Our model effectively balances the interplay between language and vision by aligning textual embeddings with visual information while simultaneously preserving the distributional statistics that characterize word usage in text corpora. By applying a learned alignment, we are able to indirectly ground unseen words including abstract words. A series of evaluations on a range of behavioral datasets shows that visual grounding is beneficial not only for concrete words but also for abstract words, lending support to the indirect theory of abstract concepts. Moreover, our approach offers advantages for contextualized embeddings, such as those generated by BERT (Devlin et al, 2018), but only when trained on corpora of modest, cognitively plausible sizes. Code and grounded embeddings for English are available at (https://github.com/Hazel1994/Visually\_Grounded\_Word\_Embeddings\_ 2).},
  langid = {english},
  annotation = {https://github.com/Hazel1994/Visually\_Grounded\_Word\_Embeddings\_2},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Shahmohammadi et al_2023_Language with vision.pdf}
}

@article{shahNavigatingWebDisinformation2024,
  title = {Navigating the {{Web}} of {{Disinformation}} and {{Misinformation}}: {{Large Language Models}} as {{Double-Edged Swords}}},
  shorttitle = {Navigating the {{Web}} of {{Disinformation}} and {{Misinformation}}},
  author = {Shah, Siddhant Bikram and Thapa, Surendrabikram and Acharya, Ashish and Rauniyar, Kritesh and Poudel, Sweta and Jain, Sandesh and Masood, Anum and Naseem, Usman},
  year = {2024},
  journal = {IEEE Access},
  pages = {1--1},
  issn = {2169-3536},
  doi = {10.1109/ACCESS.2024.3406644},
  urldate = {2024-07-23},
  abstract = {This paper explores the dual role of Large Language Models (LLMs) in the context of online misinformation and disinformation. In today's digital landscape, where the internet and social media facilitate the rapid dissemination of information, discerning between accurate content and falsified information presents a formidable challenge. Misinformation, often arising unintentionally, and disinformation, crafted deliberately, are at the forefront of this challenge. LLMs such as OpenAI's GPT-4, equipped with advanced language generation abilities, present a double-edged sword in this scenario. While they hold promise in combating misinformation by fact-checking and detecting LLM-generated text, their ability to generate realistic, contextually relevant text also poses risks for creating and propagating misinformation. Further, LLMs are plagued with many problems such as biases, knowledge cutoffs, and hallucinations, which may further perpetuate misinformation and disinformation. The paper outlines historical developments in misinformation detection and how it affects social media consumption, especially among youth, and introduces LLMs and their applications in various domains. It then critically analyzes the potential of LLMs to generate and counter misinformation and disinformation in sensitive topics such as healthcare, COVID-19, and political agendas. Further, it discusses mitigation strategies, ethical considerations, and regulatory measures, summarizing previous methods and proposing future research direction toward leveraging the benefits of LLMs while minimizing misuse risks. The paper concludes by acknowledging LLMs as powerful tools with significant implications in both spreading and combating misinformation in the digital age.},
  keywords = {ChatGPT,Computational Social Sciences,Disinformation,Fake news,Feature extraction,Hallucinations in LLMs,Information integrity,Large language models,Large Language Models,Market research,Navigation,Neural networks,Social networking (online),Social sciences},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Shah et al_2024_Navigating the Web of Disinformation and Misinformation.pdf;/Users/thomasgorman/Zotero/storage/4E3HH8YR/10540581.html}
}

@article{shahRoleMachineLearning2022,
  title = {The {{Role}} of {{Machine Learning}} and the {{Internet}} of {{Things}} in {{Smart Buildings}} for {{Energy Efficiency}}},
  author = {Shah, Syed Faisal Abbas and Iqbal, Muhammad and Aziz, Zeeshan and Rana, Toqir A. and Khalid, Adnan and Cheah, Yu-N. and Arif, Muhammad},
  year = {2022},
  month = jan,
  journal = {Applied Sciences},
  volume = {12},
  number = {15},
  pages = {7882},
  publisher = {Multidisciplinary Digital Publishing Institute},
  issn = {2076-3417},
  doi = {10.3390/app12157882},
  urldate = {2024-07-02},
  abstract = {Machine learning can be used to automate a wide range of tasks. Smart buildings, which use the Internet of Things (IoT) to connect building operations, enable activities, such as monitoring temperature, safety, and maintenance, for easier controlling via mobile devices and computers. Smart buildings are becoming core aspects in larger system integrations as the IoT is becoming increasingly widespread. The IoT plays an important role in smart buildings and provides facilities that improve human security by using effective technology-based life-saving strategies. This review highlights the role of IoT devices in smart buildings. The IoT devices platform and its components are highlighted in this review. Furthermore, this review provides security challenges regarding IoT and smart buildings. The main factors pertaining to smart buildings are described and the different methods of machine learning in combination with IoT technologies are also described to improve the effectiveness of smart buildings to make them energy efficient.},
  copyright = {http://creativecommons.org/licenses/by/3.0/},
  langid = {english},
  keywords = {challenges in smart buildings,Internet of Things,IoT applications,machine learning,smart buildings},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Shah et al_2022_The Role of Machine Learning and the Internet of Things in Smart Buildings for.pdf}
}

@misc{shaiTransformersRepresentBelief2024,
  title = {Transformers Represent Belief State Geometry in Their Residual Stream},
  author = {Shai, Adam S. and Marzen, Sarah E. and Teixeira, Lucas and Oldenziel, Alexander Gietelink and Riechers, Paul M.},
  year = {2024},
  month = may,
  number = {arXiv:2405.15943},
  eprint = {2405.15943},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.15943},
  urldate = {2024-07-15},
  abstract = {What computational structure are we building into large language models when we train them on next-token prediction? Here, we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data-generating process. Leveraging the theory of optimal prediction, we anticipate and then find that belief states are linearly represented in the residual stream of transformers, even in cases where the predicted belief state geometry has highly nontrivial fractal structure. We investigate cases where the belief state geometry is represented in the final residual stream or distributed across the residual streams of multiple layers, providing a framework to explain these observations. Furthermore we demonstrate that the inferred belief states contain information about the entire future, beyond the local next-token prediction that the transformers are explicitly trained on. Our work provides a framework connecting the structure of training data to the computational structure and representations that transformers use to carry out their behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Shai et al_2024_Transformers represent belief state geometry in their residual stream.pdf;/Users/thomasgorman/Zotero/storage/MZ9VFDFD/2405.html}
}

@article{sheaUseGPT4Analyze2023,
  title = {Use of {{GPT-4}} to {{Analyze Medical Records}} of {{Patients With Extensive Investigations}} and {{Delayed Diagnosis}}},
  author = {Shea, Yat-Fung and Lee, Cynthia Min Yao and Ip, Whitney Chin Tung and Luk, Dik Wai Anderson and Wong, Stephanie Sze Wing},
  year = {2023},
  month = aug,
  journal = {JAMA Network Open},
  volume = {6},
  number = {8},
  pages = {e2325000},
  issn = {2574-3805},
  doi = {10.1001/jamanetworkopen.2023.25000},
  urldate = {2023-09-12},
  abstract = {This case series investigates whether analysis of clinical history via a language model system improves diagnostic accuracy in patients with complex and delayed diagnoses.},
  pmcid = {PMC10425828},
  pmid = {37578798},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Shea et al_2023_Use of GPT-4 to Analyze Medical Records of Patients With Extensive.pdf}
}

@article{shiffrinProbingPsychologyAI2023,
  title = {Probing the Psychology of {{AI}} Models},
  author = {Shiffrin, Richard and Mitchell, Melanie},
  year = {2023},
  month = mar,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {10},
  pages = {e2300963120},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2300963120},
  urldate = {2024-08-16},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Shiffrin_Mitchell_2023_Probing the psychology of AI models.pdf}
}

@article{shimadaPerformanceFlankerTask2016,
  title = {Performance on the Flanker Task Predicts Driving Cessation in Older Adults},
  author = {Shimada, Hiroyuki and Uemura, Kazuki and Makizako, Hyuma and Doi, Takehiko and Lee, Sangyoon and Suzuki, Takao},
  year = {2016},
  journal = {International Journal of Geriatric Psychiatry},
  volume = {31},
  number = {2},
  pages = {169--175},
  issn = {1099-1166},
  doi = {10.1002/gps.4308},
  urldate = {2021-09-25},
  abstract = {Objectives This study examined the predictive validity of flanker tasks on driving cessation in older drivers. The flanker task comprises a set of response inhibition tests used to assess the ability to suppress responses. Methods A total of 2805 older drivers aged {$\geq$}65 years at baseline participated in this study. We conducted several baseline assessments focused on physical and psychological health as well as cognitive performance. Fifteen months after the baseline measurements, we collected information about the current driving status of the participants. Results Forty-eight participants (1.7\%) ceased driving during the 15-month period following the assessment. Logistic regression models identified the following as significant predictors of driving cessation: performance on the trail-making test (parts A and B), digit symbol substitution test scores, story memory, and flanker task scores for the total, congruent, and incongruent conditions. The flanker task scores for the total, congruent, and incongruent conditions were significant predictors in the fully adjusted logistic model. Conclusion The flanker task was more important than assessments of general cognition, including memory, attention, executive control, and processing speed, in predicting driving cessation. The flanker task may be useful for identifying driving cessation in older adults. Copyright {\copyright} 2015 John Wiley \& Sons, Ltd.},
  langid = {english},
  keywords = {cognitive function,driving cessation,flanker task,older adults,selective attention},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Shimada et al_2016_Performance on the flanker task predicts driving cessation in older adults.pdf;/Users/thomasgorman/Zotero/storage/M8VKFSGU/gps.html}
}

@misc{shiMedAdapterEfficientTestTime2024,
  title = {{{MedAdapter}}: {{Efficient Test-Time Adaptation}} of {{Large Language Models}} towards {{Medical Reasoning}}},
  shorttitle = {{{MedAdapter}}},
  author = {Shi, Wenqi and Xu, Ran and Zhuang, Yuchen and Yu, Yue and Wu, Hang and Yang, Carl and Wang, May D.},
  year = {2024},
  month = may,
  number = {arXiv:2405.03000},
  eprint = {2405.03000},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-05},
  abstract = {Despite their improved capabilities in generation and reasoning, adapting large language models (LLMs) to the biomedical domain remains challenging due to their immense size and corporate privacy. In this work, we propose MedAdapter, a unified post-hoc adapter for test-time adaptation of LLMs towards biomedical applications. Instead of fine-tuning the entire LLM, MedAdapter effectively adapts the original model by fine-tuning only a small BERT-sized adapter to rank candidate solutions generated by LLMs. Experiments demonstrate that MedAdapter effectively adapts both white-box and black-box LLMs in biomedical reasoning, achieving average performance improvements of 25.48\% and 11.31\%, respectively, without requiring extensive computational resources or sharing data with third parties. MedAdapter also yields superior performance when combined with train-time adaptation, highlighting a flexible and complementary solution to existing adaptation methods. Faced with the challenges of balancing model performance, computational resources, and data privacy, MedAdapter provides an efficient, privacy-preserving, cost-effective, and transparent solution for adapting LLMs to the biomedical domain.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Shi et al_2024_MedAdapter.pdf}
}

@misc{siCanLLMsGenerate2024,
  title = {Can {{LLMs Generate Novel Research Ideas}}? {{A Large-Scale Human Study}} with 100+ {{NLP Researchers}}},
  shorttitle = {Can {{LLMs Generate Novel Research Ideas}}?},
  author = {Si, Chenglei and Yang, Diyi and Hashimoto, Tatsunori},
  year = {2024},
  month = sep,
  number = {arXiv:2409.04109},
  eprint = {2409.04109},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2409.04109},
  urldate = {2024-09-09},
  abstract = {Recent advancements in large language models (LLMs) have sparked optimism about their potential to accelerate scientific discovery, with a growing number of works proposing research agents that autonomously generate and validate new ideas. Despite this, no evaluations have shown that LLM systems can take the very first step of producing novel, expert-level ideas, let alone perform the entire research process. We address this by establishing an experimental design that evaluates research idea generation while controlling for confounders and performs the first head-to-head comparison between expert NLP researchers and an LLM ideation agent. By recruiting over 100 NLP researchers to write novel ideas and blind reviews of both LLM and human ideas, we obtain the first statistically significant conclusion on current LLM capabilities for research ideation: we find LLM-generated ideas are judged as more novel (p {$<$} 0.05) than human expert ideas while being judged slightly weaker on feasibility. Studying our agent baselines closely, we identify open problems in building and evaluating research agents, including failures of LLM self-evaluation and their lack of diversity in generation. Finally, we acknowledge that human judgements of novelty can be difficult, even by experts, and propose an end-to-end study design which recruits researchers to execute these ideas into full projects, enabling us to study whether these novelty and feasibility judgements result in meaningful differences in research outcome.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Si et al_2024_Can LLMs Generate Novel Research Ideas.pdf;/Users/thomasgorman/Zotero/storage/9GM9ZLKY/2409.html}
}

@article{sintovUnlockingPotentialSmart2015,
  title = {Unlocking the Potential of Smart Grid Technologies with Behavioral Science},
  author = {Sintov, Nicole D. and Schultz, P. Wesley},
  year = {2015},
  month = apr,
  journal = {Frontiers in Psychology},
  volume = {6},
  publisher = {Frontiers},
  issn = {1664-1078},
  doi = {10.3389/fpsyg.2015.00410},
  urldate = {2024-07-02},
  abstract = {{$<$}p{$>$}Smart grid systems aim to provide a more stable and adaptable electricity infrastructure, and to maximize energy efficiency. Grid-linked technologies vary widely in form and function, but generally share common potentials: to reduce energy consumption via efficiency and/or curtailment, to shift use to off-peak times of day, and to enable distributed storage and generation options. Although end users are central players in these systems, they are sometimes not central considerations in technology or program design, and in some cases, their motivations for participating in such systems are not fully appreciated. Behavioral science can be instrumental in engaging end-users and maximizing the impact of smart grid technologies. In this paper, we present emerging technologies made possible by a smart grid infrastructure, and for each we highlight ways in which behavioral science can be applied to enhance their impact on energy savings.{$<$}/p{$>$}},
  langid = {english},
  keywords = {Behavioral Science,Energy conservation,energy efficiency,human factors,Smart Grid,Technology Adoption},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Sintov_Schultz_2015_Unlocking the potential of smart grid technologies with behavioral science.pdf}
}

@article{slossPatientCaregiverPerceptions2024,
  title = {Patient and {{Caregiver Perceptions}} of an {{Interface Design}} to {{Communicate Artificial Intelligence}}--{{Based Prognosis}} for {{Patients With Advanced Solid Tumors}}},
  author = {Sloss, Elizabeth A. and McPherson, Jordan P. and Beck, Anna C. and Guo, Jia-Wen and Scheese, Carolyn H. and Flake, Naomi R. and Chalkidis, George and Staes, Catherine J.},
  year = {2024},
  month = apr,
  journal = {JCO Clinical Cancer Informatics},
  number = {8},
  pages = {e2300187},
  issn = {2473-4276},
  doi = {10.1200/CCI.23.00187},
  urldate = {2024-09-10},
  abstract = {PURPOSE               Use of artificial intelligence (AI) in cancer care is increasing. What remains unclear is how best to design patient-facing systems that communicate AI output. With oncologist input, we designed an interface that presents patient-specific, machine learning--based 6-month survival prognosis information designed to aid oncology providers in preparing for and discussing prognosis with patients with advanced solid tumors and their caregivers. The primary purpose of this study was to assess patient and caregiver perceptions and identify enhancements of the interface for communicating 6-month survival and other prognosis information when making treatment decisions concerning anticancer and supportive therapy.                                         METHODS               This qualitative study included interviews and focus groups conducted between November and December 2022. Purposive sampling was used to recruit former patients with cancer and/or former caregivers of patients with cancer who had participated in cancer treatment decisions from Utah or elsewhere in the United States. Categories and themes related to perceptions of the interface were identified.                                         RESULTS               We received feedback from 20 participants during eight individual interviews and two focus groups, including four cancer survivors, 13 caregivers, and three representing both. Overall, most participants expressed positive perceptions about the tool and identified its value for supporting decision making, feeling less alone, and supporting communication among oncologists, patients, and their caregivers. Participants identified areas for improvement and implementation considerations, particularly that oncologists should share the tool and guide discussions about prognosis with patients who want to receive the information.                                         CONCLUSION               This study revealed important patient and caregiver perceptions of and enhancements for the proposed interface. Originally designed with input from oncology providers, patient and caregiver participants identified additional interface design recommendations and implementation considerations to support communication about prognosis.                        ,              Cancer patient and caregiver perceptions affect interface design for sharing AI-based prognosis.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Sloss et al_2024_Patient and Caregiver Perceptions of an Interface Design to Communicate.pdf}
}

@article{srinivasanComparingMergingBehaviors2021,
  title = {Comparing Merging Behaviors Observed in Naturalistic Data with Behaviors Generated by a Machine Learned Model},
  author = {Srinivasan, Aravinda Ramakrishnan and Hasan, Mohamed and Lin, Yi-Shin and Leonetti, Matteo and Billington, Jac and Romano, Richard and Markkula, Gustav},
  year = {2021},
  month = sep,
  journal = {2021 IEEE International Intelligent Transportation Systems Conference (ITSC)},
  eprint = {2104.10496},
  pages = {3787--3792},
  doi = {10.1109/ITSC48978.2021.9564791},
  urldate = {2022-04-14},
  abstract = {There is quickly growing literature on machine-learned models that predict human driving trajectories in road traffic. These models focus their learning on low-dimensional error metrics, for example average distance between model-generated and observed trajectories. Such metrics permit relative comparison of models, but do not provide clearly interpretable information on how close to human behavior the models actually come, for example in terms of higher-level behavior phenomena that are known to be present in human driving. We study highway driving as an example scenario, and introduce metrics to quantitatively demonstrate the presence, in a naturalistic dataset, of two familiar behavioral phenomena: (1) The kinematics-dependent contest, between on-highway and on-ramp vehicles, of who passes the merging point first. (2) Courtesy lane changes away from the outermost lane, to leave space for a merging vehicle. Applying the exact same metrics to the output of a state-of-the-art machine-learned model, we show that the model is capable of reproducing the former phenomenon, but not the latter. We argue that this type of behavioral analysis provides information that is not available from conventional model-fitting metrics, and that it may be useful to analyze (and possibly fit) models also based on these types of behavioral criteria.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Srinivasan et al_2021_Comparing merging behaviors observed in naturalistic data with behaviors.pdf;/Users/thomasgorman/Zotero/storage/JLHK6WXC/2104.html}
}

@article{stadlerCognitiveEaseCost2024,
  title = {Cognitive Ease at a Cost: {{LLMs}} Reduce Mental Effort but Compromise Depth in Student Scientific Inquiry},
  shorttitle = {Cognitive Ease at a Cost},
  author = {Stadler, Matthias and Bannert, Maria and Sailer, Michael},
  year = {2024},
  month = nov,
  journal = {Computers in Human Behavior},
  volume = {160},
  pages = {108386},
  issn = {0747-5632},
  doi = {10.1016/j.chb.2024.108386},
  urldate = {2024-08-16},
  abstract = {This study explores the cognitive load and learning outcomes associated with using large language models (LLMs) versus traditional search engines for information gathering during learning. A total of 91 university students were randomly assigned to either use ChatGPT3.5 or Google to research the socio-scientific issue of nanoparticles in sunscreen to derive valid recommendations and justifications. The study aimed to investigate potential differences in cognitive load, as well as the quality and homogeneity of the students' recommendations and justifications. Results indicated that students using LLMs experienced significantly lower cognitive load. However, despite this reduction, these students demonstrated lower-quality reasoning and argumentation in their final recommendations compared to those who used traditional search engines. Further, the homogeneity of the recommendations and justifications did not differ significantly between the two groups, suggesting that LLMs did not restrict the diversity of students' perspectives. These findings highlight the nuanced implications of digital tools on learning, suggesting that while LLMs can decrease the cognitive burden associated with information gathering during a learning task, they may not promote deeper engagement with content necessary for high-quality learning per se.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Stadler et al_2024_Cognitive ease at a cost.pdf;/Users/thomasgorman/Zotero/storage/KY7F2M4X/S0747563224002541.html}
}

@article{steinert-threlkeldEaseLearningExplains2020,
  title = {Ease of Learning Explains Semantic Universals},
  author = {{Steinert-Threlkeld}, Shane and Szymanik, Jakub},
  year = {2020},
  month = feb,
  journal = {Cognition},
  volume = {195},
  pages = {104076},
  issn = {0010-0277},
  doi = {10.1016/j.cognition.2019.104076},
  urldate = {2022-01-21},
  abstract = {Semantic universals are properties of meaning shared by the languages of the world. We offer an explanation of the presence of such universals by measuring simplicity in terms of ease of learning, showing that expressions satisfying universals are simpler than those that do not according to this criterion. We measure ease of learning using tools from machine learning and analyze universals in a domain of function words (quantifiers) and content words (color terms). Our results provide strong evidence that semantic universals across both function and content words reflect simplicity as measured by ease of learning.},
  langid = {english},
  keywords = {Python code},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Steinert-Threlkeld_Szymanik_2020_Ease of learning explains semantic universals.pdf;/Users/thomasgorman/Zotero/storage/HX8L2BE9/S0010027719302495.html}
}

@article{stellaUsingCognitivePsychology2023,
  title = {Using Cognitive Psychology to Understand {{GPT-like}} Models Needs to Extend beyond Human Biases},
  author = {Stella, Massimo and Hills, Thomas T. and Kenett, Yoed N.},
  year = {2023},
  month = oct,
  journal = {Proceedings of the National Academy of Sciences},
  volume = {120},
  number = {43},
  pages = {e2312911120},
  publisher = {Proceedings of the National Academy of Sciences},
  doi = {10.1073/pnas.2312911120},
  urldate = {2023-11-19},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Stella et al_2023_Using cognitive psychology to understand GPT-like models needs to extend beyond.pdf}
}

@misc{steyversCalibrationGapModel2024,
  title = {The {{Calibration Gap}} between {{Model}} and {{Human Confidence}} in {{Large Language Models}}},
  author = {Steyvers, Mark and Tejeda, Heliodoro and Kumar, Aakriti and Belem, Catarina and Karny, Sheer and Hu, Xinyue and Mayer, Lukas and Smyth, Padhraic},
  year = {2024},
  month = jan,
  number = {arXiv:2401.13835},
  eprint = {2401.13835},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-30},
  abstract = {For large language models (LLMs) to be trusted by humans they need to be well-calibrated in the sense that they can accurately assess and communicate how likely it is that their predictions are correct. Recent work has focused on the quality of internal LLM confidence assessments, but the question remains of how well LLMs can communicate this internal model confidence to human users. This paper explores the disparity between external human confidence in an LLM's responses and the internal confidence of the model. Through experiments involving multiple-choice questions, we systematically examine human users' ability to discern the reliability of LLM outputs. Our study focuses on two key areas: (1) assessing users' perception of true LLM confidence and (2) investigating the impact of tailored explanations on this perception. The research highlights that default explanations from LLMs often lead to user overestimation of both the model's confidence and its' accuracy. By modifying the explanations to more accurately reflect the LLM's internal confidence, we observe a significant shift in user perception, aligning it more closely with the model's actual confidence levels. This adjustment in explanatory approach demonstrates potential for enhancing user trust and accuracy in assessing LLM outputs. The findings underscore the importance of transparent communication of confidence levels in LLMs, particularly in high-stakes applications where understanding the reliability of AI-generated information is essential.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Steyvers et al_2024_The Calibration Gap between Model and Human Confidence in Large Language Models.pdf;/Users/thomasgorman/Zotero/storage/MKS262KD/2401.html}
}

@article{strachanTestingTheoryMind2024,
  title = {Testing Theory of Mind in Large Language Models and Humans},
  author = {Strachan, James W. A. and Albergo, Dalila and Borghini, Giulia and Pansardi, Oriana and Scaliti, Eugenio and Gupta, Saurabh and Saxena, Krati and Rufo, Alessandro and Panzeri, Stefano and Manzi, Guido and Graziano, Michael S. A. and Becchio, Cristina},
  year = {2024},
  month = may,
  journal = {Nature Human Behaviour},
  pages = {1--11},
  issn = {2397-3374},
  doi = {10.1038/s41562-024-01882-z},
  urldate = {2024-05-26},
  abstract = {At the core of what defines us as humans is the concept of theory of mind: the ability to track other people's mental states. The recent development of large language models (LLMs) such as ChatGPT has led to intense debate about the possibility that these models exhibit behaviour that is indistinguishable from human behaviour in theory of mind tasks. Here we compare human and LLM performance on a comprehensive battery of measurements that aim to measure different theory of mind abilities, from understanding false beliefs to interpreting indirect requests and recognizing irony and faux pas. We tested two families of LLMs (GPT and LLaMA2) repeatedly against these measures and compared their performance with those from a sample of 1,907 human participants. Across the battery of theory of mind tests, we found that GPT-4 models performed at, or even sometimes above, human levels at identifying indirect requests, false beliefs and misdirection, but struggled with detecting faux pas. Faux pas, however, was the only test where LLaMA2 outperformed humans. Follow-up manipulations of the belief likelihood revealed that the superiority of LLaMA2 was illusory, possibly reflecting a bias towards attributing ignorance. By contrast, the poor performance of GPT originated from a hyperconservative approach towards committing to conclusions rather than from a genuine failure of inference. These findings not only demonstrate that LLMs exhibit behaviour that is consistent with the outputs of mentalistic inference in humans but also highlight the importance of systematic testing to ensure a non-superficial comparison between human and artificial intelligences.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Technology},
  annotation = {https://osf.io/fwj6v/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Strachan et al_2024_Testing theory of mind in large language models and humans.pdf}
}

@misc{sucholutskyGettingAlignedRepresentational2023,
  title = {Getting Aligned on Representational Alignment},
  author = {Sucholutsky, Ilia and Muttenthaler, Lukas and Weller, Adrian and Peng, Andi and Bobu, Andreea and Kim, Been and Love, Bradley C. and Grant, Erin and Groen, Iris and Achterberg, Jascha and Tenenbaum, Joshua B. and Collins, Katherine M. and Hermann, Katherine L. and Oktar, Kerem and Greff, Klaus and Hebart, Martin N. and Jacoby, Nori and Zhang, Qiuyi and Marjieh, Raja and Geirhos, Robert and Chen, Sherol and Kornblith, Simon and Rane, Sunayana and Konkle, Talia and O'Connell, Thomas P. and Unterthiner, Thomas and Lampinen, Andrew K. and M{\"u}ller, Klaus-Robert and Toneva, Mariya and Griffiths, Thomas L.},
  year = {2023},
  month = nov,
  number = {arXiv:2310.13018},
  eprint = {2310.13018},
  primaryclass = {cs, q-bio},
  publisher = {arXiv},
  urldate = {2024-09-07},
  abstract = {Biological and artificial information processing systems form representations of the world that they can use to categorize, reason, plan, navigate, and make decisions. How can we measure the extent to which the representations formed by these diverse systems agree? Do similarities in representations then translate into similar behavior? And if so, then how can a system's representations be modified to better match those of another system? These questions pertaining to the study of representational alignment are at the heart of some of the most active research areas in contemporary cognitive science, neuroscience, and machine learning. For example, cognitive scientists seek to measure the representational alignment of multiple individuals to identify shared cognitive priors, neuroscientists seek to align fMRI responses from multiple individuals into a shared representational space to boost the signal for group-level analyses, and machine learning researchers seek to distill knowledge from large teacher models into small student models by increasing their representational alignment. Unfortunately, there is limited knowledge transfer between research communities interested in representational alignment, so progress in one field often ends up being rediscovered independently in another. Thus, greater cross-field communication would be advantageous. To improve communication between fields, we propose a unifying framework that can serve as a common language between researchers studying representational alignment. We survey the literature from the fields of cognitive science, neuroscience, and machine learning, and demonstrate how prior work fits into this framework. Finally, we lay out open problems in representational alignment where progress can benefit all three of these fields. We hope that our work can catalyze cross-disciplinary collaboration and accelerate progress for all communities studying and developing information processing systems. We note that this is a working paper and encourage readers to reach out with their suggestions for future revisions.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing,Quantitative Biology - Neurons and Cognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Sucholutsky et al_2023_Getting aligned on representational alignment.pdf}
}

@misc{sumersCognitiveArchitecturesLanguage2023,
  title = {Cognitive {{Architectures}} for {{Language Agents}}},
  author = {Sumers, Theodore and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L.},
  year = {2023},
  month = sep,
  number = {arXiv:2309.02427},
  eprint = {2309.02427},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2309.02427},
  urldate = {2023-09-12},
  abstract = {Recent efforts have incorporated large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning. However, these efforts have largely been piecemeal, lacking a systematic framework for constructing a fully-fledged language agent. To address this challenge, we draw on the rich history of agent design in symbolic artificial intelligence to develop a blueprint for a new wave of cognitive language agents. We first show that LLMs have many of the same properties as production systems, and recent efforts to improve their grounding or reasoning mirror the development of cognitive architectures built around production systems. We then propose Cognitive Architectures for Language Agents (CoALA), a conceptual framework to systematize diverse methods for LLM-based reasoning, grounding, learning, and decision making as instantiations of language agents in the framework. Finally, we use the CoALA framework to highlight gaps and propose actionable directions toward more capable language agents in the future.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Symbolic Computation},
  annotation = {https://github.com/ysymyth/awesome-language-agents},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Sumers et al_2023_Cognitive Architectures for Language Agents.pdf;/Users/thomasgorman/Zotero/storage/AAPS7WR9/2309.html}
}

@article{sunCanCognitiveArchitecture,
  title = {Can {{A Cognitive Architecture Fundamentally Enhance LLMs}}? {{Or Vice Versa}}?},
  author = {Sun, Ron},
  abstract = {The paper discusses what is needed to address the limitations of current LLM-centered AI systems. The paper argues that incorporating insights from human cognition and psychology, as embodied by a computational cognitive architecture, can help develop systems that are more capable, more reliable, and more human-like. It emphasizes the importance of the dual-process architecture and the hybrid neuro-symbolic approach in addressing the limitations of current LLMs. In the opposite direction, the paper also highlights the need for an overhaul of computational cognitive architectures to better reflect advances in AI and computing technology. Overall, the paper advocates for a multidisciplinary, mutually beneficial approach towards developing better models both for AI and for understanding the human mind.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Sun_Can A Cognitive Architecture Fundamentally Enhance LLMs.pdf}
}

@misc{sunDecodingSilentMajority2023,
  title = {Decoding the {{Silent Majority}}: {{Inducing Belief Augmented Social Graph}} with {{Large Language Model}} for {{Response Forecasting}}},
  shorttitle = {Decoding the {{Silent Majority}}},
  author = {Sun, Chenkai and Li, Jinning and Fung, Yi R. and Chan, Hou Pong and Abdelzaher, Tarek and Zhai, ChengXiang and Ji, Heng},
  year = {2023},
  month = oct,
  number = {arXiv:2310.13297},
  eprint = {2310.13297},
  publisher = {arXiv},
  urldate = {2024-04-07},
  abstract = {Automatic response forecasting for news media plays a crucial role in enabling content producers to efficiently predict the impact of news releases and prevent unexpected negative outcomes such as social conflict and moral injury. To effectively forecast responses, it is essential to develop measures that leverage the social dynamics and contextual information surrounding individuals, especially in cases where explicit profiles or historical actions of the users are limited (referred to as lurkers). As shown in a previous study, 97\% of all tweets are produced by only the most active 25\% of users. However, existing approaches have limited exploration of how to best process and utilize these important features. To address this gap, we propose a novel framework, named SocialSense, that leverages a large language model to induce a belief-centered graph on top of an existent social network, along with graph-based propagation to capture social dynamics. We hypothesize that the induced graph that bridges the gap between distant users who share similar beliefs allows the model to effectively capture the response patterns. Our method surpasses existing state-of-the-art in experimental evaluations for both zero-shot and supervised settings, demonstrating its effectiveness in response forecasting. Moreover, the analysis reveals the framework's capability to effectively handle unseen user and lurker scenarios, further highlighting its robustness and practical applicability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/chenkaisun/SocialSense},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Sun et al_2023_Decoding the Silent Majority.pdf;/Users/thomasgorman/Zotero/storage/EM2TIP35/2310.html}
}

@article{sunRandomSiliconSampling,
  title = {Random {{Silicon Sampling}}: {{Simulating Human Sub-Population Opinion Using}} a {{Large Language Model Based}} on {{Group-Level Demographic Information}}},
  author = {Sun, Seungjong and Lee, Eungu and Nan, Dongyan and Zhao, Xiangying and Lee, Wonbyung and Jansen, Bernard J and Kim, Jang Hyun},
  abstract = {Large language models exhibit societal biases associated with demographic information, including race, gender, and others. Endowing such language models with personalities based on demographic data can enable generating opinions that align with those of humans. Building on this idea, we propose "random silicon sampling," a method to emulate the opinions of the human population subgroup. Our study analyzed 1) a language model that generates the survey responses that correspond with a human group based solely on its demographic distribution and 2) the applicability of our methodology across various demographic subgroups and thematic questions. Through random silicon sampling and using only group-level demographic information, we discovered that language models can generate response distributions that are remarkably similar to the actual U.S. public opinion polls. Moreover, we found that the replicability of language models varies depending on the demographic group and topic of the question, and this can be attributed to inherent societal biases in the models. Our findings demonstrate the feasibility of mirroring a group's opinion using only demographic distribution and elucidate the effect of social biases in language models on such simulations.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Sun et al_Random Silicon Sampling.pdf}
}

@misc{sunTrustingSearchUnraveling2024,
  title = {Trusting the {{Search}}: {{Unraveling Human Trust}} in {{Health Information}} from {{Google}} and {{ChatGPT}}},
  shorttitle = {Trusting the {{Search}}},
  author = {Sun, Xin and Ma, Rongjun and Zhao, Xiaochang and Li, Zhuying and Lindqvist, Janne and Ali, Abdallah El and Bosch, Jos A.},
  year = {2024},
  month = mar,
  number = {arXiv:2403.09987},
  eprint = {2403.09987},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-23},
  abstract = {People increasingly rely on online sources for health information seeking due to their convenience and timeliness, traditionally using search engines like Google as the primary search agent. Recently, the emergence of generative Artificial Intelligence (AI) has made Large Language Model (LLM) powered conversational agents such as ChatGPT a viable alternative for health information search. However, while trust is crucial for adopting the online health advice, the factors influencing people's trust judgments in health information provided by LLM-powered conversational agents remain unclear. To address this, we conducted a mixed-methods, within-subjects lab study (N=21) to explore how interactions with different agents (ChatGPT vs. Google) across three health search tasks influence participants' trust judgments of the search results as well as the search agents themselves. Our key findings showed that: (a) participants' trust levels in ChatGPT were significantly higher than Google in the context of health information seeking; (b) there is a significant correlation between trust in health-related information and trust in the search agent, however only for Google; (c) the type of search tasks did not affect participants' perceived trust; and (d) participants' prior knowledge, the style of information presentation, and the interactive manner of using search agents were key determinants of trust in the health-related information. Our study taps into differences in trust perceptions when using traditional search engines compared to LLM-powered conversational agents. We highlight the potential role LLMs play in health-related information-seeking contexts, where they excel as stepping stones for further search. We contribute key factors and considerations for ensuring effective and reliable personal health information seeking in the age of generative AI.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Human-Computer Interaction,F.2.2 I.2.7},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Sun et al_2024_Trusting the Search.pdf}
}

@inproceedings{sureshConceptualStructureCoheres2023,
  title = {Conceptual Structure Coheres in Human Cognition but Not in Large Language Models},
  booktitle = {Proceedings of the 2023 {{Conference}} on {{Empirical Methods}} in {{Natural Language Processing}}},
  author = {Suresh, Siddharth and Mukherjee, Kushin and Yu, Xizheng and Huang, Wei-Chun and Padua, Lisa and Rogers, Timothy},
  editor = {Bouamor, Houda and Pino, Juan and Bali, Kalika},
  year = {2023},
  month = dec,
  pages = {722--738},
  publisher = {Association for Computational Linguistics},
  address = {Singapore},
  doi = {10.18653/v1/2023.emnlp-main.47},
  urldate = {2024-06-06},
  abstract = {Neural network models of language have long been used as a tool for developing hypotheses about conceptual representation in the mind and brain. For many years, such use involved extracting vector-space representations of words and using distances among these to predict or understand human behavior in various semantic tasks. In contemporary language models, however, it is possible to interrogate the latent structure of conceptual representations using methods nearly identical to those commonly used with human participants. The current work uses three common techniques borrowed from cognitive psychology to estimate and compare lexical-semantic structure in both humans and a well-known large language model, the DaVinci variant of GPT-3. In humans, we show that conceptual structure is robust to differences in culture, language, and method of estimation. Structures estimated from the LLM behavior, while individually fairly consistent with those estimated from human behavior, depend much more upon the particular task used to generate behavior responses--responses generated by the very same model in the three tasks yield estimates of conceptual structure that cohere less with one another than do human structure estimates. The results suggest one important way that knowledge inhering in contemporary LLMs can differ from human cognition.},
  annotation = {https://github.com/siddsuresh97/conceptual\_structure\_emnlp\_2023},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Suresh et al_2023_Conceptual structure coheres in human cognition but not in large language models.pdf}
}

@article{suriLargeLanguageModels2024,
  title = {Do Large Language Models Show Decision Heuristics Similar to Humans? {{A}} Case Study Using {{GPT-35}}},
  shorttitle = {Do Large Language Models Show Decision Heuristics Similar to Humans?},
  author = {Suri, Gaurav and Slater, Lily R. and Ziaee, Ali and Nguyen, Morgan},
  year = {2024},
  month = apr,
  journal = {Journal of Experimental Psychology: General},
  volume = {153},
  number = {4},
  pages = {1066--1075},
  issn = {0096-3445},
  doi = {10.1037/xge0001547},
  urldate = {2024-06-29},
  abstract = {A Large Language Model (LLM) is an artificial intelligence system trained on vast amounts of natural language data, enabling it to generate human-like responses to written or spoken language input. Generative Pre-Trained Transformer (GPT)-3.5 is an example of an LLM that supports a conversational agent called ChatGPT. In this work, we used a series of novel prompts to determine whether ChatGPT shows heuristics and other context-sensitive responses. We also tested the same prompts on human participants. Across four studies, we found that ChatGPT was influenced by random anchors in making estimates (anchoring, Study 1); it judged the likelihood of two events occurring together to be higher than the likelihood of either event occurring alone, and it was influenced by anecdotal information (representativeness and availability heuristic, Study 2); it found an item to be more efficacious when its features were presented positively rather than negatively---even though both presentations contained statistically equivalent information (framing effect, Study 3); and it valued an owned item more than a newly found item even though the two items were objectively identical (endowment effect, Study 4). In each study, human participants showed similar effects. Heuristics and context-sensitive responses in humans are thought to be driven by cognitive and affective processes such as loss aversion and effort reduction. The fact that an LLM---which lacks these processes---also shows such responses invites consideration of the possibility that language is sufficiently rich to carry these effects and may play a role in generating these effects in humans. (PsycInfo Database Record (c) 2024 APA, all rights reserved)},
  keywords = {Artificial Intelligence,artificial intelligence system,ChatGPT 3.5,Cognitive Computing,conversational agent,decision heuristics,Decision Making,Heuristics,human-like responses,Large Language Models,natural language processing,Natural Language Processing,Responses},
  annotation = {https://osf.io/kx69y/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Suri et al_2024_Do large language models show decision heuristics similar to humans.pdf}
}

@misc{swaminathanSchemalearningRebindingMechanisms2023,
  title = {Schema-Learning and Rebinding as Mechanisms of in-Context Learning and Emergence},
  author = {Swaminathan, Sivaramakrishnan and Dedieu, Antoine and Raju, Rajkumar Vasudeva and Shanahan, Murray and {Lazaro-Gredilla}, Miguel and George, Dileep},
  year = {2023},
  month = jun,
  number = {arXiv:2307.01201},
  eprint = {2307.01201},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-03},
  abstract = {In-context learning (ICL) is one of the most powerful and most unexpected capabilities to emerge in recent transformer-based large language models (LLMs). Yet the mechanisms that underlie it are poorly understood. In this paper, we demonstrate that comparable ICL capabilities can be acquired by an alternative sequence prediction learning method using clone-structured causal graphs (CSCGs). Moreover, a key property of CSCGs is that, unlike transformer-based LLMs, they are \{{\textbackslash}em interpretable\}, which considerably simplifies the task of explaining how ICL works. Specifically, we show that it uses a combination of (a) learning template (schema) circuits for pattern completion, (b) retrieving relevant templates in a context-sensitive manner, and (c) rebinding of novel tokens to appropriate slots in the templates. We go on to marshall evidence for the hypothesis that similar mechanisms underlie ICL in LLMs. For example, we find that, with CSCGs as with LLMs, different capabilities emerge at different levels of overparameterization, suggesting that overparameterization helps in learning more complex template (schema) circuits. By showing how ICL can be achieved with small models and datasets, we open up a path to novel architectures, and take a vital step towards a more general understanding of the mechanics behind this important capability.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Swaminathan et al_2023_Schema-learning and rebinding as mechanisms of in-context learning and emergence.pdf;/Users/thomasgorman/Zotero/storage/G7PBURS3/2307.html}
}

@article{szewczykContextbasedFacilitationSemantic2022,
  title = {Context-Based Facilitation of Semantic Access Follows Both Logarithmic and Linear Functions of Stimulus Probability},
  author = {Szewczyk, Jakub M. and Federmeier, Kara D.},
  year = {2022},
  month = apr,
  journal = {Journal of Memory and Language},
  volume = {123},
  pages = {104311},
  issn = {0749-596X},
  doi = {10.1016/j.jml.2021.104311},
  urldate = {2023-08-15},
  abstract = {Stimuli are easier to process when context makes them predictable, but does context-based facilitation arise from preactivation of a limited set of relatively probable upcoming stimuli (with facilitation then linearly related to probability) or, instead, because the system maintains and updates a probability distribution across all items (with facilitation logarithmically related to probability)? We measured the N400, an index of semantic access, to words of varying probability, including unpredictable words. Word predictability was measured using both cloze probabilities and a state-of-the-art machine learning language model (GPT-2). We reanalyzed five datasets (n~=~138) to demonstrate and then replicate that context-based facilitation on the N400 is graded, even among unpredictable words. Furthermore, we established that the relationship between word predictability and context-based facilitation combines linear and logarithmic functions. We argue that this composite function reveals properties of the mapping between words and semantic features and how feature- and word-related information is activated on-line.},
  keywords = {Context-based facilitation,GPT-2,N400,Semantic access},
  annotation = {https://osf.io/urvax/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Szewczyk_Federmeier_2022_Context-based facilitation of semantic access follows both logarithmic and.pdf;/Users/thomasgorman/Zotero/storage/6RTCPDCR/S0749596X21000942.html}
}

@article{tanComputationalCognitiveModel2024,
  title = {A {{Computational Cognitive Model}} of {{Driver Response Time}} for {{Scheduled Freeway Exiting Takeovers}} in {{Conditionally Automated Vehicles}}},
  author = {Tan, Xiaomei and Zhang, Yiqi},
  year = {2024},
  month = may,
  journal = {Human Factors},
  volume = {66},
  number = {5},
  pages = {1583--1599},
  publisher = {SAGE Publications Inc},
  issn = {0018-7208},
  doi = {10.1177/00187208221143028},
  urldate = {2024-09-16},
  abstract = {ObjectiveThis study develops a computational model to predict drivers? response time and understand the underlying cognitive mechanism for freeway exiting takeovers in conditionally automated vehicles (AVs).BackgroundPrevious research has modeled drivers? takeover response time in emergency scenarios that demand a quick response. However, existing models may not be applicable for scheduled, non-time-critical takeovers as drivers take longer to resume control when there is no time pressure. A model of driver response time in non-time-critical takeovers is lacking.MethodA computational cognitive model of driver takeover response time is developed based on Queuing Network-Model Human Processor (QN-MHP) architecture. The model quantifies gaze redirection in response to takeover request (ToR), task prioritization, driver situation awareness, and driver trust to address the complexities of drivers' takeover strategies when sufficient time budget exists.ResultsExperimental data of a preliminary driving simulator study were used to validate the model. The model accounted for 97\% of the experimental takeover response time for freeway exiting.ConclusionThe current model can successfully predict drivers? response time for scheduled, non-time-critical freeway exiting takeovers in conditionally AVs.ApplicationThis model can be applied to the human-machine interface design with respect to ToR lead time for enhancing safe freeway exiting takeovers in conditionally AVs. It also provides a foundation for future modeling work towards an integrated driver model of freeway exiting takeover performance.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Tan_Zhang_2024_A Computational Cognitive Model of Driver Response Time for Scheduled Freeway.pdf}
}

@misc{tangEvaluationEstimativeUncertainty2024,
  title = {An {{Evaluation}} of {{Estimative Uncertainty}} in {{Large Language Models}}},
  author = {Tang, Zhisheng and Shen, Ke and Kejriwal, Mayank},
  year = {2024},
  month = may,
  number = {arXiv:2405.15185},
  eprint = {2405.15185},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-03},
  abstract = {Words of estimative probability (WEPs), such as ``maybe'' or ``probably not'' are ubiquitous in natural language for communicating estimative uncertainty, compared with direct statements involving numerical probability. Human estimative uncertainty, and its calibration with numerical estimates, has long been an area of study -- including by intelligence agencies like the CIA. This study compares estimative uncertainty in commonly used large language models (LLMs) like GPT-4 and ERNIE-4 to that of humans, and to each other. Here we show that LLMs like GPT-3.5 and GPT-4 align with human estimates for some, but not all, WEPs presented in English. Divergence is also observed when the LLM is presented with gendered roles and Chinese contexts. Further study shows that an advanced LLM like GPT-4 can consistently map between statistical and estimative uncertainty, but a significant performance gap remains. The results contribute to a growing body of research on human-LLM alignment.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Tang et al_2024_An Evaluation of Estimative Uncertainty in Large Language Models.pdf}
}

@article{tangoRealTimeDetectionSystem2013,
  title = {Real-{{Time Detection System}} of {{Driver Distraction Using Machine Learning}}},
  author = {Tango, Fabio and Botta, Marco},
  year = {2013},
  month = jun,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {14},
  number = {2},
  pages = {894--905},
  issn = {1558-0016},
  doi = {10.1109/TITS.2013.2247760},
  urldate = {2024-09-16},
  abstract = {There is accumulating evidence that driver distraction is a leading cause of vehicle crashes and incidents. In particular, increased use of so-called in-vehicle information systems (IVIS) and partially autonomous driving assistance systems (PADAS) have raised important and growing safety concerns. Thus, detecting the driver's state is of paramount importance, to adapt IVIS and PADAS accordingly, therefore avoiding or mitigating their possible negative effects. The purpose of this paper is to show a method for the nonintrusive and real-time detection of visual distraction, using vehicle dynamics data and without using the eye-tracker data as inputs to classifiers. Specifically, we present and compare different models that are based on well-known machine learning (ML) methods. Data for training the models were collected using a static driving simulator, with real human subjects performing a specific secondary task [i.e., a surrogate visual research task (SURT)] while driving. Different training methods, model characteristics, and feature selection criteria have been compared. Based on our results, using a support vector machine (SVM) has outperformed all the other ML methods, providing the highest classification rate for most of the subjects. Potential applications of this paper include the design of an adaptive IVIS and of a ``smarter'' PADAS.},
  keywords = {Accident prevention,artificial intelligence and machine learning (ML),Artificial neural networks,driver distraction and inattention,Fuzzy logic,intelligent supporting systems,Real-time systems,Safety,Training,Vehicles,Visualization},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Tango_Botta_2013_Real-Time Detection System of Driver Distraction Using Machine Learning.pdf;/Users/thomasgorman/Zotero/storage/QBGZX2WD/6484166.html}
}

@misc{taubenfeldSystematicBiasesLLM2024,
  title = {Systematic {{Biases}} in {{LLM Simulations}} of {{Debates}}},
  author = {Taubenfeld, Amir and Dover, Yaniv and Reichart, Roi and Goldstein, Ariel},
  year = {2024},
  month = feb,
  number = {arXiv:2402.04049},
  eprint = {2402.04049},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-02-10},
  abstract = {Recent advancements in natural language processing, especially the emergence of Large Language Models (LLMs), have opened exciting possibilities for constructing computational simulations designed to replicate human behavior accurately. However, LLMs are complex statistical learners without straightforward deductive rules, making them prone to unexpected behaviors. In this study, we highlight the limitations of LLMs in simulating human interactions, particularly focusing on LLMs' ability to simulate political debates. Our findings indicate a tendency for LLM agents to conform to the model's inherent social biases despite being directed to debate from certain political perspectives. This tendency results in behavioral patterns that seem to deviate from well-established social dynamics among humans. We reinforce these observations using an automatic self-fine-tuning method, which enables us to manipulate the biases within the LLM and demonstrate that agents subsequently align with the altered biases. These results underscore the need for further research to develop methods that help agents overcome these biases, a critical step toward creating more realistic simulations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Taubenfeld et al_2024_Systematic Biases in LLM Simulations of Debates.pdf;/Users/thomasgorman/Zotero/storage/CK6MFAWC/2402.html}
}

@inproceedings{tawariComputationalFrameworkDrivers2017,
  title = {A Computational Framework for Driver's Visual Attention Using a Fully Convolutional Architecture},
  booktitle = {2017 {{IEEE Intelligent Vehicles Symposium}} ({{IV}})},
  author = {Tawari, Ashish and Kang, Byeongkeun},
  year = {2017},
  month = jun,
  pages = {887--894},
  doi = {10.1109/IVS.2017.7995828},
  urldate = {2024-09-16},
  abstract = {It is a challenging and important task to perceive and interact with other traffic participants in a complex driving environment. The human vision system plays one of the crucial roles to achieve this task. Particularly, visual attention mechanisms allow a human driver to cleverly attend to the salient and relevant regions of the scene to further make necessary decisions for the safe driving. Thus, it is significant to investigate human vision systems with great potential to improve assistive, and even autonomous, vehicular technologies. In this paper, we investigate driver's gaze behavior to understand visual attention. We, first, present a Bayesian framework to model visual attention of a human driver. Further, based on the framework, we develop a fully convolutional neural network to estimate the salient region in a novel driving scene. We systematically evaluate the proposed method using on-road driving data and compare it with other state-of-the-art saliency estimation approaches. Our analyses show promising results.},
  keywords = {Bayes methods,Deconvolution,Kernel,Mathematical model,Neural networks,Vehicles,Visualization},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Tawari_Kang_2017_A computational framework for driver's visual attention using a fully.pdf;/Users/thomasgorman/Zotero/storage/FMC6AE9L/7995828.html}
}

@article{teigenPersuasivenessArgumentsAIsource,
  title = {Persuasiveness of Arguments with {{AI-source}} Labels},
  author = {Teigen, Cassandra},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Teigen_Persuasiveness of arguments with AI-source labels.pdf}
}

@article{testoniEfficiencyQuestionAskingStrategies2023,
  title = {The {{Efficiency}} of {{Question-Asking Strategies}} in a {{Real-World Visual Search Task}}},
  author = {Testoni, Alberto and Bernardi, Raffaella and Ruggeri, Azzurra},
  year = {2023},
  journal = {Cognitive Science},
  volume = {47},
  number = {12},
  pages = {e13396},
  issn = {1551-6709},
  doi = {10.1111/cogs.13396},
  urldate = {2023-12-29},
  abstract = {In recent years, a multitude of datasets of human--human conversations has been released for the main purpose of training conversational agents based on data-hungry artificial neural networks. In this paper, we argue that datasets of this sort represent a useful and underexplored source to validate, complement, and enhance cognitive studies on human behavior and language use. We present a method that leverages the recent development of powerful computational models to obtain the fine-grained annotation required to apply metrics and techniques from Cognitive Science to large datasets. Previous work in Cognitive Science has investigated the question-asking strategies of human participants by employing different variants of the so-called 20-question-game setting and proposing several evaluation methods. In our work, we focus on GuessWhat, a task proposed within the Computer Vision and Natural Language Processing communities that is similar in structure to the 20-question-game setting. Crucially, the GuessWhat dataset contains tens of thousands of dialogues based on real-world images, making it a suitable setting to investigate the question-asking strategies of human players on a large scale and in a natural setting. Our results demonstrate the effectiveness of computational tools to automatically code how the hypothesis space changes throughout the dialogue in complex visual scenes. On the one hand, we confirm findings from previous work on smaller and more controlled settings. On the other hand, our analyses allow us to highlight the presence of ``uninformative'' questions (in terms of Expected Information Gain) at specific rounds of the dialogue. We hypothesize that these questions fulfill pragmatic constraints that are exploited by human players to solve visual tasks in complex scenes successfully. Our work illustrates a method that brings together efforts and findings from different disciplines to gain a better understanding of human question-asking strategies on large-scale datasets, while at the same time posing new questions about the development of conversational systems.},
  copyright = {{\copyright} 2023 The Authors. Cognitive Science published by Wiley Periodicals LLC on behalf of Cognitive Science Society (CSS).},
  langid = {english},
  keywords = {20-Questions game,Expected information gain,Information search,Question asking,Visual search},
  annotation = {https://osf.io/xywa6/?view\_only=6d186c8947ba4c59b860dec1e460e4d5},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Testoni et al_2023_The Efficiency of Question-Asking Strategies in a Real-World Visual Search Task.pdf;/Users/thomasgorman/Zotero/storage/ETPYHEMT/cogs.html}
}

@article{thomasImprovingStudentLearning2024,
  title = {Improving {{Student Learning}} with {{Hybrid Human-AI Tutoring}}: {{A Three-Study Quasi-Experimental Investigation}}},
  author = {Thomas, Danielle R and Lin, Jionghao and Gatz, Erin and Gurung, Ashish and Gupta, Shivang and Norberg, Kole and Fancsali, Stephen E and Aleven, Vincent and Branstetter, Lee and Brunskill, Emma and Koedinger, Kenneth R},
  year = {2024},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Thomas et al_2024_Improving Student Learning with Hybrid Human-AI Tutoring.pdf}
}

@misc{thomasUsingGenerativeAI2024,
  title = {Using {{Generative AI}} to {{Provide Feedback}} to {{Adult Tutors}} in {{Training}} and {{Assess Real-life Performance}}},
  author = {Thomas, Danielle R. and Gatz, Erin and Gupta, Shivang and Lin, Jionghao and Tipper, Cindy and Koedinger, Kenneth},
  year = {2024},
  month = apr,
  doi = {10.35542/osf.io/n9w4b},
  urldate = {2024-04-30},
  abstract = {Tutoring remains among the most impactful academic interventions known to improve student achievement. Despite its success, there are few available opportunities for training and professional development for adult tutors. Furthermore, the task of assessing tutor performance during actual tutoring sessions is challenging and time-consuming for human evaluators---enter artificial intelligence (AI). In this work, we harness generative AI, specifically large language models (LLMs) like GPT-4, to introduce an innovative approach for delivering tutors real-time, explanatory feedback while tutors engage in online scenariobased lessons. Hosted within the Personalized Learning Squared (PLUS) tutoring platform, these lessons offer tutors opportunities to practice responding to common tutoring scenarios. For example, in the Giving Effective Praise lesson, tutors practice responding to students by providing praise and then receiving immediate and templated feedback generated by LLMs. Beyond demonstrating tutor training containing AI-generated feedback, we report past work using LLMs to assess tutors providing praise, revealing moderate performance, and explaining several prompting methods. While using generative AI shows promise as a low-cost and efficient method for giving adult tutors feedback on their performance in training and actual tutoring, it is not without limitations. Practical and ethical considerations are discussed. Human tutoring remains a robust and irreplaceable influence on student learning, with generative AI serving as a valuable tool to complement tutors and enhance the impact of tutoring on student learning.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Thomas et al_2024_Using Generative AI to Provide Feedback to Adult Tutors in Training and Assess.pdf}
}

@misc{tianChartGPTLeveragingLLMs2023,
  title = {{{ChartGPT}}: {{Leveraging LLMs}} to {{Generate Charts}} from {{Abstract Natural Language}}},
  shorttitle = {{{ChartGPT}}},
  author = {Tian, Yuan and Cui, Weiwei and Deng, Dazhen and Yi, Xinjing and Yang, Yurun and Zhang, Haidong and Wu, Yingcai},
  year = {2023},
  month = nov,
  number = {arXiv:2311.01920},
  eprint = {2311.01920},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {The use of natural language interfaces (NLIs) for the creation of charts is becoming increasingly popular due to the intuitiveness of natural language interactions. One key challenge in this approach is to accurately capture user intents and transform them to proper chart specifications. This obstructs the wide use of NLI in chart generation, as users' natural language inputs are generally abstract (i.e., ambiguous or under-specified), without a clear specification of visual encodings. Recently, pre-trained large language models (LLMs) have exhibited superior performance in understanding and generating natural language, demonstrating great potential for downstream tasks. Inspired by this major trend, we propose ChartGPT, generating charts from abstract natural language inputs. However, LLMs are struggling to address complex logic problems. To enable the model to accurately specify the complex parameters and perform operations in chart generation, we decompose the generation process into a step-by-step reasoning pipeline, so that the model only needs to reason a single and specific sub-task during each run. Moreover, LLMs are pre-trained on general datasets, which might be biased for the task of chart generation. To provide adequate visualization knowledge, we create a dataset consisting of abstract utterances and charts and improve model performance through fine-tuning. We further design an interactive interface for ChartGPT that allows users to check and modify the intermediate outputs of each step. The effectiveness of the proposed system is evaluated through quantitative evaluations and a user study.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Human-Computer Interaction},
  annotation = {https://huggingface.co/yuan-tian/chartgpt\\
\\
https://dengdazhen.github.io/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Tian et al_2023_ChartGPT.pdf;/Users/thomasgorman/Zotero/storage/PLDYNMAJ/2311.html}
}

@misc{timkeyLanguageModelLimited2023,
  title = {A {{Language Model}} with {{Limited Memory Capacity Captures Interference}} in {{Human Sentence Processing}}},
  author = {Timkey, William and Linzen, Tal},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16142},
  eprint = {2310.16142},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-05-23},
  abstract = {Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors relied on the parallels between the self-attention mechanism of transformer language models and cuebased retrieval theories of working memory in human sentence processing (Ryu and Lewis, 2021). While Ryu and Lewis show that attention patterns in specialized attention heads of GPT-2 are consistent with similarity-based interference, a key prediction of cue-based retrieval models, their method requires identifying syntactically specialized attention heads, and makes the cognitively implausible assumption that hundreds of memory retrieval operations take place in parallel. In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories. We show that our model's single attention head captures semantic and syntactic interference effects observed in human experiments.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Timkey_Linzen_2023_A Language Model with Limited Memory Capacity Captures Interference in Human2.pdf}
}

@misc{timkeyLanguageModelLimited2023a,
  title = {A {{Language Model}} with {{Limited Memory Capacity Captures Interference}} in {{Human Sentence Processing}}},
  author = {Timkey, William and Linzen, Tal},
  year = {2023},
  month = oct,
  number = {arXiv:2310.16142},
  eprint = {2310.16142},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-21},
  abstract = {Two of the central factors believed to underpin human sentence processing difficulty are expectations and retrieval from working memory. A recent attempt to create a unified cognitive model integrating these two factors relied on the parallels between the self-attention mechanism of transformer language models and cue-based retrieval theories of working memory in human sentence processing (Ryu and Lewis 2021). While Ryu and Lewis show that attention patterns in specialized attention heads of GPT-2 are consistent with similarity-based interference, a key prediction of cue-based retrieval models, their method requires identifying syntactically specialized attention heads, and makes the cognitively implausible assumption that hundreds of memory retrieval operations take place in parallel. In the present work, we develop a recurrent neural language model with a single self-attention head, which more closely parallels the memory system assumed by cognitive theories. We show that our model's single attention head captures semantic and syntactic interference effects observed in human experiments.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Timkey_Linzen_2023_A Language Model with Limited Memory Capacity Captures Interference in Human.pdf;/Users/thomasgorman/Zotero/storage/I56YDGG7/2310.html}
}

@article{tjuatjaLLMsExhibitHumanlike2024,
  title = {Do {{LLMs Exhibit Human-like Response Biases}}? {{A Case Study}} in {{Survey Design}}},
  shorttitle = {Do {{LLMs Exhibit Human-like Response Biases}}?},
  author = {Tjuatja, Lindia and Chen, Valerie and Wu, Tongshuang and Talwalkwar, Ameet and Neubig, Graham},
  year = {2024},
  month = sep,
  journal = {Transactions of the Association for Computational Linguistics},
  volume = {12},
  pages = {1011--1026},
  issn = {2307-387X},
  doi = {10.1162/tacl_a_00685},
  urldate = {2024-09-23},
  abstract = {One widely cited barrier to the adoption of LLMs as proxies for humans in subjective tasks is their sensitivity to prompt wording---but interestingly, humans also display sensitivities to instruction changes in the form of response biases. We investigate the extent to which LLMs reflect human response biases, if at all. We look to survey design, where human response biases caused by changes in the wordings of ``prompts'' have been extensively explored in social psychology literature. Drawing from these works, we design a dataset and framework to evaluate whether LLMs exhibit human-like response biases in survey questionnaires. Our comprehensive evaluation of nine models shows that popular open and commercial LLMs generally fail to reflect human-like behavior, particularly in models that have undergone RLHF. Furthermore, even if a model shows a significant change in the same direction as humans, we find that they are sensitive to perturbations that do not elicit significant changes in humans. These results highlight the pitfalls of using LLMs as human proxies, and underscore the need for finer-grained characterizations of model behavior.1},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Tjuatja et al_2024_Do LLMs Exhibit Human-like Response Biases.pdf;/Users/thomasgorman/Zotero/storage/YLXZLA2S/Do-LLMs-Exhibit-Human-like-Response-Biases-A-Case.html}
}

@article{toblerSmartGradingGenerative2024,
  title = {Smart Grading: {{A}} Generative {{AI-based}} Tool for Knowledge-Grounded Answer Evaluation in Educational Assessments},
  shorttitle = {Smart Grading},
  author = {Tobler, Samuel},
  year = {2024},
  month = jun,
  journal = {MethodsX},
  volume = {12},
  pages = {102531},
  issn = {2215-0161},
  doi = {10.1016/j.mex.2023.102531},
  urldate = {2024-01-29},
  abstract = {Evaluating text-based answers obtained in educational settings or behavioral studies is time-consuming and resource-intensive. Applying novel artificial intelligence tools such as ChatGPT might support the process. Still, currently available implementations do not allow for automated and case-specific evaluations of large numbers of student answers. To counter this limitation, we developed a flexible software and user-friendly web application that enables researchers and educators to use cutting-edge artificial intelligence technologies by providing an interface that combines large language models with options to specify questions of interest, sample solutions, and evaluation instructions for automated answer scoring. We validated the method in an empirical study and found the software with expert ratings to have high reliability. Hence, the present software constitutes a valuable tool to facilitate and enhance text-based answer evaluation.{$\bullet$}Generative AI-enhanced software for customizable, case-specific, and automized grading of large amounts of text-based answers.{$\bullet$}Open-source software and web application for direct implementation and adaptation.},
  keywords = {Artificial intelligence,Automated grading,Educational assessment,GPT,Large language model,Test evaluation},
  annotation = {https://github.com/samueltobler/smartgrading},
  file = {/Users/thomasgorman/Zotero/storage/QS4GV397/S2215016123005277.html}
}

@article{topfEvidenceGreenBehaviours2022,
  title = {Evidence of `{{Green}}' Behaviours: {{Exploring}} Behavioural Traces of pro- and Anti-Environmental Behaviors},
  shorttitle = {Evidence of `{{Green}}' Behaviours},
  author = {Topf, Sabine and Speekenbrink, Maarten},
  year = {2022},
  month = dec,
  journal = {Journal of Environmental Psychology},
  volume = {84},
  pages = {101886},
  issn = {0272-4944},
  doi = {10.1016/j.jenvp.2022.101886},
  urldate = {2024-07-11},
  abstract = {The current climate crisis requires pro-environmental behaviours (PEBs) to be developed, engaged in, and spread to other people. Behavioural traces, i.e. evidence of other people's pro-environmental behaviour left in the shared environment, have shown to influence people towards being more pro-environmental. However, systematic research into behavioural traces of PEBs is missing. In a set of three surveys, we investigate which behavioural traces correspond to a number of pro- and anti-environmental behaviours identified from previous literature, how frequently these behavioural traces are encountered, their relation with engagement in behaviours, and whether behaviours can be inferred from traces. All studies are survey-based with a mix of open-ended questions (Surveys 1 \& 3) and rating scales (Survey 2). We use network analysis to identify partial correlations between behaviours and traces. A total of 66 traces uniquely attributed to 36 pro- and anti-environmental behaviours were identified. On average, each trace is observed monthly. Noticing traces correlated with engaging in related behaviours in 24 instances. Participants report that if they saw a trace more frequently, they expect they would be more likely to adopt the behaviour that produced the trace. Finally, participants were generally able to infer the causing behaviours when only presented the traces. We show that unique behavioural traces exist for a number of pro- and anti-environmental behaviours. Traces are noticed and relate to the constituting behaviours based on correlational and self-report evidence. Because of the wide variation between behaviours and their traces, further research into specific behaviours is warranted. Use of these findings for interventions are discussed.},
  keywords = {Behavioural traces,Pro-environmental behaviour,Stigmergy,Sustainability,Visibility},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Topf_Speekenbrink_2022_Evidence of ‘Green’ behaviours.pdf;/Users/thomasgorman/Zotero/storage/AVYB3TAE/S0272494422001311.html}
}

@misc{tornbergSimulatingSocialMedia2023,
  title = {Simulating {{Social Media Using Large Language Models}} to {{Evaluate Alternative News Feed Algorithms}}},
  author = {T{\"o}rnberg, Petter and Valeeva, Diliara and Uitermark, Justus and Bail, Christopher},
  year = {2023},
  month = oct,
  number = {arXiv:2310.05984},
  eprint = {2310.05984},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-04-07},
  abstract = {Social media is often criticized for amplifying toxic discourse and discouraging constructive conversations. But designing social media platforms to promote better conversations is inherently challenging. This paper asks whether simulating social media through a combination of Large Language Models (LLM) and Agent-Based Modeling can help researchers study how different news feed algorithms shape the quality of online conversations. We create realistic personas using data from the American National Election Study to populate simulated social media platforms. Next, we prompt the agents to read and share news articles - and like or comment upon each other's messages - within three platforms that use different news feed algorithms. In the first platform, users see the most liked and commented posts from users whom they follow. In the second, they see posts from all users - even those outside their own network. The third platform employs a novel "bridging" algorithm that highlights posts that are liked by people with opposing political views. We find this bridging algorithm promotes more constructive, non-toxic, conversation across political divides than the other two models. Though further research is needed to evaluate these findings, we argue that LLMs hold considerable potential to improve simulation research on social media and many other complex social settings.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Multiagent Systems,Computer Science - Social and Information Networks},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Törnberg et al_2023_Simulating Social Media Using Large Language Models to Evaluate Alternative.pdf;/Users/thomasgorman/Zotero/storage/ZMMBAUIK/2310.html}
}

@misc{treutleinConnectingDotsLLMs2024,
  title = {Connecting the {{Dots}}: {{LLMs}} Can {{Infer}} and {{Verbalize Latent Structure}} from {{Disparate Training Data}}},
  shorttitle = {Connecting the {{Dots}}},
  author = {Treutlein, Johannes and Choi, Dami and Betley, Jan and Anil, Cem and Marks, Samuel and Grosse, Roger Baker and Evans, Owain},
  year = {2024},
  month = jun,
  number = {arXiv:2406.14546},
  eprint = {2406.14546},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.14546},
  urldate = {2024-06-30},
  abstract = {One way to address safety risks from large language models (LLMs) is to censor dangerous knowledge from their training data. While this removes the explicit information, implicit information can remain scattered across various training documents. Could an LLM infer the censored knowledge by piecing together these implicit hints? As a step towards answering this question, we study inductive out-of-context reasoning (OOCR), a type of generalization in which LLMs infer latent information from evidence distributed across training documents and apply it to downstream tasks without in-context learning. Using a suite of five tasks, we demonstrate that frontier LLMs can perform inductive OOCR. In one experiment we finetune an LLM on a corpus consisting only of distances between an unknown city and other known cities. Remarkably, without in-context examples or Chain of Thought, the LLM can verbalize that the unknown city is Paris and use this fact to answer downstream questions. Further experiments show that LLMs trained only on individual coin flip outcomes can verbalize whether the coin is biased, and those trained only on pairs \$(x,f(x))\$ can articulate a definition of \$f\$ and compute inverses. While OOCR succeeds in a range of cases, we also show that it is unreliable, particularly for smaller LLMs learning complex structures. Overall, the ability of LLMs to "connect the dots" without explicit in-context learning poses a potential obstacle to monitoring and controlling the knowledge acquired by LLMs.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Treutlein et al_2024_Connecting the Dots.pdf;/Users/thomasgorman/Zotero/storage/QADA5LHN/2406.html}
}

@article{trottLargeLanguageModels2023,
  title = {Do {{Large Language Models Know What Humans Know}}?},
  author = {Trott, Sean and Jones, Cameron and Chang, Tyler and Michaelov, James and Bergen, Benjamin},
  year = {2023},
  journal = {Cognitive Science},
  volume = {47},
  number = {7},
  pages = {e13309},
  issn = {1551-6709},
  doi = {10.1111/cogs.13309},
  urldate = {2024-04-07},
  abstract = {Humans can attribute beliefs to others. However, it is unknown to what extent this ability results from an innate biological endowment or from experience accrued through child development, particularly exposure to language describing others' mental states. We test the viability of the language exposure hypothesis by assessing whether models exposed to large quantities of human language display sensitivity to the implied knowledge states of characters in written passages. In pre-registered analyses, we present a linguistic version of the False Belief Task to both human participants and a large language model, GPT-3. Both are sensitive to others' beliefs, but while the language model significantly exceeds chance behavior, it does not perform as well as the humans nor does it explain the full extent of their behavior---despite being exposed to more language than a human would in a lifetime. This suggests that while statistical learning from language exposure may in part explain how humans develop the ability to reason about the mental states of others, other mechanisms are also responsible.},
  langid = {english},
  keywords = {Belief attribution,False Belief Task,Language,Large language models},
  annotation = {https://osf.io/hu865/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Trott et al_2023_Do Large Language Models Know What Humans Know.pdf;/Users/thomasgorman/Zotero/storage/SDSEQ7PT/cogs.html}
}

@article{trottLargeLanguageModels2024,
  title = {Large {{Language Models}} and the {{Wisdom}} of {{Small Crowds}}},
  author = {Trott, Sean},
  year = {2024},
  month = may,
  journal = {Open Mind},
  volume = {8},
  pages = {723--738},
  issn = {2470-2986},
  doi = {10.1162/opmi_a_00144},
  urldate = {2024-07-09},
  abstract = {Recent advances in Large Language Models (LLMs) have raised the question of replacing human subjects with LLM-generated data. While some believe that LLMs capture the ``wisdom of the crowd''---due to their vast training data---empirical evidence for this hypothesis remains scarce. We present a novel methodological framework to test this: the ``number needed to beat'' (NNB), which measures how many humans are needed for a sample's quality to rival the quality achieved by GPT-4, a state-of-the-art LLM. In a series of pre-registered experiments, we collect novel human data and demonstrate the utility of this method for four psycholinguistic datasets for English. We find that NNB \&gt; 1 for each dataset, but also that NNB varies across tasks (and in some cases is quite small, e.g., 2). We also introduce two ``centaur'' methods for combining LLM and human data, which outperform both stand-alone LLMs and human samples. Finally, we analyze the trade-offs in data cost and quality for each approach. While clear limitations remain, we suggest that this framework could guide decision-making about whether and how to integrate LLM-generated data into the research pipeline.},
  annotation = {https://github.com/seantrott/llm\_clt/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Trott_2024_Large Language Models and the Wisdom of Small Crowds.pdf;/Users/thomasgorman/Zotero/storage/293C49JQ/121179.html}
}

@article{truebloodDisentanglingPrevalenceInduced2021,
  title = {Disentangling Prevalence Induced Biases in Medical Image Decision-Making},
  author = {Trueblood, Jennifer S. and Eichbaum, Quentin and Seegmiller, Adam C. and Stratton, Charles and O'Daniels, Payton and Holmes, William R.},
  year = {2021},
  month = jul,
  journal = {Cognition},
  volume = {212},
  pages = {104713},
  issn = {00100277},
  doi = {10.1016/j.cognition.2021.104713},
  urldate = {2024-07-06},
  abstract = {Many important real-world decision tasks involve the detection of rarely occurring targets (e.g., weapons in luggage, potentially cancerous abnormalities in radiographs). Over the past decade, it has been repeatedly demonstrated that extreme prevalence (both high and low) leads to an increase in errors. While this ``prevalence effect'' is well established, the cognitive and/or perceptual mechanisms responsible for it are not. One reason for this is that the most common tool for analyzing prevalence effects, Signal Detection Theory, cannot distinguish between different biases that might be present. Through an application to pathology image-based decisionmaking, we illustrate that an evidence accumulation modeling framework can be used to disentangle different types of biases. Importantly, our results show that prevalence influences both response expectancy and stimulus evaluation biases, with novices (students, N = 96) showing a more pronounced response expectancy bias and experts (medical laboratory professionals, N = 19) showing a more pronounced stimulus evaluation bias.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Trueblood et al_2021_Disentangling prevalence induced biases in medical image decision-making.pdf}
}

@article{truebloodImpactSpeedBias2018,
  title = {The Impact of Speed and Bias on the Cognitive Processes of Experts and Novices in Medical Image Decision-Making},
  author = {Trueblood, Jennifer S. and Holmes, William R. and Seegmiller, Adam C. and Douds, Jonathan and Compton, Margaret and Szentirmai, Eszter and Woodruff, Megan and Huang, Wenrui and Stratton, Charles and Eichbaum, Quentin},
  year = {2018},
  month = jul,
  journal = {Cognitive Research: Principles and Implications},
  volume = {3},
  number = {1},
  pages = {28},
  issn = {2365-7464},
  doi = {10.1186/s41235-018-0119-2},
  urldate = {2024-07-06},
  abstract = {Training individuals to make accurate decisions from medical images is a critical component of education in diagnostic pathology. We describe a joint experimental and computational modeling approach to examine the similarities and differences in the cognitive processes of novice participants and experienced participants (pathology residents and pathology faculty) in cancer cell image identification. For this study we collected a bank of hundreds of digital images that were identified by cell type and classified by difficulty by a panel of expert hematopathologists. The key manipulations in our study included examining the speed-accuracy tradeoff as well as the impact of prior expectations on decisions. In addition, our study examined individual differences in decision-making by comparing task performance to domain general visual ability (as measured using the Novel Object Memory Test (NOMT) (Richler et al. Cognition 166:42--55, 2017). Using signal detection theory and the diffusion decision model (DDM), we found many similarities between experts and novices in our task. While experts tended to have better discriminability, the two groups responded similarly to time pressure (i.e., reduced caution under speed instructions in the DDM) and to the introduction of a probabilistic cue (i.e., increased response bias in the DDM). These results have important implications for training in this area as well as using novice participants in research on medical image perception and decision-making.},
  langid = {english},
  keywords = {Cancer image detection,Diagnostic pathology,Diffusion decision model,General object recognition,Medical decision-making,Signal detection theory},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Trueblood et al_2018_The impact of speed and bias on the cognitive processes of experts and novices.pdf}
}

@misc{tsirtsisComputationalModelResponsibility2024,
  title = {Towards a Computational Model of Responsibility Judgments in Sequential Human-{{AI}} Collaboration},
  author = {Tsirtsis, Stratis and Rodriguez, Manuel Gomez and Gerstenberg, Tobias},
  year = {2024},
  month = may,
  doi = {10.31234/osf.io/m4yad},
  urldate = {2024-05-22},
  abstract = {When a human and an AI agent collaborate to complete a task and something goes wrong, who is responsible? Prior work has developed theories to describe how people assign responsibility to individuals in teams. However, there has been little work studying the cognitive processes that underlie responsibility judgments in human-AI collaborations, especially for tasks comprising a sequence of interdependent actions. In this work, we take a step towards filling this gap. Using semiautonomous driving as a paradigm, we develop an environment that simulates stylized cases of human-AI collaboration using a generative model of agent behavior. We propose a model of responsibility that considers how unexpected an agent's action was, and what would have happened had they acted differently. We test the model's predictions empirically and find that in addition to action expectations and counterfactual considerations, participants' responsibility judgments are also affected by how much each agent actually contributed to the outcome.},
  copyright = {https://creativecommons.org/licenses/by/4.0/legalcode},
  langid = {english},
  annotation = {https://github.com/cicl-stanford/responsibility\_sequential\\
\\
https://cicl-stanford.github.io/responsibility\_sequential/experiment1/\\
\\
https://osf.io/5ajzd},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Tsirtsis et al_2024_Towards a computational model of responsibility judgments in sequential.pdf}
}

@misc{tsvilodubPredictionsLanguageModels2024,
  title = {Predictions from Language Models for Multiple-Choice Tasks Are Not Robust under Variation of Scoring Methods},
  author = {Tsvilodub, Polina and Wang, Hening and Grosch, Sharon and Franke, Michael},
  year = {2024},
  month = mar,
  number = {arXiv:2403.00998},
  eprint = {2403.00998},
  publisher = {arXiv},
  urldate = {2024-05-25},
  abstract = {This paper systematically compares different methods of deriving item-level predictions of language models for multiple-choice tasks. It compares scoring methods for answer options based on free generation of responses, various probability-based scores, a Likert-scale style rating method, and embedding similarity. In a case study on pragmatic language interpretation, we find that LLM predictions are not robust under variation of method choice, both within a single LLM and across different LLMs. As this variability entails pronounced researcher degrees of freedom in reporting results, knowledge of the variability is crucial to secure robustness of results and research integrity.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://osf.io/nepvm/?view\_only=1628d00609bc45e3b0264de2b59591be},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Tsvilodub et al_2024_Predictions from language models for multiple-choice tasks are not robust under.pdf}
}

@article{tuliAreConvolutionalNeural2021,
  title = {Are {{Convolutional Neural Networks}} or {{Transformers}} More like Human Vision?},
  author = {Tuli, Shikhar and Dasgupta, Ishita and Grant, Erin and Griffiths, Thomas L.},
  year = {2021},
  month = jul,
  journal = {arXiv:2105.07197 [cs]},
  eprint = {2105.07197},
  primaryclass = {cs},
  urldate = {2021-10-19},
  abstract = {Modern machine learning models for computer vision exceed humans in accuracy on specific visual recognition tasks, notably on datasets like ImageNet. However, high accuracy can be achieved in many ways. The particular decision function found by a machine learning system is determined not only by the data to which the system is exposed, but also the inductive biases of the model, which are typically harder to characterize. In this work, we follow a recent trend of in-depth behavioral analyses of neural network models that go beyond accuracy as an evaluation metric by looking at patterns of errors. Our focus is on comparing a suite of standard Convolutional Neural Networks (CNNs) and a recently-proposed attention-based network, the Vision Transformer (ViT), which relaxes the translation-invariance constraint of CNNs and therefore represents a model with a weaker set of inductive biases. Attention-based networks have previously been shown to achieve higher accuracy than CNNs on vision tasks, and we demonstrate, using new metrics for examining error consistency with more granularity, that their errors are also more consistent with those of humans. These results have implications both for building more human-like vision models, as well as for understanding visual object recognition in humans.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Python code},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Tuli et al_2021_Are Convolutional Neural Networks or Transformers more like human vision.pdf;/Users/thomasgorman/Zotero/storage/NYYW7UHK/2105.html}
}

@book{udenDesignAIEnabledExperienceBased2024,
  title = {The {{Design}} of {{AI-Enabled Experience-Based Knowledge Management System}} to {{Facilitate Knowing}} and {{Doing}} in {{Communities}} of {{Practice}}},
  shorttitle = {Knowledge {{Management}} in {{Organisations}}},
  editor = {Uden, Lorna and Ting, I-Hsien},
  year = {2024},
  series = {Communications in {{Computer}} and {{Information Science}}},
  volume = {2152},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-63269-3},
  urldate = {2024-09-05},
  copyright = {https://www.springernature.com/gp/researchers/text-and-data-mining},
  isbn = {978-3-031-63268-6 978-3-031-63269-3},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Uden_Ting_2024_Knowledge Management in Organisations.pdf}
}

@article{ullahRoleLLMsSustainable,
  title = {The {{Role}} of {{LLMs}} in {{Sustainable Smart Cities}}: {{Applications}}, {{Challenges}}, and {{Future Directions}}},
  author = {Ullah, Amin and Qi, Guilin and Hussain, Saddam and Ullah, Irfan and Ali, Zafar},
  abstract = {Smart cities stand as pivotal components in the ongoing pursuit of elevating urban living standards, facilitating the rapid expansion of urban areas while efficiently managing resources through sustainable and scalable innovations. In this regard, as emerging technologies like Artificial Intelligence (AI), the Internet of Things (IoT), big data analytics, and fog and edge computing have become increasingly prevalent, smart city applications grapple with various challenges, including the potential for unauthorized disclosure of confidential and sensitive data. The seamless integration of emerging technologies has played a vital role in sustaining the dynamic pace of their development. This paper explores the substantial potential and applications of Deep Learning (DL), Federated Learning (FL), IoT, Blockchain, Natural Language Processing (NLP), and large language models (LLMs) in optimizing ICT processes within smart cities. We aim to spotlight the vast potential of these technologies as foundational elements that technically strengthen the realization and advancement of smart cities, underscoring their significance in driving innovation within this transformative urban milieu. Our discourse culminates with an exploration of the formidable challenges that DL, FL, IoT, Blockchain, NLP, and LLMs face within these contexts, and we offer insights into potential future directions.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ullah et al_The Role of LLMs in Sustainable Smart Cities.pdf}
}

@misc{upretyInvestigatingContextEffects2024,
  title = {Investigating {{Context Effects}} in {{Similarity Judgements}} in {{Large Language Models}}},
  author = {Uprety, Sagar and Jaiswal, Amit Kumar and Liu, Haiming and Song, Dawei},
  year = {2024},
  month = aug,
  number = {arXiv:2408.10711},
  eprint = {2408.10711},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-23},
  abstract = {Large Language Models (LLMs) have revolutionised the capability of AI models in comprehending and generating natural language text. They are increasingly being used to empower and deploy agents in real-world scenarios, which make decisions and take actions based on their understanding of the context. Therefore researchers, policy makers and enterprises alike are working towards ensuring that the decisions made by these agents align with human values and user expectations. That being said, human values and decisions are not always straightforward to measure and are subject to different cognitive biases. There is a vast section of literature in Behavioural Science which studies biases in human judgements. In this work we report an ongoing investigation on alignment of LLMs with human judgements affected by order bias. Specifically, we focus on a famous human study which showed evidence of order effects in similarity judgements, and replicate it with various popular LLMs. We report the different settings where LLMs exhibit human-like order effect bias and discuss the implications of these findings to inform the design and development of LLM based applications.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Uprety et al_2024_Investigating Context Effects in Similarity Judgements in Large Language Models.pdf}
}

@misc{vaidyaHumansLanguageModels2023,
  title = {Humans and Language Models Diverge When Predicting Repeating Text},
  author = {Vaidya, Aditya R. and Turek, Javier and Huth, Alexander G.},
  year = {2023},
  month = oct,
  number = {arXiv:2310.06408},
  eprint = {2310.06408},
  publisher = {arXiv},
  urldate = {2023-10-14},
  abstract = {Language models that are trained on the next-word prediction task have been shown to accurately model human behavior in word prediction and reading speed. In contrast with these findings, we present a scenario in which the performance of humans and LMs diverges. We collected a dataset of human next-word predictions for five stimuli that are formed by repeating spans of text. Human and GPT-2 LM predictions are strongly aligned in the first presentation of a text span, but their performance quickly diverges when memory (or in-context learning) begins to play a role. We traced the cause of this divergence to specific attention heads in a middle layer. Adding a power-law recency bias to these attention heads yielded a model that performs much more similarly to humans. We hope that this scenario will spur future work in bringing LMs closer to human behavior.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/HuthLab/lm-repeating-text},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Vaidya et al_2023_Humans and language models diverge when predicting repeating text.pdf;/Users/thomasgorman/Zotero/storage/R5I6E3H6/2310.html}
}

@misc{vartanianLearningInteractionsBoost2023,
  title = {Learning Interactions to Boost Human Creativity with Bandits and {{GPT-4}}},
  author = {Vartanian, Ara and Sun, Xiaoxi and Chuang, Yun-Shiuan and Suresh, Siddharth and Zhu, Xiaojin and Rogers, Timothy T.},
  year = {2023},
  month = nov,
  number = {arXiv:2311.10127},
  eprint = {2311.10127},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-11-23},
  abstract = {This paper considers how interactions with AI algorithms can boost human creative thought. We employ a psychological task that demonstrates limits on human creativity, namely semantic feature generation: given a concept name, respondents must list as many of its features as possible. Human participants typically produce only a fraction of the features they know before getting "stuck." In experiments with humans and with a language AI (GPT-4) we contrast behavior in the standard task versus a variant in which participants can ask for algorithmically-generated hints. Algorithm choice is administered by a multi-armed bandit whose reward indicates whether the hint helped generating more features. Humans and the AI show similar benefits from hints, and remarkably, bandits learning from AI responses prefer the same prompting strategy as those learning from human behavior. The results suggest that strategies for boosting human creativity via computer interactions can be learned by bandits run on groups of simulated participants.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Vartanian et al_2023_Learning interactions to boost human creativity with bandits and GPT-4.pdf;/Users/thomasgorman/Zotero/storage/RP4A2RI4/2311.html}
}

@misc{vatsSurveyHumanAITeaming2024,
  title = {A {{Survey}} on {{Human-AI Teaming}} with {{Large Pre-Trained Models}}},
  author = {Vats, Vanshika and Nizam, Marzia Binta and Liu, Minghao and Wang, Ziyuan and Ho, Richard and Prasad, Mohnish Sai and Titterton, Vincent and Malreddy, Sai Venkat and Aggarwal, Riya and Xu, Yanwen and Ding, Lei and Mehta, Jay and Grinnell, Nathan and Liu, Li and Zhong, Sijia and Gandamani, Devanathan Nallur and Tang, Xinyi and Ghosalkar, Rohan and Shen, Celeste and Shen, Rachel and Hussain, Nafisa and Ravichandran, Kesav and Davis, James},
  year = {2024},
  month = jun,
  number = {arXiv:2403.04931},
  eprint = {2403.04931},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-05},
  abstract = {In the rapidly evolving landscape of artificial intelligence (AI), the collaboration between human intelligence and AI systems, known as Human-AI (HAI) Teaming, has emerged as a cornerstone for advancing problem-solving and decision-making processes. The advent of Large Pre-trained Models (LPtM) has significantly transformed this landscape, offering unprecedented capabilities by leveraging vast amounts of data to understand and predict complex patterns. This paper surveys the pivotal integration of LPtMs with HAI, emphasizing how these models enhance collaborative intelligence beyond traditional approaches. It examines the potential of LPtMs in augmenting human capabilities, discussing this collaboration for AI model improvements, effective teaming, ethical considerations, and their broad applied implications in various sectors. Through this exploration, the study sheds light on the transformative impact of LPtM-enhanced HAI Teaming, providing insights for future research, policy development, and strategic implementations aimed at harnessing the full potential of this collaboration for research and societal benefit.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Vats et al_2024_A Survey on Human-AI Teaming with Large Pre-Trained Models.pdf}
}

@article{vermaPreferenceProxiesEvaluating,
  title = {Preference {{Proxies}}: {{Evaluating Large Language Models}} in Capturing {{Human Preferences}} in {{Human-AI Tasks}}},
  author = {Verma, Mudit and Bhambri, Siddhant and Kambhampati, Subbarao},
  abstract = {In this work, we investigate the potential of Large Language Models (LLMs) to serve as effective human proxies by capturing human preferences in the context of collaboration with AI agents. Focusing on two key aspects of human preferences - explicability and sub-task specification in team settings - we explore LLMs' ability to not only model mental states but also understand human reasoning processes. By developing scenarios where optimal AI performance relies on modeling human mental states and reasoning, our investigation involving two different preference types and a user study (with 17 participants) contributes valuable insights into the suitability of LLMs as ``Preference Proxies'' in various human-AI applications, paving the way for future research on the integration of AI agents with human users in Human-Aware AI tasks.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Verma et al_Preference Proxies.pdf}
}

@article{vongGroundedLanguageAcquisition2024,
  title = {Grounded Language Acquisition through the Eyes and Ears of a Single Child},
  author = {Vong, Wai Keen and Wang, Wentao and Orhan, A. Emin and Lake, Brenden M.},
  year = {2024},
  month = feb,
  journal = {Science},
  volume = {383},
  number = {6682},
  pages = {504--511},
  issn = {0036-8075, 1095-9203},
  doi = {10.1126/science.adi1374},
  urldate = {2024-02-06},
  abstract = {Starting around 6 to 9 months of age, children begin acquiring their first words, linking spoken words to their visual counterparts. How much of this knowledge is learnable from sensory input with relatively generic learning mechanisms, and how much requires stronger inductive biases? Using longitudinal head-mounted camera recordings from one child aged 6 to 25 months, we trained a relatively generic neural network on 61 hours of correlated visual-linguistic data streams, learning feature-based representations and cross-modal associations. Our model acquires many word-referent mappings present in the child's everyday experience, enables zero-shot generalization to new visual referents, and aligns its visual and linguistic conceptual systems. These results show how critical aspects of grounded word meaning are learnable through joint representation and associative learning from one child's input.           ,              Editor's summary                            How do young children learn to associate new words with specific objects or visually represented concepts? This hotly debated question in early language acquisition has been traditionally examined in laboratories, limiting generalizability to real-world settings. Vong               et al               . investigated the question in an unprecedented, longitudinal manner using head-mounted video recordings from a single child's first-person experiences in naturalistic settings. By applying machine learning, they introduced the Child's View for Contrastive Learning (CVCL) model, pairing video frames that co-occurred with uttered words, and embedded the images and words in shared representational spaces. CVCL represents sets of visually similar things from one concept (e.g., puzzles) through distinct subclusters (animal versus alphabet puzzles). It combines associative and representation learning that fills gaps in language acquisition research and theories. ---Ekeoma Uzogara                        ,              Machine learning advances research into early language acquisition in children.},
  langid = {english},
  annotation = {https://github.com/wkvong/multimodal-baby},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Vong et al_2024_Grounded language acquisition through the eyes and ears of a single child.pdf}
}

@inproceedings{vylomovaEvaluationSemanticChange2019,
  title = {Evaluation of {{Semantic Change}} of {{Harm-Related Concepts}} in {{Psychology}}},
  booktitle = {Proceedings of the 1st {{International Workshop}} on {{Computational Approaches}} to {{Historical Language Change}}},
  author = {Vylomova, Ekaterina and Murphy, Sean and Haslam, Nicholas},
  year = {2019},
  month = aug,
  pages = {29--34},
  publisher = {Association for Computational Linguistics},
  address = {Florence, Italy},
  doi = {10.18653/v1/W19-4704},
  urldate = {2023-02-18},
  abstract = {The paper focuses on diachronic evaluation of semantic changes of harm-related concepts in psychology. More specifically, we investigate a hypothesis that certain concepts such as ``addiction'', ``bullying'', ``harassment'', ``prejudice'', and ``trauma'' became broader during the last four decades. We evaluate semantic changes using two models: an LSA-based model from Sagi et al. (2009) and a diachronic adaptation of word2vec from Hamilton et al. (2016), that are trained on a large corpus of journal abstracts covering the period of 1980-- 2019. Several concepts showed evidence of broadening. ``Addiction'' moved from physiological dependency on a substance to include psychological dependency on gaming and the Internet. Similarly, ``harassment'' and ``trauma'' shifted towards more psychological meanings. On the other hand, ``bullying'' has transformed into a more victim-related concept and expanded to new areas such as workplaces.},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/vylomovaEvaluationSemanticChange2019-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Vylomova et al_2019_Evaluation of Semantic Change of Harm-Related Concepts in Psychology.pdf}
}

@misc{wangCanLLMsUnderstand2024,
  title = {Can {{LLMs Understand Social Norms}} in {{Autonomous Driving Games}}?},
  author = {Wang, Boxuan and Duan, Haonan and Feng, Yanhao and Chen, Xu and Fu, Yongjie and Mo, Zhaobin and Di, Xuan},
  year = {2024},
  month = sep,
  number = {arXiv:2408.12680},
  eprint = {2408.12680},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-23},
  abstract = {Social norm is defined as a shared standard of acceptable behavior in a society. The emergence of social norms fosters coordination among agents without any hard-coded rules, which is crucial for the large-scale deployment of autonomous vehicles (AVs) in an intelligent transportation system. This paper explores the application of large language models (LLMs) in understanding and modeling social norms in autonomous driving games. We introduce LLMs into autonomous driving games as intelligent agents who make decisions according to text prompts. These agents are referred to as LLM agents. Our framework involves LLM agents playing Markov games in a multi-agent system (MAS), allowing us to investigate the emergence of social norms among individual agents. We aim to identify social norms by designing prompts and utilizing LLMs on textual information related to the environment setup and the observations of LLM agents. Using the OpenAI Chat API powered by GPT-4.0, we conduct experiments to simulate interactions and evaluate the performance of LLM agents in two driving scenarios: unsignalized intersection and highway platoon. The results show that LLM agents can handle dynamically changing environments in Markov games, and social norms evolve among LLM agents in both scenarios. In the intersection game, LLM agents tend to adopt a conservative driving policy when facing a potential car crash. The advantage of LLM agents in games lies in their strong operability and analyzability, which facilitate experimental design.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wang et al_2024_Can LLMs Understand Social Norms in Autonomous Driving Games.pdf}
}

@inproceedings{wangEvaluatingLargeLanguage2024,
  title = {Evaluating {{Large Language Models}} on {{Academic Literature Understanding}} and {{Review}}: {{An Empirical Study}} among {{Early-stage Scholars}}},
  shorttitle = {Evaluating {{Large Language Models}} on {{Academic Literature Understanding}} and {{Review}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wang, Jiyao and Hu, Haolong and Wang, Zuyuan and Yan, Song and Sheng, Youyu and He, Dengbo},
  year = {2024},
  month = may,
  pages = {1--18},
  publisher = {ACM},
  address = {Honolulu HI USA},
  doi = {10.1145/3613904.3641917},
  urldate = {2024-07-03},
  isbn = {9798400703300},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wang et al_2024_Evaluating Large Language Models on Academic Literature Understanding and Review.pdf}
}

@misc{wangEvaluatingModelingSocial2024,
  title = {Evaluating and {{Modeling Social Intelligence}}: {{A Comparative Study}} of {{Human}} and {{AI Capabilities}}},
  shorttitle = {Evaluating and {{Modeling Social Intelligence}}},
  author = {Wang, Junqi and Zhang, Chunhui and Li, Jiapeng and Ma, Yuxi and Niu, Lixing and Han, Jiaheng and Peng, Yujia and Zhu, Yixin and Fan, Lifeng},
  year = {2024},
  month = may,
  number = {arXiv:2405.11841},
  eprint = {2405.11841},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.11841},
  urldate = {2024-07-05},
  abstract = {Facing the current debate on whether Large Language Models (LLMs) attain near-human intelligence levels (Mitchell \& Krakauer, 2023; Bubeck et al., 2023; Kosinski, 2023; Shiffrin \& Mitchell, 2023; Ullman, 2023), the current study introduces a benchmark for evaluating social intelligence, one of the most distinctive aspects of human cognition. We developed a comprehensive theoretical framework for social dynamics and introduced two evaluation tasks: Inverse Reasoning (IR) and Inverse Inverse Planning (IIP). Our approach also encompassed a computational model based on recursive Bayesian inference, adept at elucidating diverse human behavioral patterns. Extensive experiments and detailed analyses revealed that humans surpassed the latest GPT models in overall performance, zero-shot learning, one-shot generalization, and adaptability to multi-modalities. Notably, GPT models demonstrated social intelligence only at the most basic order (order = 0), in stark contrast to human social intelligence (order {$>$}= 2). Further examination indicated a propensity of LLMs to rely on pattern recognition for shortcuts, casting doubt on their possession of authentic human-level social intelligence. Our codes, dataset, appendix and human data are released at https://github.com/bigai-ai/Evaluate-n-Model-Social-Intelligence.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence},
  annotation = {https://github.com/bigai-ai/Evaluate-n-Model-Social-Intelligence},
  file = {/Users/thomasgorman/Zotero/storage/N5VKGTPR/Wang et al. - 2024 - Evaluating and Modeling Social Intelligence A Com.pdf;/Users/thomasgorman/Zotero/storage/T238VITW/2405.html}
}

@misc{wangGrokkedTransformersAre2024,
  title = {Grokked {{Transformers}} Are {{Implicit Reasoners}}: {{A Mechanistic Journey}} to the {{Edge}} of {{Generalization}}},
  shorttitle = {Grokked {{Transformers}} Are {{Implicit Reasoners}}},
  author = {Wang, Boshi and Yue, Xiang and Su, Yu and Sun, Huan},
  year = {2024},
  month = may,
  number = {arXiv:2405.15071},
  eprint = {2405.15071},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2405.15071},
  urldate = {2024-07-05},
  abstract = {We study whether transformers can learn to implicitly reason over parametric knowledge, a skill that even the most capable language models struggle with. Focusing on two representative reasoning types, composition and comparison, we consistently find that transformers can learn implicit reasoning, but only through grokking, i.e., extended training far beyond overfitting. The levels of generalization also vary across reasoning types: when faced with out-of-distribution examples, transformers fail to systematically generalize for composition but succeed for comparison. We delve into the model's internals throughout training, conducting analytical experiments that reveal: 1) the mechanism behind grokking, such as the formation of the generalizing circuit and its relation to the relative efficiency of generalizing and memorizing circuits, and 2) the connection between systematicity and the configuration of the generalizing circuit. Our findings guide data and training setup to better induce implicit reasoning and suggest potential improvements to the transformer architecture, such as encouraging cross-layer knowledge sharing. Furthermore, we demonstrate that for a challenging reasoning task with a large search space, GPT-4-Turbo and Gemini-1.5-Pro based on non-parametric memory fail badly regardless of prompting styles or retrieval augmentation, while a fully grokked transformer can achieve near-perfect accuracy, showcasing the power of parametric memory for complex reasoning.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wang et al_2024_Grokked Transformers are Implicit Reasoners.pdf;/Users/thomasgorman/Zotero/storage/63AAW872/2405.html}
}

@inproceedings{wangHumanLLMCollaborativeAnnotation2024,
  title = {Human-{{LLM Collaborative Annotation Through Effective Verification}} of {{LLM Labels}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Wang, Xinru and Kim, Hannah and Rahman, Sajjadur and Mitra, Kushan and Miao, Zhengjie},
  year = {2024},
  month = may,
  pages = {1--21},
  publisher = {ACM},
  address = {Honolulu HI USA},
  doi = {10.1145/3613904.3641960},
  urldate = {2024-07-03},
  abstract = {Large language models (LLMs) have shown remarkable perfor- mance across various natural language processing (NLP) tasks, indicating their significant potential as data annotators. Although LLM-generated annotations are more cost-effective and efficient to obtain, they are often erroneous for complex or domain-specific tasks and may introduce bias when compared to human annota- tions. Therefore, instead of completely replacing human annotators with LLMs, we need to leverage the strengths of both LLMs and humans to ensure the accuracy and reliability of annotations. This paper presents a multi-step human-LLM collaborative approach where (1) LLMs generate labels and provide explanations, (2) a ver- ifier assesses the quality of LLM-generated labels, and (3) human annotators re-annotate a subset of labels with lower verification scores. To facilitate human-LLM collaboration, we make use of LLM's ability to rationalize its decisions. LLM-generated explana- tions can provide additional information to the verifier model as well as help humans better understand LLM labels. We demonstrate that our verifier is able to identify potentially incorrect LLM labels for human re-annotation. Furthermore, we investigate the impact of presenting LLM labels and explanations on human re-annotation through crowdsourced studies.},
  isbn = {9798400703300},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wang et al_2024_Human-LLM Collaborative Annotation Through Effective Verification of LLM Labels.pdf}
}

@article{wangLargeLanguageModels,
  title = {Large {{Language Models Are Latent Variable Models}}: {{Explaining}} and {{Finding Good Demonstrations}} for {{In-Context Learning}}},
  author = {Wang, Xinyi and Zhu, Wanrong and Saxon, Michael and Steyvers, Mark and Wang, William Yang},
  langid = {english},
  annotation = {https://github.com/WANGXinyiLinda/concept-based-demonstration-selection},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wang et al_Large Language Models Are Latent Variable Models.pdf}
}

@misc{wangLargeLanguageModels2024,
  title = {Large Language Models Cannot Replace Human Participants Because They Cannot Portray Identity Groups},
  author = {Wang, Angelina and Morgenstern, Jamie and Dickerson, John P.},
  year = {2024},
  month = feb,
  number = {arXiv:2402.01908},
  eprint = {2402.01908},
  publisher = {arXiv},
  urldate = {2024-05-25},
  abstract = {Large language models (LLMs) are increasing in capability and popularity, propelling their application in new domains -- including as replacements for human participants in computational social science, user testing, annotation tasks, and more. Traditionally, in all of these settings survey distributors are careful to find representative samples of the human population to ensure the validity of their results and understand potential demographic differences. This means in order to be a suitable replacement, LLMs will need to be able to capture the influence of positionality (i.e., relevance of social identities like gender and race). However, we show that there are two inherent limitations in the way current LLMs are trained that prevent this. We argue analytically for why LLMs are doomed to both misportray and flatten the representations of demographic groups, then empirically show this to be true on 4 LLMs through a series of human studies with 3200 participants across 16 demographic identities. We also discuss a third consideration about how identity prompts can essentialize identities. Throughout, we connect each of these limitations to a pernicious history that shows why each is harmful for marginalized demographic groups. Overall, we urge caution in use cases where LLMs are intended to replace human participants whose identities are relevant to the task at hand. At the same time, in cases where the goal is to supplement rather than replace (e.g., pilot studies), we provide empirically-better inference-time techniques to reduce, but not remove, these harms.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society},
  annotation = {https://osf.io/7gmzq/?view\_only=4e0c5680b0e8434eab3733115d4e506d},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wang et al_2024_Large language models cannot replace human participants because they cannot.pdf}
}

@article{wangLearningBasedApproachLane2018,
  title = {A {{Learning-Based Approach}} for {{Lane Departure Warning Systems With}} a {{Personalized Driver Model}}},
  author = {Wang, Wenshuo and Zhao, Ding and Han, Wei and Xi, Junqiang},
  year = {2018},
  month = oct,
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {67},
  number = {10},
  pages = {9145--9157},
  issn = {1939-9359},
  doi = {10.1109/TVT.2018.2854406},
  urldate = {2024-09-16},
  abstract = {Misunderstanding of driver correction behaviors is the primary reason for false warnings of lane-departure-prediction systems. We proposed a learning-based approach to predict unintended lane-departure behaviors and chances of drivers to bring vehicles back to the lane. First, a personalized driver model for lane-departure and lane-keeping behavior is established by combining the Gaussian mixture model and the hidden Markov model. Second, based on this model, we developed an online model-based prediction algorithm to predict the forthcoming vehicle trajectory and judge whether the driver will act a lane departure behavior or correction behavior. We also develop a warning strategy based on the model-based prediction algorithm that allows the lane-departure warning system to be acceptable for drivers according to the predicted trajectory. In addition, the naturalistic driving data of ten drivers were collected to train the personalized driver model and validate this approach. We compared the proposed method with a basic time-to-lane-crossing (TLC) method and a TLC-directional sequence of piecewise lateral slopes (TLC-DSPLS) method. Experimental results show that the proposed approach can reduce the false-warning rate to 3.13\% on average at 1-s prediction time.},
  keywords = {Gaussian mixture model,hidden Markov model,Hidden Markov models,lane departure warning system,Learning-based approach,Mechanical engineering,personalized driver model,Prediction algorithms,Predictive models,Roads,Trajectory,Vehicles},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wang et al_2018_A Learning-Based Approach for Lane Departure Warning Systems With a.pdf;/Users/thomasgorman/Zotero/storage/WLNLU8M9/8408761.html}
}

@misc{wangLifeSpanCognitiveSystems2024,
  title = {Towards {{LifeSpan Cognitive Systems}}},
  author = {Wang, Yu and Han, Chi and Wu, Tongtong and He, Xiaoxin and Zhou, Wangchunshu and Sadeq, Nafis and Chen, Xiusi and He, Zexue and Wang, Wei and Haffari, Gholamreza and Ji, Heng and McAuley, Julian},
  year = {2024},
  month = sep,
  number = {arXiv:2409.13265},
  eprint = {2409.13265},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-28},
  abstract = {Building a human-like system that continuously interacts with complex environments---whether simulated digital worlds or human society---presents several key challenges. Central to this is enabling continuous, high-frequency interactions, where the interactions are termed experiences. We refer to this envisioned system as the LifeSpan Cognitive System (LSCS). A critical feature of LSCS is its ability to engage in incremental and rapid updates while retaining and accurately recalling past experiences. We identify two major challenges in achieving this: (1) Abstraction and Experience Merging, and (2) Long-term Retention with Accurate Recalling. These properties are essential for storing new experiences, organizing past experiences, and responding to the environment in ways that leverage relevant historical data. Unlike language models with continual learning, which typically rely on large corpora for fine-tuning and focus on improving performance within specific domains or tasks, LSCS must rapidly and incrementally update with new information from its environment at a high frequency. Existing technologies with the potential of solving the above two major challenges can be classified into four classes based on a conceptual metric called Storage Complexity, which measures the relative space required to store past experiences. Each of these four classes of technologies has its own strengths and limitations. Given that none of the existing technologies can achieve LSCS alone, we propose a novel paradigm for LSCS that integrates all four classes of technologies. The new paradigm operates through two core processes: Absorbing Experiences and Generating Responses.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wang et al_2024_Towards LifeSpan Cognitive Systems.pdf}
}

@misc{wangMixtureAgentsEnhancesLarge2024,
  title = {Mixture-of-{{Agents Enhances Large Language Model Capabilities}}},
  author = {Wang, Junlin and Wang, Jue and Athiwaratkun, Ben and Zhang, Ce and Zou, James},
  year = {2024},
  month = jun,
  number = {arXiv:2406.04692},
  eprint = {2406.04692},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-07-03},
  abstract = {Recent advances in large language models (LLMs) demonstrate substantial capabilities in natural language understanding and generation tasks. With the growing number of LLMs, how to harness the collective expertise of multiple LLMs is an exciting open direction. Toward this goal, we propose a new approach that leverages the collective strengths of multiple LLMs through a Mixture-of-Agents (MoA) methodology. In our approach, we construct a layered MoA architecture wherein each layer comprises multiple LLM agents. Each agent takes all the outputs from agents in the previous layer as auxiliary information in generating its response. MoA models achieves state-of-art performance on AlpacaEval 2.0, MT-Bench and FLASK, surpassing GPT-4 Omni. For example, our MoA using only open-source LLMs is the leader of AlpacaEval 2.0 by a substantial gap, achieving a score of 65.1\% compared to 57.5\% by GPT-4 Omni.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wang et al_2024_Mixture-of-Agents Enhances Large Language Model Capabilities.pdf}
}

@article{wangScientificDiscoveryAge2023,
  title = {Scientific Discovery in the Age of Artificial Intelligence},
  author = {Wang, Hanchen and Fu, Tianfan and Du, Yuanqi and Gao, Wenhao and Huang, Kexin and Liu, Ziming and Chandak, Payal and Liu, Shengchao and Van Katwyk, Peter and Deac, Andreea and Anandkumar, Anima and Bergen, Karianne and Gomes, Carla P. and Ho, Shirley and Kohli, Pushmeet and Lasenby, Joan and Leskovec, Jure and Liu, Tie-Yan and Manrai, Arjun and Marks, Debora and Ramsundar, Bharath and Song, Le and Sun, Jimeng and Tang, Jian and Veli{\v c}kovi{\'c}, Petar and Welling, Max and Zhang, Linfeng and Coley, Connor W. and Bengio, Yoshua and Zitnik, Marinka},
  year = {2023},
  month = aug,
  journal = {Nature},
  volume = {620},
  number = {7972},
  pages = {47--60},
  publisher = {Nature Publishing Group},
  issn = {1476-4687},
  doi = {10.1038/s41586-023-06221-2},
  urldate = {2024-01-11},
  abstract = {Artificial intelligence (AI) is being increasingly integrated into scientific discovery to augment and accelerate research, helping scientists to generate hypotheses, design experiments, collect and interpret large datasets, and gain insights that might not have been possible using traditional scientific methods alone. Here we examine breakthroughs over the past decade that include self-supervised learning, which allows models to be trained on vast amounts of unlabelled data, and geometric deep learning, which leverages knowledge about the structure of scientific data to enhance model accuracy and efficiency. Generative AI methods can create designs, such as small-molecule drugs and proteins, by analysing diverse data modalities, including images and sequences. We discuss how these methods can help scientists throughout the scientific process and the central issues that remain despite such advances. Both developers and users of AI tools need a better understanding of when such approaches need improvement, and challenges posed by poor data quality and stewardship remain. These issues cut across scientific disciplines and require developing foundational algorithmic approaches that can contribute to scientific understanding or acquire it autonomously, making them critical areas of focus for AI innovation.},
  copyright = {2023 Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Machine learning,Scientific community,Statistics},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Wang et al_2023_Scientific discovery in the age of artificial intelligence.pdf}
}

@misc{wangVoyagerOpenEndedEmbodied2023,
  title = {Voyager: {{An Open-Ended Embodied Agent}} with {{Large Language Models}}},
  shorttitle = {Voyager},
  author = {Wang, Guanzhi and Xie, Yuqi and Jiang, Yunfan and Mandlekar, Ajay and Xiao, Chaowei and Zhu, Yuke and Fan, Linxi and Anandkumar, Anima},
  year = {2023},
  month = may,
  number = {arXiv:2305.16291},
  eprint = {2305.16291},
  publisher = {arXiv},
  urldate = {2023-05-26},
  abstract = {We introduce Voyager, the first LLM-powered embodied lifelong learning agent in Minecraft that continuously explores the world, acquires diverse skills, and makes novel discoveries without human intervention. Voyager consists of three key components: 1) an automatic curriculum that maximizes exploration, 2) an ever-growing skill library of executable code for storing and retrieving complex behaviors, and 3) a new iterative prompting mechanism that incorporates environment feedback, execution errors, and self-verification for program improvement. Voyager interacts with GPT-4 via blackbox queries, which bypasses the need for model parameter fine-tuning. The skills developed by Voyager are temporally extended, interpretable, and compositional, which compounds the agent's abilities rapidly and alleviates catastrophic forgetting. Empirically, Voyager shows strong in-context lifelong learning capability and exhibits exceptional proficiency in playing Minecraft. It obtains 3.3x more unique items, travels 2.3x longer distances, and unlocks key tech tree milestones up to 15.3x faster than prior SOTA. Voyager is able to utilize the learned skill library in a new Minecraft world to solve novel tasks from scratch, while other techniques struggle to generalize. We open-source our full codebase and prompts at https://voyager.minedojo.org/.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {https://voyager.minedojo.org/},
  file = {/Users/thomasgorman/Documents/Zotero_Markdown/wangVoyagerOpenEndedEmbodied2023-zotero.md;/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Wang et al_2023_Voyager.pdf;/Users/thomasgorman/Zotero/storage/D2WPF7R9/2305.html}
}

@incollection{warstadtWhatArtificialNeural2022,
  title = {What {{Artificial Neural Networks Can Tell Us}} about {{Human Language Acquisition}}},
  booktitle = {Algebraic {{Structures}} in {{Natural Language}}},
  author = {Warstadt, Alex and Bowman, Samuel R.},
  year = {2022},
  publisher = {CRC Press},
  abstract = {Rapid progress in machine learning for natural language processing has the potential to transform debates about how humans learn language. However, the learning environments and biases of current artificial learners and humans diverge in ways that weaken the impact of the evidence gleaned from learning simulations. Today's most effective neural language models are trained on roughly one thousand times the amount of linguistic data available to a typical child. To increase the relevance of learnability results from computational models, we need to train model learners without significant advantages over humans. If an appropriate model successfully acquires some target linguistic knowledge, it can provide a proof of concept that the target is learnable in a hypothesised human learning scenario. Plausible model learners will enable us to carry out experimental manipulations to make causal inferences about variables in the learning environment, and to rigorously test poverty-of-the-stimulus-style claims arguing for innate linguistic knowledge in humans on the basis of speculations about learnability. Comparable experiments will never be possible with human subjects due to practical and ethical considerations, making model learners an indispensable resource. So far, attempts to deprive current models of unfair advantages obtain sub-human results for key grammatical behaviours such as acceptability judgements. But before we can justifiably conclude that language learning requires more prior domain-specific knowledge than current models possess, we must first explore non-linguistic inputs in the form of multimodal stimuli and multi-agent interaction as ways to make our learners more efficient at learning from limited linguistic input.},
  isbn = {978-1-00-320538-8},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Warstadt_Bowman_2022_What Artificial Neural Networks Can Tell Us about Human Language Acquisition.pdf}
}

@article{webbEmergentAnalogicalReasoning2023,
  title = {Emergent Analogical Reasoning in Large Language Models},
  author = {Webb, Taylor and Holyoak, Keith J. and Lu, Hongjing},
  year = {2023},
  month = sep,
  journal = {Nature Human Behaviour},
  volume = {7},
  number = {9},
  pages = {1526--1541},
  issn = {2397-3374},
  doi = {10.1038/s41562-023-01659-w},
  urldate = {2024-02-03},
  abstract = {The recent advent of large language models has reinvigorated debate over whether human cognitive capacities might emerge in such generic models given sufficient training data. Of particular interest is the ability of these models to reason about novel problems zero-shot, without any direct training. In human cognition, this capacity is closely tied to an ability to reason by analogy. Here we performed a direct comparison between human reasoners and a large language model (the text-davinci-003 variant of Generative Pre-trained Transformer (GPT)-3) on a range of analogical tasks, including a non-visual matrix reasoning task based on the rule structure of Raven's Standard Progressive Matrices. We found that GPT-3 displayed a surprisingly strong capacity for abstract pattern induction, matching or even surpassing human capabilities in most settings; preliminary tests of GPT-4 indicated even better performance. Our results indicate that large language models such as GPT-3 have acquired an emergent ability to find zero-shot solutions to a broad range of analogy problems.},
  copyright = {2023 The Author(s), under exclusive licence to Springer Nature Limited},
  langid = {english},
  keywords = {Computer science,Human behaviour},
  annotation = {https://github.com/taylorwwebb/emergent\_analogies\_LLM},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Webb et al_2023_Emergent analogical reasoning in large language models.pdf}
}

@misc{weiCoCoG2ControllableGeneration2024,
  title = {{{CoCoG-2}}: {{Controllable}} Generation of Visual Stimuli for Understanding Human Concept Representation},
  shorttitle = {{{CoCoG-2}}},
  author = {Wei, Chen and Zou, Jiachen and Heinke, Dietmar and Liu, Quanying},
  year = {2024},
  month = jul,
  number = {arXiv:2407.14949},
  eprint = {2407.14949},
  publisher = {arXiv},
  urldate = {2024-08-08},
  abstract = {Humans interpret complex visual stimuli using abstract concepts that facilitate decision-making tasks such as food selection and risk avoidance. Similarity judgment tasks are effective for exploring these concepts. However, methods for controllable image generation in concept space are underdeveloped. In this study, we present a novel framework called CoCoG-2, which integrates generated visual stimuli into similarity judgment tasks. CoCoG-2 utilizes a training-free guidance algorithm to enhance generation flexibility. CoCoG-2 framework is versatile for creating experimental stimuli based on human concepts, supporting various strategies for guiding visual stimuli generation, and demonstrating how these stimuli can validate various experimental hypotheses. CoCoG-2 will advance our understanding of the causal relationship between concept representations and behaviors by generating visual stimuli. The code is available at https://github.com/ncclab-sustech/CoCoG-2.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computer Vision and Pattern Recognition,Computer Science - Human-Computer Interaction,Quantitative Biology - Neurons and Cognition},
  annotation = {https://github.com/ncclab-sustech/CoCoG-2},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wei et al_2024_CoCoG-2.pdf}
}

@article{westphalDecisionControlExplanations2023,
  title = {Decision Control and Explanations in Human-{{AI}} Collaboration: {{Improving}} User Perceptions and Compliance},
  shorttitle = {Decision Control and Explanations in Human-{{AI}} Collaboration},
  author = {Westphal, Monika and V{\"o}ssing, Michael and Satzger, Gerhard and {Yom-Tov}, Galit B. and Rafaeli, Anat},
  year = {2023},
  month = jul,
  journal = {Computers in Human Behavior},
  volume = {144},
  pages = {107714},
  issn = {0747-5632},
  doi = {10.1016/j.chb.2023.107714},
  urldate = {2024-07-02},
  abstract = {Human-AI collaboration has become common, integrating highly complex AI systems into the workplace. Still, it is often ineffective; impaired perceptions -- such as low trust or limited understanding -- reduce compliance with recommendations provided by the AI system. Drawing from cognitive load theory, we examine two techniques of human-AI collaboration as potential remedies. In three experimental studies, we grant users decision control by empowering them to adjust the system's recommendations, and we offer explanations for the system's reasoning. We find decision control positively affects user perceptions of trust and understanding, and improves user compliance with system recommendations. Next, we isolate different effects of providing explanations that may help explain inconsistent findings in recent literature: while explanations help reenact the system's reasoning, they also increase task complexity. Further, the effectiveness of providing an explanation depends on the specific user's cognitive ability to handle complex tasks. In summary, our study shows that users benefit from enhanced decision control, while explanations -- unless appropriately designed for the specific user -- may even harm user perceptions and compliance. This work bears both theoretical and practical implications for the management of human-AI collaboration.},
  keywords = {Decision control,Explanations,Human-AI collaboration,Task complexity,User compliance,User trust},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Westphal et al_2023_Decision control and explanations in human-AI collaboration.pdf;/Users/thomasgorman/Zotero/storage/5852FJJN/S0747563223000651.html}
}

@misc{wickstromRELAXRepresentationLearning2022,
  title = {{{RELAX}}: {{Representation Learning Explainability}}},
  shorttitle = {{{RELAX}}},
  author = {Wickstr{\o}m, Kristoffer K. and Trosten, Daniel J. and L{\o}kse, Sigurd and Boubekki, Ahc{\`e}ne and Mikalsen, Karl {\O}yvind and Kampffmeyer, Michael C. and Jenssen, Robert},
  year = {2022},
  month = feb,
  number = {arXiv:2112.10161},
  eprint = {2112.10161},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2112.10161},
  urldate = {2023-09-12},
  abstract = {Despite the significant improvements that representation learning via self-supervision has led to when learning from unlabeled data, no methods exist that explain what influences the learned representation. We address this need through our proposed approach, RELAX, which is the first approach for attribution-based explanations of representations. Our approach can also model the uncertainty in its explanations, which is essential to produce trustworthy explanations. RELAX explains representations by measuring similarities in the representation space between an input and masked out versions of itself, providing intuitive explanations and significantly outperforming the gradient-based baseline. We provide theoretical interpretations of RELAX and conduct a novel analysis of feature extractors trained using supervised and unsupervised learning, providing insights into different learning strategies. Finally, we illustrate the usability of RELAX in multi-view clustering and highlight that incorporating uncertainty can be essential for providing low-complexity explanations, taking a crucial step towards explaining representations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wickstrøm et al_2022_RELAX.pdf;/Users/thomasgorman/Zotero/storage/AME4C7YA/2112.html}
}

@misc{wilkinsAutomatedTitleAbstract2023,
  title = {Automated Title and Abstract Screening for Scoping Reviews Using the {{GPT-4 Large Language Model}}},
  author = {Wilkins, David},
  year = {2023},
  month = nov,
  number = {arXiv:2311.07918},
  eprint = {2311.07918},
  publisher = {arXiv},
  urldate = {2024-01-29},
  abstract = {Scoping reviews, a type of literature review, require intensive human effort to screen large numbers of scholarly sources for their relevance to the review objectives. This manuscript introduces GPTscreenR, a package for the R statistical programming language that uses the GPT-4 Large Language Model (LLM) to automatically screen sources. The package makes use of the chain-of-thought technique with the goal of maximising performance on complex screening tasks. In validation against consensus human reviewer decisions, GPTscreenR performed similarly to an alternative zero-shot technique, with a sensitivity of 71\%, specificity of 89\%, and overall accuracy of 84\%. Neither method achieved perfect accuracy nor human levels of intraobserver agreement. GPTscreenR demonstrates the potential for LLMs to support scholarly work and provides a user-friendly software framework that can be integrated into existing review processes.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/wilkox/GPTscreenR},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wilkins_2023_Automated title and abstract screening for scoping reviews using the GPT-4.pdf;/Users/thomasgorman/Zotero/storage/WME6TI8L/2311.html}
}

@article{wilsonBenefitsRisksSmart2017,
  title = {Benefits and Risks of Smart Home Technologies},
  author = {Wilson, Charlie and Hargreaves, Tom and {Hauxwell-Baldwin}, Richard},
  year = {2017},
  month = apr,
  journal = {Energy Policy},
  volume = {103},
  pages = {72--83},
  issn = {0301-4215},
  doi = {10.1016/j.enpol.2016.12.047},
  urldate = {2024-07-02},
  abstract = {Smart homes are a priority area of strategic energy planning and national policy. The market adoption of smart home technologies (SHTs) relies on prospective users perceiving clear benefits with acceptable levels of risk. This paper characterises the perceived benefits and risks of SHTs from multiple perspectives. A representative national survey of UK homeowners (n=1025) finds prospective users have positive perceptions of the multiple functionality of SHTs including energy management. Ceding autonomy and independence in the home for increased technological control are the main perceived risks. An additional survey of actual SHT users (n=42) participating in a smart home field trial identifies the key role of early adopters in lowering perceived SHT risks for the mass market. Content analysis of SHT marketing material (n=62) finds the SHT industry are insufficiently emphasising measures to build consumer confidence on data security and privacy. Policymakers can play an important role in mitigating perceived risks, and supporting the energy-management potential of a smart-home future. Policy measures to support SHT market development include design and operating standards, guidelines on data and privacy, quality control, and in situ research programmes. Policy experiences with domestic energy efficiency technologies and with national smart meter roll-outs offer useful precedents.},
  keywords = {Consumer research,Early adopters,Smart homes},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wilson et al_2017_Benefits and risks of smart home technologies.pdf}
}

@article{woodSpatialAllocationAttention2019,
  title = {The Spatial Allocation of Attention in an Interactive Environment},
  author = {Wood, Katherine and Simons, Daniel J.},
  year = {2019},
  month = dec,
  journal = {Cognitive Research: Principles and Implications},
  volume = {4},
  number = {1},
  pages = {13},
  issn = {2365-7464},
  doi = {10.1186/s41235-019-0164-5},
  urldate = {2020-08-31},
  abstract = {Inattentional blindness methods allow for an unobtrusive measure of the spatial distribution of attention; because subjects do not expect the critical object, they have no reason to devote attention to task-irrelevant regions in anticipation of it. We used inattentional blindness to examine the spatial allocation of attention in an interactive game in which subjects navigated through a dynamic environment and avoided hazards. Subjects were most likely to notice unexpected objects in the areas with the greatest risk of contact with a hazard, and less likely to notice equally proximal objects in inaccessible areas of the display or areas in which hazards no longer posed a threat. These results suggest that both the content of the environment and how a subject can interact with it influence the spatial allocation of attention.},
  langid = {english},
  file = {/Users/thomasgorman/Zotero/storage/GIA73PDL/Wood and Simons - 2019 - The spatial allocation of attention in an interact.pdf}
}

@misc{wuHowWellLLMs2024,
  title = {How Well Do {{LLMs}} Cite Relevant Medical References? {{An}} Evaluation Framework and Analyses},
  shorttitle = {How Well Do {{LLMs}} Cite Relevant Medical References?},
  author = {Wu, Kevin and Wu, Eric and Cassasola, Ally and Zhang, Angela and Wei, Kevin and Nguyen, Teresa and Riantawan, Sith and Riantawan, Patricia Shi and Ho, Daniel E. and Zou, James},
  year = {2024},
  month = feb,
  number = {arXiv:2402.02008},
  eprint = {2402.02008},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-14},
  abstract = {Large language models (LLMs) are currently being used to answer medical questions across a variety of clinical domains. Recent top-performing commercial LLMs, in particular, are also capable of citing sources to support their responses. In this paper, we ask: do the sources that LLMs generate actually support the claims that they make? To answer this, we propose three contributions. First, as expert medical annotations are an expensive and time-consuming bottleneck for scalable evaluation, we demonstrate that GPT-4 is highly accurate in validating source relevance, agreeing 88\% of the time with a panel of medical doctors. Second, we develop an end-to-end, automated pipeline called SourceCheckup and use it to evaluate five top-performing LLMs on a dataset of 1200 generated questions, totaling over 40K pairs of statements and sources. Interestingly, we find that between {$\sim$}50\% to 90\% of LLM responses are not fully supported by the sources they provide. We also evaluate GPT-4 with retrieval augmented generation (RAG) and find that, even still, around 30\% of individual statements are unsupported, while nearly half of its responses are not fully supported. Third, we open-source our curated dataset of medical questions and expert annotations for future evaluations. Given the rapid pace of LLM development and the potential harms of incorrect or outdated medical information, it is crucial to also understand and quantify their capability to produce relevant, trustworthy medical references.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wu et al_2024_How well do LLMs cite relevant medical references.pdf}
}

@misc{wuReasoningRecitingExploring2024,
  title = {Reasoning or {{Reciting}}? {{Exploring}} the {{Capabilities}} and {{Limitations}} of {{Language Models Through Counterfactual Tasks}}},
  shorttitle = {Reasoning or {{Reciting}}?},
  author = {Wu, Zhaofeng and Qiu, Linlu and Ross, Alexis and Aky{\"u}rek, Ekin and Chen, Boyuan and Wang, Bailin and Kim, Najoung and Andreas, Jacob and Kim, Yoon},
  year = {2024},
  month = mar,
  number = {arXiv:2307.02477},
  eprint = {2307.02477},
  publisher = {arXiv},
  urldate = {2024-05-22},
  abstract = {The impressive performance of recent language models across a wide range of tasks suggests that they possess a degree of abstract reasoning skills. Are these skills general and transferable, or specialized to specific tasks seen during pretraining? To disentangle these effects, we propose an evaluation framework based on "counterfactual" task variants that deviate from the default assumptions underlying standard tasks. Across a suite of 11 tasks, we observe nontrivial performance on the counterfactual variants, but nevertheless find that performance substantially and consistently degrades compared to the default conditions. This suggests that while current LMs may possess abstract task-solving skills to an extent, they often also rely on narrow, non-transferable procedures for task-solving. These results motivate a more careful interpretation of language model performance that teases apart these aspects of behavior.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  annotation = {https://github.com/ZhaofengWu/counterfactual-evaluation},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wu et al_2024_Reasoning or Reciting.pdf}
}

@misc{wynnLearningHumanlikeRepresentations2024,
  title = {Learning {{Human-like Representations}} to {{Enable Learning Human Values}}},
  author = {Wynn, Andrea and Sucholutsky, Ilia and Griffiths, Thomas L.},
  year = {2024},
  month = mar,
  number = {arXiv:2312.14106},
  eprint = {2312.14106},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-06},
  abstract = {How can we build AI systems that are aligned with human values to avoid causing harm or violating societal standards for acceptable behavior? We argue that representational alignment between humans and AI agents facilitates value alignment. Making AI systems learn human-like representations of the world has many known benefits, including improving generalization, robustness to domain shifts, and few-shot learning performance. We propose that this kind of representational alignment between machine learning (ML) models and humans can also support value alignment, allowing ML systems to conform to human values and societal norms. We focus on ethics as one aspect of value alignment and train ML agents using a variety of methods in a multi-armed bandit setting, where rewards reflect the moral acceptability of the chosen action. We use a synthetic experiment to demonstrate that agents' representational alignment with the environment bounds their learning performance. We then repeat this procedure in a realistic setting, using textual action descriptions and similarity judgments collected from humans and a variety of language models, to show that the results generalize and are model-agnostic when grounded in an ethically relevant context.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Wynn et al_2024_Learning Human-like Representations to Enable Learning Human Values.pdf}
}

@misc{xieCanLargeLanguage2024,
  title = {Can {{Large Language Model Agents Simulate Human Trust Behaviors}}?},
  author = {Xie, Chengxing and Chen, Canyu and Jia, Feiran and Ye, Ziyu and Shu, Kai and Bibi, Adel and Hu, Ziniu and Torr, Philip and Ghanem, Bernard and Li, Guohao},
  year = {2024},
  month = feb,
  number = {arXiv:2402.04559},
  eprint = {2402.04559},
  publisher = {arXiv},
  urldate = {2024-02-10},
  abstract = {Large Language Model (LLM) agents have been increasingly adopted as simulation tools to model humans in applications such as social science. However, one fundamental question remains: can LLM agents really simulate human behaviors? In this paper, we focus on one of the most critical behaviors in human interactions, trust, and aim to investigate whether or not LLM agents can simulate human trust behaviors. We first find that LLM agents generally exhibit trust behaviors, referred to as agent trust, under the framework of Trust Games, which are widely recognized in behavioral economics. Then, we discover that LLM agents can have high behavioral alignment with humans regarding trust behaviors, indicating the feasibility to simulate human trust behaviors with LLM agents. In addition, we probe into the biases in agent trust and the differences in agent trust towards agents and humans. We also explore the intrinsic properties of agent trust under conditions including advanced reasoning strategies and external manipulations. We further offer important implications for various scenarios where trust is paramount. Our study represents a significant step in understanding the behaviors of LLM agents and the LLM-human analogy.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  annotation = {https://github.com/camel-ai/agent-trust},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Xie et al_2024_Can Large Language Model Agents Simulate Human Trust Behaviors.pdf;/Users/thomasgorman/Zotero/storage/QLU6NEQJ/2402.html}
}

@article{xieEvaluatingPredictivePerformance2024,
  title = {Evaluating {{Predictive Performance}} and {{Learning Efficiency}} of {{Large Language Models}} with {{Think Aloud}} in {{Risky Decision Making}}},
  author = {Xie, Hanbo and Xiong, Huadong and Wilson, Robert},
  year = {2024},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Xie et al_Evaluating Predictive Performance and Learning Efficiency of Large Language.pdf}
}

@misc{xieLLMsDoctorsLeveraging2024,
  title = {{{LLMs}} for {{Doctors}}: {{Leveraging Medical LLMs}} to {{Assist Doctors}}, {{Not Replace Them}}},
  shorttitle = {{{LLMs}} for {{Doctors}}},
  author = {Xie, Wenya and Xiao, Qingying and Zheng, Yu and Wang, Xidong and Chen, Junying and Ji, Ke and Gao, Anningzhe and Wan, Xiang and Jiang, Feng and Wang, Benyou},
  year = {2024},
  month = jun,
  number = {arXiv:2406.18034},
  eprint = {2406.18034},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-11},
  abstract = {The recent success of Large Language Models (LLMs) has had a significant impact on the healthcare field, providing patients with medical advice, diagnostic information, and more. However, due to a lack of professional medical knowledge, patients are easily misled by generated erroneous information from LLMs, which may result in serious medical problems. To address this issue, we focus on tuning the LLMs to be medical assistants who collaborate with more experienced doctors. We first conduct a two-stage survey by inspiration-feedback to gain a broad understanding of the real needs of doctors for medical assistants. Based on this, we construct a Chinese medical dataset called DoctorFLAN to support the entire workflow of doctors, which includes 92K Q\&A samples from 22 tasks and 27 specialists. Moreover, we evaluate LLMs in doctor-oriented scenarios by constructing the DoctorFLAN-test containing 550 single-turn Q\&A and DotaBench containing 74 multi-turn conversations. The evaluation results indicate that being a medical assistant still poses challenges for existing open-source models, but DoctorFLAN can help them significantly. It demonstrates that the doctor-oriented dataset and benchmarks we construct can complement existing patient-oriented work and better promote medical LLMs research.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Xie et al_2024_LLMs for Doctors.pdf}
}

@article{xingDriverLaneChange2019,
  title = {Driver {{Lane Change Intention Inference}} for {{Intelligent Vehicles}}: {{Framework}}, {{Survey}}, and {{Challenges}}},
  shorttitle = {Driver {{Lane Change Intention Inference}} for {{Intelligent Vehicles}}},
  author = {Xing, Yang and Lv, Chen and Wang, Huaji and Wang, Hong and Ai, Yunfeng and Cao, Dongpu and Velenis, Efstathios and Wang, Fei-Yue},
  year = {2019},
  month = may,
  journal = {IEEE Transactions on Vehicular Technology},
  volume = {68},
  number = {5},
  pages = {4377--4390},
  issn = {1939-9359},
  doi = {10.1109/TVT.2019.2903299},
  urldate = {2024-09-16},
  abstract = {Intelligent vehicles and advanced driver assistance systems (ADAS) need to have proper awareness of the traffic context, as well as the driver status since ADAS share the vehicle control authorities with the human driver. This paper provides an overview of the ego-vehicle driver intention inference (DII), which mainly focuses on the lane change intention on highways. First, a human intention mechanism is discussed in the beginning to gain an overall understanding of the driver intention. Next, the ego-vehicle driver intention is classified into different categories based on various criteria. A complete DII system can be separated into different modules, which consist of traffic context awareness, driver states monitoring, and the vehicle dynamic measurement module. The relationship between these modules and the corresponding impacts on the DII are analyzed. Then, the lane change intention inference system is reviewed from the perspective of input signals, algorithms, and evaluation. Finally, future concerns and emerging trends in this area are highlighted.},
  keywords = {Accidents,ADAS,Brain modeling,driver intention,Intelligent vehicle,lane change,Monitoring,parallel driving,Roads,Task analysis,Vehicle dynamics,Vehicles},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Xing et al_2019_Driver Lane Change Intention Inference for Intelligent Vehicles.pdf;/Users/thomasgorman/Zotero/storage/VVDYKW6W/8661600.html}
}

@article{xingHumanvehicleCollaborationReview2021,
  title = {Toward Human-Vehicle Collaboration: {{Review}} and Perspectives on Human-Centered Collaborative Automated Driving},
  shorttitle = {Toward Human-Vehicle Collaboration},
  author = {Xing, Yang and Lv, Chen and Cao, Dongpu and Hang, Peng},
  year = {2021},
  month = jul,
  journal = {Transportation Research Part C: Emerging Technologies},
  volume = {128},
  pages = {103199},
  issn = {0968-090X},
  doi = {10.1016/j.trc.2021.103199},
  urldate = {2024-09-16},
  abstract = {The last decade witnessed a great development of automated driving vehicles (ADVs) and vehicle intelligence. The significant increment of machine intelligence poses a new challenge to the community, which is the collaboration between human drivers and vehicle autonomy. As vehicle autonomy is gaining more control authority, the roles that human drivers can play in the future need to be further clarified. In this study, literature review and perspectives on the human behaviors and cognition (HBC) for ADVs toward human-autonomy (H-A) collaboration are proposed. First, the H-A collaboration basics and key factors are reviewed. Then, the HBC issues in driver behavior modeling and understanding are discussed. Specifically, two key factors are reviewed, which are human trust and situation awareness (SA). Next, HBC in two H-A collaboration-enabled vehicle control methods, namely, shared control and take-over control, are analyzed. It is shown that the human-centered collaboration strategy that integrates driver states will largely increase the acceptance of the ADVs. Then, the HBC issues in the design of human-machine-interface (HMI) for future autonomous and collaboration-enabled ADVs are discussed. Last, challenges and future works for H-A collaboration on ADVs are analyzed to contribute to the development of understandable, trustable, and acceptable ADVs.},
  keywords = {Automated driving vehicle,Cognition,Driver behaviors,Human-autonomy collaboration,Mutual understanding},
  file = {/Users/thomasgorman/Zotero/storage/7P9ZV9G6/S0968090X2100214X.html}
}

@misc{xiongConvergingParadigmsSynergy2024,
  title = {Converging {{Paradigms}}: {{The Synergy}} of {{Symbolic}} and {{Connectionist AI}} in {{LLM-Empowered Autonomous Agents}}},
  shorttitle = {Converging {{Paradigms}}},
  author = {Xiong, Haoyi and Wang, Zhiyuan and Li, Xuhong and Bian, Jiang and Xie, Zeke and Mumtaz, Shahid and Barnes, Laura E.},
  year = {2024},
  month = aug,
  number = {arXiv:2407.08516},
  eprint = {2407.08516},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-16},
  abstract = {This article explores the convergence of connectionist and symbolic artificial intelligence (AI), from historical debates to contemporary advancements. Traditionally considered distinct paradigms, connectionist AI focuses on neural networks, while symbolic AI emphasizes symbolic representation and logic. Recent advancements in large language models (LLMs), exemplified by ChatGPT and GPT-4, highlight the potential of connectionist architectures in handling human language as a form of symbols. The study argues that LLM-empowered Autonomous Agents (LAAs) embody this paradigm convergence. By utilizing LLMs for text-based knowledge modeling and representation, LAAs integrate neuro-symbolic AI principles, showcasing enhanced reasoning and decision-making capabilities. Comparing LAAs with Knowledge Graphs within the neuro-symbolic AI theme highlights the unique strengths of LAAs in mimicking human-like reasoning processes, scaling effectively with large datasets, and leveraging in-context samples without explicit re-training. The research underscores promising avenues in neuro-vector-symbolic integration, instructional encoding, and implicit reasoning, aimed at further enhancing LAA capabilities. By exploring the progression of neuro-symbolic AI and proposing future research trajectories, this work advances the understanding and development of AI technologies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Xiong et al_2024_Converging Paradigms.pdf}
}

@article{xuAISocialScience2024,
  title = {{{AI}} for Social Science and Social Science of {{AI}}: {{A Survey}}},
  shorttitle = {{{AI}} for Social Science and Social Science of {{AI}}},
  author = {Xu, Ruoxi and Sun, Yingfei and Ren, Mengjie and Guo, Shiguang and Pan, Ruotong and Lin, Hongyu and Sun, Le and Han, Xianpei},
  year = {2024},
  month = jan,
  journal = {Information Processing \& Management,},
  volume = {61},
  number = {3},
  eprint = {2401.11839},
  primaryclass = {cs},
  pages = {103665},
  urldate = {2024-09-23},
  abstract = {Recent advancements in artificial intelligence, particularly with the emergence of large language models (LLMs), have sparked a rethinking of artificial general intelligence possibilities. The increasing human-like capabilities of AI are also attracting attention in social science research, leading to various studies exploring the combination of these two fields. In this survey, we systematically categorize previous explorations in the combination of AI and social science into two directions that share common technical approaches but differ in their research objectives. The first direction is focused on AI for social science, where AI is utilized as a powerful tool to enhance various stages of social science research. While the second direction is the social science of AI, which examines AI agents as social entities with their human-like cognitive and linguistic capabilities. By conducting a thorough review, particularly on the substantial progress facilitated by recent advancements in large language models, this paper introduces a fresh perspective to reassess the relationship between AI and social science, provides a cohesive framework that allows researchers to understand the distinctions and connections between AI for social science and social science of AI, and also summarized state-of-art experiment simulation platforms to facilitate research in these two directions. We believe that as AI technology continues to advance and intelligent agents find increasing applications in our daily lives, the significance of the combination of AI and social science will become even more prominent.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Xu et al_2024_AI for social science and social science of AI.pdf}
}

@article{xuVisionFlanScalingHumanLabeled,
  title = {Vision-{{Flan}}: {{Scaling Human-Labeled Tasks}} in {{Visual Instruction Tuning}}},
  author = {Xu, Zhiyang and Feng, Chao and Shao, Rulin and Ashby, Trevor and Shen, Ying and Jin, Di and Cheng, Yu and Wang, Qifan and Huang, Lifu},
  abstract = {Despite vision-language models' (VLMs) remarkable capabilities as versatile visual assistants, two substantial challenges persist within the existing VLM frameworks: (1) lacking task diversity in pretraining and visual instruction tuning, and (2) annotation error and bias in GPT-4 synthesized instruction tuning data. Both challenges lead to issues such as poor generalizability, hallucination, and catastrophic forgetting. To address these challenges, we construct VISION-FLAN, the most diverse publicly available visual instruction tuning dataset to date, comprising 187 diverse tasks and 1,664,261 instances sourced from academic datasets, and each task is accompanied by an expert-written instruction. In addition, we propose a two-stage instruction tuning framework, in which VLMs are firstly finetuned on VISIONFLAN and further tuned on GPT-4 synthesized data. We find this two-stage tuning framework significantly outperforms the traditional single-stage visual instruction tuning framework and achieves the state-of-the-art performance across a wide range of multi-modal evaluation benchmarks. Finally, we conduct indepth analyses to understand visual instruction tuning and our findings reveal that: (1) GPT-4 synthesized data does not substantially enhance VLMs' capabilities but rather modulates the model's responses to human-preferred formats; (2) A minimal quantity (e.g., 1,000) of GPT4 synthesized data can effectively align VLM responses with human-preference; (3) Visual instruction tuning mainly helps large-language models (LLMs) to understand visual features.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Xu et al_Vision-Flan.pdf}
}

@misc{yadlowskyPretrainingDataMixtures2023,
  title = {Pretraining {{Data Mixtures Enable Narrow Model Selection Capabilities}} in {{Transformer Models}}},
  author = {Yadlowsky, Steve and Doshi, Lyric and Tripuraneni, Nilesh},
  year = {2023},
  month = nov,
  number = {arXiv:2311.00871},
  eprint = {2311.00871},
  primaryclass = {cs, stat},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2311.00871},
  urldate = {2023-11-19},
  abstract = {Transformer models, notably large language models (LLMs), have the remarkable ability to perform in-context learning (ICL) -- to perform new tasks when prompted with unseen input-output examples without any explicit model training. In this work, we study how effectively transformers can bridge between their pretraining data mixture, comprised of multiple distinct task families, to identify and learn new tasks in-context which are both inside and outside the pretraining distribution. Building on previous work, we investigate this question in a controlled setting, where we study transformer models trained on sequences of \$(x, f(x))\$ pairs rather than natural language. Our empirical results show transformers demonstrate near-optimal unsupervised model selection capabilities, in their ability to first in-context identify different task families and in-context learn within them when the task families are well-represented in their pretraining data. However when presented with tasks or functions which are out-of-domain of their pretraining data, we demonstrate various failure modes of transformers and degradation of their generalization for even simple extrapolation tasks. Together our results highlight that the impressive ICL abilities of high-capacity sequence models may be more closely tied to the coverage of their pretraining data mixtures than inductive biases that create fundamental generalization capabilities.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning,Statistics - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yadlowsky et al_2023_Pretraining Data Mixtures Enable Narrow Model Selection Capabilities in.pdf;/Users/thomasgorman/Zotero/storage/WNKT98JH/2311.html}
}

@misc{yangDawnLMMsPreliminary2023,
  title = {The {{Dawn}} of {{LMMs}}: {{Preliminary Explorations}} with {{GPT-4V}}(Ision)},
  shorttitle = {The {{Dawn}} of {{LMMs}}},
  author = {Yang, Zhengyuan and Li, Linjie and Lin, Kevin and Wang, Jianfeng and Lin, Chung-Ching and Liu, Zicheng and Wang, Lijuan},
  year = {2023},
  month = oct,
  number = {arXiv:2309.17421},
  eprint = {2309.17421},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-10-13},
  abstract = {Large multimodal models (LMMs) extend large language models (LLMs) with multi-sensory skills, such as visual understanding, to achieve stronger generic intelligence. In this paper, we analyze the latest model, GPT-4V(ision), to deepen the understanding of LMMs. The analysis focuses on the intriguing tasks that GPT-4V can perform, containing test samples to probe the quality and genericity of GPT-4V's capabilities, its supported inputs and working modes, and the effective ways to prompt the model. In our approach to exploring GPT-4V, we curate and organize a collection of carefully designed qualitative samples spanning a variety of domains and tasks. Observations from these samples demonstrate that GPT-4V's unprecedented ability in processing arbitrarily interleaved multimodal inputs and the genericity of its capabilities together make GPT-4V a powerful multimodal generalist system. Furthermore, GPT-4V's unique capability of understanding visual markers drawn on input images can give rise to new human-computer interaction methods such as visual referring prompting. We conclude the report with in-depth discussions on the emerging application scenarios and the future research directions for GPT-4V-based systems. We hope that this preliminary exploration will inspire future research on the next-generation multimodal task formulation, new ways to exploit and enhance LMMs to solve real-world problems, and gaining better understanding of multimodal foundation models. Finally, we acknowledge that the model under our study is solely the product of OpenAI's innovative work, and they should be fully credited for its development. Please see the GPT-4V contributions paper for the authorship and credit attribution: https://cdn.openai.com/contributions/gpt-4v.pdf},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Computer Vision and Pattern Recognition},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yang et al_2023_The Dawn of LMMs.pdf;/Users/thomasgorman/Zotero/storage/327MS9ZV/2309.html}
}

@misc{yangLLMAgentsPsychology2024,
  title = {{{LLM Agents}} for {{Psychology}}: {{A Study}} on {{Gamified Assessments}}},
  shorttitle = {{{LLM Agents}} for {{Psychology}}},
  author = {Yang, Qisen and Wang, Zekun and Chen, Honghui and Wang, Shenzhi and Pu, Yifan and Gao, Xin and Huang, Wenhao and Song, Shiji and Huang, Gao},
  year = {2024},
  month = feb,
  number = {arXiv:2402.12326},
  eprint = {2402.12326},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-06-29},
  abstract = {Psychological measurement is essential for mental health, self-understanding, and personal development. Traditional methods, such as selfreport scales and psychologist interviews, often face challenges with engagement and accessibility. While game-based and LLM-based tools have been explored to improve user interest and automate assessment, they struggle to balance engagement with generalizability. In this work, we propose PsychoGAT (Psychological Game AgenTs) to achieve a generic gamification of psychological assessment. The main insight is that powerful LLMs can function both as adept psychologists and innovative game designers. By incorporating LLM agents into designated roles and carefully managing their interactions, PsychoGAT can transform any standardized scales into personalized and engaging interactive fiction games. To validate the proposed method, we conduct psychometric evaluations to assess its effectiveness and employ human evaluators to examine the generated content across various psychological constructs, including depression, cognitive distortions, and personality traits. Results demonstrate that PsychoGAT serves as an effective assessment tool, achieving statistically significant excellence in psychometric metrics such as reliability, convergent validity, and discriminant validity. Moreover, human evaluations confirm PsychoGAT's enhancements in content coherence, interactivity, interest, immersion, and satisfaction.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Human-Computer Interaction,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yang et al_2024_LLM Agents for Psychology.pdf}
}

@misc{yangLLMBasedDigitalTwin2024,
  title = {An {{LLM-Based Digital Twin}} for {{Optimizing Human-in-the Loop Systems}}},
  author = {Yang, Hanqing and Siew, Marie and {Joe-Wong}, Carlee},
  year = {2024},
  month = mar,
  number = {arXiv:2403.16809},
  eprint = {2403.16809},
  publisher = {arXiv},
  urldate = {2024-07-03},
  abstract = {The increasing prevalence of Cyber-Physical Systems and the Internet of Things (CPS-IoT) applications and Foundation Models are enabling new applications that leverage real-time control of the environment. For example, real-time control of Heating, Ventilation and Air-Conditioning (HVAC) systems can reduce its usage when not needed for the comfort of human occupants, hence reducing energy consumption. Collecting realtime feedback on human preferences in such human-in-the-loop (HITL) systems, however, is difficult in practice. We propose the use of large language models (LLMs) to deal with the challenges of dynamic environments and difficult-to-obtain data in CPS optimization. In this paper, we present a case study that employs LLM agents to mimic the behaviors and thermal preferences of various population groups (e.g. young families, the elderly) in a shopping mall. The aggregated thermal preferences are integrated into an agent-in-the-loop based reinforcement learning algorithm AitL-RL, which employs the LLM as a dynamic simulation of the physical environment to learn how to balance between energy savings and occupant comfort. Our results show that LLMs are capable of simulating complex population movements within large open spaces. Besides, AitL-RLdemonstrates superior performance compared to the popular existing policy of set point control, suggesting that adaptive and personalized decision-making is critical for efficient optimization in CPS-IoT applications. Through this case study, we demonstrate the potential of integrating advanced Foundation Models like LLMs into CPS-IoT to enhance system adaptability and efficiency. The project's code can be found on our GitHub repository.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning,Electrical Engineering and Systems Science - Systems and Control},
  annotation = {https://github.com/HappyEureka/LLM\_digital\_twin},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yang et al_2024_An LLM-Based Digital Twin for Optimizing Human-in-the Loop Systems.pdf}
}

@misc{yangLLMVotingHuman2024,
  title = {{{LLM Voting}}: {{Human Choices}} and {{AI Collective Decision Making}}},
  shorttitle = {{{LLM Voting}}},
  author = {Yang, Joshua C. and Dailisan, Damian and Korecki, Marcin and Hausladen, Carina I. and Helbing, Dirk},
  year = {2024},
  month = aug,
  number = {arXiv:2402.01766},
  eprint = {2402.01766},
  primaryclass = {cs, econ, q-fin},
  publisher = {arXiv},
  urldate = {2024-09-30},
  abstract = {This paper investigates the voting behaviors of Large Language Models (LLMs), specifically GPT-4 and LLaMA-2, their biases, and how they align with human voting patterns. Our methodology involved using a dataset from a human voting experiment to establish a baseline for human preferences and conducting a corresponding experiment with LLM agents. We observed that the choice of voting methods and the presentation order influenced LLM voting outcomes. We found that varying the persona can reduce some of these biases and enhance alignment with human choices. While the Chain-of-Thought approach did not improve prediction accuracy, it has potential for AI explainability in the voting process. We also identified a trade-off between preference diversity and alignment accuracy in LLMs, influenced by different temperature settings. Our findings indicate that LLMs may lead to less diverse collective outcomes and biased assumptions when used in voting scenarios, emphasizing the need for cautious integration of LLMs into democratic processes.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {68T05 91B14 91C20,Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning,Economics - General Economics,I.2.7,J.4,K.4.1},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yang et al_2024_LLM Voting.pdf}
}

@article{yannierAIAdaptivityMixedReality2024,
  title = {{{AI Adaptivity}} in a {{Mixed-Reality System Improves Learning}}},
  author = {Yannier, Nesra and Hudson, Scott E. and Chang, Henry and Koedinger, Kenneth R.},
  year = {2024},
  month = jan,
  journal = {International Journal of Artificial Intelligence in Education},
  issn = {1560-4306},
  doi = {10.1007/s40593-023-00388-5},
  urldate = {2024-01-14},
  abstract = {Adaptivity in advanced learning technologies offer the possibility to adapt to different student backgrounds, which is difficult to do in a traditional classroom setting. However, there are mixed results on the effectiveness of adaptivity based on different implementations and contexts. In this paper, we introduce AI adaptivity in the context of a new genre of Intelligent Science Stations that bring intelligent tutoring into the physical world. Intelligent Science Stations are mixed-reality systems that bridge the physical and virtual worlds to improve children's inquiry-based STEM learning. Automated reactive guidance is made possible by a specialized AI computer vision algorithm, providing personalized interactive feedback to children as they experiment and make discoveries in their physical environment. We report on a randomized controlled experiment where we compare learning outcomes of children interacting with the Intelligent Science Station that has task-loop adaptivity incorporated, compared to another version that provides tasks randomly without adaptivity. Our results show that adaptivity using Bayesian Knowledge Tracing in the context of a mixed-reality system leads to better learning of scientific principles, without sacrificing enjoyment. These results demonstrate benefits of adaptivity in a mixed-reality setting to improve children's science learning.},
  langid = {english},
  keywords = {Adaptivity,Computer vision,Educational technology,Learning sciences,Mixed-reality learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yannier et al_2024_AI Adaptivity in a Mixed-Reality System Improves Learning.pdf}
}

@misc{yaoFederatedLargeLanguage2024,
  title = {Federated {{Large Language Models}}: {{Current Progress}} and {{Future Directions}}},
  shorttitle = {Federated {{Large Language Models}}},
  author = {Yao, Yuhang and Zhang, Jianyi and Wu, Junda and Huang, Chengkai and Xia, Yu and Yu, Tong and Zhang, Ruiyi and Kim, Sungchul and Rossi, Ryan and Li, Ang and Yao, Lina and McAuley, Julian and Chen, Yiran and {Joe-Wong}, Carlee},
  year = {2024},
  month = sep,
  number = {arXiv:2409.15723},
  eprint = {2409.15723},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-29},
  abstract = {Large language models are rapidly gaining popularity and have been widely adopted in real-world applications. While the quality of training data is essential, privacy concerns arise during data collection. Federated learning offers a solution by allowing multiple clients to collaboratively train LLMs without sharing local data. However, FL introduces new challenges, such as model convergence issues due to heterogeneous data and high communication costs. A comprehensive study is required to address these challenges and guide future research. This paper surveys Federated learning for LLMs (FedLLM), highlighting recent advances and future directions. We focus on two key aspects: fine-tuning and prompt learning in a federated setting, discussing existing work and associated research challenges. We finally propose potential research directions for federated LLMs, including pre-training and how LLMs can further enhance federated learning.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Zotero/storage/KYSSSRXS/Yao et al. - 2024 - Federated Large Language Models Current Progress .pdf}
}

@article{yaxStudyingImprovingReasoning2024,
  title = {Studying and Improving Reasoning in Humans and Machines},
  author = {Yax, Nicolas and Anll{\'o}, Hern{\'a}n and Palminteri, Stefano},
  year = {2024},
  month = jun,
  journal = {Communications Psychology},
  volume = {2},
  number = {1},
  pages = {1--16},
  issn = {2731-9121},
  doi = {10.1038/s44271-024-00091-8},
  urldate = {2024-07-09},
  abstract = {In the present study, we investigate and compare reasoning in large language models (LLMs) and humans, using a selection of cognitive psychology tools traditionally dedicated to the study of (bounded) rationality. We presented to human participants and an array of pretrained LLMs new variants of classical cognitive experiments, and cross-compared their performances. Our results showed that most of the included models presented reasoning errors akin to those frequently ascribed to error-prone, heuristic-based human reasoning. Notwithstanding this superficial similarity, an in-depth comparison between humans and LLMs indicated important differences with human-like reasoning, with models' limitations disappearing almost entirely in more recent LLMs' releases. Moreover, we show that while it is possible to devise strategies to induce better performance, humans and machines are not equally responsive to the same prompting schemes. We conclude by discussing the epistemological implications and challenges of comparing human and machine behavior for both artificial intelligence and cognitive psychology.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Language and linguistics,Philosophy,Problem solving},
  annotation = {https://github.com/hrl-team/ReasoningGPT},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yax et al_2024_Studying and improving reasoning in humans and machines.pdf}
}

@article{yildirimTaskStructuresWorld2024,
  title = {From Task Structures to World Models: What Do {{LLMs}} Know?},
  shorttitle = {From Task Structures to World Models},
  author = {Yildirim, Ilker and Paul, L. A.},
  year = {2024},
  month = may,
  journal = {Trends in Cognitive Sciences},
  volume = {28},
  number = {5},
  pages = {404--415},
  publisher = {Elsevier},
  issn = {1364-6613, 1879-307X},
  doi = {10.1016/j.tics.2024.02.008},
  urldate = {2024-06-06},
  langid = {english},
  pmid = {38443199},
  keywords = {instrumental knowledge,intelligence,large language models,resource rational,world models,worldly knowledge},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yildirim_Paul_2024_From task structures to world models.pdf}
}

@article{yinAccountingHumanEngagement2024,
  title = {Accounting for {{Human Engagement Behavior}} to {{Enhance AI-Assisted Decision Making}}},
  author = {Yin, Ming},
  year = {2024},
  month = may,
  journal = {Proceedings of the AAAI Symposium Series},
  volume = {3},
  number = {1},
  pages = {68--70},
  issn = {2994-4317},
  doi = {10.1609/aaaiss.v3i1.31184},
  urldate = {2024-09-26},
  abstract = {Artificial intelligence (AI) technologies have been increasingly integrated into human workflows. For example, the usage of AI-based decision aids in human decision-making processes has resulted in a new paradigm of AI-assisted decision making---that is, the AI-based decision aid provides a decision recommendation to the human decision makers, while humans make the final decision. The increasing prevalence of human-AI collaborative decision making highlights the need to understand how humans engage with the AI-based decision aid in these decision-making processes, and how to promote the effectiveness of the human-AI team in decision making. In this talk, I'll discuss a few examples illustrating that when AI is used to assist humans---both an individual decision maker or a group of decision makers---in decision making, people's engagement with the AI assistance is largely subject to their heuristics and biases, rather than careful deliberation of the respective strengths and limitations of AI and themselves. I'll then describe how to enhance AI-assisted decision making by accounting for human engagement behavior in the designs of AI-based decision aids. For example, AI recommendations can be presented to decision makers in a way that promotes their appropriate trust and reliance on AI by leveraging or mitigating human biases, informed by the analysis of human competence in decision making. Alternatively, AI-assisted decision making can be improved by developing AI models that can anticipate and adapt to the engagement behavior of human decision makers.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yin_2024_Accounting for Human Engagement Behavior to Enhance AI-Assisted Decision Making.pdf}
}

@article{yongsatianchotInvestigatingLargeLanguage,
  title = {Investigating {{Large Language Models}}' {{Perception}} of {{Emotion Using Appraisal Theory}}},
  author = {Yongsatianchot, Nutchanon and Torshizi, Parisa Ghanad and Marsella, Stacy},
  abstract = {Large Language Models (LLM) like ChatGPT have significantly advanced in recent years and are now being used by the general public. As more people interact with these systems, improving our understanding of these black box models is crucial, especially regarding their understanding of human psychological aspects. In this work, we investigate their emotion perception through the lens of appraisal and coping theory using the Stress and Coping Process Questionaire (SCPQ). SCPQ is a validated clinical instrument consisting of multiple stories that evolve over time and differ in key appraisal variables such as controllability and changeability. We applied SCPQ to three recent LLMs from OpenAI, davinci-003, ChatGPT, and GPT-4 and compared the results with predictions from the appraisal theory and human data. The results show that LLMs' responses are similar to humans in terms of dynamics of appraisal and coping, but their responses did not differ along key appraisal dimensions as predicted by the theory and data. The magnitude of their responses is also quite different from humans in several variables. We also found that GPTs can be quite sensitive to instruction and how questions are asked. This work adds to the growing literature evaluating the psychological aspects of LLMs and helps enrich our understanding of the current models.},
  langid = {english},
  annotation = {https://github.com/yongsa-nut/PerrezSAIWS},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yongsatianchot et al_Investigating Large Language Models’ Perception of Emotion Using Appraisal.pdf}
}

@article{yoonHowCanUsers2024,
  title = {How {{Can Users Maintain Self-Determination}} in {{AI Recommender Systems}}? {{The Role}} of {{Explainable AI}} ({{XAI}})},
  shorttitle = {How {{Can Users Maintain Self-Determination}} in {{AI Recommender Systems}}?},
  author = {Yoon, YoungHo and Lee, One-Ki and HAOXI, {\relax WU} and Koh, Joon},
  year = {2024},
  month = aug,
  journal = {AMCIS 2024 Proceedings},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Yoon et al_2024_How Can Users Maintain Self-Determination in AI Recommender Systems.pdf;/Users/thomasgorman/Zotero/storage/998AXE69/5.html}
}

@incollection{yueHumanTrajectoryPrediction2022,
  title = {Human {{Trajectory Prediction}} via {{Neural Social Physics}}},
  booktitle = {Computer {{Vision}} -- {{ECCV}} 2022},
  author = {Yue, Jiangbei and Manocha, Dinesh and Wang, He},
  editor = {Avidan, Shai and Brostow, Gabriel and Ciss{\'e}, Moustapha and Farinella, Giovanni Maria and Hassner, Tal},
  year = {2022},
  volume = {13694},
  pages = {376--394},
  publisher = {Springer Nature Switzerland},
  address = {Cham},
  doi = {10.1007/978-3-031-19830-4_22},
  urldate = {2024-09-16},
  abstract = {Trajectory prediction has been widely pursued in many fields, and many model-based and model-free methods have been explored. The former include rule-based, geometric or optimization-based models, and the latter are mainly comprised of deep learning approaches. In this paper, we propose a new method combining both methodologies based on a new Neural Differential Equation model. Our new model (Neural Social Physics or NSP) is a deep neural network within which we use an explicit physics model with learnable parameters. The explicit physics model serves as a strong inductive bias in modeling pedestrian behaviors, while the rest of the network provides a strong data-fitting capability in terms of system parameter estimation and dynamics stochasticity modeling. We compare NSP with 15 recent deep learning methods on 6 datasets and improve the state-of-the-art performance by 5.56\%-70\%. Besides, we show that NSP has better generalizability in predicting plausible trajectories in drastically different scenarios where the density is 2-5 times as high as the testing data. Finally, we show that the physics model in NSP can provide plausible explanations for pedestrian behaviors, as opposed to black-box deep learning. Code is available: https://github.com/realcrane/HumanTrajectory-Prediction-via-Neural-Social-Physics.},
  isbn = {978-3-031-19829-8 978-3-031-19830-4},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yue et al_2022_Human Trajectory Prediction via Neural Social Physics.pdf}
}

@misc{yuFinMePerformanceEnhancedLarge2023,
  title = {{{FinMe}}: {{A Performance-Enhanced Large Language Model Trading Agent}} with {{Layered Memory}} and {{Character Design}}},
  shorttitle = {{{FinMe}}},
  author = {Yu, Yangyang and Li, Haohang and Chen, Zhi and Jiang, Yuechen and Li, Yang and Zhang, Denghui and Liu, Rong and Suchow, Jordan W. and Khashanah, Khaldoun},
  year = {2023},
  month = nov,
  number = {arXiv:2311.13743},
  eprint = {2311.13743},
  primaryclass = {cs, q-fin},
  publisher = {arXiv},
  urldate = {2023-11-30},
  abstract = {Recent advancements in Large Language Models (LLMs) have exhibited notable efficacy in question-answering (QA) tasks across diverse domains. Their prowess in integrating extensive web knowledge has fueled interest in developing LLM autonomous agents. While LLMs are efficient in decoding human instructions and deriving solutions by holistically processing historical inputs, transitioning to purpose-driven agents requires a supplementary rational architecture to process multi-source information, establish reasoning chains, and prioritize critical tasks. Addressing this, we introduce {\textbackslash}textsc\{FinMe\}, a novel LLM-based agent framework devised for financial decision-making, encompassing three core modules: Profiling, to outline the agent's characteristics; Memory, with layered processing, to aid the agent in assimilating realistic hierarchical financial data; and Decision-making, to convert insights gained from memories into investment decisions. Notably, {\textbackslash}textsc\{FinMe\}'s memory module aligns closely with the cognitive structure of human traders, offering robust interpretability and real-time tuning. Its adjustable cognitive span allows for the retention of critical information beyond human perceptual limits, thereby enhancing trading outcomes. This framework enables the agent to self-evolve its professional knowledge, react agilely to new investment cues, and continuously refine trading decisions in the volatile financial environment. We first compare {\textbackslash}textsc\{FinMe\} with various algorithmic agents on a scalable real-world financial dataset, underscoring its leading trading performance in stocks and funds. We then fine-tuned the agent's perceptual spans to achieve a significant trading performance. Collectively, {\textbackslash}textsc\{FinMe\} presents a cutting-edge LLM agent framework for automated trading, boosting cumulative investment returns.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computational Engineering Finance and Science,Computer Science - Machine Learning,Quantitative Finance - Computational Finance},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Yu et al_2023_FinMe.pdf;/Users/thomasgorman/Zotero/storage/WZ2TP7GX/2311.html}
}

@misc{yuSkillMixFlexibleExpandable2023,
  title = {Skill-{{Mix}}: A {{Flexible}} and {{Expandable Family}} of {{Evaluations}} for {{AI}} Models},
  shorttitle = {Skill-{{Mix}}},
  author = {Yu, Dingli and Kaur, Simran and Gupta, Arushi and {Brown-Cohen}, Jonah and Goyal, Anirudh and Arora, Sanjeev},
  year = {2023},
  month = oct,
  number = {arXiv:2310.17567},
  eprint = {2310.17567},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2310.17567},
  urldate = {2024-02-03},
  abstract = {With LLMs shifting their role from statistical modeling of language to serving as general-purpose AI agents, how should LLM evaluations change? Arguably, a key ability of an AI agent is to flexibly combine, as needed, the basic skills it has learned. The capability to combine skills plays an important role in (human) pedagogy and also in a paper on emergence phenomena (Arora \& Goyal, 2023). This work introduces Skill-Mix, a new evaluation to measure ability to combine skills. Using a list of \$N\$ skills the evaluator repeatedly picks random subsets of \$k\$ skills and asks the LLM to produce text combining that subset of skills. Since the number of subsets grows like \$N{\textasciicircum}k\$, for even modest \$k\$ this evaluation will, with high probability, require the LLM to produce text significantly different from any text in the training set. The paper develops a methodology for (a) designing and administering such an evaluation, and (b) automatic grading (plus spot-checking by humans) of the results using GPT-4 as well as the open LLaMA-2 70B model. Administering a version of to popular chatbots gave results that, while generally in line with prior expectations, contained surprises. Sizeable differences exist among model capabilities that are not captured by their ranking on popular LLM leaderboards ("cramming for the leaderboard"). Furthermore, simple probability calculations indicate that GPT-4's reasonable performance on \$k=5\$ is suggestive of going beyond "stochastic parrot" behavior (Bender et al., 2021), i.e., it combines skills in ways that it had not seen during training. We sketch how the methodology can lead to a Skill-Mix based eco-system of open evaluations for AI capabilities of future models.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning,Computer Science - Neural and Evolutionary Computing},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Yu et al_2023_Skill-Mix.pdf;/Users/thomasgorman/Zotero/storage/YE7MKM7N/2310.html}
}

@misc{zanartuGenerativeDebunkingClimate2024,
  title = {Generative {{Debunking}} of {{Climate Misinformation}}},
  author = {Zanartu, Francisco and Otmakhova, Yulia and Cook, John and Frermann, Lea},
  year = {2024},
  month = jul,
  number = {arXiv:2407.05599},
  eprint = {2407.05599},
  publisher = {arXiv},
  urldate = {2024-07-23},
  abstract = {Misinformation about climate change causes numerous negative impacts, necessitating corrective responses. Psychological research has offered various strategies for reducing the influence of climate misinformation, such as the fact-myth-fallacy-fact-structure. However, practically implementing corrective interventions at scale represents a challenge. Automatic detection and correction of misinformation offers a solution to the misinformation problem. This study documents the development of large language models that accept as input a climate myth and produce a debunking that adheres to the fact-myth-fallacy-fact (``truth sandwich'') structure, by incorporating contrarian claim classification and fallacy detection into an LLM prompting framework. We combine open (Mixtral, Palm2) and proprietary (GPT-4) LLMs with prompting strategies of varying complexity. Experiments reveal promising performance of GPT-4 and Mixtral if combined with structured prompts. We identify specific challenges of debunking generation and human evaluation, and map out avenues for future work. We release a dataset of high-quality truth-sandwich debunkings, source code and a demo of the debunking system.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Computers and Society},
  annotation = {https://huggingface.co/spaces/fzanartu/flicc-agent\\
\\
https://huggingface.co/spaces/fzanartu/flicc-agent/tree/main\\
\\
https://huggingface.co/datasets/fzanartu/CARDSexamples},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zanartu et al_2024_Generative Debunking of Climate Misinformation.pdf}
}

@inproceedings{zavolokinaThinkFastThink2024,
  title = {Think {{Fast}}, {{Think Slow}}, {{Think Critical}}: {{Designing}} an {{Automated Propaganda Detection Tool}}},
  shorttitle = {Think {{Fast}}, {{Think Slow}}, {{Think Critical}}},
  booktitle = {Proceedings of the {{CHI Conference}} on {{Human Factors}} in {{Computing Systems}}},
  author = {Zavolokina, Liudmila and Sprenkamp, Kilian and Katashinskaya, Zoya and Jones, Daniel Gordon and Schwabe, Gerhard},
  year = {2024},
  month = may,
  pages = {1--24},
  publisher = {ACM},
  address = {Honolulu HI USA},
  doi = {10.1145/3613904.3642805},
  urldate = {2024-07-23},
  isbn = {9798400703300},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zavolokina et al_2024_Think Fast, Think Slow, Think Critical.pdf}
}

@misc{zenilFutureFundamentalScience2023,
  title = {The {{Future}} of {{Fundamental Science Led}} by {{Generative Closed-Loop Artificial Intelligence}}},
  author = {Zenil, Hector and Tegn{\'e}r, Jesper and Abrah{\~a}o, Felipe S. and Lavin, Alexander and Kumar, Vipin and Frey, Jeremy G. and Weller, Adrian and Soldatova, Larisa and Bundy, Alan R. and Jennings, Nicholas R. and Takahashi, Koichi and Hunter, Lawrence and Dzeroski, Saso and Briggs, Andrew and Gregory, Frederick D. and Gomes, Carla P. and Williams, Christopher K. I. and Rowe, Jon and Evans, James and Kitano, Hiroaki and Tenenbaum, Joshua B. and King, Ross},
  year = {2023},
  month = jul,
  number = {arXiv:2307.07522},
  eprint = {2307.07522},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2023-07-24},
  abstract = {Recent advances in machine learning and AI, including Generative AI and LLMs, are disrupting technological innovation, product development, and society as a whole. AI's contribution to technology can come from multiple approaches that require access to large training data sets and clear performance evaluation criteria, ranging from pattern recognition and classification to generative models. Yet, AI has contributed less to fundamental science in part because large data sets of high-quality data for scientific practice and model discovery are more difficult to access. Generative AI, in general, and Large Language Models in particular, may represent an opportunity to augment and accelerate the scientific discovery of fundamental deep science with quantitative models. Here we explore and investigate aspects of an AI-driven, automated, closed-loop approach to scientific discovery, including self-driven hypothesis generation and open-ended autonomous exploration of the hypothesis space. Integrating AI-driven automation into the practice of science would mitigate current problems, including the replication of findings, systematic production of data, and ultimately democratisation of the scientific process. Realising these possibilities requires a vision for augmented AI coupled with a diversity of AI approaches able to deal with fundamental aspects of causality analysis and model discovery while enabling unbiased search across the space of putative explanations. These advances hold the promise to unleash AI's potential for searching and discovering the fundamental structure of our world beyond what human scientists have been able to achieve. Such a vision would push the boundaries of new fundamental science rather than automatize current workflows and instead open doors for technological innovation to tackle some of the greatest challenges facing humanity today.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zenil et al_2023_The Future of Fundamental Science Led by Generative Closed-Loop Artificial.pdf;/Users/thomasgorman/Zotero/storage/86FA9GWR/2307.html}
}

@article{zgonnikovShouldStayShould2024,
  title = {Should {{I Stay}} or {{Should I Go}}? {{Cognitive Modeling}} of {{Left-Turn Gap Acceptance Decisions}} in {{Human Drivers}}},
  shorttitle = {Should {{I Stay}} or {{Should I Go}}?},
  author = {Zgonnikov, Arkady and Abbink, David and Markkula, Gustav},
  year = {2024},
  month = may,
  journal = {Human Factors},
  volume = {66},
  number = {5},
  pages = {1399--1413},
  publisher = {SAGE Publications Inc},
  issn = {0018-7208},
  doi = {10.1177/00187208221144561},
  urldate = {2024-09-16},
  abstract = {ObjectiveWe aim to bridge the gap between naturalistic studies of driver behavior and modern cognitive and neuroscientific accounts of decision making by modeling the cognitive processes underlying left-turn gap acceptance by human drivers.BackgroundUnderstanding decisions of human drivers is essential for the development of safe and efficient transportation systems. Current models of decision making in drivers provide little insight into the underlying cognitive processes. On the other hand, laboratory studies of abstract, highly controlled tasks point towards noisy evidence accumulation as a key mechanism governing decision making. However, it is unclear whether the cognitive processes implicated in these tasks are as paramount to decisions that are ingrained in more complex behaviors, such as driving.ResultsThe drivers? probability of accepting the available gap increased with the size of the gap; importantly, response time increased with time gap but not distance gap. The generalized drift-diffusion model explained the observed decision outcomes and response time distributions, as well as substantial individual differences in those. Through cross-validation, we demonstrate that the model not only explains the data, but also generalizes to out-of-sample conditions.ConclusionOur results suggest that dynamic evidence accumulation is an essential mechanism underlying left-turn gap acceptance decisions in human drivers, and exemplify how simple cognitive process models can help to understand human behavior in complex real-world tasks.ApplicationPotential applications of our results include real-time prediction of human behavior by automated vehicles and simulating realistic human-like behaviors in virtual environments for automated vehicles.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zgonnikov et al_2024_Should I Stay or Should I Go.pdf}
}

@misc{zhanBanalDeceptionHumanAI2024,
  title = {Banal {{Deception Human-AI Ecosystems}}: {{A Study}} of {{People}}'s {{Perceptions}} of {{LLM-generated Deceptive Behaviour}}},
  shorttitle = {Banal {{Deception Human-AI Ecosystems}}},
  author = {Zhan, Xiao and Xu, Yifan and Abdi, Noura and Collenette, Joe and {Abu-Salma}, Ruba and Sarkadi, Stefan},
  year = {2024},
  month = jun,
  number = {arXiv:2406.08386},
  eprint = {2406.08386},
  publisher = {arXiv},
  urldate = {2024-07-01},
  abstract = {Large language models (LLMs) can provide users with false, inaccurate, or misleading information, and we consider the output of this type information as what Natale (2021) calls `banal' deceptive behaviour. Here, we investigate peoples' perceptions of ChatGPT-generated deceptive behaviour and how this affects peoples' own behaviour and trust. To do this, we use a mixed-methods approach comprising of (i) an online survey with 220 participants and (ii) semi-structured interviews with 12 participants. Our results show that (i) the most common types of deceptive information encountered were over-simplifications and outdated information; (ii) humans' perceptions of trust and `worthiness' of talking to ChatGPT are impacted by `banal' deceptive behaviour; (iii) the perceived responsibility for deception is influenced by education level and the frequency of deceptive information; and (iv) users become more cautious after encountering deceptive information, but they come to trust the technology more when they identify advantages of using it. Our findings contribute to the understanding of human-AI interaction dynamics in the context of Deceptive AI Ecosystems, and highlight the importance of user-centric approaches to mitigating the potential harms of deceptive AI technologies.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society},
  annotation = {https://osf.io/uf5v3/?view\_only=da0b14aaabe34bca811f85e0e5f65882},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhan et al_2024_Banal Deception Human-AI Ecosystems.pdf}
}

@misc{zhangClusterLLMLargeLanguage2023,
  title = {{{ClusterLLM}}: {{Large Language Models}} as a {{Guide}} for {{Text Clustering}}},
  shorttitle = {{{ClusterLLM}}},
  author = {Zhang, Yuwei and Wang, Zihan and Shang, Jingbo},
  year = {2023},
  month = nov,
  number = {arXiv:2305.14871},
  eprint = {2305.14871},
  publisher = {arXiv},
  urldate = {2024-05-25},
  abstract = {We introduce CLUSTERLLM, a novel text clustering framework that leverages feedback from an instruction-tuned large language model, such as ChatGPT. Compared with traditional unsupervised methods that builds upon ``small'' embedders, CLUSTERLLM exhibits two intriguing advantages: (1) it enjoys the emergent capability of LLM even if its embeddings are inaccessible; and (2) it understands the user's preference on clustering through textual instruction and/or a few annotated data. First, we prompt ChatGPT for insights on clustering perspective by constructing hard triplet questions {$<$}does A better correspond to B than C{$>$}, where A, B and C are similar data points that belong to different clusters according to small embedder. We empirically show that this strategy is both effective for fine-tuning small embedder and cost-efficient to query ChatGPT. Second, we prompt ChatGPT for helps on clustering granularity by carefully designed pairwise questions {$<$}do A and B belong to the same category{$>$}, and tune the granularity from cluster hierarchies that is the most consistent with the ChatGPT answers. Extensive experiments on 14 datasets show that CLUSTERLLM consistently improves clustering quality, at an average cost of {$\sim\$$}0.61 per dataset. The code will be available at https: //github.com/zhang-yu-wei/ClusterLLM.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  annotation = {https://github.com/zhang-yu-wei/ClusterLLM},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhang et al_2023_ClusterLLM.pdf}
}

@misc{zhangExploringCollaborationMechanisms2024,
  title = {Exploring {{Collaboration Mechanisms}} for {{LLM Agents}}: {{A Social Psychology View}}},
  shorttitle = {Exploring {{Collaboration Mechanisms}} for {{LLM Agents}}},
  author = {Zhang, Jintian and Xu, Xin and Zhang, Ningyu and Liu, Ruibo and Hooi, Bryan and Deng, Shumin},
  year = {2024},
  month = may,
  number = {arXiv:2310.02124},
  eprint = {2310.02124},
  publisher = {arXiv},
  urldate = {2024-09-30},
  abstract = {As Natural Language Processing (NLP) systems are increasingly employed in intricate social environments, a pressing query emerges: Can these NLP systems mirror human-esque collaborative intelligence, in a multi-agent society consisting of multiple large language models (LLMs)? This paper probes the collaboration mechanisms among contemporary NLP systems by melding practical experiments with theoretical insights. We fabricate four unique `societies' comprised of LLM agents, where each agent is characterized by a specific `trait' (easy-going or overconfident) and engages in collaboration with a distinct `thinking pattern' (debate or reflection). Through evaluating these multi-agent societies on three benchmark datasets, we discern that certain collaborative strategies not only outshine previous top-tier approaches but also optimize efficiency (using fewer API tokens). Moreover, our results further illustrate that LLM agents manifest humanlike social behaviors, such as conformity and consensus reaching, mirroring foundational social psychology theories. In conclusion, we integrate insights from social psychology to contextualize the collaboration of LLM agents, inspiring further investigations into the collaboration mechanism for LLMs. We have shared our code and datasets1, hoping to catalyze further research in this promising avenue.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Computers and Society,Computer Science - Machine Learning,Computer Science - Multiagent Systems},
  annotation = {https://www.zjukg.org/project/MachineSoM/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhang et al_2024_Exploring Collaboration Mechanisms for LLM Agents.pdf}
}

@article{zhangLLMMastermindSurvey2024,
  title = {{{LLM}} as a {{Mastermind}}: {{A Survey}} of {{Strategic Reasoning}} with {{Large Language Models}}},
  author = {Zhang, Yadong and Mao, Shaoguang and Ge, Tao and Wang, Xun and {de Wynter}, Adrian and Xia, Yan and Wu, Wenshan and Song, Ting and Lan, Man and Wei, Furu},
  year = {2024},
  abstract = {This paper presents a comprehensive survey of the current status and opportunities for Large Language Models (LLMs) in strategic reasoning, a sophisticated form of reasoning that necessitates understanding and predicting adversary actions in multi-agent settings while adjusting strategies accordingly. Strategic reasoning is distinguished by its focus on the dynamic and uncertain nature of interactions among multi-agents, where comprehending the environment and anticipating the behavior of others is crucial. We explore the scopes, applications, methodologies, and evaluation metrics related to strategic reasoning with LLMs, highlighting the burgeoning development in this area and the interdisciplinary approaches enhancing their decision-making performance. It aims to systematize and clarify the scattered literature on this subject, providing a systematic review that underscores the importance of strategic reasoning as a critical cognitive capability and offers insights into future research directions and potential improvements.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhang et al_Zhang, Y.pdf}
}

@misc{zhangSimulatingClassroomEducation2024,
  title = {Simulating {{Classroom Education}} with {{LLM-Empowered Agents}}},
  author = {Zhang, Zheyuan and {Zhang-Li}, Daniel and Yu, Jifan and Gong, Linlu and Zhou, Jinchang and Liu, Zhiyuan and Hou, Lei and Li, Juanzi},
  year = {2024},
  month = jun,
  number = {arXiv:2406.19226},
  eprint = {2406.19226},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-23},
  abstract = {Large language models (LLMs) have been employed in various intelligent educational tasks to assist teaching. While preliminary explorations have focused on independent LLMempowered agents for specific educational tasks, the potential for LLMs within a multiagent collaborative framework to simulate a classroom with real user participation remains unexplored. In this work, we propose SimClass, a multi-agent classroom simulation framework involving user participation. We recognize representative class roles and introduce a novel class control mechanism for automatic classroom teaching, and conduct user experiments in two real-world courses. Utilizing the Flanders Interactive Analysis System and Community of Inquiry theoretical frame works from educational analysis, we demonstrate that LLMs can simulate traditional classroom interaction patterns effectively while enhancing user's experience. We also observe emergent group behaviors among agents in SimClass, where agents collaborate to create enlivening interactions in classrooms to improve user learning process. We hope this work pioneers the application of LLM-empowered multi-agent systems in virtual classroom teaching.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language,Computer Science - Human-Computer Interaction},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhang et al_2024_Simulating Classroom Education with LLM-Empowered Agents.pdf}
}

@misc{zhangTranscendenceGenerativeModels2024,
  title = {Transcendence: {{Generative Models Can Outperform The Experts That Train Them}}},
  shorttitle = {Transcendence},
  author = {Zhang, Edwin and Zhu, Vincent and Saphra, Naomi and Kleiman, Anat and Edelman, Benjamin L. and Tambe, Milind and Kakade, Sham M. and Malach, Eran},
  year = {2024},
  month = jun,
  number = {arXiv:2406.11741},
  eprint = {2406.11741},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.11741},
  urldate = {2024-06-23},
  abstract = {Generative models are trained with the simple objective of imitating the conditional probability distribution induced by the data they are trained on. Therefore, when trained on data generated by humans, we may not expect the artificial model to outperform the humans on their original objectives. In this work, we study the phenomenon of transcendence: when a generative model achieves capabilities that surpass the abilities of the experts generating its data. We demonstrate transcendence by training an autoregressive transformer to play chess from game transcripts, and show that the trained model can sometimes achieve better performance than all players in the dataset. We theoretically prove that transcendence is enabled by low-temperature sampling, and rigorously assess this experimentally. Finally, we discuss other sources of transcendence, laying the groundwork for future investigation of this phenomenon in a broader setting.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning},
  annotation = {https://transcendence.eddie.win/\\
\\
https://www.youtube.com/watch?v=gMUo\_kUxPMg\&ab\_channel=Tunadorable},
  file = {/Users/thomasgorman/OneDrive - Indiana University/resources/Zotero/Zhang et al_2024_Transcendence.pdf;/Users/thomasgorman/Zotero/storage/ZBJIBDG7/2406.html}
}

@misc{zhangTREACLEThriftyReasoning2024,
  title = {{{TREACLE}}: {{Thrifty Reasoning}} via {{Context-Aware LLM}} and {{Prompt Selection}}},
  shorttitle = {{{TREACLE}}},
  author = {Zhang, Xuechen and Huang, Zijian and Taga, Ege Onur and {Joe-Wong}, Carlee and Oymak, Samet and Chen, Jiasi},
  year = {2024},
  month = apr,
  number = {arXiv:2404.13082},
  eprint = {2404.13082},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-11},
  abstract = {Recent successes in natural language processing have led to the proliferation of large language models (LLMs) by multiple providers. Each LLM offering has different inference accuracy, monetary cost, and latency, and their accuracy further depends on the exact wording of the question (i.e., the specific prompt). At the same time, users often have a limit on monetary budget and latency to answer all their questions, and they do not know which LLMs to choose for each question to meet their accuracy and long term budget requirements. To navigate this rich design space, we propose TREACLE (Thrifty Reasoning via Context-Aware LLM and Prompt Selection), a reinforcement learning policy that jointly selects the model and prompting scheme while respecting the user's monetary cost and latency constraints. TREACLE uses the problem context, including question text embeddings (reflecting the type or difficulty of a query) and the response history (reflecting the consistency of previous responses) to make smart decisions. Our evaluations on standard reasoning datasets (GSM8K, CSQA, and LLC ) with various LLMs and prompts show that TREACLE enables cost savings of up to 85\% compared to baselines, while maintaining high accuracy. Importantly, it provides the user with the ability to gracefully trade off accuracy for cost.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Computer Science - Machine Learning},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhang et al_2024_TREACLE.pdf}
}

@article{zhangYouCanOnly2024,
  title = {You {{Can Only Verify When You Know}} the {{Answer}}: {{Feature-Based Explanations Reduce Overreliance}} on {{AI}} for {{Easy Decisions}}, but {{Not}} for {{Hard Ones}}},
  author = {Zhang, Zelun Tony and Buchner, Felicitas and Liu, Yuanting and Butz, Andreas},
  year = {2024},
  abstract = {Explaining the mechanisms behind model predictions is a common strategy in AI-assisted decision-making to help users rely appropriately on AI. However, recent research shows that the effectiveness of explanations depends on numerous factors, leading to mixed results, with many studies finding no effect or even an increase in overreliance, while explanations do improve appropriate reliance in other studies. We consider the factor of decision difficulty to better understand when feature-based explanations can mitigate overreliance. To this end, we conducted an online experiment ({$N$} = 200) with carefully selected task instances that cover a wide range of difficulties. We found that explanations reduce overreliance for easy decisions, but that this effect vanishes with increasing decision difficulty. For the most difficult decisions, explanations might even increase overreliance. Our results imply that explanations of the model's inner workings are only helpful for a limited set of decision tasks where users easily know the answer themselves.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhang et al_2024_You Can Only Verify When You Know the Answer.pdf}
}

@article{zhaoAcceleratedEvaluationAutomated2017,
  title = {Accelerated {{Evaluation}} of {{Automated Vehicles Safety}} in {{Lane-Change Scenarios Based}} on {{Importance Sampling Techniques}}},
  author = {Zhao, Ding and Lam, Henry and Peng, Huei and Bao, Shan and LeBlanc, David J. and Nobukawa, Kazutoshi and Pan, Christopher S.},
  year = {2017},
  month = mar,
  journal = {IEEE Transactions on Intelligent Transportation Systems},
  volume = {18},
  number = {3},
  pages = {595--607},
  issn = {1558-0016},
  doi = {10.1109/TITS.2016.2582208},
  urldate = {2024-09-16},
  abstract = {Automated vehicles (AVs) must be thoroughly evaluated before their release and deployment. A widely used evaluation approach is the Naturalistic-Field Operational Test (N-FOT), which tests prototype vehicles directly on the public roads. Due to the low exposure to safety-critical scenarios, N-FOTs are time consuming and expensive to conduct. In this paper, we propose an accelerated evaluation approach for AVs. The results can be used to generate motions of the other primary vehicles to accelerate the verification of AVs in simulations and controlled experiments. Frontal collision due to unsafe cut-ins is the target crash type of this paper. Human-controlled vehicles making unsafe lane changes are modeled as the primary disturbance to AVs based on data collected by the University of Michigan Safety Pilot Model Deployment Program. The cut-in scenarios are generated based on skewed statistics of collected human driver behaviors, which generate risky testing scenarios while preserving the statistical information so that the safety benefits of AVs in nonaccelerated cases can be accurately estimated. The cross-entropy method is used to recursively search for the optimal skewing parameters. The frequencies of the occurrences of conflicts, crashes, and injuries are estimated for a modeled AV, and the achieved accelerated rate is around 2000 to 20 000. In other words, in the accelerated simulations, driving for 1000 miles will expose the AV with challenging scenarios that will take about 2 to 20 million miles of real-world driving to encounter. This technique thus has the potential to greatly reduce the development and validation time for AVs.},
  keywords = {Acceleration,Active safety systems,automated vehicles (AVs),autonomous emergency braking (AEB),crash avoidance,Data models,Databases,importance sampling (IS),lane change,Monte Carlo methods,Safety,Vehicle crash testing,Vehicles},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhao et al_2017_Accelerated Evaluation of Automated Vehicles Safety in Lane-Change Scenarios.pdf;/Users/thomasgorman/Zotero/storage/N35IR9YG/7534875.html}
}

@article{zhaoRiskProsocialBehavioural2024,
  title = {Risk and Prosocial Behavioural Cues Elicit Human-like Response Patterns from {{AI}} Chatbots},
  author = {Zhao, Yukun and Huang, Zhen and Seligman, Martin and Peng, Kaiping},
  year = {2024},
  month = mar,
  journal = {Scientific Reports},
  volume = {14},
  number = {1},
  pages = {7095},
  issn = {2045-2322},
  doi = {10.1038/s41598-024-55949-y},
  urldate = {2024-06-29},
  abstract = {Emotions, long deemed a distinctly human characteristic, guide a repertoire of behaviors, e.g., promoting risk-aversion under negative emotional states or generosity under positive ones. The question of whether Artificial Intelligence (AI) can possess emotions remains elusive, chiefly due to the absence of an operationalized consensus on what constitutes 'emotion' within AI. Adopting a pragmatic approach, this study investigated the response patterns of AI chatbots---specifically, large language models (LLMs)---to various emotional primes. We engaged AI chatbots as one would human participants, presenting scenarios designed to elicit positive, negative, or neutral emotional states. Multiple accounts of OpenAI's ChatGPT Plus were then tasked with responding to inquiries concerning investment decisions and prosocial behaviors. Our analysis revealed that ChatGPT-4 bots, when primed with positive, negative, or neutral emotions, exhibited distinct response patterns in both risk-taking and prosocial decisions, a phenomenon less evident in the ChatGPT-3.5 iterations. This observation suggests an enhanced capacity for modulating responses based on emotional cues in more advanced LLMs. While these findings do not suggest the presence of emotions in AI, they underline the feasibility of swaying AI responses by leveraging emotional indicators.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Human behaviour,Psychology},
  annotation = {https://osf.io/yrc4p/},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhao et al_2024_Risk and prosocial behavioural cues elicit human-like response patterns from AI.pdf}
}

@article{zhaoWhenBraininspiredAI2023,
  title = {When Brain-Inspired {{AI}} Meets {{AGI}}},
  author = {Zhao, Lin and Zhang, Lu and Wu, Zihao and Chen, Yuzhong and Dai, Haixing and Yu, Xiaowei and Liu, Zhengliang and Zhang, Tuo and Hu, Xintao and Jiang, Xi and Li, Xiang and Zhu, Dajiang and Shen, Dinggang and Liu, Tianming},
  year = {2023},
  month = jun,
  journal = {Meta-Radiology},
  volume = {1},
  number = {1},
  pages = {100005},
  issn = {29501628},
  doi = {10.1016/j.metrad.2023.100005},
  urldate = {2024-08-15},
  abstract = {Artificial General Intelligence (AGI) has been a long-standing goal of humanity, with the aim of creating machines capable of performing any intellectual task that humans can do. To achieve this, AGI researchers draw inspiration from the human brain and seek to replicate its principles in intelligent machines. Brain-inspired artificial intelligence is a field that has emerged from this endeavor, combining insights from neuroscience, psychology, and computer science to develop more efficient and powerful AI systems. In this article, we provide a comprehensive overview of brain-inspired AI from the perspective of AGI. We begin with the current progress in brain-inspired AI and its extensive connection with AGI. We then cover the important characteristics for both human intelligence and AGI (e.g., scaling, multimodality, and reasoning). We discuss important technologies toward achieving AGI in current AI systems, such as in-context learning and prompt tuning. We also investigate the evolution of AGI systems from both algorithmic and infrastructural perspectives. Finally, we explore the limitations and future of AGI.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhao et al_2023_When brain-inspired AI meets AGI.pdf}
}

@misc{zhaoWildHallucinationsEvaluatingLongform2024,
  title = {{{WildHallucinations}}: {{Evaluating Long-form Factuality}} in {{LLMs}} with {{Real-World Entity Queries}}},
  shorttitle = {{{WildHallucinations}}},
  author = {Zhao, Wenting and Goyal, Tanya and Chiu, Yu Ying and Jiang, Liwei and Newman, Benjamin and Ravichander, Abhilasha and Chandu, Khyathi and Bras, Ronan Le and Cardie, Claire and Deng, Yuntian and Choi, Yejin},
  year = {2024},
  month = jul,
  number = {arXiv:2407.17468},
  eprint = {2407.17468},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2407.17468},
  urldate = {2024-08-14},
  abstract = {While hallucinations of large language models (LLMs) prevail as a major challenge, existing evaluation benchmarks on factuality do not cover the diverse domains of knowledge that the real-world users of LLMs seek information about. To bridge this gap, we introduce WildHallucinations, a benchmark that evaluates factuality. It does so by prompting LLMs to generate information about entities mined from user-chatbot conversations in the wild. These generations are then automatically fact-checked against a systematically curated knowledge source collected from web search. Notably, half of these real-world entities do not have associated Wikipedia pages. We evaluate 118,785 generations from 15 LLMs on 7,919 entities. We find that LLMs consistently hallucinate more on entities without Wikipedia pages and exhibit varying hallucination rates across different domains. Finally, given the same base models, adding a retrieval component only slightly reduces hallucinations but does not eliminate hallucinations.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhao et al_2024_WildHallucinations.pdf;/Users/thomasgorman/Zotero/storage/NL7JCM76/2407.html}
}

@article{zhongMemoryBankEnhancingLarge2024,
  title = {{{MemoryBank}}: {{Enhancing Large Language Models}} with {{Long-Term Memory}}},
  shorttitle = {{{MemoryBank}}},
  author = {Zhong, Wanjun and Guo, Lianghong and Gao, Qiqi and Ye, He and Wang, Yanlin},
  year = {2024},
  month = mar,
  journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume = {38},
  number = {17},
  pages = {19724--19731},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v38i17.29946},
  urldate = {2024-09-18},
  abstract = {Large Language Models (LLMs) have drastically reshaped our interactions with artificial intelligence (AI) systems, showcasing impressive performance across an extensive array of tasks. Despite this, a notable hindrance remains---the deficiency of a long-term memory mechanism within these models. This shortfall becomes increasingly evident in situations demanding sustained interaction, such as personal companion systems, psychological counseling, and secretarial assistance. Recognizing the necessity for long-term memory, we propose MemoryBank, a novel memory mechanism tailored for LLMs. MemoryBank enables the models to summon relevant memories, continually evolve through continuous memory updates, comprehend, and adapt to a user's personality over time by synthesizing information from previous interactions. To mimic anthropomorphic behaviors and selectively preserve memory, MemoryBank incorporates a memory updating mechanism, inspired by the Ebbinghaus Forgetting Curve theory. This mechanism permits the AI to forget and reinforce memory based on time elapsed and the relative significance of the memory, thereby offering a more human-like memory mechanism and enriched user experience. MemoryBank is versatile in accommodating both closed-source models like ChatGPT and open-source models such as ChatGLM. To validate MemoryBank's effectiveness, we exemplify its application through the creation of an LLM-based chatbot named SiliconFriend in a long-term AI Companion scenario. Further tuned with psychological dialog data, SiliconFriend displays heightened empathy and discernment in its interactions. Experiment involves both qualitative analysis with realworld user dialogs and quantitative analysis with simulated dialogs. In the latter, ChatGPT acts as multiple users with diverse characteristics and generates long-term dialog contexts covering a wide array of topics. The results of our analysis reveal that SiliconFriend, equipped with MemoryBank, exhibits a strong capability for long-term companionship as it can provide emphatic response, recall relevant memories and understand user personality.},
  langid = {english},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhong et al_2024_MemoryBank.pdf}
}

@misc{zhouChatGPTVsSocial2024,
  title = {{{ChatGPT}} vs {{Social Surveys}}: {{Probing}} the {{Objective}} and {{Subjective Human Society}}},
  shorttitle = {{{ChatGPT}} vs {{Social Surveys}}},
  author = {Zhou, Muzhi and Yu, Lu and Geng, Xiaomin and Luo, Lan},
  year = {2024},
  month = sep,
  number = {arXiv:2409.02601},
  eprint = {2409.02601},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-09-28},
  abstract = {The extent to which Large Language Models (LLMs) can simulate the data-generating process for social surveys remains unclear. Current research has not thoroughly assessed potential biases in the sociodemographic population represented within the language model's framework. Additionally, the subjective worlds of LLMs often show inconsistencies in how closely their responses match those of groups of human respondents. In this paper, we used ChatGPT-3.5 to simulate the sampling process and generated six socioeconomic characteristics from the 2020 US population. We also analyzed responses to questions about income inequality and gender roles to explore GPT's subjective attitudes. By using repeated random sampling, we created a sampling distribution to identify the parameters of the GPT-generated population and compared these with Census data. Our findings show some alignment in gender and age means with the actual 2020 US population, but we also found mismatches in the distributions of racial and educational groups. Furthermore, there were significant differences between the distribution of GPT's responses and human self-reported attitudes. While the overall point estimates of GPT's income attitudinal responses seem to align with the mean of the population occasionally, their response distributions follow a normal distribution that diverges from human responses. In terms of gender relations, GPT's answers tend to cluster around a single category, demonstrating a deterministic pattern. We conclude by emphasizing the distinct design philosophies of LLMs and social surveys: LLMs aim to predict the most suitable answers, while social surveys seek to reveal the heterogeneity among social groups.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computers and Society},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhou et al_2024_ChatGPT vs Social Surveys.pdf}
}

@misc{zhouContextLearningType2024,
  title = {Is {{In-Context Learning}} a {{Type}} of {{Gradient-Based Learning}}? {{Evidence}} from the {{Inverse Frequency Effect}} in {{Structural Priming}}},
  shorttitle = {Is {{In-Context Learning}} a {{Type}} of {{Gradient-Based Learning}}?},
  author = {Zhou, Zhenghao and Frank, Robert and McCoy, R. Thomas},
  year = {2024},
  month = jun,
  number = {arXiv:2406.18501},
  eprint = {2406.18501},
  primaryclass = {cs},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2406.18501},
  urldate = {2024-07-01},
  abstract = {Large language models (LLMs) have shown the emergent capability of in-context learning (ICL). One line of research has explained ICL as functionally performing gradient descent. In this paper, we introduce a new way of diagnosing whether ICL is functionally equivalent to gradient-based learning. Our approach is based on the inverse frequency effect (IFE) -- a phenomenon in which an error-driven learner is expected to show larger updates when trained on infrequent examples than frequent ones. The IFE has previously been studied in psycholinguistics because humans show this effect in the context of structural priming (the tendency for people to produce sentence structures they have encountered recently); the IFE has been used as evidence that human structural priming must involve error-driven learning mechanisms. In our experiments, we simulated structural priming within ICL and found that LLMs display the IFE, with the effect being stronger in larger models. We conclude that ICL is indeed a type of gradient-based learning, supporting the hypothesis that a gradient component is implicitly computed in the forward pass during ICL. Our results suggest that both humans and LLMs make use of gradient-based, error-driven processing mechanisms.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhou et al_2024_Is In-Context Learning a Type of Gradient-Based Learning.pdf;/Users/thomasgorman/Zotero/storage/ZN3PSMSZ/2406.html}
}

@article{zhouLargerMoreInstructable2024,
  title = {Larger and More Instructable Language Models Become Less Reliable},
  author = {Zhou, Lexin and Schellaert, Wout and {Mart{\'i}nez-Plumed}, Fernando and {Moros-Daval}, Yael and Ferri, C{\`e}sar and {Hern{\'a}ndez-Orallo}, Jos{\'e}},
  year = {2024},
  month = sep,
  journal = {Nature},
  pages = {1--8},
  issn = {1476-4687},
  doi = {10.1038/s41586-024-07930-y},
  urldate = {2024-09-28},
  abstract = {The prevailing methods to make large language models more powerful and amenable have been based on continuous scaling up (that is, increasing their size, data volume and computational resources1) and bespoke shaping up (including post-filtering2,3, fine tuning or use of human feedback4,5). However, larger and more instructable large language models may have become less reliable. By studying the relationship between difficulty concordance, task avoidance and prompting stability of several language model families, here we show that easy instances for human participants are also easy for the models, but scaled-up, shaped-up models do not secure areas of low difficulty in which either the model does not err or human supervision can spot the errors. We also find that early models often avoid user questions but scaled-up, shaped-up models tend to give an apparently sensible yet wrong answer much more often, including errors on difficult questions that human supervisors frequently overlook. Moreover, we observe that stability to different natural phrasings of the same question is improved by scaling-up and shaping-up interventions, but pockets of variability persist across difficulty levels. These findings highlight the need for a fundamental shift in the design and development of general-purpose artificial intelligence, particularly in high-stakes areas for which a predictable distribution of errors is paramount.},
  copyright = {2024 The Author(s)},
  langid = {english},
  keywords = {Computer science,Information technology,Technology},
  annotation = {https://github.com/wschella/llm-reliability},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhou et al_2024_Larger and more instructable language models become less reliable.pdf}
}

@inproceedings{zhouLLMReliableReviewer2024,
  title = {Is {{LLM}} a {{Reliable Reviewer}}? {{A Comprehensive Evaluation}} of {{LLM}} on {{Automatic Paper Reviewing Tasks}}},
  shorttitle = {Is {{LLM}} a {{Reliable Reviewer}}?},
  booktitle = {Proceedings of the 2024 {{Joint International Conference}} on {{Computational Linguistics}}, {{Language Resources}} and {{Evaluation}} ({{LREC-COLING}} 2024)},
  author = {Zhou, Ruiyang and Chen, Lu and Yu, Kai},
  editor = {Calzolari, Nicoletta and Kan, Min-Yen and Hoste, Veronique and Lenci, Alessandro and Sakti, Sakriani and Xue, Nianwen},
  year = {2024},
  month = may,
  pages = {9340--9351},
  publisher = {{ELRA and ICCL}},
  address = {Torino, Italia},
  urldate = {2024-08-14},
  abstract = {The use of large language models (LLM), especially ChatGPT, to help with research has come into practice. Researchers use it for timely advice and hope to obtain in-depth feedback. However, can LLM be a qualified and reliable reviewer? Although there already exist several review-related datasets, few works have carefully and thoroughly inspected model's capability as a reviewer, especially the correctness of generated reviews. In this paper, we first evaluate GPT-3.5 and GPT-4 (the current top-performing LLM) on 2 types of tasks under different settings: the score prediction task and the review generation task. In addition, we propose a dataset containing 197 review-revision multiple-choice questions (RR-MCQ) with detailed labels from the review-rebuttal forum in ICLR-2023. By asking questions from technical details to the overall presentation and quality, our RR-MCQ data provides a more complete model ability assessment. The results show that LLM is generally helpful, but great caution is needed as it always makes mistakes. Although it can give passable decisions ({\textbackslash}textgreater 60\% accuracy) on single options, completely correct answers are still rare (about 20\%); models are still weak on long paper processing, zero-shot scoring, and giving critical feedback like human reviewers.},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhou et al_2024_Is LLM a Reliable Reviewer.pdf}
}

@misc{zhuangStaticBenchmarksAdaptive2024,
  title = {From {{Static Benchmarks}} to {{Adaptive Testing}}: {{Psychometrics}} in {{AI Evaluation}}},
  shorttitle = {From {{Static Benchmarks}} to {{Adaptive Testing}}},
  author = {Zhuang, Yan and Liu, Qi and Ning, Yuting and Huang, Weizhe and Pardos, Zachary A. and Kyllonen, Patrick C. and Zu, Jiyun and Mao, Qingyang and Lv, Rui and Huang, Zhenya and Zhao, Guanhao and Zhang, Zheng and Wang, Shijin and Chen, Enhong},
  year = {2024},
  month = aug,
  number = {arXiv:2306.10512},
  eprint = {2306.10512},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-08-16},
  abstract = {As AI systems continue to grow, particularly generative models like Large Language Models (LLMs), their rigorous evaluation is crucial for development and deployment. To determine their adequacy, researchers have developed various large-scale benchmarks against a so-called gold-standard test set and report metrics averaged across all items. However, this static evaluation paradigm increasingly shows its limitations, including high computational costs, data contamination, and the impact of low-quality or erroneous items on evaluation reliability and efficiency. In this Perspective, drawing from human psychometrics, we discuss a paradigm shift from static evaluation methods to adaptive testing. This involves estimating the characteristics and value of each test item in the benchmark and dynamically adjusting items in real-time, tailoring the evaluation based on the model's ongoing performance instead of relying on a fixed test set. This paradigm not only provides a more robust ability estimation but also significantly reduces the number of test items required. We analyze the current approaches, advantages, and underlying reasons for adopting psychometrics in AI evaluation. We propose that adaptive testing will become the new norm in AI model evaluation, enhancing both the efficiency and effectiveness of assessing advanced intelligence systems.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhuang et al_2024_From Static Benchmarks to Adaptive Testing.pdf}
}

@misc{zhuLanguageModelsTrained2024,
  title = {Language {{Models Trained}} to Do {{Arithmetic Predict Human Risky}} and {{Intertemporal Choice}}},
  author = {Zhu, Jian-Qiao and Yan, Haijiang and Griffiths, Thomas L.},
  year = {2024},
  month = may,
  number = {arXiv:2405.19313},
  eprint = {2405.19313},
  primaryclass = {cs, econ, q-fin},
  publisher = {arXiv},
  urldate = {2024-08-16},
  abstract = {The observed similarities in the behavior of humans and Large Language Models (LLMs) have prompted researchers to consider the potential of using LLMs as models of human cognition. However, several significant challenges must be addressed before LLMs can be legitimately regarded as cognitive models. For instance, LLMs are trained on far more data than humans typically encounter, and may have been directly trained on human data in specific cognitive tasks or aligned with human preferences. Consequently, the origins of these behavioral similarities are not well understood. In this paper, we propose a novel way to enhance the utility of LLMs as cognitive models. This approach involves (i) leveraging computationally equivalent tasks that both an LLM and a rational agent need to master for solving a cognitive problem and (ii) examining the specific task distributions required for an LLM to exhibit human-like behaviors. We apply this approach to decision-making -- specifically risky and intertemporal choice --where the key computationally equivalent task is the arithmetic of expected value calculations. We show that an LLM pretrained on an ecologically valid arithmetic dataset, which we call Arithmetic-GPT, predicts human behavior better than many traditional cognitive models. Pretraining LLMs on ecologically valid arithmetic datasets is sufficient to produce a strong correspondence between these models and human decision-making. Our results also suggest that LLMs used as cognitive models should be carefully investigated via ablation studies of the pretraining data.},
  archiveprefix = {arXiv},
  langid = {english},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Computation and Language,Economics - General Economics},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhu et al_2024_Language Models Trained to do Arithmetic Predict Human Risky and Intertemporal.pdf}
}

@misc{zhuLanguageModelsUnderstand2024,
  title = {Language {{Models Understand Numbers}}, at {{Least Partially}}},
  author = {Zhu, Fangwei and Dai, Damai and Sui, Zhifang},
  year = {2024},
  month = jan,
  number = {arXiv:2401.03735},
  eprint = {2401.03735},
  primaryclass = {cs},
  publisher = {arXiv},
  urldate = {2024-01-14},
  abstract = {Large language models (LLMs) have exhibited impressive competency in various text-related tasks. However, their opaque internal mechanisms become a hindrance to leveraging them in mathematical problems. In this paper, we study a fundamental question: whether language models understand numbers, which play a basic element in mathematical problems. We assume that to solve mathematical problems, language models should be capable of understanding numbers and compressing these numbers in their hidden states. We construct a synthetic dataset comprising addition problems and utilize linear probes to read out input numbers from the hidden states of models. Experimental results demonstrate evidence supporting the existence of compressed numbers in the LLaMA-2 model family from early layers. However, the compression process seems to be not lossless, presenting difficulty in precisely reconstructing the original numbers. Further experiments show that language models can utilize the encoded numbers to perform arithmetic computations, and the computational ability scales up with the model size. Our preliminary research suggests that language models exhibit a partial understanding of numbers, offering insights into future investigations about the models' capability of solving mathematical problems.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Zhu et al_2024_Language Models Understand Numbers, at Least Partially.pdf;/Users/thomasgorman/Zotero/storage/UG7P7WJ5/2401.html}
}

@misc{ziemsCanLargeLanguage2023,
  title = {Can {{Large Language Models Transform Computational Social Science}}?},
  author = {Ziems, Caleb and Held, William and Shaikh, Omar and Chen, Jiaao and Zhang, Zhehao and Yang, Diyi},
  year = {2023},
  month = apr,
  number = {arXiv:2305.03514},
  eprint = {2305.03514},
  publisher = {arXiv},
  doi = {10.48550/arXiv.2305.03514},
  urldate = {2023-10-18},
  abstract = {Large Language Models (LLMs) like ChatGPT are capable of successfully performing many language processing tasks zero-shot (without the need for training data). If this capacity also applies to the coding of social phenomena like persuasiveness and political ideology, then LLMs could effectively transform Computational Social Science (CSS). This work provides a road map for using LLMs as CSS tools. Towards this end, we contribute a set of prompting best practices and an extensive evaluation pipeline to measure the zero-shot performance of 13 language models on 24 representative CSS benchmarks. On taxonomic labeling tasks (classification), LLMs fail to outperform the best fine-tuned models but still achieve fair levels of agreement with humans. On free-form coding tasks (generation), LLMs produce explanations that often exceed the quality of crowdworkers' gold references. We conclude that today's LLMs can radically augment the CSS research pipeline in two ways: (1) serving as zero-shot data annotators on human annotation teams, and (2) bootstrapping challenging creative generation tasks (e.g., explaining the hidden meaning behind text). In summary, LLMs can significantly reduce costs and increase efficiency of social science analysis in partnership with humans.},
  archiveprefix = {arXiv},
  keywords = {Computer Science - Computation and Language,Computer Science - Machine Learning},
  annotation = {https://github.com/SALT-NLP/LLMs\_for\_CSS/tree/main},
  file = {/Users/thomasgorman/Library/CloudStorage/OneDrive-IndianaUniversity/Resources/Zotero/Ziems et al_2023_Can Large Language Models Transform Computational Social Science.pdf;/Users/thomasgorman/Zotero/storage/I6BENS9N/2305.html}
}
